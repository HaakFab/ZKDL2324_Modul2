{"query":{"0":"artificial intelligence","1":"artificial intelligence","2":"artificial intelligence","3":"artificial intelligence","4":"artificial intelligence","5":"artificial intelligence","6":"artificial intelligence","7":"artificial intelligence","8":"artificial intelligence","9":"artificial intelligence","10":"artificial intelligence","11":"artificial intelligence","12":"artificial intelligence","13":"artificial intelligence","14":"artificial intelligence","15":"artificial intelligence","16":"artificial intelligence","17":"artificial intelligence","18":"artificial intelligence","19":"artificial intelligence","20":"artificial intelligence","21":"artificial intelligence","22":"artificial intelligence","23":"artificial intelligence","24":"artificial intelligence","25":"artificial intelligence","26":"artificial intelligence","27":"artificial intelligence","28":"artificial intelligence","29":"artificial intelligence","30":"artificial intelligence","31":"artificial intelligence","32":"artificial intelligence","33":"artificial intelligence","34":"artificial intelligence","35":"artificial intelligence","36":"artificial intelligence","37":"artificial intelligence","38":"artificial intelligence","39":"artificial intelligence","40":"artificial intelligence","41":"artificial intelligence","42":"artificial intelligence","43":"artificial intelligence","44":"artificial intelligence","45":"artificial intelligence","46":"artificial intelligence","47":"artificial intelligence","48":"artificial intelligence","49":"artificial intelligence","50":"artificial intelligence","51":"artificial intelligence","52":"artificial intelligence","53":"artificial intelligence","54":"artificial intelligence","55":"artificial intelligence","56":"artificial intelligence","57":"artificial intelligence","58":"artificial intelligence","59":"artificial intelligence","60":"artificial intelligence","61":"artificial intelligence","62":"artificial intelligence","63":"artificial intelligence","64":"artificial intelligence","65":"artificial intelligence","66":"artificial intelligence","67":"artificial intelligence","68":"artificial intelligence","69":"artificial intelligence","70":"artificial intelligence","71":"artificial intelligence","72":"artificial intelligence","73":"artificial intelligence","74":"artificial intelligence","75":"artificial intelligence","76":"artificial intelligence","77":"artificial intelligence","78":"artificial intelligence","79":"artificial intelligence","80":"artificial intelligence","81":"artificial intelligence","82":"artificial intelligence","83":"artificial intelligence","84":"artificial intelligence","85":"artificial intelligence","86":"artificial intelligence","87":"artificial intelligence","88":"artificial intelligence","89":"artificial intelligence","90":"artificial intelligence","91":"artificial intelligence","92":"artificial intelligence","93":"artificial intelligence","94":"artificial intelligence","95":"artificial intelligence","96":"artificial intelligence","97":"artificial intelligence","98":"artificial intelligence","99":"artificial intelligence","100":"artificial intelligence","101":"artificial intelligence","102":"artificial intelligence","103":"artificial intelligence","104":"artificial intelligence","105":"artificial intelligence","106":"artificial intelligence","107":"artificial intelligence","108":"artificial intelligence","109":"artificial intelligence","110":"artificial intelligence","111":"artificial intelligence","112":"artificial intelligence","113":"artificial intelligence","114":"artificial intelligence","115":"artificial intelligence","116":"artificial intelligence","117":"artificial intelligence","118":"artificial intelligence","119":"artificial intelligence","120":"artificial intelligence","121":"artificial intelligence","122":"artificial intelligence","123":"artificial intelligence","124":"artificial intelligence","125":"artificial intelligence","126":"artificial intelligence","127":"artificial intelligence","128":"artificial intelligence","129":"artificial intelligence","130":"artificial intelligence","131":"artificial intelligence","132":"artificial intelligence","133":"artificial intelligence","134":"artificial intelligence","135":"artificial intelligence","136":"artificial intelligence","137":"artificial intelligence","138":"artificial intelligence","139":"artificial intelligence","140":"artificial intelligence","141":"artificial intelligence","142":"artificial intelligence","143":"artificial intelligence","144":"artificial intelligence","145":"artificial intelligence","146":"artificial intelligence","147":"artificial intelligence","148":"artificial intelligence","149":"artificial intelligence","150":"artificial intelligence","151":"artificial intelligence","152":"artificial intelligence","153":"artificial intelligence","154":"artificial intelligence","155":"artificial intelligence","156":"artificial intelligence","157":"artificial intelligence","158":"artificial intelligence","159":"artificial intelligence","160":"artificial intelligence","161":"artificial intelligence","162":"artificial intelligence","163":"artificial intelligence","164":"artificial intelligence","165":"artificial intelligence","166":"artificial intelligence","167":"artificial intelligence","168":"artificial intelligence","169":"artificial intelligence","170":"artificial intelligence","171":"artificial intelligence","172":"artificial intelligence","173":"artificial intelligence","174":"artificial intelligence","175":"artificial intelligence","176":"artificial intelligence","177":"artificial intelligence","178":"artificial intelligence","179":"artificial intelligence","180":"artificial intelligence","181":"artificial intelligence","182":"artificial intelligence","183":"artificial intelligence","184":"artificial intelligence","185":"artificial intelligence","186":"artificial intelligence","187":"artificial intelligence","188":"artificial intelligence","189":"artificial intelligence","190":"artificial intelligence","191":"artificial intelligence","192":"artificial intelligence","193":"artificial intelligence","194":"artificial intelligence","195":"artificial intelligence","196":"artificial intelligence","197":"artificial intelligence","198":"artificial intelligence","199":"artificial intelligence","200":"artificial intelligence","201":"artificial intelligence","202":"artificial intelligence","203":"artificial intelligence","204":"artificial intelligence","205":"artificial intelligence","206":"artificial intelligence","207":"artificial intelligence","208":"artificial intelligence","209":"artificial intelligence","210":"artificial intelligence","211":"artificial intelligence","212":"artificial intelligence","213":"artificial intelligence","214":"artificial intelligence","215":"artificial intelligence","216":"artificial intelligence","217":"artificial intelligence","218":"artificial intelligence","219":"artificial intelligence","220":"artificial intelligence","221":"artificial intelligence","222":"artificial intelligence","223":"artificial intelligence","224":"artificial intelligence","225":"artificial intelligence","226":"artificial intelligence","227":"artificial intelligence","228":"artificial intelligence","229":"artificial intelligence","230":"artificial intelligence","231":"artificial intelligence","232":"artificial intelligence","233":"artificial intelligence","234":"artificial intelligence","235":"artificial intelligence","236":"artificial intelligence","237":"artificial intelligence","238":"artificial intelligence","239":"artificial intelligence","240":"artificial intelligence","241":"artificial intelligence","242":"artificial intelligence","243":"artificial intelligence","244":"artificial intelligence","245":"artificial intelligence","246":"artificial intelligence","247":"artificial intelligence","248":"artificial intelligence","249":"artificial intelligence","250":"artificial intelligence","251":"artificial intelligence","252":"artificial intelligence","253":"artificial intelligence","254":"artificial intelligence","255":"artificial intelligence","256":"artificial intelligence","257":"artificial intelligence","258":"artificial intelligence","259":"artificial intelligence","260":"artificial intelligence","261":"artificial intelligence","262":"artificial intelligence","263":"artificial intelligence","264":"artificial intelligence","265":"artificial intelligence","266":"artificial intelligence","267":"artificial intelligence","268":"artificial intelligence","269":"artificial intelligence","270":"artificial intelligence","271":"artificial intelligence","272":"artificial intelligence","273":"artificial intelligence","274":"artificial intelligence","275":"artificial intelligence","276":"artificial intelligence","277":"artificial intelligence","278":"artificial intelligence","279":"artificial intelligence","280":"artificial intelligence","281":"artificial intelligence","282":"artificial intelligence","283":"artificial intelligence","284":"artificial intelligence","285":"artificial intelligence","286":"artificial intelligence","287":"artificial intelligence","288":"artificial intelligence","289":"artificial intelligence","290":"artificial intelligence","291":"artificial intelligence","292":"artificial intelligence","293":"artificial intelligence","294":"artificial intelligence","295":"artificial intelligence","296":"artificial intelligence","297":"artificial intelligence","298":"artificial intelligence","299":"artificial intelligence","300":"artificial intelligence","301":"artificial intelligence","302":"artificial intelligence","303":"artificial intelligence","304":"artificial intelligence","305":"artificial intelligence","306":"artificial intelligence","307":"artificial intelligence","308":"artificial intelligence","309":"artificial intelligence","310":"artificial intelligence","311":"artificial intelligence","312":"artificial intelligence","313":"artificial intelligence","314":"artificial intelligence","315":"artificial intelligence","316":"artificial intelligence","317":"artificial intelligence","318":"artificial intelligence","319":"artificial intelligence","320":"artificial intelligence","321":"artificial intelligence","322":"artificial intelligence","323":"artificial intelligence","324":"artificial intelligence","325":"artificial intelligence","326":"artificial intelligence","327":"artificial intelligence","328":"artificial intelligence","329":"artificial intelligence","330":"artificial intelligence","331":"artificial intelligence","332":"artificial intelligence","333":"artificial intelligence","334":"artificial intelligence","335":"artificial intelligence","336":"artificial intelligence","337":"artificial intelligence","338":"artificial intelligence","339":"artificial intelligence","340":"artificial intelligence","341":"artificial intelligence","342":"artificial intelligence","343":"artificial intelligence","344":"artificial intelligence","345":"artificial intelligence","346":"artificial intelligence","347":"artificial intelligence","348":"artificial intelligence","349":"artificial intelligence","350":"artificial intelligence","351":"artificial intelligence","352":"artificial intelligence","353":"artificial intelligence","354":"artificial intelligence","355":"artificial intelligence","356":"artificial intelligence","357":"artificial intelligence","358":"artificial intelligence","359":"artificial intelligence","360":"artificial intelligence","361":"artificial intelligence","362":"artificial intelligence","363":"artificial intelligence","364":"artificial intelligence","365":"artificial intelligence","366":"artificial intelligence","367":"artificial intelligence","368":"artificial intelligence","369":"artificial intelligence","370":"artificial intelligence","371":"artificial intelligence","372":"artificial intelligence","373":"artificial intelligence","374":"artificial intelligence","375":"artificial intelligence","376":"artificial intelligence","377":"artificial intelligence","378":"artificial intelligence","379":"artificial intelligence","380":"artificial intelligence","381":"artificial intelligence","382":"artificial intelligence","383":"artificial intelligence","384":"artificial intelligence","385":"artificial intelligence","386":"artificial intelligence","387":"artificial intelligence","388":"artificial intelligence","389":"artificial intelligence","390":"artificial intelligence","391":"artificial intelligence","392":"artificial intelligence","393":"artificial intelligence","394":"artificial intelligence","395":"artificial intelligence","396":"artificial intelligence","397":"artificial intelligence","398":"artificial intelligence","399":"artificial intelligence","400":"artificial intelligence","401":"artificial intelligence","402":"artificial intelligence","403":"artificial intelligence","404":"artificial intelligence","405":"artificial intelligence","406":"artificial intelligence","407":"artificial intelligence","408":"artificial intelligence","409":"artificial intelligence","410":"artificial intelligence","411":"artificial intelligence","412":"artificial intelligence","413":"artificial intelligence","414":"artificial intelligence","415":"artificial intelligence","416":"artificial intelligence","417":"artificial intelligence","418":"artificial intelligence","419":"artificial intelligence","420":"artificial intelligence","421":"artificial intelligence","422":"artificial intelligence","423":"artificial intelligence","424":"artificial intelligence","425":"artificial intelligence","426":"artificial intelligence","427":"artificial intelligence","428":"artificial intelligence","429":"artificial intelligence","430":"artificial intelligence","431":"artificial intelligence","432":"artificial intelligence","433":"artificial intelligence","434":"artificial intelligence","435":"artificial intelligence","436":"artificial intelligence","437":"artificial intelligence","438":"artificial intelligence","439":"artificial intelligence","440":"artificial intelligence","441":"artificial intelligence","442":"artificial intelligence","443":"artificial intelligence","444":"artificial intelligence","445":"artificial intelligence","446":"artificial intelligence","447":"artificial intelligence","448":"artificial intelligence","449":"artificial intelligence","450":"artificial intelligence","451":"artificial intelligence","452":"artificial intelligence","453":"artificial intelligence","454":"artificial intelligence","455":"artificial intelligence","456":"artificial intelligence","457":"artificial intelligence","458":"artificial intelligence","459":"artificial intelligence","460":"artificial intelligence","461":"artificial intelligence","462":"artificial intelligence","463":"artificial intelligence","464":"artificial intelligence","465":"artificial intelligence","466":"artificial intelligence","467":"artificial intelligence","468":"artificial intelligence","469":"artificial intelligence","470":"artificial intelligence","471":"artificial intelligence","472":"artificial intelligence","473":"artificial intelligence","474":"artificial intelligence","475":"artificial intelligence","476":"artificial intelligence","477":"artificial intelligence","478":"artificial intelligence","479":"artificial intelligence","480":"artificial intelligence","481":"artificial intelligence","482":"artificial intelligence","483":"artificial intelligence","484":"artificial intelligence","485":"artificial intelligence","486":"artificial intelligence","487":"artificial intelligence","488":"artificial intelligence","489":"artificial intelligence","490":"artificial intelligence","491":"artificial intelligence","492":"artificial intelligence","493":"artificial intelligence","494":"artificial intelligence","495":"artificial intelligence","496":"artificial intelligence","497":"artificial intelligence","498":"artificial intelligence","499":"artificial intelligence","500":"artificial intelligence","501":"artificial intelligence","502":"artificial intelligence","503":"artificial intelligence","504":"artificial intelligence","505":"artificial intelligence","506":"artificial intelligence","507":"artificial intelligence","508":"artificial intelligence","509":"artificial intelligence","510":"artificial intelligence","511":"artificial intelligence","512":"artificial intelligence","513":"artificial intelligence","514":"artificial intelligence","515":"artificial intelligence","516":"artificial intelligence","517":"artificial intelligence","518":"artificial intelligence","519":"artificial intelligence","520":"artificial intelligence","521":"artificial intelligence","522":"artificial intelligence","523":"artificial intelligence","524":"artificial intelligence","525":"artificial intelligence","526":"artificial intelligence","527":"artificial intelligence","528":"artificial intelligence","529":"artificial intelligence","530":"artificial intelligence","531":"artificial intelligence","532":"artificial intelligence","533":"artificial intelligence","534":"artificial intelligence","535":"artificial intelligence","536":"artificial intelligence","537":"artificial intelligence","538":"artificial intelligence","539":"artificial intelligence","540":"artificial intelligence","541":"artificial intelligence","542":"artificial intelligence","543":"artificial intelligence","544":"artificial intelligence","545":"artificial intelligence","546":"artificial intelligence","547":"artificial intelligence","548":"artificial intelligence","549":"artificial intelligence","550":"artificial intelligence","551":"artificial intelligence","552":"artificial intelligence","553":"artificial intelligence","554":"artificial intelligence","555":"artificial intelligence","556":"artificial intelligence","557":"artificial intelligence","558":"artificial intelligence","559":"artificial intelligence","560":"artificial intelligence","561":"artificial intelligence","562":"artificial intelligence","563":"artificial intelligence","564":"artificial intelligence","565":"artificial intelligence","566":"artificial intelligence","567":"artificial intelligence","568":"artificial intelligence","569":"artificial intelligence","570":"artificial intelligence","571":"artificial intelligence","572":"artificial intelligence","573":"artificial intelligence","574":"artificial intelligence","575":"artificial intelligence","576":"artificial intelligence","577":"artificial intelligence","578":"artificial intelligence","579":"artificial intelligence","580":"artificial intelligence","581":"artificial intelligence","582":"artificial intelligence","583":"artificial intelligence","584":"artificial intelligence","585":"artificial intelligence","586":"artificial intelligence","587":"artificial intelligence","588":"artificial intelligence","589":"artificial intelligence","590":"artificial intelligence","591":"artificial intelligence","592":"artificial intelligence","593":"artificial intelligence","594":"artificial intelligence","595":"artificial intelligence","596":"artificial intelligence","597":"artificial intelligence","598":"artificial intelligence","599":"artificial intelligence","600":"artificial intelligence","601":"artificial intelligence","602":"artificial intelligence","603":"artificial intelligence","604":"artificial intelligence","605":"artificial intelligence","606":"artificial intelligence","607":"artificial intelligence","608":"artificial intelligence","609":"artificial intelligence","610":"artificial intelligence","611":"artificial intelligence","612":"artificial intelligence","613":"artificial intelligence","614":"artificial intelligence","615":"artificial intelligence","616":"artificial intelligence","617":"artificial intelligence","618":"artificial intelligence","619":"artificial intelligence","620":"artificial intelligence","621":"artificial intelligence","622":"artificial intelligence","623":"artificial intelligence","624":"artificial intelligence","625":"artificial intelligence","626":"artificial intelligence","627":"artificial intelligence","628":"artificial intelligence","629":"artificial intelligence","630":"artificial intelligence","631":"artificial intelligence","632":"artificial intelligence","633":"artificial intelligence","634":"artificial intelligence","635":"artificial intelligence","636":"artificial intelligence","637":"artificial intelligence","638":"artificial intelligence","639":"artificial intelligence","640":"artificial intelligence","641":"artificial intelligence","642":"artificial intelligence","643":"artificial intelligence","644":"artificial intelligence","645":"artificial intelligence","646":"artificial intelligence","647":"artificial intelligence","648":"artificial intelligence","649":"artificial intelligence","650":"artificial intelligence","651":"artificial intelligence","652":"artificial intelligence","653":"artificial intelligence","654":"artificial intelligence","655":"artificial intelligence","656":"artificial intelligence","657":"artificial intelligence","658":"artificial intelligence","659":"artificial intelligence","660":"artificial intelligence","661":"artificial intelligence","662":"artificial intelligence","663":"artificial intelligence","664":"artificial intelligence","665":"artificial intelligence","666":"artificial intelligence","667":"artificial intelligence","668":"artificial intelligence","669":"artificial intelligence","670":"artificial intelligence","671":"artificial intelligence","672":"artificial intelligence","673":"artificial intelligence","674":"artificial intelligence","675":"artificial intelligence","676":"artificial intelligence","677":"artificial intelligence","678":"artificial intelligence","679":"artificial intelligence","680":"artificial intelligence","681":"artificial intelligence","682":"artificial intelligence","683":"artificial intelligence","684":"artificial intelligence","685":"artificial intelligence","686":"artificial intelligence","687":"artificial intelligence","688":"artificial intelligence","689":"artificial intelligence","690":"artificial intelligence","691":"artificial intelligence","692":"artificial intelligence","693":"artificial intelligence","694":"artificial intelligence","695":"artificial intelligence","696":"artificial intelligence","697":"artificial intelligence","698":"artificial intelligence","699":"artificial intelligence","700":"artificial intelligence","701":"artificial intelligence","702":"artificial intelligence","703":"artificial intelligence","704":"artificial intelligence","705":"artificial intelligence","706":"artificial intelligence","707":"artificial intelligence","708":"artificial intelligence","709":"artificial intelligence","710":"artificial intelligence","711":"artificial intelligence","712":"artificial intelligence","713":"artificial intelligence","714":"artificial intelligence","715":"artificial intelligence","716":"artificial intelligence","717":"artificial intelligence","718":"artificial intelligence","719":"artificial intelligence","720":"artificial intelligence","721":"artificial intelligence","722":"artificial intelligence","723":"artificial intelligence","724":"artificial intelligence","725":"artificial intelligence","726":"artificial intelligence","727":"artificial intelligence","728":"artificial intelligence","729":"artificial intelligence","730":"artificial intelligence","731":"artificial intelligence","732":"artificial intelligence","733":"artificial intelligence","734":"artificial intelligence","735":"artificial intelligence","736":"artificial intelligence","737":"artificial intelligence","738":"artificial intelligence","739":"artificial intelligence","740":"artificial intelligence","741":"artificial intelligence","742":"artificial intelligence","743":"artificial intelligence","744":"artificial intelligence","745":"artificial intelligence","746":"artificial intelligence","747":"artificial intelligence","748":"artificial intelligence","749":"AI","750":"AI","751":"AI","752":"AI","753":"AI","754":"AI","755":"AI","756":"AI","757":"AI","758":"AI","759":"AI","760":"AI","761":"AI","762":"AI","763":"AI","764":"AI","765":"AI","766":"AI","767":"AI","768":"AI","769":"AI","770":"AI","771":"AI","772":"AI","773":"AI","774":"AI","775":"AI","776":"AI","777":"AI","778":"AI","779":"AI","780":"AI","781":"AI","782":"AI","783":"AI","784":"AI","785":"AI","786":"AI","787":"AI","788":"AI","789":"AI","790":"AI","791":"AI","792":"AI","793":"AI","794":"AI","795":"AI","796":"AI","797":"AI","798":"AI","799":"AI","800":"AI","801":"AI","802":"AI","803":"AI","804":"AI","805":"AI","806":"AI","807":"AI","808":"AI","809":"AI","810":"AI","811":"AI","812":"AI","813":"AI","814":"AI","815":"AI","816":"AI","817":"AI","818":"AI","819":"AI","820":"AI","821":"AI","822":"AI","823":"AI","824":"AI","825":"AI","826":"AI","827":"AI","828":"AI","829":"AI","830":"AI","831":"AI","832":"AI","833":"AI","834":"AI","835":"AI","836":"AI","837":"AI","838":"AI","839":"AI","840":"AI","841":"AI","842":"AI","843":"AI","844":"AI","845":"AI","846":"AI","847":"AI","848":"AI","849":"AI","850":"AI","851":"AI","852":"AI","853":"AI","854":"AI","855":"AI","856":"AI","857":"AI","858":"AI","859":"AI","860":"AI","861":"AI","862":"AI","863":"AI","864":"AI","865":"AI","866":"AI","867":"AI","868":"AI","869":"AI","870":"AI","871":"AI","872":"AI","873":"AI","874":"AI","875":"AI","876":"AI","877":"AI","878":"AI","879":"AI","880":"AI","881":"AI","882":"AI","883":"AI","884":"AI","885":"AI","886":"AI","887":"AI","888":"AI","889":"AI","890":"AI","891":"AI","892":"AI","893":"AI","894":"AI","895":"AI","896":"AI","897":"AI","898":"AI","899":"AI","900":"AI","901":"AI","902":"AI","903":"AI","904":"AI","905":"AI","906":"AI","907":"AI","908":"AI","909":"AI","910":"AI","911":"AI","912":"AI","913":"AI","914":"AI","915":"AI","916":"AI","917":"AI","918":"AI","919":"AI","920":"AI","921":"AI","922":"AI","923":"AI","924":"AI","925":"AI","926":"AI","927":"AI","928":"AI","929":"AI","930":"AI","931":"AI","932":"AI","933":"AI","934":"AI","935":"AI","936":"AI","937":"AI","938":"AI","939":"AI","940":"AI","941":"AI","942":"AI","943":"AI","944":"AI","945":"AI","946":"AI","947":"AI","948":"AI","949":"AI","950":"AI","951":"AI","952":"AI","953":"AI","954":"AI","955":"AI","956":"AI","957":"AI","958":"AI","959":"AI","960":"AI","961":"AI","962":"AI","963":"AI","964":"AI","965":"AI","966":"AI","967":"AI","968":"AI","969":"AI","970":"AI","971":"AI","972":"AI","973":"AI","974":"AI","975":"AI","976":"AI","977":"AI","978":"AI","979":"AI","980":"AI","981":"AI","982":"AI","983":"AI","984":"AI","985":"AI","986":"AI","987":"AI","988":"AI","989":"AI","990":"AI","991":"AI","992":"AI","993":"AI","994":"AI","995":"AI","996":"AI","997":"AI","998":"AI","999":"AI","1000":"AI","1001":"AI","1002":"AI","1003":"AI","1004":"AI","1005":"AI","1006":"AI","1007":"AI","1008":"AI","1009":"AI","1010":"AI","1011":"AI","1012":"AI","1013":"AI","1014":"AI","1015":"AI","1016":"AI","1017":"AI","1018":"AI","1019":"AI","1020":"AI","1021":"AI","1022":"AI","1023":"AI","1024":"AI","1025":"AI","1026":"AI","1027":"AI","1028":"AI","1029":"AI","1030":"AI","1031":"AI","1032":"AI","1033":"AI","1034":"AI","1035":"AI","1036":"AI","1037":"AI","1038":"AI","1039":"AI","1040":"AI","1041":"AI","1042":"AI","1043":"AI","1044":"AI","1045":"AI","1046":"AI","1047":"AI","1048":"AI","1049":"AI","1050":"AI","1051":"AI","1052":"AI","1053":"AI","1054":"AI","1055":"AI","1056":"AI","1057":"AI","1058":"AI","1059":"AI","1060":"AI","1061":"AI","1062":"AI","1063":"AI","1064":"AI","1065":"AI","1066":"AI","1067":"AI","1068":"AI","1069":"AI","1070":"AI","1071":"AI","1072":"AI","1073":"AI","1074":"AI","1075":"AI","1076":"AI","1077":"AI","1078":"AI","1079":"AI","1080":"AI","1081":"AI","1082":"AI","1083":"AI","1084":"AI","1085":"AI","1086":"AI","1087":"AI","1088":"AI","1089":"AI","1090":"AI","1091":"AI","1092":"AI","1093":"AI","1094":"AI","1095":"AI","1096":"AI","1097":"AI","1098":"AI","1099":"AI","1100":"AI","1101":"AI","1102":"AI","1103":"AI","1104":"AI","1105":"AI","1106":"AI","1107":"AI","1108":"AI","1109":"AI","1110":"AI","1111":"AI","1112":"AI","1113":"AI","1114":"AI","1115":"AI","1116":"AI","1117":"AI","1118":"AI","1119":"AI","1120":"AI","1121":"AI","1122":"AI","1123":"AI","1124":"AI","1125":"AI","1126":"AI","1127":"AI","1128":"AI","1129":"AI","1130":"AI","1131":"AI","1132":"AI","1133":"AI","1134":"AI","1135":"AI","1136":"AI","1137":"AI","1138":"AI","1139":"AI","1140":"AI","1141":"AI","1142":"AI","1143":"AI","1144":"AI","1145":"AI","1146":"AI","1147":"AI","1148":"AI","1149":"AI","1150":"AI","1151":"AI","1152":"AI","1153":"AI","1154":"AI","1155":"AI","1156":"AI","1157":"AI","1158":"AI","1159":"AI","1160":"AI","1161":"AI","1162":"AI","1163":"AI","1164":"AI","1165":"AI","1166":"AI","1167":"AI","1168":"AI","1169":"AI","1170":"AI","1171":"AI","1172":"AI","1173":"AI","1174":"AI","1175":"AI","1176":"AI","1177":"AI","1178":"AI","1179":"AI","1180":"AI","1181":"AI","1182":"AI","1183":"AI","1184":"AI","1185":"AI","1186":"AI","1187":"AI","1188":"AI","1189":"AI","1190":"AI","1191":"AI","1192":"AI","1193":"AI","1194":"AI","1195":"AI","1196":"AI","1197":"AI","1198":"AI","1199":"AI","1200":"AI","1201":"AI","1202":"AI","1203":"AI","1204":"AI","1205":"AI","1206":"AI","1207":"AI","1208":"AI","1209":"AI","1210":"AI","1211":"AI","1212":"AI","1213":"AI","1214":"AI","1215":"AI","1216":"AI","1217":"AI","1218":"AI","1219":"AI","1220":"AI","1221":"AI","1222":"AI","1223":"AI","1224":"AI","1225":"AI","1226":"AI","1227":"AI","1228":"AI","1229":"AI","1230":"AI","1231":"AI","1232":"AI","1233":"AI","1234":"AI","1235":"AI","1236":"AI","1237":"AI","1238":"AI","1239":"AI","1240":"AI","1241":"AI","1242":"AI","1243":"AI","1244":"AI","1245":"AI","1246":"AI","1247":"AI","1248":"AI","1249":"AI","1250":"AI","1251":"AI","1252":"AI","1253":"AI","1254":"AI","1255":"AI","1256":"AI","1257":"AI","1258":"AI","1259":"AI","1260":"AI","1261":"AI","1262":"AI","1263":"AI","1264":"AI","1265":"AI","1266":"AI","1267":"AI","1268":"AI","1269":"AI","1270":"AI","1271":"AI","1272":"AI","1273":"AI","1274":"AI","1275":"AI","1276":"AI","1277":"AI","1278":"AI","1279":"AI","1280":"AI","1281":"AI","1282":"AI","1283":"AI","1284":"AI","1285":"AI","1286":"AI","1287":"AI","1288":"AI","1289":"AI","1290":"AI","1291":"AI","1292":"AI","1293":"AI","1294":"AI","1295":"AI","1296":"AI","1297":"AI","1298":"AI","1299":"AI","1300":"AI","1301":"AI","1302":"AI","1303":"AI","1304":"AI","1305":"AI","1306":"AI","1307":"AI","1308":"AI","1309":"AI","1310":"AI","1311":"AI","1312":"AI","1313":"AI","1314":"AI","1315":"AI","1316":"AI","1317":"AI","1318":"AI","1319":"AI","1320":"AI","1321":"AI","1322":"AI","1323":"AI","1324":"AI","1325":"AI","1326":"AI","1327":"AI","1328":"AI","1329":"AI","1330":"AI","1331":"AI","1332":"AI","1333":"AI","1334":"AI","1335":"AI","1336":"AI","1337":"AI","1338":"AI","1339":"AI","1340":"AI","1341":"AI","1342":"AI","1343":"AI","1344":"AI","1345":"AI","1346":"AI","1347":"AI","1348":"AI","1349":"AI","1350":"AI","1351":"AI","1352":"AI","1353":"AI","1354":"AI","1355":"AI","1356":"AI","1357":"AI","1358":"AI","1359":"AI","1360":"AI","1361":"AI","1362":"AI","1363":"AI","1364":"AI","1365":"AI","1366":"AI","1367":"AI","1368":"AI","1369":"AI","1370":"AI","1371":"AI","1372":"AI","1373":"AI","1374":"AI","1375":"AI","1376":"AI","1377":"AI","1378":"AI","1379":"AI","1380":"AI","1381":"AI","1382":"AI","1383":"AI","1384":"AI","1385":"AI","1386":"AI","1387":"AI","1388":"AI","1389":"AI","1390":"AI","1391":"AI","1392":"AI","1393":"AI","1394":"AI","1395":"AI","1396":"AI","1397":"AI","1398":"AI","1399":"AI","1400":"AI","1401":"AI","1402":"AI","1403":"AI","1404":"AI","1405":"AI","1406":"AI","1407":"AI","1408":"AI","1409":"AI","1410":"AI","1411":"AI","1412":"AI","1413":"AI","1414":"AI","1415":"AI","1416":"AI","1417":"AI","1418":"AI","1419":"AI","1420":"AI","1421":"AI","1422":"AI","1423":"AI","1424":"AI","1425":"AI","1426":"AI","1427":"AI","1428":"AI","1429":"AI","1430":"AI","1431":"AI","1432":"AI","1433":"AI","1434":"AI","1435":"AI","1436":"AI","1437":"AI","1438":"AI","1439":"AI","1440":"AI","1441":"AI","1442":"AI","1443":"AI","1444":"AI","1445":"AI","1446":"AI","1447":"AI","1448":"AI","1449":"AI","1450":"AI","1451":"AI","1452":"AI","1453":"AI","1454":"AI","1455":"AI","1456":"AI","1457":"AI","1458":"AI","1459":"AI","1460":"AI","1461":"AI","1462":"AI","1463":"AI","1464":"AI","1465":"AI","1466":"AI","1467":"AI","1468":"AI","1469":"AI","1470":"AI","1471":"AI","1472":"AI","1473":"AI","1474":"AI","1475":"AI","1476":"AI","1477":"AI","1478":"AI","1479":"AI","1480":"AI","1481":"AI","1482":"AI","1483":"AI","1484":"AI","1485":"AI","1486":"AI","1487":"AI","1488":"AI","1489":"AI","1490":"AI","1491":"AI","1492":"AI","1493":"AI","1494":"AI","1495":"AI","1496":"AI","1497":"AI","1498":"AI","1499":"AI","1500":"AI","1501":"AI","1502":"AI","1503":"AI","1504":"AI","1505":"AI","1506":"AI","1507":"AI","1508":"AI","1509":"AI","1510":"AI","1511":"AI","1512":"AI","1513":"AI","1514":"AI","1515":"AI","1516":"AI","1517":"AI","1518":"AI","1519":"AI","1520":"AI","1521":"AI","1522":"AI","1523":"AI","1524":"AI","1525":"AI","1526":"AI","1527":"AI","1528":"AI","1529":"AI","1530":"AI","1531":"AI","1532":"AI","1533":"AI","1534":"AI","1535":"AI","1536":"AI","1537":"AI","1538":"AI","1539":"AI","1540":"AI","1541":"AI","1542":"AI","1543":"AI","1544":"AI","1545":"AI","1546":"AI","1547":"AI","1548":"AI","1549":"AI","1550":"AI","1551":"AI","1552":"AI","1553":"AI","1554":"AI","1555":"AI","1556":"AI","1557":"AI","1558":"AI","1559":"AI","1560":"AI","1561":"AI","1562":"AI","1563":"AI","1564":"AI","1565":"AI","1566":"AI","1567":"AI","1568":"AI","1569":"AI","1570":"AI","1571":"AI","1572":"AI","1573":"AI","1574":"AI","1575":"AI","1576":"AI","1577":"AI","1578":"AI","1579":"AI","1580":"AI","1581":"AI","1582":"AI","1583":"AI","1584":"AI","1585":"AI","1586":"AI","1587":"AI","1588":"AI","1589":"AI","1590":"AI","1591":"AI","1592":"AI","1593":"AI","1594":"AI","1595":"AI","1596":"AI","1597":"AI","1598":"AI","1599":"AI","1600":"AI","1601":"AI","1602":"AI","1603":"AI","1604":"AI","1605":"AI","1606":"AI","1607":"AI","1608":"AI","1609":"AI","1610":"AI","1611":"AI","1612":"AI","1613":"AI","1614":"AI","1615":"AI","1616":"AI","1617":"AI","1618":"AI","1619":"AI","1620":"AI","1621":"AI","1622":"AI","1623":"AI","1624":"AI","1625":"AI","1626":"AI","1627":"AI","1628":"AI","1629":"AI","1630":"AI","1631":"AI","1632":"AI","1633":"AI","1634":"AI","1635":"AI","1636":"AI","1637":"AI","1638":"AI","1639":"AI","1640":"AI","1641":"AI","1642":"AI","1643":"AI","1644":"AI","1645":"AI","1646":"AI","1647":"AI","1648":"AI","1649":"AI","1650":"AI","1651":"AI","1652":"AI","1653":"AI","1654":"AI","1655":"AI","1656":"machine learning","1657":"machine learning","1658":"machine learning","1659":"machine learning","1660":"machine learning","1661":"machine learning","1662":"machine learning","1663":"machine learning","1664":"machine learning","1665":"machine learning","1666":"machine learning","1667":"machine learning","1668":"machine learning","1669":"machine learning","1670":"machine learning","1671":"machine learning","1672":"machine learning","1673":"machine learning","1674":"machine learning","1675":"machine learning","1676":"machine learning","1677":"machine learning","1678":"machine learning","1679":"machine learning","1680":"machine learning","1681":"machine learning","1682":"machine learning","1683":"machine learning","1684":"machine learning","1685":"machine learning","1686":"machine learning","1687":"machine learning","1688":"machine learning","1689":"machine learning","1690":"machine learning","1691":"machine learning","1692":"machine learning","1693":"machine learning","1694":"machine learning","1695":"machine learning","1696":"machine learning","1697":"machine learning","1698":"machine learning","1699":"machine learning","1700":"machine learning","1701":"machine learning","1702":"machine learning","1703":"machine learning","1704":"machine learning","1705":"machine learning","1706":"machine learning","1707":"machine learning","1708":"machine learning","1709":"machine learning","1710":"machine learning","1711":"machine learning","1712":"machine learning","1713":"machine learning","1714":"machine learning","1715":"machine learning","1716":"machine learning","1717":"machine learning","1718":"machine learning","1719":"machine learning","1720":"machine learning","1721":"machine learning","1722":"machine learning","1723":"machine learning","1724":"machine learning","1725":"machine learning","1726":"machine learning","1727":"machine learning","1728":"machine learning","1729":"machine learning","1730":"machine learning","1731":"machine learning","1732":"machine learning","1733":"machine learning","1734":"machine learning","1735":"machine learning","1736":"machine learning","1737":"machine learning","1738":"machine learning","1739":"machine learning","1740":"machine learning","1741":"machine learning","1742":"machine learning","1743":"machine learning","1744":"machine learning","1745":"machine learning","1746":"machine learning","1747":"machine learning","1748":"machine learning","1749":"machine learning","1750":"machine learning","1751":"machine learning","1752":"machine learning","1753":"machine learning","1754":"machine learning","1755":"machine learning","1756":"machine learning","1757":"machine learning","1758":"machine learning","1759":"machine learning","1760":"machine learning","1761":"machine learning","1762":"machine learning","1763":"machine learning","1764":"machine learning","1765":"machine learning","1766":"machine learning","1767":"machine learning","1768":"machine learning","1769":"machine learning","1770":"machine learning","1771":"machine learning","1772":"machine learning","1773":"machine learning","1774":"machine learning","1775":"machine learning","1776":"machine learning","1777":"machine learning","1778":"machine learning","1779":"machine learning","1780":"machine learning","1781":"machine learning","1782":"machine learning","1783":"machine learning","1784":"machine learning","1785":"machine learning","1786":"machine learning","1787":"machine learning","1788":"machine learning","1789":"machine learning","1790":"machine learning","1791":"machine learning","1792":"machine learning","1793":"machine learning","1794":"machine learning","1795":"machine learning","1796":"machine learning","1797":"machine learning","1798":"machine learning","1799":"machine learning","1800":"machine learning","1801":"machine learning","1802":"machine learning","1803":"machine learning","1804":"machine learning","1805":"machine learning","1806":"machine learning","1807":"machine learning","1808":"machine learning","1809":"machine learning","1810":"machine learning","1811":"machine learning","1812":"machine learning","1813":"machine learning","1814":"machine learning","1815":"machine learning","1816":"machine learning","1817":"machine learning","1818":"machine learning","1819":"machine learning","1820":"machine learning","1821":"machine learning","1822":"machine learning","1823":"machine learning","1824":"machine learning","1825":"machine learning","1826":"machine learning","1827":"machine learning","1828":"machine learning","1829":"machine learning","1830":"machine learning","1831":"machine learning","1832":"machine learning","1833":"machine learning","1834":"machine learning","1835":"machine learning","1836":"machine learning","1837":"machine learning","1838":"machine learning","1839":"machine learning","1840":"machine learning","1841":"machine learning","1842":"machine learning","1843":"machine learning","1844":"machine learning","1845":"machine learning","1846":"machine learning","1847":"machine learning","1848":"machine learning","1849":"machine learning","1850":"machine learning","1851":"machine learning","1852":"machine learning","1853":"machine learning","1854":"machine learning","1855":"machine learning","1856":"machine learning","1857":"machine learning","1858":"machine learning","1859":"machine learning","1860":"machine learning","1861":"machine learning","1862":"machine learning","1863":"machine learning","1864":"machine learning","1865":"machine learning","1866":"machine learning","1867":"machine learning","1868":"machine learning","1869":"machine learning","1870":"machine learning","1871":"machine learning","1872":"machine learning","1873":"machine learning","1874":"machine learning","1875":"machine learning","1876":"machine learning","1877":"machine learning","1878":"machine learning","1879":"machine learning","1880":"machine learning","1881":"machine learning","1882":"machine learning","1883":"machine learning","1884":"machine learning","1885":"machine learning","1886":"machine learning","1887":"machine learning","1888":"machine learning","1889":"machine learning","1890":"machine learning","1891":"machine learning","1892":"machine learning","1893":"machine learning","1894":"machine learning","1895":"machine learning","1896":"machine learning","1897":"machine learning","1898":"machine learning","1899":"machine learning","1900":"machine learning","1901":"machine learning","1902":"machine learning","1903":"machine learning","1904":"machine learning","1905":"machine learning","1906":"machine learning","1907":"machine learning","1908":"machine learning","1909":"machine learning","1910":"machine learning","1911":"machine learning","1912":"machine learning","1913":"machine learning","1914":"machine learning","1915":"machine learning","1916":"machine learning","1917":"machine learning","1918":"machine learning","1919":"machine learning","1920":"machine learning","1921":"machine learning","1922":"machine learning","1923":"machine learning","1924":"machine learning","1925":"machine learning","1926":"machine learning","1927":"machine learning","1928":"machine learning","1929":"machine learning","1930":"machine learning","1931":"machine learning","1932":"machine learning","1933":"machine learning","1934":"machine learning","1935":"machine learning","1936":"machine learning","1937":"machine learning","1938":"machine learning","1939":"machine learning","1940":"machine learning","1941":"machine learning","1942":"machine learning","1943":"machine learning","1944":"machine learning","1945":"machine learning","1946":"machine learning","1947":"machine learning","1948":"machine learning","1949":"machine learning","1950":"machine learning","1951":"machine learning","1952":"machine learning","1953":"machine learning","1954":"machine learning","1955":"machine learning","1956":"machine learning","1957":"machine learning","1958":"machine learning","1959":"machine learning","1960":"machine learning","1961":"machine learning","1962":"machine learning","1963":"machine learning","1964":"machine learning","1965":"machine learning","1966":"machine learning","1967":"machine learning","1968":"machine learning","1969":"machine learning","1970":"machine learning","1971":"machine learning","1972":"machine learning","1973":"machine learning","1974":"machine learning","1975":"machine learning","1976":"machine learning","1977":"machine learning","1978":"machine learning","1979":"machine learning","1980":"machine learning","1981":"machine learning","1982":"machine learning","1983":"machine learning","1984":"machine learning","1985":"machine learning","1986":"machine learning","1987":"machine learning","1988":"machine learning","1989":"machine learning","1990":"machine learning","1991":"machine learning","1992":"machine learning","1993":"machine learning","1994":"machine learning","1995":"machine learning","1996":"machine learning","1997":"machine learning","1998":"machine learning","1999":"machine learning","2000":"machine learning","2001":"machine learning","2002":"machine learning","2003":"machine learning","2004":"machine learning","2005":"machine learning","2006":"machine learning","2007":"machine learning","2008":"machine learning","2009":"machine learning","2010":"machine learning","2011":"machine learning","2012":"machine learning","2013":"machine learning","2014":"machine learning","2015":"machine learning","2016":"machine learning","2017":"machine learning","2018":"machine learning","2019":"machine learning","2020":"machine learning","2021":"machine learning","2022":"machine learning","2023":"machine learning","2024":"machine learning","2025":"machine learning","2026":"machine learning","2027":"machine learning","2028":"machine learning","2029":"machine learning","2030":"machine learning","2031":"machine learning","2032":"machine learning","2033":"machine learning","2034":"machine learning","2035":"machine learning","2036":"machine learning","2037":"machine learning","2038":"machine learning","2039":"machine learning","2040":"machine learning","2041":"machine learning","2042":"machine learning","2043":"machine learning","2044":"machine learning","2045":"machine learning","2046":"machine learning","2047":"machine learning","2048":"machine learning","2049":"machine learning","2050":"machine learning","2051":"machine learning","2052":"machine learning","2053":"machine learning","2054":"machine learning","2055":"machine learning","2056":"machine learning","2057":"machine learning","2058":"machine learning","2059":"machine learning","2060":"machine learning","2061":"machine learning","2062":"machine learning","2063":"machine learning","2064":"machine learning","2065":"machine learning","2066":"machine learning","2067":"machine learning","2068":"machine learning","2069":"machine learning","2070":"machine learning","2071":"machine learning","2072":"machine learning","2073":"machine learning","2074":"machine learning","2075":"machine learning","2076":"machine learning","2077":"machine learning","2078":"machine learning","2079":"machine learning","2080":"machine learning","2081":"machine learning","2082":"machine learning","2083":"machine learning","2084":"machine learning","2085":"machine learning","2086":"machine learning","2087":"machine learning","2088":"machine learning","2089":"machine learning","2090":"machine learning","2091":"machine learning","2092":"machine learning","2093":"machine learning","2094":"machine learning","2095":"machine learning","2096":"machine learning","2097":"machine learning","2098":"machine learning","2099":"machine learning","2100":"machine learning","2101":"machine learning","2102":"machine learning","2103":"machine learning","2104":"machine learning","2105":"machine learning","2106":"machine learning","2107":"machine learning","2108":"machine learning","2109":"machine learning","2110":"machine learning","2111":"machine learning","2112":"machine learning","2113":"machine learning","2114":"machine learning","2115":"machine learning","2116":"machine learning","2117":"machine learning","2118":"machine learning","2119":"machine learning","2120":"machine learning","2121":"machine learning","2122":"machine learning","2123":"machine learning","2124":"machine learning","2125":"machine learning","2126":"machine learning","2127":"machine learning","2128":"machine learning","2129":"machine learning","2130":"machine learning","2131":"machine learning","2132":"machine learning","2133":"machine learning","2134":"machine learning","2135":"machine learning","2136":"machine learning","2137":"machine learning","2138":"machine learning","2139":"machine learning","2140":"machine learning","2141":"machine learning","2142":"machine learning","2143":"machine learning","2144":"machine learning","2145":"machine learning","2146":"machine learning","2147":"machine learning","2148":"machine learning","2149":"machine learning","2150":"machine learning","2151":"machine learning","2152":"machine learning","2153":"machine learning","2154":"machine learning","2155":"machine learning","2156":"machine learning","2157":"machine learning","2158":"machine learning","2159":"machine learning","2160":"machine learning","2161":"machine learning","2162":"machine learning","2163":"machine learning","2164":"machine learning","2165":"machine learning","2166":"machine learning","2167":"machine learning","2168":"machine learning","2169":"machine learning","2170":"machine learning","2171":"machine learning","2172":"machine learning","2173":"machine learning","2174":"machine learning","2175":"machine learning","2176":"machine learning","2177":"machine learning","2178":"machine learning","2179":"machine learning","2180":"machine learning","2181":"machine learning","2182":"machine learning","2183":"machine learning","2184":"machine learning","2185":"machine learning","2186":"machine learning","2187":"machine learning","2188":"machine learning","2189":"machine learning","2190":"machine learning","2191":"machine learning","2192":"machine learning","2193":"machine learning","2194":"machine learning","2195":"machine learning","2196":"machine learning","2197":"machine learning","2198":"machine learning","2199":"machine learning","2200":"machine learning","2201":"machine learning","2202":"machine learning","2203":"machine learning","2204":"machine learning","2205":"machine learning","2206":"machine learning","2207":"machine learning","2208":"machine learning","2209":"machine learning","2210":"machine learning","2211":"machine learning","2212":"machine learning","2213":"machine learning","2214":"machine learning","2215":"machine learning","2216":"machine learning","2217":"machine learning","2218":"machine learning","2219":"machine learning","2220":"machine learning","2221":"machine learning","2222":"machine learning","2223":"machine learning","2224":"machine learning","2225":"machine learning","2226":"machine learning","2227":"machine learning","2228":"machine learning","2229":"machine learning","2230":"machine learning","2231":"machine learning","2232":"machine learning","2233":"machine learning","2234":"machine learning","2235":"machine learning","2236":"machine learning","2237":"machine learning","2238":"machine learning","2239":"machine learning","2240":"machine learning","2241":"machine learning","2242":"machine learning","2243":"machine learning","2244":"machine learning","2245":"machine learning","2246":"machine learning","2247":"machine learning","2248":"machine learning","2249":"machine learning","2250":"machine learning","2251":"machine learning","2252":"machine learning","2253":"machine learning","2254":"machine learning","2255":"machine learning","2256":"machine learning","2257":"machine learning","2258":"machine learning","2259":"machine learning","2260":"machine learning","2261":"machine learning","2262":"machine learning","2263":"machine learning","2264":"machine learning","2265":"machine learning","2266":"machine learning","2267":"machine learning","2268":"machine learning","2269":"machine learning","2270":"machine learning","2271":"machine learning","2272":"machine learning","2273":"machine learning","2274":"machine learning","2275":"machine learning","2276":"machine learning","2277":"machine learning","2278":"machine learning","2279":"machine learning","2280":"machine learning","2281":"machine learning","2282":"machine learning","2283":"machine learning","2284":"machine learning","2285":"machine learning","2286":"machine learning","2287":"machine learning","2288":"machine learning","2289":"machine learning","2290":"machine learning","2291":"machine learning","2292":"machine learning","2293":"machine learning","2294":"machine learning","2295":"machine learning","2296":"machine learning","2297":"machine learning","2298":"machine learning","2299":"machine learning","2300":"machine learning","2301":"machine learning","2302":"machine learning","2303":"machine learning","2304":"machine learning","2305":"machine learning","2306":"machine learning","2307":"machine learning","2308":"machine learning","2309":"machine learning","2310":"machine learning","2311":"machine learning","2312":"machine learning","2313":"machine learning","2314":"machine learning","2315":"machine learning","2316":"machine learning","2317":"machine learning","2318":"machine learning","2319":"machine learning","2320":"machine learning","2321":"machine learning","2322":"machine learning","2323":"machine learning","2324":"machine learning","2325":"machine learning","2326":"machine learning","2327":"machine learning","2328":"machine learning","2329":"machine learning","2330":"machine learning","2331":"machine learning","2332":"machine learning","2333":"machine learning","2334":"machine learning","2335":"machine learning","2336":"machine learning","2337":"machine learning","2338":"machine learning","2339":"machine learning","2340":"machine learning","2341":"machine learning","2342":"machine learning","2343":"machine learning","2344":"machine learning","2345":"machine learning","2346":"machine learning","2347":"machine learning","2348":"machine learning","2349":"machine learning","2350":"machine learning","2351":"machine learning","2352":"machine learning","2353":"machine learning","2354":"machine learning","2355":"machine learning","2356":"machine learning","2357":"machine learning","2358":"machine learning","2359":"machine learning","2360":"machine learning","2361":"machine learning","2362":"machine learning","2363":"machine learning","2364":"machine learning","2365":"machine learning","2366":"machine learning","2367":"machine learning","2368":"machine learning","2369":"machine learning","2370":"machine learning","2371":"machine learning","2372":"machine learning","2373":"machine learning","2374":"machine learning","2375":"machine learning","2376":"machine learning","2377":"machine learning","2378":"machine learning","2379":"machine learning","2380":"machine learning","2381":"machine learning","2382":"machine learning","2383":"machine learning","2384":"machine learning","2385":"machine learning","2386":"machine learning","2387":"machine learning","2388":"machine learning","2389":"machine learning","2390":"machine learning","2391":"machine learning","2392":"machine learning","2393":"machine learning","2394":"machine learning","2395":"machine learning","2396":"machine learning","2397":"machine learning","2398":"machine learning","2399":"machine learning","2400":"machine learning","2401":"machine learning","2402":"machine learning","2403":"machine learning","2404":"machine learning","2405":"machine learning","2406":"machine learning","2407":"machine learning","2408":"machine learning","2409":"machine learning","2410":"machine learning","2411":"machine learning","2412":"machine learning","2413":"machine learning","2414":"machine learning","2415":"machine learning","2416":"machine learning","2417":"machine learning","2418":"machine learning","2419":"machine learning","2420":"machine learning","2421":"machine learning","2422":"machine learning","2423":"machine learning","2424":"machine learning","2425":"machine learning","2426":"machine learning","2427":"machine learning","2428":"machine learning","2429":"machine learning","2430":"machine learning","2431":"machine learning","2432":"machine learning","2433":"machine learning","2434":"machine learning","2435":"machine learning","2436":"machine learning","2437":"machine learning","2438":"machine learning","2439":"machine learning","2440":"machine learning","2441":"machine learning","2442":"machine learning","2443":"machine learning","2444":"machine learning","2445":"machine learning","2446":"machine learning","2447":"machine learning","2448":"machine learning","2449":"machine learning","2450":"machine learning","2451":"machine learning","2452":"machine learning","2453":"machine learning","2454":"machine learning","2455":"machine learning","2456":"machine learning","2457":"machine learning","2458":"machine learning","2459":"machine learning","2460":"machine learning","2461":"machine learning","2462":"machine learning","2463":"machine learning","2464":"machine learning","2465":"machine learning","2466":"machine learning","2467":"machine learning","2468":"machine learning","2469":"machine learning","2470":"machine learning","2471":"machine learning","2472":"machine learning","2473":"machine learning","2474":"machine learning","2475":"machine learning","2476":"machine learning","2477":"machine learning","2478":"machine learning","2479":"machine learning","2480":"machine learning","2481":"machine learning","2482":"machine learning","2483":"language model","2484":"language model","2485":"language model","2486":"language model","2487":"language model","2488":"language model","2489":"language model","2490":"language model","2491":"language model","2492":"language model","2493":"language model","2494":"language model","2495":"language model","2496":"language model","2497":"language model","2498":"language model","2499":"language model","2500":"language model","2501":"language model","2502":"language model","2503":"language model","2504":"language model","2505":"language model","2506":"language model","2507":"language model","2508":"language model","2509":"language model","2510":"language model","2511":"language model","2512":"language model","2513":"language model","2514":"language model","2515":"language model","2516":"language model","2517":"language model","2518":"language model","2519":"language model","2520":"language model","2521":"language model","2522":"language model","2523":"language model","2524":"language model","2525":"language model","2526":"language model","2527":"language model","2528":"language model","2529":"language model","2530":"language model","2531":"language model","2532":"language model","2533":"language model","2534":"language model","2535":"language model","2536":"language model","2537":"language model","2538":"language model","2539":"language model","2540":"language model","2541":"language model","2542":"language model","2543":"language model","2544":"language model","2545":"language model","2546":"language model","2547":"language model","2548":"language model","2549":"language model","2550":"language model","2551":"language model","2552":"language model","2553":"language model","2554":"language model","2555":"language model","2556":"language model","2557":"language model","2558":"language model","2559":"language model","2560":"language model","2561":"language model","2562":"language model","2563":"language model","2564":"language model","2565":"language model","2566":"language model","2567":"language model","2568":"language model","2569":"language model","2570":"language model","2571":"language model","2572":"language model","2573":"language model","2574":"language model","2575":"language model","2576":"language model","2577":"language model","2578":"language model","2579":"language model","2580":"language model","2581":"language model","2582":"language model","2583":"language model","2584":"language model","2585":"language model","2586":"language model","2587":"language model","2588":"language model","2589":"language model","2590":"language model","2591":"language model","2592":"language model","2593":"language model","2594":"language model","2595":"language model","2596":"language model","2597":"language model","2598":"language model","2599":"language model","2600":"language model","2601":"language model","2602":"language model","2603":"language model","2604":"language model","2605":"language model","2606":"language model","2607":"language model","2608":"language model","2609":"language model","2610":"language model","2611":"language model","2612":"language model","2613":"language model","2614":"language model","2615":"language model","2616":"language model","2617":"language model","2618":"language model","2619":"language model","2620":"language model","2621":"language model","2622":"language model","2623":"language model","2624":"language model","2625":"language model","2626":"language model","2627":"language model","2628":"language model","2629":"language model","2630":"language model","2631":"language model","2632":"language model","2633":"language model","2634":"language model","2635":"language model","2636":"language model","2637":"language model","2638":"language model","2639":"language model","2640":"language model","2641":"language model","2642":"language model","2643":"language model","2644":"language model","2645":"language model","2646":"language model","2647":"language model","2648":"language model","2649":"language model","2650":"language model","2651":"language model","2652":"language model","2653":"language model","2654":"language model","2655":"language model","2656":"language model","2657":"language model","2658":"language model","2659":"language model","2660":"language model","2661":"language model","2662":"language model","2663":"language model","2664":"language model","2665":"language model","2666":"language model","2667":"language model","2668":"language model","2669":"language model","2670":"language model","2671":"language model","2672":"language model","2673":"language model","2674":"language model","2675":"language model","2676":"language model","2677":"language model","2678":"language model","2679":"language model","2680":"language model","2681":"language model","2682":"language model","2683":"language model","2684":"language model","2685":"language model","2686":"language model","2687":"language model","2688":"language model","2689":"language model","2690":"language model","2691":"language model","2692":"language model","2693":"language model","2694":"language model","2695":"language model","2696":"language model","2697":"language model","2698":"language model","2699":"language model","2700":"language model","2701":"language model","2702":"language model","2703":"language model","2704":"language model","2705":"language model","2706":"language model","2707":"language model","2708":"language model","2709":"language model","2710":"language model","2711":"language model","2712":"language model","2713":"language model","2714":"language model","2715":"language model","2716":"language model","2717":"language model","2718":"language model","2719":"language model","2720":"language model","2721":"language model","2722":"language model","2723":"language model","2724":"language model","2725":"language model","2726":"language model","2727":"language model","2728":"language model","2729":"language model","2730":"language model","2731":"language model","2732":"language model","2733":"language model","2734":"language model","2735":"language model","2736":"language model","2737":"language model","2738":"language model","2739":"language model","2740":"language model","2741":"language model","2742":"language model","2743":"language model","2744":"language model","2745":"language model","2746":"language model","2747":"language model","2748":"language model","2749":"language model","2750":"language model","2751":"language model","2752":"language model","2753":"language model","2754":"language model","2755":"language model","2756":"language model","2757":"language model","2758":"language model","2759":"language model","2760":"language model","2761":"language model","2762":"language model","2763":"language model","2764":"language model","2765":"language model","2766":"language model","2767":"language model","2768":"language model","2769":"language model","2770":"language model","2771":"language model","2772":"language model","2773":"language model","2774":"language model","2775":"language model","2776":"language model","2777":"language model","2778":"language model","2779":"language model","2780":"language model","2781":"language model","2782":"language model","2783":"language model","2784":"language model","2785":"language model","2786":"language model","2787":"language model","2788":"language model","2789":"language model","2790":"language model","2791":"language model","2792":"language model","2793":"language model","2794":"language model","2795":"language model","2796":"language model","2797":"language model","2798":"language model","2799":"language model","2800":"language model","2801":"language model","2802":"language model","2803":"language model","2804":"language model","2805":"language model","2806":"language model","2807":"language model","2808":"language model","2809":"language model","2810":"language model","2811":"language model","2812":"language model","2813":"language model","2814":"language model","2815":"language model","2816":"language model","2817":"language model","2818":"language model","2819":"language model","2820":"language model","2821":"language model","2822":"language model","2823":"language model","2824":"language model","2825":"language model","2826":"language model","2827":"language model","2828":"language model","2829":"language model","2830":"language model","2831":"language model","2832":"language model","2833":"language model","2834":"language model","2835":"language model","2836":"language model","2837":"language model","2838":"language model","2839":"language model","2840":"language model","2841":"language model","2842":"language model","2843":"language model","2844":"language model","2845":"language model","2846":"language model","2847":"language model","2848":"language model","2849":"language model","2850":"language model","2851":"language model","2852":"language model","2853":"language model","2854":"language model","2855":"language model","2856":"language model","2857":"language model","2858":"language model","2859":"language model","2860":"language model","2861":"language model","2862":"language model","2863":"language model","2864":"language model","2865":"language model","2866":"language model","2867":"language model","2868":"language model","2869":"language model","2870":"language model","2871":"language model","2872":"language model","2873":"language model","2874":"language model","2875":"language model","2876":"language model","2877":"language model","2878":"language model","2879":"language model","2880":"language model","2881":"language model","2882":"language model","2883":"language model","2884":"language model","2885":"language model","2886":"language model","2887":"language model","2888":"language model","2889":"language model","2890":"language model","2891":"language model","2892":"language model","2893":"language model","2894":"language model","2895":"language model","2896":"language model","2897":"language model","2898":"language model","2899":"language model","2900":"language model","2901":"language model","2902":"language model","2903":"language model","2904":"language model","2905":"language model","2906":"language model","2907":"language model","2908":"language model","2909":"language model","2910":"language model","2911":"language model","2912":"language model","2913":"language model","2914":"language model","2915":"language model","2916":"language model","2917":"language model","2918":"language model","2919":"language model","2920":"language model","2921":"language model","2922":"language model","2923":"language model","2924":"language model","2925":"language model","2926":"language model","2927":"language model","2928":"language model","2929":"language model","2930":"language model","2931":"language model","2932":"language model","2933":"language model","2934":"language model","2935":"language model","2936":"language model","2937":"language model","2938":"language model","2939":"language model","2940":"language model","2941":"language model","2942":"language model","2943":"language model","2944":"language model","2945":"language model","2946":"language model","2947":"language model","2948":"language model","2949":"language model","2950":"language model","2951":"language model","2952":"language model","2953":"language model","2954":"language model","2955":"language model","2956":"language model","2957":"language model","2958":"language model","2959":"language model","2960":"language model","2961":"language model","2962":"language model","2963":"language model","2964":"language model","2965":"language model","2966":"language model","2967":"language model","2968":"language model","2969":"language model","2970":"language model","2971":"language model","2972":"language model","2973":"language model","2974":"language model","2975":"language model","2976":"language model","2977":"language model","2978":"language model","2979":"language model","2980":"language model","2981":"language model","2982":"language model","2983":"language model","2984":"language model","2985":"language model","2986":"language model","2987":"language model","2988":"language model","2989":"language model","2990":"language model","2991":"language model","2992":"language model","2993":"language model","2994":"language model","2995":"language model","2996":"language model","2997":"language model","2998":"language model","2999":"language model","3000":"language model","3001":"language model","3002":"language model","3003":"language model","3004":"language model","3005":"language model","3006":"language model","3007":"language model","3008":"language model","3009":"language model","3010":"language model","3011":"language model","3012":"language model","3013":"language model","3014":"language model","3015":"language model","3016":"language model","3017":"language model","3018":"language model","3019":"language model","3020":"language model","3021":"language model","3022":"language model","3023":"language model","3024":"language model","3025":"language model","3026":"language model","3027":"language model","3028":"language model","3029":"language model","3030":"language model","3031":"language model","3032":"language model","3033":"language model","3034":"language model","3035":"language model","3036":"language model","3037":"language model","3038":"language model","3039":"language model","3040":"language model","3041":"language model","3042":"language model","3043":"language model","3044":"language model","3045":"language model","3046":"language model","3047":"language model","3048":"language model","3049":"language model","3050":"language model","3051":"language model","3052":"language model","3053":"language model","3054":"language model","3055":"language model","3056":"language model","3057":"language model","3058":"language model","3059":"language model","3060":"language model","3061":"language model","3062":"language model","3063":"language model","3064":"language model","3065":"language model","3066":"language model","3067":"language model","3068":"language model","3069":"language model","3070":"language model","3071":"language model","3072":"language model","3073":"language model","3074":"language model","3075":"language model","3076":"language model","3077":"language model","3078":"language model","3079":"language model","3080":"language model","3081":"language model","3082":"language model","3083":"language model","3084":"language model","3085":"language model","3086":"language model","3087":"language model","3088":"language model","3089":"language model","3090":"language model","3091":"language model","3092":"language model","3093":"language model","3094":"language model","3095":"language model","3096":"language model","3097":"language model","3098":"language model","3099":"language model","3100":"language model","3101":"language model","3102":"language model","3103":"language model","3104":"language model","3105":"language model","3106":"language model","3107":"language model","3108":"language model","3109":"language model","3110":"language model","3111":"language model","3112":"language model","3113":"language model","3114":"language model","3115":"language model","3116":"language model","3117":"language model","3118":"language model","3119":"language model","3120":"language model","3121":"language model","3122":"language model","3123":"language model","3124":"language model","3125":"language model","3126":"language model","3127":"language model","3128":"language model","3129":"language model","3130":"language model","3131":"language model","3132":"language model","3133":"language model","3134":"language model","3135":"language model","3136":"language model","3137":"language model","3138":"language model","3139":"language model","3140":"language model","3141":"language model","3142":"language model","3143":"language model","3144":"language model","3145":"language model","3146":"language model","3147":"language model","3148":"language model","3149":"language model","3150":"language model","3151":"language model","3152":"language model","3153":"language model","3154":"language model","3155":"language model","3156":"language model","3157":"language model","3158":"language model","3159":"language model","3160":"language model","3161":"language model","3162":"language model","3163":"language model","3164":"language model","3165":"language model","3166":"language model","3167":"language model","3168":"language model","3169":"language model","3170":"language model","3171":"language model","3172":"language model","3173":"language model","3174":"language model","3175":"language model","3176":"language model","3177":"language model","3178":"language model","3179":"language model","3180":"language model","3181":"language model","3182":"language model","3183":"language model","3184":"language model","3185":"language model","3186":"language model","3187":"language model","3188":"language model","3189":"language model","3190":"language model","3191":"language model","3192":"language model","3193":"language model","3194":"language model","3195":"language model","3196":"language model","3197":"language model","3198":"language model","3199":"language model","3200":"language model","3201":"language model","3202":"language model","3203":"language model","3204":"language model","3205":"language model","3206":"language model","3207":"language model","3208":"language model","3209":"language model","3210":"language model","3211":"language model","3212":"language model","3213":"language model","3214":"language model","3215":"language model","3216":"language model","3217":"language model","3218":"language model","3219":"language model","3220":"language model","3221":"language model","3222":"language model","3223":"language model","3224":"language model","3225":"language model","3226":"language model","3227":"language model","3228":"language model","3229":"language model","3230":"language model","3231":"language model","3232":"language model","3233":"language model","3234":"language model","3235":"language model","3236":"language model","3237":"language model","3238":"language model","3239":"language model","3240":"language model","3241":"language model","3242":"language model","3243":"language model","3244":"language model","3245":"language model","3246":"language model","3247":"language model","3248":"language model","3249":"language model","3250":"language model","3251":"language model","3252":"language model","3253":"language model","3254":"language model","3255":"language model","3256":"language model","3257":"language model","3258":"language model","3259":"language model","3260":"language model","3261":"language model","3262":"language model","3263":"language model","3264":"language model","3265":"language model","3266":"language model","3267":"language model","3268":"language model","3269":"language model","3270":"language model","3271":"language model","3272":"language model","3273":"language model","3274":"language model","3275":"language model","3276":"language model","3277":"language model","3278":"language model","3279":"language model","3280":"language model","3281":"language model","3282":"language model","3283":"language model","3284":"language model","3285":"language model","3286":"language model","3287":"language model","3288":"language model","3289":"language model","3290":"language model","3291":"language model","3292":"language model","3293":"language model","3294":"language model","3295":"language model","3296":"language model","3297":"language model","3298":"language model","3299":"language model","3300":"language model","3301":"language model","3302":"language model","3303":"language model","3304":"language model","3305":"language model","3306":"language model","3307":"language model","3308":"language model","3309":"language model","3310":"language model","3311":"language model","3312":"language model","3313":"language model","3314":"language model","3315":"language model","3316":"language model","3317":"language model","3318":"language model","3319":"language model","3320":"language model","3321":"language model","3322":"language model","3323":"language model","3324":"language model","3325":"language model","3326":"language model","3327":"language model","3328":"language model","3329":"language model","3330":"language model","3331":"language model","3332":"language model","3333":"language model","3334":"language model","3335":"language model","3336":"language model","3337":"language model","3338":"language model","3339":"language model","3340":"language model","3341":"language model","3342":"language model","3343":"language model","3344":"language model","3345":"language model","3346":"language model","3347":"language model","3348":"language model","3349":"language model","3350":"language model","3351":"language model","3352":"language model","3353":"language model","3354":"language model","3355":"language model","3356":"language model","3357":"language model","3358":"language model","3359":"language model","3360":"language model","3361":"language model","3362":"language model","3363":"language model","3364":"artificial intelligence","3365":"artificial intelligence","3366":"artificial intelligence","3367":"artificial intelligence","3368":"artificial intelligence","3369":"artificial intelligence","3370":"artificial intelligence","3371":"artificial intelligence","3372":"artificial intelligence","3373":"artificial intelligence","3374":"artificial intelligence","3375":"artificial intelligence","3376":"artificial intelligence","3377":"artificial intelligence","3378":"artificial intelligence","3379":"artificial intelligence","3380":"artificial intelligence","3381":"artificial intelligence","3382":"artificial intelligence","3383":"artificial intelligence","3384":"artificial intelligence","3385":"artificial intelligence","3386":"artificial intelligence","3387":"artificial intelligence","3388":"artificial intelligence","3389":"artificial intelligence","3390":"artificial intelligence","3391":"artificial intelligence","3392":"artificial intelligence","3393":"artificial intelligence","3394":"artificial intelligence","3395":"artificial intelligence","3396":"artificial intelligence","3397":"artificial intelligence","3398":"artificial intelligence","3399":"artificial intelligence","3400":"artificial intelligence","3401":"artificial intelligence","3402":"artificial intelligence","3403":"artificial intelligence","3404":"artificial intelligence","3405":"artificial intelligence","3406":"artificial intelligence","3407":"artificial intelligence","3408":"artificial intelligence","3409":"artificial intelligence","3410":"artificial intelligence","3411":"artificial intelligence","3412":"artificial intelligence","3413":"artificial intelligence","3414":"artificial intelligence","3415":"artificial intelligence","3416":"artificial intelligence","3417":"artificial intelligence","3418":"artificial intelligence","3419":"artificial intelligence","3420":"artificial intelligence","3421":"artificial intelligence","3422":"artificial intelligence","3423":"artificial intelligence","3424":"artificial intelligence","3425":"artificial intelligence","3426":"artificial intelligence","3427":"artificial intelligence","3428":"artificial intelligence","3429":"artificial intelligence","3430":"artificial intelligence","3431":"artificial intelligence","3432":"artificial intelligence","3433":"artificial intelligence","3434":"artificial intelligence","3435":"artificial intelligence","3436":"artificial intelligence","3437":"artificial intelligence","3438":"artificial intelligence","3439":"artificial intelligence","3440":"artificial intelligence","3441":"artificial intelligence","3442":"artificial intelligence","3443":"artificial intelligence","3444":"artificial intelligence","3445":"artificial intelligence","3446":"artificial intelligence","3447":"artificial intelligence","3448":"artificial intelligence","3449":"artificial intelligence","3450":"artificial intelligence","3451":"artificial intelligence","3452":"artificial intelligence","3453":"artificial intelligence","3454":"artificial intelligence","3455":"artificial intelligence","3456":"artificial intelligence","3457":"artificial intelligence","3458":"artificial intelligence","3459":"artificial intelligence","3460":"artificial intelligence","3461":"artificial intelligence","3462":"artificial intelligence","3463":"artificial intelligence","3464":"artificial intelligence","3465":"artificial intelligence","3466":"artificial intelligence","3467":"artificial intelligence","3468":"artificial intelligence","3469":"artificial intelligence","3470":"artificial intelligence","3471":"artificial intelligence","3472":"artificial intelligence","3473":"artificial intelligence","3474":"artificial intelligence","3475":"artificial intelligence","3476":"artificial intelligence","3477":"artificial intelligence","3478":"artificial intelligence","3479":"artificial intelligence","3480":"artificial intelligence","3481":"artificial intelligence","3482":"artificial intelligence","3483":"artificial intelligence","3484":"artificial intelligence","3485":"artificial intelligence","3486":"artificial intelligence","3487":"artificial intelligence","3488":"artificial intelligence","3489":"artificial intelligence","3490":"artificial intelligence","3491":"artificial intelligence","3492":"artificial intelligence","3493":"artificial intelligence","3494":"artificial intelligence","3495":"artificial intelligence","3496":"artificial intelligence","3497":"artificial intelligence","3498":"artificial intelligence","3499":"artificial intelligence","3500":"artificial intelligence","3501":"artificial intelligence","3502":"artificial intelligence","3503":"artificial intelligence","3504":"artificial intelligence","3505":"artificial intelligence","3506":"artificial intelligence","3507":"artificial intelligence","3508":"artificial intelligence","3509":"artificial intelligence","3510":"artificial intelligence","3511":"artificial intelligence","3512":"artificial intelligence","3513":"artificial intelligence","3514":"artificial intelligence","3515":"artificial intelligence","3516":"artificial intelligence","3517":"artificial intelligence","3518":"artificial intelligence","3519":"artificial intelligence","3520":"artificial intelligence","3521":"artificial intelligence","3522":"artificial intelligence","3523":"artificial intelligence","3524":"artificial intelligence","3525":"artificial intelligence","3526":"artificial intelligence","3527":"artificial intelligence","3528":"artificial intelligence","3529":"artificial intelligence","3530":"artificial intelligence","3531":"artificial intelligence","3532":"artificial intelligence","3533":"artificial intelligence","3534":"artificial intelligence","3535":"artificial intelligence","3536":"artificial intelligence","3537":"artificial intelligence","3538":"artificial intelligence","3539":"artificial intelligence","3540":"artificial intelligence","3541":"artificial intelligence","3542":"artificial intelligence","3543":"artificial intelligence","3544":"artificial intelligence","3545":"artificial intelligence","3546":"artificial intelligence","3547":"artificial intelligence","3548":"artificial intelligence","3549":"artificial intelligence","3550":"artificial intelligence","3551":"artificial intelligence","3552":"artificial intelligence","3553":"artificial intelligence","3554":"artificial intelligence","3555":"artificial intelligence","3556":"artificial intelligence","3557":"artificial intelligence","3558":"artificial intelligence","3559":"artificial intelligence","3560":"artificial intelligence","3561":"artificial intelligence","3562":"artificial intelligence","3563":"artificial intelligence","3564":"artificial intelligence","3565":"artificial intelligence","3566":"artificial intelligence","3567":"artificial intelligence","3568":"artificial intelligence","3569":"artificial intelligence","3570":"artificial intelligence","3571":"artificial intelligence","3572":"artificial intelligence","3573":"artificial intelligence","3574":"artificial intelligence","3575":"artificial intelligence","3576":"artificial intelligence","3577":"artificial intelligence","3578":"artificial intelligence","3579":"artificial intelligence","3580":"artificial intelligence","3581":"artificial intelligence","3582":"artificial intelligence","3583":"artificial intelligence","3584":"artificial intelligence","3585":"artificial intelligence","3586":"artificial intelligence","3587":"artificial intelligence","3588":"artificial intelligence","3589":"artificial intelligence","3590":"artificial intelligence","3591":"artificial intelligence","3592":"artificial intelligence","3593":"artificial intelligence","3594":"artificial intelligence","3595":"artificial intelligence","3596":"artificial intelligence","3597":"artificial intelligence","3598":"artificial intelligence","3599":"artificial intelligence","3600":"artificial intelligence","3601":"artificial intelligence","3602":"artificial intelligence","3603":"artificial intelligence","3604":"artificial intelligence","3605":"artificial intelligence","3606":"artificial intelligence","3607":"artificial intelligence","3608":"artificial intelligence","3609":"artificial intelligence","3610":"artificial intelligence","3611":"artificial intelligence","3612":"artificial intelligence","3613":"artificial intelligence","3614":"artificial intelligence","3615":"artificial intelligence","3616":"artificial intelligence","3617":"artificial intelligence","3618":"artificial intelligence","3619":"artificial intelligence","3620":"artificial intelligence","3621":"artificial intelligence","3622":"artificial intelligence","3623":"artificial intelligence","3624":"artificial intelligence","3625":"artificial intelligence","3626":"artificial intelligence","3627":"artificial intelligence","3628":"artificial intelligence","3629":"artificial intelligence","3630":"artificial intelligence","3631":"artificial intelligence","3632":"artificial intelligence","3633":"artificial intelligence","3634":"artificial intelligence","3635":"artificial intelligence","3636":"artificial intelligence","3637":"artificial intelligence","3638":"artificial intelligence","3639":"artificial intelligence","3640":"artificial intelligence","3641":"artificial intelligence","3642":"artificial intelligence","3643":"artificial intelligence","3644":"artificial intelligence","3645":"artificial intelligence","3646":"artificial intelligence","3647":"artificial intelligence","3648":"artificial intelligence","3649":"artificial intelligence","3650":"artificial intelligence","3651":"artificial intelligence","3652":"artificial intelligence","3653":"artificial intelligence","3654":"artificial intelligence","3655":"artificial intelligence","3656":"artificial intelligence","3657":"artificial intelligence","3658":"artificial intelligence","3659":"artificial intelligence","3660":"artificial intelligence","3661":"artificial intelligence","3662":"artificial intelligence","3663":"artificial intelligence","3664":"artificial intelligence","3665":"artificial intelligence","3666":"artificial intelligence","3667":"artificial intelligence","3668":"artificial intelligence","3669":"artificial intelligence","3670":"artificial intelligence","3671":"artificial intelligence","3672":"artificial intelligence","3673":"artificial intelligence","3674":"artificial intelligence","3675":"artificial intelligence","3676":"artificial intelligence","3677":"artificial intelligence","3678":"artificial intelligence","3679":"artificial intelligence","3680":"artificial intelligence","3681":"artificial intelligence","3682":"artificial intelligence","3683":"artificial intelligence","3684":"artificial intelligence","3685":"artificial intelligence","3686":"artificial intelligence","3687":"artificial intelligence","3688":"artificial intelligence","3689":"artificial intelligence","3690":"artificial intelligence","3691":"artificial intelligence","3692":"artificial intelligence","3693":"artificial intelligence","3694":"artificial intelligence","3695":"artificial intelligence","3696":"artificial intelligence","3697":"artificial intelligence","3698":"artificial intelligence","3699":"artificial intelligence","3700":"artificial intelligence","3701":"artificial intelligence","3702":"artificial intelligence","3703":"artificial intelligence","3704":"artificial intelligence","3705":"artificial intelligence","3706":"artificial intelligence","3707":"artificial intelligence","3708":"artificial intelligence","3709":"artificial intelligence","3710":"artificial intelligence","3711":"artificial intelligence","3712":"artificial intelligence","3713":"artificial intelligence","3714":"artificial intelligence","3715":"artificial intelligence","3716":"artificial intelligence","3717":"artificial intelligence","3718":"artificial intelligence","3719":"artificial intelligence","3720":"artificial intelligence","3721":"artificial intelligence","3722":"artificial intelligence","3723":"artificial intelligence","3724":"artificial intelligence","3725":"artificial intelligence","3726":"artificial intelligence","3727":"artificial intelligence","3728":"artificial intelligence","3729":"artificial intelligence","3730":"artificial intelligence","3731":"artificial intelligence","3732":"artificial intelligence","3733":"artificial intelligence","3734":"artificial intelligence","3735":"artificial intelligence","3736":"artificial intelligence","3737":"artificial intelligence","3738":"artificial intelligence","3739":"artificial intelligence","3740":"artificial intelligence","3741":"artificial intelligence","3742":"artificial intelligence","3743":"artificial intelligence","3744":"artificial intelligence","3745":"artificial intelligence","3746":"artificial intelligence","3747":"artificial intelligence","3748":"artificial intelligence","3749":"artificial intelligence","3750":"artificial intelligence","3751":"artificial intelligence","3752":"artificial intelligence","3753":"artificial intelligence","3754":"artificial intelligence","3755":"artificial intelligence","3756":"artificial intelligence","3757":"artificial intelligence","3758":"artificial intelligence","3759":"artificial intelligence","3760":"artificial intelligence","3761":"artificial intelligence","3762":"artificial intelligence","3763":"artificial intelligence","3764":"artificial intelligence","3765":"artificial intelligence","3766":"artificial intelligence","3767":"artificial intelligence","3768":"artificial intelligence","3769":"artificial intelligence","3770":"artificial intelligence","3771":"artificial intelligence","3772":"artificial intelligence","3773":"artificial intelligence","3774":"artificial intelligence","3775":"artificial intelligence","3776":"artificial intelligence","3777":"artificial intelligence","3778":"artificial intelligence","3779":"artificial intelligence","3780":"artificial intelligence","3781":"artificial intelligence","3782":"artificial intelligence","3783":"artificial intelligence","3784":"artificial intelligence","3785":"artificial intelligence","3786":"artificial intelligence","3787":"artificial intelligence","3788":"artificial intelligence","3789":"artificial intelligence","3790":"artificial intelligence","3791":"artificial intelligence","3792":"artificial intelligence","3793":"artificial intelligence","3794":"artificial intelligence","3795":"artificial intelligence","3796":"artificial intelligence","3797":"artificial intelligence","3798":"artificial intelligence","3799":"artificial intelligence","3800":"artificial intelligence","3801":"artificial intelligence","3802":"artificial intelligence","3803":"artificial intelligence","3804":"artificial intelligence","3805":"artificial intelligence","3806":"artificial intelligence","3807":"artificial intelligence","3808":"artificial intelligence","3809":"artificial intelligence","3810":"artificial intelligence","3811":"artificial intelligence","3812":"artificial intelligence","3813":"artificial intelligence","3814":"artificial intelligence","3815":"artificial intelligence","3816":"artificial intelligence","3817":"artificial intelligence","3818":"artificial intelligence","3819":"artificial intelligence","3820":"artificial intelligence","3821":"artificial intelligence","3822":"artificial intelligence","3823":"artificial intelligence","3824":"artificial intelligence","3825":"artificial intelligence","3826":"artificial intelligence","3827":"artificial intelligence","3828":"artificial intelligence","3829":"artificial intelligence","3830":"artificial intelligence","3831":"artificial intelligence","3832":"artificial intelligence","3833":"artificial intelligence","3834":"artificial intelligence","3835":"artificial intelligence","3836":"artificial intelligence","3837":"artificial intelligence","3838":"artificial intelligence","3839":"ai","3840":"ai","3841":"ai","3842":"ai","3843":"ai","3844":"ai","3845":"ai","3846":"ai","3847":"ai","3848":"ai","3849":"ai","3850":"ai","3851":"ai","3852":"ai","3853":"ai","3854":"ai","3855":"ai","3856":"ai","3857":"ai","3858":"ai","3859":"ai","3860":"ai","3861":"ai","3862":"ai","3863":"ai","3864":"ai","3865":"ai","3866":"ai","3867":"ai","3868":"ai","3869":"ai","3870":"ai","3871":"ai","3872":"ai","3873":"ai","3874":"ai","3875":"ai","3876":"ai","3877":"ai","3878":"ai","3879":"ai","3880":"ai","3881":"ai","3882":"ai","3883":"ai","3884":"ai","3885":"ai","3886":"ai","3887":"ai","3888":"ai","3889":"ai","3890":"ai","3891":"ai","3892":"ai","3893":"ai","3894":"ai","3895":"ai","3896":"ai","3897":"ai","3898":"ai","3899":"ai","3900":"ai","3901":"ai","3902":"ai","3903":"ai","3904":"ai","3905":"ai","3906":"ai","3907":"ai","3908":"ai","3909":"ai","3910":"ai","3911":"ai","3912":"ai","3913":"ai","3914":"ai","3915":"ai","3916":"ai","3917":"ai","3918":"ai","3919":"ai","3920":"ai","3921":"ai","3922":"ai","3923":"ai","3924":"ai","3925":"ai","3926":"ai","3927":"ai","3928":"ai","3929":"ai","3930":"ai","3931":"ai","3932":"ai","3933":"ai","3934":"ai","3935":"ai","3936":"ai","3937":"ai","3938":"ai","3939":"ai","3940":"ai","3941":"ai","3942":"ai","3943":"ai","3944":"ai","3945":"ai","3946":"ai","3947":"ai","3948":"ai","3949":"ai","3950":"ai","3951":"ai","3952":"ai","3953":"ai","3954":"ai","3955":"ai","3956":"ai","3957":"ai","3958":"ai","3959":"ai","3960":"ai","3961":"ai","3962":"ai","3963":"ai","3964":"ai","3965":"ai","3966":"ai","3967":"ai","3968":"ai","3969":"ai","3970":"ai","3971":"ai","3972":"ai","3973":"ai","3974":"ai","3975":"ai","3976":"ai","3977":"ai","3978":"ai","3979":"ai","3980":"ai","3981":"ai","3982":"ai","3983":"ai","3984":"ai","3985":"ai","3986":"ai","3987":"ai","3988":"ai","3989":"ai","3990":"ai","3991":"ai","3992":"ai","3993":"ai","3994":"ai","3995":"ai","3996":"ai","3997":"ai","3998":"ai","3999":"ai","4000":"ai","4001":"ai","4002":"ai","4003":"ai","4004":"ai","4005":"ai","4006":"ai","4007":"ai","4008":"ai","4009":"ai","4010":"ai","4011":"ai","4012":"ai","4013":"ai","4014":"ai","4015":"ai","4016":"ai","4017":"ai","4018":"ai","4019":"ai","4020":"ai","4021":"ai","4022":"ai","4023":"ai","4024":"ai","4025":"ai","4026":"ai","4027":"ai","4028":"ai","4029":"ai","4030":"ai","4031":"ai","4032":"ai","4033":"ai","4034":"ai","4035":"ai","4036":"ai","4037":"ai","4038":"ai","4039":"ai","4040":"ai","4041":"ai","4042":"ai","4043":"ai","4044":"ai","4045":"ai","4046":"ai","4047":"ai","4048":"ai","4049":"ai","4050":"ai","4051":"ai","4052":"ai","4053":"ai","4054":"ai","4055":"ai","4056":"ai","4057":"ai","4058":"ai","4059":"ai","4060":"ai","4061":"ai","4062":"ai","4063":"ai","4064":"ai","4065":"ai","4066":"ai","4067":"ai","4068":"ai","4069":"ai","4070":"ai","4071":"ai","4072":"ai","4073":"ai","4074":"ai","4075":"ai","4076":"ai","4077":"ai","4078":"ai","4079":"ai","4080":"ai","4081":"ai","4082":"ai","4083":"ai","4084":"ai","4085":"ai","4086":"ai","4087":"ai","4088":"ai","4089":"ai","4090":"ai","4091":"ai","4092":"ai","4093":"ai","4094":"ai","4095":"ai","4096":"ai","4097":"ai","4098":"ai","4099":"ai","4100":"ai","4101":"ai","4102":"ai","4103":"ai","4104":"ai","4105":"ai","4106":"ai","4107":"ai","4108":"ai","4109":"ai","4110":"ai","4111":"ai","4112":"ai","4113":"ai","4114":"ai","4115":"ai","4116":"ai","4117":"ai","4118":"ai","4119":"ai","4120":"ai","4121":"ai","4122":"ai","4123":"ai","4124":"ai","4125":"ai","4126":"ai","4127":"ai","4128":"ai","4129":"ai","4130":"ai","4131":"ai","4132":"ai","4133":"ai","4134":"ai","4135":"ai","4136":"ai","4137":"ai","4138":"ai","4139":"ai","4140":"ai","4141":"ai","4142":"ai","4143":"ai","4144":"ai","4145":"ai","4146":"ai","4147":"ai","4148":"ai","4149":"ai","4150":"ai","4151":"ai","4152":"ai","4153":"ai","4154":"ai","4155":"ai","4156":"ai","4157":"ai","4158":"ai","4159":"ai","4160":"ai","4161":"ai","4162":"ai","4163":"ai","4164":"ai","4165":"ai","4166":"ai","4167":"ai","4168":"ai","4169":"ai","4170":"ai","4171":"ai","4172":"ai","4173":"ai","4174":"ai","4175":"ai","4176":"ai","4177":"ai","4178":"ai","4179":"ai","4180":"ai","4181":"ai","4182":"ai","4183":"ai","4184":"ai","4185":"ai","4186":"ai","4187":"ai","4188":"ai","4189":"ai","4190":"ai","4191":"ai","4192":"ai","4193":"ai","4194":"ai","4195":"ai","4196":"ai","4197":"ai","4198":"ai","4199":"ai","4200":"ai","4201":"ai","4202":"ai","4203":"ai","4204":"ai","4205":"ai","4206":"ai","4207":"ai","4208":"ai","4209":"ai","4210":"ai","4211":"ai","4212":"ai","4213":"ai","4214":"ai","4215":"ai","4216":"ai","4217":"ai","4218":"ai","4219":"ai","4220":"ai","4221":"ai","4222":"ai","4223":"ai","4224":"ai","4225":"ai","4226":"ai","4227":"ai","4228":"ai","4229":"ai","4230":"ai","4231":"ai","4232":"ai","4233":"ai","4234":"ai","4235":"ai","4236":"ai","4237":"ai","4238":"ai","4239":"ai","4240":"ai","4241":"ai","4242":"ai","4243":"ai","4244":"ai","4245":"ai","4246":"ai","4247":"ai","4248":"ai","4249":"ai","4250":"ai","4251":"ai","4252":"ai","4253":"ai","4254":"ai","4255":"ai","4256":"ai","4257":"ai","4258":"ai","4259":"ai","4260":"ai","4261":"ai","4262":"ai","4263":"ai","4264":"ai","4265":"ai","4266":"ai","4267":"ai","4268":"ai","4269":"ai","4270":"ai","4271":"ai","4272":"large language model","4273":"large language model","4274":"large language model","4275":"large language model","4276":"large language model","4277":"large language model","4278":"large language model","4279":"large language model","4280":"large language model","4281":"large language model","4282":"large language model","4283":"large language model","4284":"large language model","4285":"large language model","4286":"large language model","4287":"large language model","4288":"large language model","4289":"large language model","4290":"large language model","4291":"large language model","4292":"large language model","4293":"large language model","4294":"large language model","4295":"large language model","4296":"large language model","4297":"large language model","4298":"large language model","4299":"large language model","4300":"large language model","4301":"large language model","4302":"large language model","4303":"large language model","4304":"large language model","4305":"large language model","4306":"large language model","4307":"large language model","4308":"large language model","4309":"large language model","4310":"large language model","4311":"large language model","4312":"large language model","4313":"large language model","4314":"large language model","4315":"large language model","4316":"large language model","4317":"large language model","4318":"large language model","4319":"large language model","4320":"large language model","4321":"large language model","4322":"large language model","4323":"large language model","4324":"large language model","4325":"large language model","4326":"large language model","4327":"large language model","4328":"large language model","4329":"large language model","4330":"large language model","4331":"large language model","4332":"large language model","4333":"large language model","4334":"large language model","4335":"large language model","4336":"large language model","4337":"large language model","4338":"large language model","4339":"large language model","4340":"large language model","4341":"large language model","4342":"large language model","4343":"large language model","4344":"large language model","4345":"large language model","4346":"large language model","4347":"large language model","4348":"large language model","4349":"large language model","4350":"large language model","4351":"large language model","4352":"large language model","4353":"large language model","4354":"large language model","4355":"large language model","4356":"large language model","4357":"large language model","4358":"large language model","4359":"large language model","4360":"large language model","4361":"large language model","4362":"large language model","4363":"large language model","4364":"large language model","4365":"large language model","4366":"large language model","4367":"large language model","4368":"large language model","4369":"large language model","4370":"large language model","4371":"large language model","4372":"large language model","4373":"large language model","4374":"large language model","4375":"large language model","4376":"large language model","4377":"large language model","4378":"large language model","4379":"large language model","4380":"large language model","4381":"large language model","4382":"large language model","4383":"large language model","4384":"large language model","4385":"large language model","4386":"large language model","4387":"large language model","4388":"large language model","4389":"large language model","4390":"large language model","4391":"large language model","4392":"large language model","4393":"large language model","4394":"large language model","4395":"large language model","4396":"large language model","4397":"large language model","4398":"large language model","4399":"large language model","4400":"large language model","4401":"large language model","4402":"large language model","4403":"large language model","4404":"large language model","4405":"large language model","4406":"large language model","4407":"large language model","4408":"large language model","4409":"large language model","4410":"large language model","4411":"large language model","4412":"large language model","4413":"large language model","4414":"large language model","4415":"large language model","4416":"large language model","4417":"large language model","4418":"large language model","4419":"large language model","4420":"large language model","4421":"large language model","4422":"large language model","4423":"large language model","4424":"large language model","4425":"large language model","4426":"large language model","4427":"large language model","4428":"large language model","4429":"large language model","4430":"large language model","4431":"large language model","4432":"large language model","4433":"large language model","4434":"large language model","4435":"large language model","4436":"large language model","4437":"large language model","4438":"large language model","4439":"large language model","4440":"large language model","4441":"large language model","4442":"large language model","4443":"large language model","4444":"large language model","4445":"large language model","4446":"large language model","4447":"large language model","4448":"large language model","4449":"large language model","4450":"large language model","4451":"large language model","4452":"large language model","4453":"large language model","4454":"large language model","4455":"large language model","4456":"large language model","4457":"large language model","4458":"large language model","4459":"large language model","4460":"large language model","4461":"large language model","4462":"large language model","4463":"large language model","4464":"large language model","4465":"large language model","4466":"large language model","4467":"large language model","4468":"large language model","4469":"large language model","4470":"large language model","4471":"large language model","4472":"large language model","4473":"large language model","4474":"large language model","4475":"large language model","4476":"large language model","4477":"large language model","4478":"large language model","4479":"large language model","4480":"large language model","4481":"large language model","4482":"large language model","4483":"large language model","4484":"large language model","4485":"large language model","4486":"large language model","4487":"large language model","4488":"large language model","4489":"large language model","4490":"large language model","4491":"large language model","4492":"large language model","4493":"large language model","4494":"large language model","4495":"large language model","4496":"large language model","4497":"large language model","4498":"large language model","4499":"large language model","4500":"large language model","4501":"large language model","4502":"large language model","4503":"large language model","4504":"large language model","4505":"large language model","4506":"large language model","4507":"large language model","4508":"large language model","4509":"large language model","4510":"large language model","4511":"large language model","4512":"large language model","4513":"large language model","4514":"large language model","4515":"large language model","4516":"large language model","4517":"large language model","4518":"large language model","4519":"large language model","4520":"large language model","4521":"large language model","4522":"large language model","4523":"large language model","4524":"large language model","4525":"large language model","4526":"large language model","4527":"large language model","4528":"large language model","4529":"large language model","4530":"large language model","4531":"large language model","4532":"large language model","4533":"large language model","4534":"large language model","4535":"large language model","4536":"large language model","4537":"large language model","4538":"large language model","4539":"large language model","4540":"large language model","4541":"large language model","4542":"large language model","4543":"large language model","4544":"large language model","4545":"large language model","4546":"large language model","4547":"large language model","4548":"large language model","4549":"large language model","4550":"large language model","4551":"large language model","4552":"large language model","4553":"large language model","4554":"large language model","4555":"large language model","4556":"large language model","4557":"large language model","4558":"large language model","4559":"large language model","4560":"large language model","4561":"large language model","4562":"large language model","4563":"large language model","4564":"large language model","4565":"large language model","4566":"large language model","4567":"large language model","4568":"large language model","4569":"large language model","4570":"large language model","4571":"large language model","4572":"large language model","4573":"large language model","4574":"large language model","4575":"large language model","4576":"large language model","4577":"large language model","4578":"large language model","4579":"large language model","4580":"large language model","4581":"large language model","4582":"large language model","4583":"large language model","4584":"large language model","4585":"large language model","4586":"large language model","4587":"large language model","4588":"large language model","4589":"large language model","4590":"large language model","4591":"large language model","4592":"large language model","4593":"large language model","4594":"large language model","4595":"large language model","4596":"large language model","4597":"large language model","4598":"large language model","4599":"large language model","4600":"large language model","4601":"large language model","4602":"large language model","4603":"large language model","4604":"large language model","4605":"large language model","4606":"large language model","4607":"large language model","4608":"large language model","4609":"large language model","4610":"large language model","4611":"large language model","4612":"large language model","4613":"large language model","4614":"large language model","4615":"large language model","4616":"large language model","4617":"large language model","4618":"large language model","4619":"large language model","4620":"large language model","4621":"large language model","4622":"large language model","4623":"large language model","4624":"large language model","4625":"large language model","4626":"large language model","4627":"large language model","4628":"large language model","4629":"large language model","4630":"large language model","4631":"large language model","4632":"large language model","4633":"large language model","4634":"large language model","4635":"large language model","4636":"large language model","4637":"large language model","4638":"large language model","4639":"large language model","4640":"large language model","4641":"large language model","4642":"large language model","4643":"large language model","4644":"large language model","4645":"large language model","4646":"large language model","4647":"large language model","4648":"large language model","4649":"large language model","4650":"large language model","4651":"large language model","4652":"large language model","4653":"large language model","4654":"large language model","4655":"large language model","4656":"large language model","4657":"large language model","4658":"large language model","4659":"large language model","4660":"large language model","4661":"large language model","4662":"large language model","4663":"large language model","4664":"large language model","4665":"large language model","4666":"large language model","4667":"large language model","4668":"large language model","4669":"large language model","4670":"large language model","4671":"large language model","4672":"large language model","4673":"large language model","4674":"large language model","4675":"large language model","4676":"large language model","4677":"large language model","4678":"large language model","4679":"large language model","4680":"large language model","4681":"large language model","4682":"large language model","4683":"large language model","4684":"large language model","4685":"large language model","4686":"large language model","4687":"large language model","4688":"large language model","4689":"large language model","4690":"large language model","4691":"large language model","4692":"large language model","4693":"large language model","4694":"large language model","4695":"large language model","4696":"large language model","4697":"large language model","4698":"large language model","4699":"large language model","4700":"large language model","4701":"large language model","4702":"large language model","4703":"large language model","4704":"large language model","4705":"large language model","4706":"large language model","4707":"large language model","4708":"large language model","4709":"large language model","4710":"large language model","4711":"large language model","4712":"large language model","4713":"large language model","4714":"large language model","4715":"large language model","4716":"large language model","4717":"large language model","4718":"large language model","4719":"large language model","4720":"large language model","4721":"large language model","4722":"large language model","4723":"large language model","4724":"large language model","4725":"large language model","4726":"large language model","4727":"large language model","4728":"large language model","4729":"large language model","4730":"large language model","4731":"large language model","4732":"large language model","4733":"large language model","4734":"large language model","4735":"large language model","4736":"large language model","4737":"large language model","4738":"large language model","4739":"large language model","4740":"large language model","4741":"large language model"},"title":{"0":"Artificial Intelligence in Agriculture","1":"Artificial Intelligence Benefit and Risks","2":"Artificial Intelligence in Power Station","3":"Artificial Intelligence Based Training and Placement Management","4":"The Significance of Artificial Intelligence in Digital Marketing","5":"Explanation in artificial intelligence: Insights from the social sciences","6":"A framework for Integrating Artificial Intelligence and Simulation","7":"Artificial Intelligence Role in Modern Science Aims, Merits, Risks and Its Applications","8":"Artificial Intelligence Catalyzes a Revolution for 21st Century Human Creativity and Modern Art","9":"Artificial Intelligence Empowering the Future of Digital Transformation","10":"Analysis and Comparative Study of the Development of Technology with Artificial Intelligence in India","11":"Impact of Artificial Intelligence in the Pharmaceutical World A Review","12":"Artificial Intelligence in Gaming","13":"Artificial Intelligence and Human Computer Interaction","14":"Artificial Intelligence Basics and Terminology","15":"Video Steganography using Discrete Wavelet Transform and Artificial Intelligence","16":"Artificial Intelligence and Machine Learning's Impact on Market Design","17":"Reconciling deep learning with symbolic artificial intelligence: representing objects and relations","18":"Role of Ethics in Artificial Intelligence","19":"Artificial Intelligence in Autonomous Vehicles","20":"Explainable Artificial Intelligence in education","21":"Artificial intelligence in dentistry and its future","22":"A law on robotics and artificial intelligence in the EU?","23":"Artificial intelligence for HR: use AI to support and develop a successful workforce","24":"Artificial Intelligence and its Role in Industry","25":"Artificial Intelligence in Cardiology","26":"Application of Artificial Intelligence in Indian Banking Opportunities and Challenges","27":"A Study on the Applications and Impact of Artificial Intelligence in E Commerce Industry","28":"Intelligent Drone based Personal Assistant using Artificial Intelligence (AI)","29":"Artificial Intelligence in Smart Grid","30":"Transformational Planning for Artificial Intelligence","31":"Artificial Intelligence Assisted Weather Based Plant Disease Forecasting System","32":"Impact of Artificial Intelligence on E marketing","33":"Neuroscience-Inspired Artificial Intelligence","34":"Innateness, AlphaZero, and Artificial Intelligence","35":"Artificial Intelligence in Accordance to Human Rights","36":"Revolutionizing Education How Artificial Intelligence is transforming the Learning Landscape","37":"Techniques to Apply Artificial Intelligence in Power Plants","38":"Law and Economic Analysis of the Ownership of Artificial Intelligence Creation","39":"Application of artificial intelligence (AI) in food industry","40":"Stride for Detecting Suicidal Thoughts Using Artificial Intelligence","41":"The Exploited Labor Behind Artificial Intelligence","42":"Advances in artificial intelligence 27th Canadian Conference on Artificial Intelligence, Canadian AI 2013, Montr\u00e9al, QC, Canada, May 6 - 9, 2014 ; proceedings","43":"Role of Artificial Intelligence in Transforming Human Resource Management","44":"Artificial Intelligence for Pediatric Ophthalmology","45":"TIDDY An Artificial Intelligence Based Floor Cleaning Robot","46":"A Universal Modular ACTOR Formalism for Artificial Intelligence","47":"Review on Solar Power System with Artificial Intelligence","48":"Artificial intelligence: a game changer for the world of work","49":"Artificial intelligence approach in crude distillation unit operation","50":"Impact of Artificial Intelligence on Pharm Technology A Review","51":"Artificial Intelligence Based Stock Market Prediction Model using Technical Indicators","52":"Artificial Intelligence: A Modern Approach (Prentice Hall Series in Artificial Intelligence)","53":"The Role of Artificial Intelligence in Revolutionizing Healthcare A Comprehensive Review","54":"Artificial intelligence: An empirical science","55":"Cognitive Technical Systems -- What Is the Role of Artificial Intelligence?","56":"Artificial Intelligence A Study of Automation, and Its Impact on Data Science","57":"An Artificial Intelligence Approach to Ultra High Frequency Path Loss Modelling of the Suburban Areas of Abuja, Nigeria","58":"Review on Electricity Consumption Forecasting in Buildings using Artificial Intelligence","59":"Sparks of Artificial General Intelligence: Early experiments with GPT-4","60":"Advent of Artificial Intelligence and its Impact on Top Leading Commercial Banks in India - Case Study","61":"The Role of Artificial Intelligence Applications in Managing the Employee Performance Evaluation An Iraqi Case Study","62":"Artificial Intelligence Is Misreading Human Emotion","63":"Probabilistic machine learning and artificial intelligence","64":"Artificial Intelligence in Geography","65":"Bayesian Analysis to the experiences of corruption through Artificial Intelligence","66":"Emerging Roles of Artificial Intelligence in ecommerce","67":"A Survey of artificial immune applications","68":"Artificial intelligence in education: The three paradigms","69":"Knowledge Representation in Sanskrit and Artificial Intelligence","70":"Artificial Intelligence","71":"Educational applications of artificial intelligence in simulation-based learning: A systematic mapping review","72":"Artificial Intelligence: An Engineering Approach","73":"The concept of artificial intelligence in aerospace","74":"A Study on Google is an Artificial Encyclopedia Affecting Human Intelligence - An Empirical Study","75":"Role of Artificial Intelligence in Supply Chain Management","76":"Artificial Intelligence and Machine Learning In Business","77":"Democracy and Technology in the Age of Artificial Intelligence","78":"An appraisal of ethical issues and the effect of artificial intelligence on the cryptocurrency market","79":"Context in Artificial Intelligence: II. Key Elements of Contexts","80":"Artificial Intelligence: A Modern Approach","81":"Explaining Artificial Intelligence with Tailored Interactive Visualisations","82":"Can Artificial Intelligence Control Antimicrobial Resistance?","83":"Artificial intelligence in perspective","84":"The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence","85":"Context in Artificial Intelligence: I. A Survey of the Literature","86":"The importance of context and colloquial register in PLN translators based on Artificial Intelligence","87":"Artificial Intelligence in Medicine","88":"White Paper on Artificial Intelligence: a European approach to excellence and trust","89":"Artificial intelligence","90":"Artificial Intelligence: Genetic Programming","91":"Requirements for a Classification Expert System Shell and their Realization in MED2","92":"Artificial intelligence for fault diagnosis of rotating machinery: A review","93":"Logic and Artificial Intelligence","94":"Design and Analysis of Artificial Intelligence Based Approach for Control of Wind Turbine","95":"AI 2011 Advances in artificial intelligence : 24th Australasian Joint Conference, Perth, Australia, December 5-8, 2011, proceedings","96":"Penetration Testing Artificial Intelligence.","97":"A Review on the Impact of Artificial Intelligence and Internet of Things in the Transformation of E Business Sector","98":"Coordination Techniques for Distributed Artificial Intelligence","99":"Artificial Intelligence: Foundations of Computational Agents","100":"Artificial Intelligence in Perspective: a retrospective on fifty volumes of Artificial Intelligence","101":"A survey of constraint based scheduling systems using an artificial intelligence approach","102":"Ethics in artificial intelligence: Issues and guidelines for developing acceptable AI systems","103":"The Impact of Information and Communication Technologies on the Performance of Human Resources Management and the Mediating Role of Artificial Intelligence","104":"Artificial intelligence for microscopy: What you should know","105":"Artificial Intelligence in Education and Ethics","106":"Three Years of Using Robots in an Artificial Intelligence Course: Lessons Learned","107":"Universal Artificial Intelligence","108":"Earth Observation and Artificial Intelligence: Understanding emerging ethical issues and opportunities","109":"The Malicious Use of Artificial Intelligence : Forecasting, Prevention, and Mitigation","110":"Learning multilingual named entity recognition from Wikipedia","111":"Neuro-symbolic artificial intelligence","112":"Multifield Cosmology with Artificial Intelligence","113":"Artificial Intelligence: an empirical science","114":"Artificial Intelligence and Software Engineering","115":"Artificial Intelligence --- A Personal View","116":"Artificial intelligence and empirical science","117":"Artificial intelligence meets natural stupidity","118":"Artificial Intelligence Programming","119":"Artificial Intelligence: A Modern Approach (2nd Edition)","120":"Temporal Qualification in Artificial Intelligence.","121":"Using Artificial Intelligence to Augment Science Prioritization for\r\n  Astro2020","122":"Combining Artificial Intelligence and Databases for Data Integration","123":"Artificial Intelligence Goes Mobile.","124":"The trainer, the verifier, the imitator: Three ways in which human platform workers support artificial intelligence","125":"Principles of Artificial Intelligence","126":"Influences on the Spanish language of English translations based on Artificial Intelligence","127":"Artificial Intelligence and Literary Creativity: Inside the Mind of Brutus, A Storytelling Machine","128":"Putting intentions into cell biochemistry: an artificial intelligence perspective","129":"Will Artificial Intelligence Eat the Law? The Rise of Hybrid Social-Ordering Systems","130":"Assessment in the age of artificial intelligence","131":"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,\r\n  Opportunities and Challenges toward Responsible AI","132":"Neuro-symbolic approaches in artificial intelligence","133":"Advantages of Using Artificial Intelligence in Technological Enterprises and Organization Management","134":"Artificial Intelligence and its Application in Information Security Management","135":"Artificial Intelligence: What Everyone Needs to Know","136":"How artificial intelligence will impact K-12 teachers","137":"Expressive AI: Games and Artificial Intelligence.","138":"Massively Multi-Author Hybrid Artificial Intelligence.","139":"Assessment reform for the age of artificial intelligence","140":"Artificial intelligence: Perspectives and Predictions.","141":"Artificial Intelligence in Manufacturing: Preface.","142":"Artificial intelligence and cognitive science.","143":"Prolog Programming for Artificial Intelligence","144":"Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence","145":"Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI","146":"Artificial Intelligence Handling Through Teaching and Learning Process and It\u2019s Effect on Science-Based Economy","147":"Systematic literature review on opportunities, challenges, and future research recommendations of artificial intelligence in education","148":"Unhype Artificial 'Intelligence'! A proposal to replace the deceiving terminology of AI","149":"YouTube - Polyworld: Using Evolution to Design\n                 Artificial Intelligence","150":"Diagnosis of hyperglycemia using Artificial Neural Networks","151":"Legal Evidence Scholarship Meets Artificial Intelligence.","152":"Using Artificial Intelligence for Intrusion Detection.","153":"Machines that think : the future of artificial intelligence","154":"Mapping change in scientific specialties: a scientometric reconstruction of the development of artificial intelligence","155":"Artificial intelligence: Human focus on technology.","156":"How Much to Trust Artificial Intelligence?","157":"Applying Artificial Intelligence to Virtual Reality: Intelligent Virtual Environments.","158":"How the artificial intelligence tool iPGK-PseAAC is working in\r\npredicting lysine phosphoglycerylation sites in proteins","159":"Probabilistic graphical models in artificial intelligence","160":"The Evolutionary Emergence route to Artificial\n                 Intelligence","161":"Artificial Intelligence in Breast Cancer: Imaging and Diagnosis","162":"Artificial Intelligence in Education: A Review","163":"Blockchain Framework for Artificial Intelligence Computation","164":"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp","165":"A Study of Deep Learning Applications","166":"Swarm Intelligence: From Natural to Artificial Systems","167":"An explainable artificial intelligence system for small-unit tactical\n\tbehavior","168":"Human-Centered Artificial Intelligence: Three Fresh Ideas","169":"Artificial Intelligence: Theory and Practice","170":"Impact of Artificial Intelligence on the Automation of Digital Health System","171":"Social Situatedness of Natural and Artificial Intelligence: Vygotsky and Beyond","172":"Resisting AI: An Anti-fascist Approach to Artificial Intelligence","173":"Synopsis of the PhD Thesis - Network Computations in Artificial Intelligence","174":"Potential and Limits of Artificial Intelligence in Medicine","175":"Rigor mortis: a response to Nilsson's ``Logic and Artificial Intelligence''","176":"The cognitive computer : on language, learning, and artificial intelligence","177":"Artificial Intelligence: Understanding Diseases that People Cannot Understand?","178":"Effects of Artificial Intelligence, Big Data Analytics, and Business Intelligence on Digital Transformation in UAE Telecommunication Firms","179":"Application of Artificial Intelligence Techniques to Simulation","180":"Artificial Intelligence for Cultural Heritage and Digital Libraries.","181":"The future of artificial intelligence: learning from experience.","182":"Artificial Intelligence Applications in Cancer Diagnosis","183":"Artificial Intelligence Solution to Electricity Price Forecasting Problem.","184":"Artificial Socialization? How Artificial Intelligence Applications Can Shape A New Era of Employee Onboarding Practices","185":"Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)","186":"Artificial Intelligence as a Positive and Negative Factor in Global Risk","187":"Artificial Intelligence Safety and Cybersecurity: a Timeline of AI\r\n  Failures","188":"Search in artificial intelligence.","189":"The Quest for Artificial Intelligence: A History of Ideas and Achievements","190":"SMOTE: Synthetic Minority Over-sampling Technique","191":"The Novamente Artificial Intelligence Engine.","192":"Artificial Intelligence And Formalisms For Legal Evidence: An Introduction.","193":"Building Tools for software engineering with artificial intelligence techniques.","194":"Some semiotic reflections on the future of artificial intelligence.","195":"Wasserstein GAN-Based Small-Sample Augmentation for New-Generation Artificial Intelligence: A Case Study of Cancer-Staging Data in Biology","196":"Collaboratively built semi-structured content and Artificial Intelligence: The story so far","197":"Applications of Distributed Artificial Intelligence in Industry","198":"Legal Risks and Preventive Measures in ChatGPT Applications","199":"Guest Editorial: an Artificial Intelligence Miscellanea, Remembering Marco Somalvico.","200":"Generality in Artificial Intelligence","201":"In-Memory Computing: Towards Energy-Efficient Artificial Intelligence.","202":"An Artificial Intelligence Bibliography.","203":"An Overview of Distributed Artificial Intelligence","204":"Why Artificial Intelligence is the Future of Growth","205":"Artificial Intelligence and Education.","206":"Principles of artificial intelligence","207":"Impacts of Artificial Intelligence.","208":"We Need to Talk, AI : A Comic Essay on Artificial Intelligence","209":"Steps Toward Robust Artificial Intelligence","210":"Artificial intelligence, work, power imbalance and democracy \u2013 why co-determination is essential","211":"Statistical Relational Artificial Intelligence: Logic, Probability, and Computation","212":"Interpretable and Reliable Artificial Intelligence Systems for Brain Diseases.","213":"Constitutional democracy and technology in the age of artificial intelligence","214":"Special issue on artificial intelligence: Future, Impacts, Challenges - Part 3.","215":"Markov Logic: An Interface Layer for Artificial Intelligence","216":"Unnatural Selection: Seeing Human Intelligence in Artificial Creations.","217":"Context in Artificial Intelligence: I. A survey of the literature","218":"Artificial intelligence and education: A critical view through the lens of human rights, democracy and the rule of law","219":"The wrong kind of AI? Artificial intelligence and the future of labor demand","220":"From Data Entry to Intelligence: Artificial Intelligence's Impact on Financial System Workflows","221":"The Natural Way to Artificial Intelligence.","222":"Logics for artificial intelligence.","223":"Exploring the Benefits and Future of Artificial Intelligence","224":"Embodied Artificial Intelligence: Trends and Challenges.","225":"Themes in Distributed Artificial Intelligence Research.","226":"Contemporary Approaches to Artificial General Intelligence.","227":"German Standardization Roadmap on Artificial Intelligence","228":"Artificial Intelligence for Information Retrieval.","229":"\"Negotiating the algorithm\u201d: Automation, artificial intelligence and labour protection.","230":"Artificial intelligence in scheduling and instruction selection for digital signal processors.","231":"An artificial intelligence-based design tool for thin film composite materials.","232":"A Study and Analysis of Emotional Intelligence and its Impacts","233":"Artificial Intelligence, Employment, and Income.","234":"How can information extraction ease formalizing treatment processes in clinical practice guidelines?: A method and its evaluation","235":"Artificial Intelligence in Computer Graphics: A Constructionist Approach","236":"Artificial intelligence approach to determination of\n                 flow curve","237":"The Construction Of Judicial Proof: A Challenge For Artificial Intelligence Modelling.","238":"Defining artificial intelligence for librarians","239":"Image Based Facial Recognition","240":"A Co-Word Study of Artificial Intelligence","241":"Travel Plans in Public Transit Networks Using Artificial Intelligence Planning Models.","242":"A system and control theoretic perspective on artificial intelligence planning systems.","243":"Weighing the Milky Way and Andromeda with Artificial Intelligence","244":"What Computers can't do. The Limit of Artificial Intelligence. Revised\r\n\tedition","245":"Artificial Intelligence and Entertainment.","246":"Methods of Using Artificial Intelligence in Insurance Companies and its Promising Directions","247":"Common Sense","248":"Essentials of General Intelligence: The Direct Path to Artificial General Intelligence.","249":"Univariate and multivariate forecasting of hourly solar radiation with artificial intelligence techniques","250":"Logics For Artificial Intelligence.","251":"Speech and Language Processing (2nd Edition) (Prentice Hall Series in Artificial Intelligence)","252":"Artificial Intelligence - A","253":"Skin Cancer Detection using Image Processing in Real Time","254":"Consciousness, intentionality, and intelligence: some foundational issues for artificial intelligence","255":"K-Nearest Neighbours based diagnosis of hyperglycemia","256":"Artificial Intelligence and Robotics.","257":"Offering false security: How the draft artificial intelligence act undermines fundamental workers rights","258":"AI Marketplace - The Ecosystem for Artificial Intelligence in Product Creation.","259":"Finding universal relations in subhalo properties with artificial\r\n  intelligence","260":"Information-Theoretical Aspects of Embodied Artificial Intelligence.","261":"Genetically breeding populations of computer programs\n                 to solve problems in artificial intelligence","262":"Information-theoretical aspects of embodied artificial intelligence","263":"Artificial and crowd intelligence based recommender system framework.","264":"Face Recognition and Increased Reality System for Mobile Devices","265":"The Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence","266":"EHRs Connect Research and Practice: Where Predictive Modeling,\r\n  Artificial Intelligence, and Clinical Decision Support Intersect","267":"SOPHIE: A pragmatic use of artificial intelligence in CAI","268":"Artificial intelligence - a modern myth.","269":"Neural networks in artificial intelligence.","270":"The Advent of Artificial Intelligence.","271":"Machine Learning in Python: Main developments and technology trends in\r\n  data science, machine learning, and artificial intelligence","272":"Impacts of Artificial Intelligence: An Overview.","273":"Artificial Intelligence and Humanistic Informatics.","274":"Proposal to ARPA for Research on Artificial Intelligence at MIT, 1970-1971 (AIM-185)","275":"Artificial Intelligence : A Morden Approach","276":"Artificial Intelligence: A Personal View.","277":"Use of Artificial Intelligence Opportunities for Early Detection of Threats to Information Systems","278":"What is Artificial Intelligence","279":"Automatic text analysis by artificial intelligence","280":"Artificial Intelligence and Intelligent Systems Research in Chile.","281":"Authentic Interactive Reenactment of Cultural Heritage with 3D Virtual Worlds and Artificial Intelligence.","282":"Life 3.0 : being human in the age of artificial intelligence","283":"Autonomous Vehicles and Embedded Artificial Intelligence: The Challenges of Framing Machine Driving Decisions.","284":"Artificial Intelligence Techniques in Real-Time Strategy Games - Architecture and Combat Behavior","285":"Knowledge Processing the Hard Way: Extraction Value from Symbolic Logic for Artificial Intelligence","286":"Enhancing Distance Education through Artificial Intelligence in Teaching English","287":"JOB MATCHING USING ARTIFICIAL INTELLIGENCE","288":"Friendly artificial intelligence --- Wikipedia\\,\\ The Free Encyclopedia","289":"How to do Research At the MIT AI Lab","290":"Computers and Thought: A Practical Introduction to Artificial Intelligence (Explorations in Cognitive Science)","291":"Combibining Artificial Intelligence and Databases for Data Integration.","292":"Learning and Adaptation In Distributed Artificial Intelligence Systems.","293":"Catalogue of artificial intelligence tools, 2nd Edition.","294":"Artificial Intelligence for Artificial Artificial Intelligence.","295":"The Future of Embodied Artificial Intelligence: Machine Consciousness?.","296":"Artificial General Intelligence: Concept, State of the Art, and Future Prospects.","297":"Evolutionary Humanoids for Embodied Artificial\n                 Intelligence","298":"An application of artificial intelligence for\n                 rainfall-runoff modeling","299":"Introduction: Aspects of Artificial General Intelligence.","300":"Artificial Intelligence and Music Education.","301":"Rethinking the entwinement between artificial intelligence and human learning: What capabilities do learners need for a world with AI?","302":"Algorithms and Architectures of Artificial Intelligence","303":"Artificial Intelligence and Unmanned Aerial Vehicles.","304":"Artificial Intelligence Meets Natural Stupidity","305":"Artificial Intelligence: Modern Approach","306":"Evolving Neural Networks for Artificial Intelligence.","307":"Human-artificial-intelligence hybrid learning systems.","308":"A Concise Introduction to Multiagent Systems and Distributed Artificial Intelligence","309":"Changing Philosophy of Teaching Artificial Intelligence.","310":"Artificial Intelligence in Computer-Aided A Diagnosis.","311":"Artificial intelligence - a modern approach: the intelligent agent book.","312":"The Artificial Intelligence Debate - False Starts, Real Foundations","313":"Social and Economic Impacts of Artificial Intelligence.","314":"Press release: Neue Werkzeuge f\u00fcr die Wissensarbeit","315":"Artificial Intelligence and Rubble-Mound Breakwater Stability.","316":"Human Information Interaction, Artificial Intelligence, and Errors.","317":"Using Artificial Intelligence to Assist Psychological Testing.","318":"The Toulmin Argument Model in Artificial Intelligence.","319":"Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence","320":"Logic Programming in Artificial Intelligence Education.","321":"Artificial Intelligence in Covid-19 Diagnosis","322":"Watson (artificial intelligence software) --- Wikipedia\\,\\ The Free Encyclopedia","323":"Tracing of an Object in Video through Mean Shift Protocol","324":"Univariate and Multivariate Forecasting of Hourly Solar Radiation\n\tWith Artificial Intelligence Techniques","325":"Android dreams : the past, present and future of artificial intelligence","326":"Artificial intelligence-based prior art document identification system","327":"Editorial: Approaches and Assumptions of Self-Programming in Achieving Artificial General Intelligence.","328":"Artificial Intelligence: A modern approach","329":"Artificial intelligence as a challenge for law and regulation","330":"Program Search as a Path to Artificial General Intelligence.","331":"Artificial Intelligence for Sustainability. On theoretical limitations, practical potentials and political discourses","332":"Consciousness in Artificial Intelligence: Insights from the Science of\r\n  Consciousness","333":"Hand Written Digit Classification","334":"Artificial intelligence for human computer interaction.","335":"Autonomy and Artificial Intelligence in Aerospace Systems.","336":"Timeline of artificial intelligence --- Wikipedia\\,\\ The Free Encyclopedia","337":"JAIR at Five: Half a Decade of the Journal of Artificial Intelligence Research","338":"Ethics of artificial intelligence --- Wikipedia\\,\\ The Free Encyclopedia","339":"Explainable Artificial Intelligence for Training and Tutoring.","340":"Search, inference and dependencies in artificial intelligence.","341":"Logical Foundations of Artificial Intelligence","342":"Artificial intelligence techniques for sizing photovoltaic systems","343":"Blockchain and Artificial Intelligence: Challenges and Opportunities.","344":"Complex Systems, Artificial Intelligence and Theoretical Psychology.","345":"Artifact Compatibility for Enabling Collaboration in the Artificial Intelligence Ecosystem","346":"A Foundational Architecture for Artificial General Intelligence.","347":"Artificial intelligence - a modern approach, 2nd Edition.","348":"Artificial Intelligence in Clinical Diagnosis","349":"Intelligent tutoring systems survey","350":"Artificial intelligence and labour law","351":"Accelerating progress in Artificial General Intelligence: Choosing a benchmark for natural world interaction.","352":"Magic in Pieces: An Analysis of Magic Trick Construction Using Artificial Intelligence as a Design Aid.","353":"Artificial Intelligence-Based Early Detection Of Acute Kidney Injury After Cardiac Surgery","354":"Expressive probabilistic description logics","355":"Adopting model checking techniques for clinical guidelines verification","356":"Artificial Intelligence and Connectionism: Some Philosophical Implications.","357":"Reactive Distributed Artificial Intelligence: Principles and Applications","358":"\u00c4rtificial Intelligence in Intelligent Transportation Systems\".","359":"Markov logic: An interface layer for artificial intelligence","360":"Fashion AI","361":"Utilizing 6G Technology for Healthcare Monitoring with Machine Learning","362":"Formal techniques in artificial intelligence - a sourcebook.","363":"An Agent-Based Approach towards Automatic Service Composition in Ambient Intelligence","364":"Online Tour Booking using Fuzzy Decision Making Method","365":"Artificial Intelligence and Molecular Biology","366":"OPUS One An Artificial Intelligence - Multi Agent based Intelligent Adaptive Learning Environment (IALE)","367":"Governmental Transparency in the Era of Artificial Intelligence.","368":"Perspectives on artificial intelligence in a business environment.","369":"Logic and Artificial Intelligence for Multi-Agent Systems.","370":"Internet of Things IoT Security Perspective","371":"Artificial Intelligence Within the Bounds of Ontological Reason.","372":"Towards a Code of Ethics for Artificial Intelligence","373":"Adaptation in Natural and Artificial Systems: An Introductory Analysis\r\n\twith Applications to Biology, Control and Artificial Intelligence","374":"An open-source toolkit for mining Wikipedia","375":"Evaluating Entity Linking with Wikipedia","376":"Blockchain and AI Convergence A New Era of Possibilities","377":"Artificial General Intelligence via Finite Covering with Learning.","378":"Bayesian Methods for Artificial Intelligence and Machine Learning.","379":"Adaptive Algorithmic Hybrids for Human-Level Artificial Intelligence.","380":"In defense of PDDL axioms","381":"Evaluations: Autonomy and Artificial Intelligence: A Threat or Savior?","382":"Hampshire College Student Uses J.K. Rowling's\n                 Quidditch as Basis for Artificial Intelligence\n                 Experiment","383":"Learning and inferring transportation routines","384":"Artificial Intelligence Methods in Early Childhood Education.","385":"Adaptation in natural and artificial systems: An introductory analysis\r\n\twith applications to biology, control, and artificial intelligence","386":"WEB BASED INTELLIGENT INVENTORY MANAGEMENT SYSTEM","387":"Review on Computational Trust and Reputation Models","388":"Uses of Artificial Intelligence on Computer Based Instruction.","389":"Intelligence in Artificial Intelligence.","390":"Building Explainable Artificial Intelligence Systems","391":"Image Classification using Deep Learning","392":"Mobile Network Coverage Determination at 900MHz for Abuja Rural Areas using Artificial Neural Networks","393":"Artificial intelligence.","394":"Artificial Intelligence.","395":"Getting to know each other---artificial social intelligence for autonomous\r\n\trobots","396":"Can we Trust Machine Learning Results? Artificial Intelligence in Safety-Critical Decision Support.","397":"A Review on Introduction to Reinforcement Learning","398":"Cervical Smear Analyzer (CSA) Expert System for identification of cervical cells in Papanicolaou smear test","399":"Fashion AI Literature","400":"Artificial Intelligence based Fraud Agent to Identify Supply Chain Irregularities.","401":"Social and Economic Impacts of Artificial Intelligence - A Japanese Perspective.","402":"An Application of Algorithmic Probability to Problems in Artificial Intelligence","403":"Affective Artificial Intelligence in Education: From Detection to Adaptation.","404":"Annotated Semantics for Nonmonotonic Reasonings in Artificial Intelligence II.","405":"Stages of Ethical Development in Artificial General Intelligence Systems.","406":"A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955","407":"Preface: Artificial Intelligence of Humor - Computational Humor.","408":"Annotated Semantics for Nonmonotonic Reasonings in Artificial Intelligence IV.","409":"The Semantic Web: The Origins of Artificial Intelligence Redux","410":"A Survey of Artificial Intelligence for Prognostics.","411":"High-level perception, representation, and analogy: A critique of\n    artificial intelligence methodology","412":"OpenCog: A Software Framework for Integrative Artificial General Intelligence.","413":"Annotated Semantics for Nonmonotonic Reasonings in Artificial Intelligence III.","414":"Annotated Semantics for Nonmonotonic Reasonings in Artificial Intelligence I.","415":"How can unions use Artificial Intelligence to build power? The use of AI chatbots for labour organising in the US and Australia","416":"Artificial General Intelligence through Large-Scale, Multimodal Bayesian Learning.","417":"Statistical Relational Artificial Intelligence: Logic, Probability,\n               and Computation","418":"How to Analyze Free Text Descriptions for Recommending TV Programmes?","419":"Artificial Intelligence Architectures for Composition and Performance Environment.","420":"Artificial Intelligence in Music Education: A Critical Review.","421":"Artificial Unintelligence: Anti-intelligence of Intelligent Algorithms.","422":"ARTIFICIAL INTELLIGENCE: WHAT WILL AMERICA\r\nLOOK LIKE POLITICALLY AFTER THE 2020 CENSUS?","423":"Artificial Intelligence in Music Education: A critical review","424":"Multidisciplinary Trends in Modern Artificial Intelligence: Turing's Way.","425":"ARCHON: A Distributed Artificial Intelligence System for Industrial Applications","426":"On the Application of Artificial Intelligence Techniques to Create Network Intelligence.","427":"Issues and Approaches in Artificial Intelligence Middleware Development for Digital Games and Entertainment Products.","428":"Fundamental Mechanisms for Artificial Intelligence Programming Language - An Introduction.","429":"Modelling Artificial Intelligence on a Case of Bridge Card Play Bidding","430":"Towards an End-to-End Artificial Intelligence Driven Global Weather Forecasting System","431":"On Contrastive Divergence Learning","432":"AI in CAI: An Artificial-Intelligence Approach to Computer-Assisted\n\tInstruction","433":"Artificial Intelligence: A Modern Approach (International Edition)","434":"A Matrix Algebra Approach to Artificial Intelligence","435":"Artificial Intelligence in U.S. Health Care Delivery","436":"Intelligent Assistance without Artificial Intelligence.","437":"User-Centric Analysis of the CAPTCHA Response Time: A New Perspective in Artificial Intelligence.","438":"Computational Creativity","439":"Artificial Intelligence for Advanced Problem Solving Techniques","440":"The Essential Turing: Seminal Writings in Computing, Logic, Philosophy, Artificial Intelligence, and Artificial Life plus The Secrets of Enigma","441":"Resource Allocation in Industrial Cloud Computing Using Artificial Intelligence Algorithms.","442":"iCub: an open platform for research in robotics & artificial intelligence.","443":"Trustability in Algorithmic Systems Based on Artificial Intelligence in the Public and Private Sectors.","444":"Artificial intelligence and ambient intelligence.","445":"Human Intelligence Needs Artificial Intelligence.","446":"Web Intelligence and Artificial Intelligence.","447":"Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks","448":"Artificial intelligence ethics by design. Evaluating public perception on the importance of ethical design principles of artificial intelligence","449":"Face Recognition Technology","450":"Traffic Sign Detection and Recognition for Automated Driverless Cars Based on SSD","451":"Extending the expressive power of semantic networks","452":"How should we change teaching and assessment in response to increasingly powerful generative Artificial Intelligence? Outcomes of the ChatGPT teacher survey","453":"From Correlation to Imagination: Deep Generative Models for Artificial Intelligence.","454":"Artificial general intelligence: an organism and level based position statement.","455":"A Taxonomy of Recommender Agents on the Internet","456":"NAVPLAN: Tactical Routeplanning for Seagoing Vessels Based on Artificial Intelligence.","457":"Humor Recognition in Psychiatric Patients and Artificial Intelligence.","458":"AI planning and scheduling in the medical hospital environment.","459":"Genre Classification of Web Pages: User Study and Feasibility Analysis","460":"The scientific legacy of Marco Cadoli in Artificial Intelligence.","461":"Artificial Intelligence Based Mutual Authentication\r\nTechnique with Four Entities in 4-G Mobile\r\nCommunications","462":"MDA: A Formal Approach to Game Design and Game Research","463":"The structure of ill structured problems","464":"Combining artificial intelligence and databases for data\n\tintegration","465":"Towards a general theory of topological maps","466":"Traditional Machine Learning and No Code Machine Learning with its Features and Application","467":"The Impact of Artificial Intelligence on Education: Opening New Windows.","468":"Artificial and Computational Intelligence for Games on Mobile Platforms.","469":"Legal Risks and Preventive Measures in ChatGPT Applications in China","470":"AutoAIViz: opening the blackbox of automated artificial intelligence with conditional parallel coordinates\r\nShare on","471":"Towards Artificial-Intelligence-Based Cybersecurity for Robustifying Automated Driving Systems Against Camera Sensor Attacks","472":"Bayesian Artificial Intelligence.","473":"Is Intelligence Artificial?","474":"Testing Artificial Intelligence.","475":"Rethinking Artificial Intelligence.","476":"Generative Artificial Intelligence.","477":"Bayesian artificial intelligence","478":"Evolutionary Artificial Intelligence.","479":"Artificial General Intelligence.","480":"Apocalyptic Al: Visions of Heaven in Robotics, Artificial Intelligence, and Virtual Reality","481":"A Paradigm Shift in Artificial Intelligence: Why Social Intelligence Matters in the Design and Development of Robots with Human-Like Intelligence.","482":"AI for Dynamic Difficult Adjustment in Games","483":"Intelligent tutoring systems: An overview","484":"Emotion Detector","485":"Fake news detection within online social media using supervised artificial intelligence algorithms","486":"Artificial intelligence for online characterization of ultrashort X\u2011ray free\u2011electron laser pulses","487":"Artificial Intelligence Testing.","488":"Psychometric artificial intelligence.","489":"Artificial Intelligence Approaches.","490":"Causability and explainability of artificial intelligence in medicine.","491":"Relational Artificial Intelligence.","492":"Intelligible Artificial Intelligence.","493":"Ethical Artificial Intelligence.","494":"Advanced Artificial Intelligence","495":"Naive Artificial Intelligence.","496":"Introducing Artificial Intelligence","497":"Artificial Intelligence - Introduction.","498":"Artificial Swarm Intelligence.","499":"Industrial Artificial Intelligence.","500":"Artificial (emotional) intelligence.","501":"Evolving artificial intelligence","502":"On artificial intelligence.","503":"Artificial intelligence research.","504":"Evolving Artificial Intelligence","505":"Artificial intelligence (ai).","506":"Artificial Intelligence Corporation.","507":"Face Recognition System","508":"An Analysis of Monte Carlo Tree Search","509":"Artificial Intelligence Programming.","510":"Biomedical Artificial Intelligence.","511":"Embodied artificial intelligence.","512":"Usable artificial intelligence.","513":"Complex systems: Network thinking","514":"Artificial intelligent in China and United States","515":"Analoge Repra\u0308sentationen in der \u00c4rtificial Intelligence\"","516":"Contextsensitive feature selection for lazy learners","517":"How Artificial Intelligence Can Protect Financial Institutions From Malware Attacks","518":"Machine learning: a review of classification and combining techniques","519":"An Extensive Review on Generative Adversarial Networks GAN\u2019s","520":"Vector Symbolic Architectures: A New Building Material for Artificial General Intelligence.","521":"On Some Problems in Applying Artificial Intelligence in Large Industrial Systems.","522":"Planning and Plausible Reasoning in Artificial Intelligence: Diagrams, Planning, and Reasoning.","523":"Analoge Repr\u00e4sentationen in der \u00c4rtificial Intelligence\".","524":"Overview and Outlook on the Semantic Desktop","525":"Structure from motion of rigid and jointed objects","526":"eXtended Artificial Intelligence: New Prospects of Human-AI Interaction\r\n  Research","527":"Beyond AI The Rise of Cognitive Computing as Future of Computing ChatGPT Analysis","528":"The Italian Association for Artificial Intelligence celebrates its 25th anniversary.","529":"Future of Robotics","530":"eXtended Artificial Intelligence: New Prospects of Human-AI Interaction Research","531":"Future of Digital Natives","532":"A study of spam filtering using support vector machines","533":"Brain Intelligence: Go beyond Artificial Intelligence.","534":"From Artificial Intelligence to Augmented Intelligence.","535":"Artificial Intelligence Methods for Ambient Intelligence.","536":"From Artificial Intelligence to Cyborg Intelligence.","537":"Brain Intelligence: Go Beyond Artificial Intelligence.","538":"Brain Area V6A: A Cognitive Model for an Embodied Artificial Intelligence.","539":"IBL: An Inheritance-Based Lexicon Formalism","540":"A Network View of Human Ingestion and Health: Instrumental Artificial Intelligence.","541":"Intelligence Quotient and Intelligence Grade of Artificial Intelligence.","542":"Dealing with Large Datasets Using an Artificial Intelligence Clustering Tool.","543":"Musical Knowledge: what can Artificial Intelligence bring to the\n\tmusician?","544":"Artificial Intelligence And Big Data Technologies To Close The Achievement Gap.","545":"MaxSolver: An efficient exact algorithm for (weighted) maximum satisfiability","546":"Musical Knowledge: What can Artificial Intelligence Bring to the Musician?","547":"Investigation into the Application of Artificial\n                 Intelligence Methods to the Analysis of Medical Data","548":"Deep Learning Applications and Image Processing","549":"The computational complexity of probabilistic inference using bayesian belief networks","550":"iBuilding: Artificial Intelligence in Intelligent Buildings.","551":"Artificial Intelligence-Enabled Intelligent 6G Networks.","552":"Designing Explainability of an Artificial Intelligence System.","553":"A Model for Belief Revision","554":"Impacts of Artificial Intelligence. Scientific, Technological, Military,\r\n\tEconomic, Societal, Cultural and Political","555":"Discovering Social and Aesthetic Categories of Avatars: A Bottom-Up Artificial Intelligence Approach Using Image Clustering.","556":"Dynamic Question Answer Generator An Enhanced Approach to Question Generation","557":"From Artificial Intelligence to Natural Stupidity (and Back) in only Fifty Years.","558":"Natural language processing technologies in artificial intelligence - the science and industry perspective.","559":"Advanced Fire Monitoring System","560":"Representing ontologies using description logics, description graphs, and rules","561":"A Universal Measure of Intelligence for Artificial Agents","562":"Artificial intelligence: Structures and strategies for complex problem solving","563":"Towards the Assessment of Easy-to-Read Guidelines Using Artificial Intelligence Techniques","564":"Integrating Robotics and Artificial Intelligence in Business Processes - An Organization Theoretical Analysis.","565":"A Traffic Sign Classifier Model using Sage Maker","566":"Feature relevance XAI in anomaly detection: Reviewing approaches and challenges","567":"Arduino Controlled Robotic Arm","568":"Teaching Artificial Intelligence at the Faculty of Mathematics and Physics, Charles University.","569":"Virtual Therapist for Psychological Healthcare","570":"The Man-Machine Interaction: The Influence of Artificial Intelligence on Rehabilitation Robotics.","571":"Age Invariant Face Recognition","572":"Artificial Intelligence at Schlumberger.","573":"Intelligence, Artificial and Otherwise.","574":"Cryptography using artificial intelligence.","575":"Preferences in artificial intelligence.","576":"Artificial Intelligence in Simulation.","577":"Minimally Naturalistic Artificial Intelligence.","578":"Artificial intelligence and design.","579":"Hybrid Human-Artificial Intelligence.","580":"Artificial intelligence in space.","581":"Economics and Artificial Intelligence.","582":"Steps Toward Artificial Intelligence","583":"Generality in artificial intelligence","584":"Artificial Intelligence through Logic?","585":"Engineering artificial intelligence software.","586":"Artificial Intelligence for Games.","587":"Artificial Intelligence Search Algorithms.","588":"Conditionals and artificial intelligence.","589":"Artificial Intelligence in Government.","590":"Artificial intelligence in geography.","591":"Artificial intelligence (2. ed.).","592":"Robotics and artificial intelligence.","593":"Tractability and artificial intelligence.","594":"Artificial Intelligence in Bioinformatics.","595":"The Artificial Life Roots of Artificial Intelligence","596":"The Artificial Life Roots of Artificial Intelligence.","597":"Supporting Global Collective Intelligence via Artificial Intelligence.","598":"Blockchain Intelligence: When Blockchain Meets Artificial Intelligence.","599":"Multimedia Intelligence: When Multimedia Meets Artificial Intelligence.","600":"Bankruptcy Prediction for Banks: An Artificial Intelligence Approach to Improve Understandability.","601":"A Short Introduction to Preferences: Between Artificial Intelligence and Social Choice","602":"Web Intelligence and Artificial Intelligence in Education.","603":"Artificial intelligence in 2027.","604":"Evolution of artificial intelligence.","605":"Terminology and Artificial Intelligence.","606":"Artificial Intelligence in Transition.","607":"Introduction to Artificial Intelligence.","608":"Artificial intelligence: Retrospective\/prospective.","609":"Uncertainty in artificial intelligence.","610":"Generality in Artificial Intelligence.","611":"Artificial Intelligence Recruitment Analysis.","612":"Artificial Intelligence in Industry.","613":"eHealth and Artificial Intelligence.","614":"An Artificial Intelligence Perspective.","615":"Artificial Intelligence in CIM.","616":"Creativity and Artificial Intelligence.","617":"Artificial Intelligence Conferences Closeness.","618":"Artificial Intelligence And Aesthetics.","619":"Trends in Artificial Intelligence.","620":"Steps toward artificial intelligence","621":"Applications of Artificial Intelligence.","622":"History of Artificial Intelligence.","623":"Uncertainty in Artificial Intelligence.","624":"XAI - Explainable artificial intelligence.","625":"Argumentation in artificial intelligence.","626":"XAI\u2014Explainable artificial intelligence","627":"Artificial intelligence and networking.","628":"Artificial Intelligence and Games.","629":"Beyond Distributed Artificial Intelligence.","630":"Artificial Intelligence, 2nd Edition.","631":"Artificial intelligence - eine Einf\u00fchrung.","632":"Quo Vadis, Artificial Intelligence?","633":"Artificial Intelligence and Law.","634":"Proceedings of the 14th National Conference on Artificial Intelligence","635":"Proceedings of the 15th National Conference on Artificial Intelligence","636":"Comparison of Defuzzification Methods from a Real World Problem","637":"Intelligent Internet Systems","638":"Epistemology and artificial intelligence.","639":"Overview of Artificial Intelligence.","640":"Artificial intelligence and space.","641":"Chips for artificial intelligence.","642":"Artificial Intelligence and Industry.","643":"Symposium on Artificial Intelligence.","644":"Artificial intelligence: think again.","645":"Cleaving (Unto) Artificial Intelligence.","646":"Dynamically scripted artificial intelligence.","647":"Tutorial on artificial intelligence.","648":"Artificial Intelligence in Humans.","649":"Blockchain and Artificial Intelligence.","650":"Dilemmas of artificial intelligence.","651":"Applications in Artificial Intelligence.","652":"Artificial intelligence and simulation.","653":"Artificial Intelligence in China.","654":"Logic and Artificial Intelligence.","655":"Introduction to artificial intelligence.","656":"Logic-based artificial intelligence","657":"Introduction to Artificial Intelligence","658":"Essentials of Artificial Intelligence","659":"Artificial Intelligence in Europe.","660":"Artificial intelligence in simulation.","661":"Industry, Artificial Intelligence in.","662":"Engineering, Artificial Intelligence in.","663":"Toward Semiotic Artificial Intelligence.","664":"Artificial Intelligence for Audiences","665":"Hybrid Artificial Intelligence Systems.","666":"Artificial Intelligence and Testing.","667":"Yale Artificial Intelligence Project.","668":"Models and artificial intelligence.","669":"Artificial intelligence and imagery.","670":"Massively Parallel Artificial Intelligence.","671":"Self-adaptive artificial intelligence.","672":"Physics Enhanced Artificial Intelligence.","673":"Towards Explainable Artificial Intelligence.","674":"Artificial Intelligence with Uncertainty.","675":"Artificial Intelligence and Automation.","676":"Artificial intelligence in vision.","677":"Artificial Intelligence in Surgery.","678":"Artificial and Biological Intelligence","679":"On distributed artificial intelligence.","680":"Manufacturing and artificial intelligence.","681":"Steps towards artificial intelligence","682":"Artificial Intelligence Through Prolog","683":"Artificial Intelligence in Psychology","684":"Modeling and artificial intelligence.","685":"Artificial intelligence and statistics.","686":"Artificial intelligence (3. ed.).","687":"Artificial Phenomenology for Human-Level Artificial Intelligence.","688":"Developing Creativity: Artificial Barriers in Artificial Intelligence.","689":"Artificial Intelligence at MITRE.","690":"E. Rich, Artificial Intelligence.","691":"Piaget and Artificial Intelligence.","692":"Linking Artificial Intelligence Principles.","693":"What is artificial intelligence?","694":"Biologically Driven Artificial Intelligence.","695":"Invention and Artificial Intelligence.","696":"Artificial intelligence news letter.","697":"Medicine, Artificial Intelligence in.","698":"Artificial intelligence as law.","699":"Readiness for Artificial Intelligence.","700":"Artificial Intelligence in RoboCup.","701":"Artificial and Biological Intelligence.","702":"Human-Artificial Intelligence Partnerships.","703":"Artificial Intelligence and Statistics.","704":"Hardware-Enabled Artificial Intelligence.","705":"Towards Verified Artificial Intelligence.","706":"Artificial intelligence through Prolog.","707":"Socially Useful Artificial Intelligence.","708":"Biological and Artificial Intelligence.","709":"Modeling Belief in Dynamic Systems, Part II: Revisions and Update","710":"Artificial Intelligence and Suicide: Where Artificial Intelligence Stops and Humans Join In.","711":"The artificial life roots of artificial intelligence","712":"Proceedings of the 19th National Conference on Artificial Intelligence","713":"Proceedings of the 12th National Conference on Artificial Intelligence","714":"Updating a Protocol-Based Decision-Support System\u2019s Knowledge Base: A Breast Cancer Case Study - Springer","715":"Multi-Agent Systems: An Introduction to Distrubuted Artificial Intelligence","716":"Proceedings of the 11th National Conference on Artificial Intelligence","717":"Proceedings of the 13th National Conference on Artificial Intelligence","718":"Proceedings of the 16th National Conference on Artificial Intelligence","719":"Proceedings of the 17th National Conference on Artificial Intelligence","720":"Artificial Intelligence and Decision Making: On the \"Morals\" of Artificial Intelligence Systems.","721":"Responsible Artificial Intelligence - How to Develop and Use AI in a Responsible Way","722":"Some Elements for a Prehistory of Artificial Intelligence in the Last Four Centuries.","723":"Massively parallel models of computation - distributed parallel processing in artificial intelligence and optimization.","724":"Artificial Intelligence Research at the Artificial Intelligence Laboratory, Massachusetts Institute of Technology.","725":"Topic Detection using Machine Learning","726":"Learning to act using real-time dynamic programming","727":"On agent-based software engineering","728":"A glimpse at the metaphysics of Bongard problems","729":"RAISE 2014: Proceedings of the 3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering","730":"Commonsense Causal Explanation in a Legal Domain","731":"Use Of Artificial Intelligence Techniques To The Interpretation Of Subsurface Log Images.","732":"Estimation of some transducer parameters in a broadband piezoelectric transmitter by using an artificial intelligence technique","733":"Statistical Relational Artificial Intelligence: From Distributions through Actions to Optimization","734":"Images of Artificial Intelligence: a Blind Spot in AI Ethics","735":"PROLOG and the Teaching of Artificial Intelligence at the Technical University of Kosice.","736":"Tag-Based User Pro\ffiling for Social Media Recommendation","737":"A model for the dynamic coordination of multiple competing goals","738":"Introduction to the special issue on Artificial Intelligence for Society and Economy.","739":"Special Issue Celebrating 25 years of the Italian Association for Artificial Intelligence.","740":"The importance of Intelligent Control Systems","741":"Preface to the Special Issue on Advances in Argumentation in Artificial Intelligence.","742":"Intelligent Road Design Using Artificial Intelligence Techniques.","743":"The Art of Human Intelligence and the Technology of Artificial Intelligence: Artificial Intelligence Visual Art Research.","744":"Drug Review Sentiment Analysis using Boosting Algorithms","745":"Student Library Attendance using Face Recognition","746":"The problem of survival from an algorithmic point of view","747":"Problem Solving Techniques in Cognitive Science","748":"Mathematical Modelling and Artificial Intelligence in Luxembourg: Twenty PhD Students to be Trained in Data-Driven Modelling.","749":"Design Unique Brochures with AI: Simplified's Free AI Brochure Maker","750":"State of the art of data science in Spanish language and its application in the field of AI","751":"Pragmatic AI","752":"AI Logo Maker: Create a Logo That is Versatile","753":"Towards the AI Index","754":"User Interface Goals, AI Opportunities","755":"Ethics guidelines for trustworthy AI","756":"Building AI Applications: Yesterday, Today, and Tomorrow","757":"Conceptualizing AI literacy: An exploratory review","758":"CiteSeerX: AI in a Digital Library Search Engine","759":"AI periodic-table","760":"AI and HCI: Two Fields Divided by a Common Focus","761":"Recent advances in AI planning","762":"Explainable AI (ex-AI)","763":"AI Application Programming (Programming Series)","764":"User interface goals, AI opportunities","765":"The AI Regulation: entering an AI regulatory winter? Why an ad hoc directive on AI in employment is required","766":"Ethical Requirements for AI Systems.","767":"Poker as Testbed for AI Research.","768":"Pattern-Based AI Scripting Using ScriptEase.","769":"Towards a Science of Human-AI Decision Making: A Survey of Empirical\r\n  Studies","770":"AI Safety and Regulations Navigating the Post COVID Era Aims, Opportunities, and Challenges A ChatGPT Analysis","771":"AI-Supported Messaging: An Investigation of Human-Human Text Conversation with AI Support","772":"Designing Theory-Driven User-Centric Explainable AI","773":"Framework based on parameterized images on ResNet to identify intrusions in smartwatches or other related devices","774":"Sensitivity to Risk Profiles of Users When Developing AI Systems.","775":"How AI Boosts Industry Profits and Innovation","776":"Toward a Conceptual Framework for Understanding AI Action and Legal Reaction.","777":"The age of AI : and our human future","778":"How AI Can Help SE; or: Randomized Search Not Considered Harmful.","779":"Explainable AI: The New 42?","780":"Potential AI Strategies to Solve the Commons Game: A Position Paper.","781":"Is AI good for the planet?","782":"AI Roadmap - A human-centric approach to AI in aviation","783":"AI in Power Systems and Energy Markets","784":"Educational systems and resources based on AI which support and evaluate education","785":"State of the art and practice in AI in education","786":"An AI Planning-Based Approach to the Multi-Agent Plan Recognition Problem.","787":"Educational Design Principles of Using AI Chatbot That Supports Self-Regulated Learning in Education: Goal Setting, Feedback, and Personalization","788":"Impact of AI assistance on student agency","789":"Cambrian Intelligence: The Early History of the New AI","790":"On Quantifying and Understanding the Role of Ethics in AI Research: A\r\n  Historical Account of Flagship Conferences and Journals","791":"AI in support of Plant Disease Management.","792":"AI Research in the Benelux - Guest Editorial.","793":"AI in Web Advertising: Picking the Right Ad Ten Thousand Times a Second.","794":"Rise of Concerns about AI: Reflections and Directions","795":"Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning","796":"AI, explain yourself","797":"Metrics for Explainable AI: Challenges and Prospects","798":"Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI\r\n  2021 Challenge: Report","799":"Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction","800":"Generative AI, generating precariousness for workers?","801":"The ART of AI \u2014 Accountability, Responsibility, Transparency","802":"Guest Editors' Introduction: AI in Power Systems and Energy Markets","803":"AI technologies for education: Recent research & future directions","804":"Usable AI Requires Commonsense Knowledge","805":"AI-enabled adaptive learning systems: A systematic mapping of the literature","806":"Natural Language Translation at the Intersection of AI and HCI","807":"Resisting Dehumanization in the Age of \u201cAI\u201d","808":"I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI","809":"How Do Employees Imagine AI They Want to Work with: A Drawing Study","810":"AI and Ideas by Statistical Mechanics.","811":"Biases in AI systems","812":"Deep learning for AI","813":"AI Methods for Analyzing Microarray Data.","814":"Gendered AI: German news media discourse on the future of work","815":"Smartening up with Artificial Intelligence (AI)---What's in it for Germany and its Industrial Sector?","816":"Neural Approaches to Conversational AI","817":"Validation of AI Systems","818":"Proof-Carrying Plans: a Resource Logic for AI Planning","819":"Choose Your Weapon: Survival Strategies for Depressed AI Academics","820":"Introduction to the Special Issue on Usable AI","821":"An Agent-Oriented View of AI Teaching using a Modern C++ Programming Methodology","822":"How Evaluation Guides AI Research: The Message Still Counts More Than the Medium","823":"Open (For Business): Big Tech, Concentrated Power, and the Political Economy of Open AI","824":"Learning Deep Architectures for AI","825":"An AI Walk from Pharmacokinetics to A Marketing.","826":"Moving Beyond the Turing Test with the Allen AI Science Challenge","827":"State of AI Ethics June 2020","828":"AI for Romantic Comedies","829":"Explainable AI for Games","830":"Using AI and machine learning to study expressive music performance: project survey and first report.","831":"Smart Verification of Passenger using AI","832":"Mixed-Initiative Interface Personalization as a Case Study in Usable AI","833":"Navigating the generative AI era: Introducing the AI assessment scale for ethical GenAI assessment","834":"What should AI know? Information disclosure in human-AI collaboration","835":"Solving Hard AI Problem using CaRP as Online Network Security","836":"FactSheets: Increasing Trust in AI Services through Supplier's\r\n  Declarations of Conformity","837":"AI and the Economy","838":"Conversational AI Powered Chatbot Using Lex and AWS","839":"Educator and Student Perspectives on the Impact of Generative AI on Assessments in Higher Education","840":"Supporting High-Uncertainty Decisions through AI and Logic-Style Explanations","841":"AI Feynman: a Physics-Inspired Method for Symbolic Regression","842":"TPACK in the age of ChatGPT and Generative AI","843":"AI Poincar\u00e9: Machine Learning Conservation Laws from Trajectories","844":"Shakey: From Conception to History","845":"A Review of the Role of Causality in Developing Trustworthy AI Systems","846":"4th International Workshop on Realizing AI Synergies in Software Engineering (RAISE 2015)","847":"AI Magazine Poster: The AI Landscape.","848":"TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in\r\n  the Full Game","849":"A biomedical open knowledge network harnesses the power of AI to understand deep human biology","850":"There Is No Agency Without Attention","851":"Yuval Noah Harari argues that AI has hacked the operating system of human civilisation","852":"Using AI to Teach AI: Lessons from an Online AI Class.","853":"A Review of AI-Supported Tutoring Approaches for Learning Programming","854":"Annif and Finto AI : developing and implementing automated subject indexing","855":"FAIR Principles for data and AI models in high energy physics research\r\n  and education","856":"Labour in the age of AI: why regulation is needed to protect workers","857":"AI and Education: Grand Challenges","858":"An AI Based ATM Intelligent Security System using Open CV and YOLO","859":"AI and Similarity","860":"The Singularity May Never Be Near","861":"An AI Based Online Scheduling Controller for Highly Automated Production Systems","862":"Will There Be Superintelligence and Would It Hate Us?","863":"AI events.","864":"AI Topics.","865":"AI policy.","866":"Assigning AI: Seven Approaches for Students, with Prompts","867":"Experiential AI.","868":"AI Techniques in a Context-Aware Ubiquitous Environment","869":"Experiential AI","870":"AI@NICTA.","871":"AI education: open-access educational resources on AI.","872":"Semantics for Big Data","873":"Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface","874":"Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability","875":"Fast Camera Image Denoising on Mobile GPUs with Deep Learning, Mobile AI\r\n  2021 Challenge: Report","876":"AI in Oncology - Precision Therapy & Prognosis","877":"Ontogeny Recapitulates Ontegeny: AI and the AI Magazine - Editorial.","878":"Anatomy of an AI System: The Amazon Echo As An Anatomical Map of Human Labor, Data and Planetary Resources","879":"Data as the main focus of \u201cState of the art of data science in Spanish language and its application in the field of Artificial Intelligence\u201d","880":"AI literacy in K-12: a systematic literature review","881":"101 creative ideas to use AI in education, A crowdsourced collection","882":"AI fun matters.","883":"AI and Bioinformatics.","884":"Where's the AI?","885":"AI and Accountability.","886":"AI policy matters.","887":"AI in Switzerland.","888":"Introducing Worldwide AI.","889":"AI Rebel Agents.","890":"Rethinking AI Magazine.","891":"Applied AI News.","892":"The NY AI summit: a meeting of AI discipline leaders.","893":"AI&Society: editorial volume 35.2: the trappings of AI Agency.","894":"The Best AI Content Writer Plugin For WordPress","895":"AI is multidisciplinary.","896":"Integrated AI Systems.","897":"AI conference reports.","898":"Women and AI.","899":"MAILS - Meta AI Literacy Scale: Development and Testing of an AI Literacy Questionnaire Based on Well-Founded Competency Models and Psychological Change- and Meta-Competencies","900":"The BSM-AI project: SUSY-AI - Generalizing LHC limits on Supersymmetry with Machine Learning","901":"15 challenges for AI: or what AI (currently) can't do.","902":"AI education matters: a modular approach to AI ethics education.","903":"Introduction to the Special Issue on Question Answering","904":"Ontology Re-Engineering: A Case Study from the Automotive Industry","905":"Categorical and Continuous Features in Counterfactual Explanations of AI Systems","906":"K-12 AI curricula: a mapping of government-endorsed AI curricula","907":"Reading Race: AI Recognises Patient's Racial Identity In Medical Images","908":"Dynamic Vision-Based Intelligence","909":"Proximity dimensions and the emergence of collaboration: a\r\nHypTrails study on German AI research","910":"The global landscape of AI ethics guidelines","911":"Lessons Learned Delivering Optimized Supply Chain Planning to the Business World","912":"Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making","913":"Project Halo Update---Progress Toward Digital Aristotle","914":"The Problem With AI","915":"Leveraging AI Teaching in the Cloud for AI Teaching on Campus.","916":"AI in Computer games","917":"Is There a Future for AI Without Representation?","918":"AI in Contact Centers","919":"The partnership on AI.","920":"Uncommon voices of AI.","921":"AI in the News.","922":"AI - and Everything Else.","923":"AI and human society.","924":"AI Bridges and Dreams.","925":"From AI to Cybernetics.","926":"Welcome to AI matters.","927":"Happy Silver Anniversary, AI!","928":"Towards the AI Index.","929":"Towards an Employee-Centered Design for Human-AI Collaboration: How Work Design Theory Informs the Design of AI Systems","930":"The Applied AI Business.","931":"AI and the News.","932":"StarCraft AI Competition Report.","933":"AI education: adaptive planning.","934":"On Two AI Traditions.","935":"Empirical Methods in AI.","936":"The Israeli AI Community.","937":"AI: simply reinforcing medicine\u2019s worst biases?","938":"KBEmacs: Where's the AI?","939":"What Is AI, Anyway?","940":"Who speaks for AI?","941":"Editorial: Expository AI Applications.","942":"The Diversity of AI.","943":"AI Game-Playing Techniques.","944":"AI & Society and Society.","945":"X5Learn: A Personalised Learning Companion at the Intersection of AI and HCI","946":"NeMo: a toolkit for building AI applications using Neural Modules","947":"Usability Engineering Methods for Interactive Intelligent Systems","948":"Evaluation of\u00a0a\u00a0Hybrid AI-Human Recommender for\u00a0CS1 Instructors in\u00a0a\u00a0Real Educational Scenario","949":"AI Based Personal Learning Environments: Directions for Long Term Research","950":"Reclaiming AI as a theoretical tool for cognitive science","951":"Machine Learning: The New AI","952":"Scaling Learning Algorithms towards AI","953":"Semantic Integration","954":"Examining the Antecedents of Creative Collaboration with an AI Teammate","955":"Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems","956":"Kreuj prezentacje na nowym poziomie dzi\u0119ki darmowemu kreatorowi AI","957":"A Semantic Future for AI","958":"Recent Advances in AI planning","959":"AI in Business-Process Reengineering.","960":"User Interface Goals, AI Opportunities.","961":"AI Planning: Systems and Techniques.","962":"Epochs of an AI cosmology.","963":"Rapid Assisted Visual Search: Supporting Digital Pathologists with Imperfect AI","964":"AI in health and medicine","965":"Unemployment in the AI age.","966":"Recent Advances in AI Planning.","967":"Strong AI Is Simply Silly.","968":"The intelligence left in AI.","969":"AI amusements: my favorite marvin.","970":"AI education: machine learning resources.","971":"AI and the Human Eprom.","972":"The Angry Birds AI Competition.","973":"AI buzzwords explained: scientific workflows.","974":"Recent Advances in AI Planning","975":"AI & Society Volume 20-4.","976":"AI@50: We Are Golden!","977":"Optimizing Limousine Service with AI.","978":"AI Grand Challenges for Education.","979":"Year One of the IBM Watson AI XPRIZE: Case Studies in \u00c4I for Good\".","980":"Understanding and Dealing With Usability Side Effects of Intelligent Processing","981":"AI in Manufacturing at Digital.","982":"Is Computer Vision Still AI?","983":"Incremental Heuristic Search in AI.","984":"Is there an AI cosmology?","985":"On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications.","986":"Understanding the Role of Explanation Modality in AI-assisted Decision-making","987":"Experience AI: A Practitioner's Guide to Integrating Appreciative Inquiry With Experiential Learning","988":"Constructing Temporal Abstractions Autonomously in Reinforcement Learning","989":"AI--A multiple book review.","990":"Preferences in AI: An overview.","991":"Introduction to the Special Issue on Dialogue with Robots","992":"A Survey of Real-Time Strategy Game AI Research and Competition in StarCraft","993":"On the Ethical and Epistemological Utility of Explicable AI in Medicine","994":"AI Bookie Will a Self-Authorizing AI-Based System Take Control from a Human Operator?","995":"Explaining AI","996":"Using AI to Implement Effective Teaching Strategies in Classrooms: Five Strategies, Including Prompts","997":"Why is AI so scary?","998":"A Survey of the State of Explainable AI for Natural Language Processing","999":"Deep Sea Mining Environment, Economic and Hindu Technological Perspectives with AI Chatbots Analysis","1000":"Welcome to AI matters 4(1).","1001":"Welcome to AI Matters issue 4.","1002":"AI in the German Democratic Republic.","1003":"AI in Medicine: A Japanese Perspective.","1004":"The intersection of ethics and AI.","1005":"Analogy and Relational Representations in the Companion Cognitive Architecture","1006":"Reflections on James Bond of AI.","1007":"AI: A Strategie Technology in Japan?","1008":"AI Communications track on agreement technologies.","1009":"Welcome to AI Matters Issue 2.","1010":"Welcome to AI matters 4(2).","1011":"Cognitive Orthoses: Toward Human-Centered AI.","1012":"RoboCup: A Challenge Problem for AI.","1013":"The 1994 Florida AI Research Symposium.","1014":"Accentuating the Magazine in AI Magazine.","1015":"NewsFinder: Automating an AI News Service.","1016":"The Mario AI Championship 2009-2012.","1017":"The Application of AI to Law.","1018":"Europace AI and Expert Systems Programme.","1019":"Welcome to AI matters 5(3).","1020":"AI education matters: teaching search algorithms.","1021":"AI education: birds of a feather.","1022":"1986 Workshop on Distributed AI (Report).","1023":"Welcome to AI matters 5(1).","1024":"Four Decades of AI in Portugal.","1025":"The 1998 AI Planning Systems Competition.","1026":"Welcome to AI matters 4(3).","1027":"Happy Anniversary, AAAI and AI Magazine!","1028":"Algorithms for propagating resource constraints in AI planning and scheduling: Existing approaches and new results","1029":"Interviews on AI and Education: Education and Technology: What do we know? And where is AI?","1030":"Welcome to AI matters 5(4).","1031":"Welcome to AI matters 5(2).","1032":"The Future of AI - A Manifesto.","1033":"Editorial: AI Education for the World.","1034":"Often, It's not About the AI.","1035":"The Coevolution of AI and AAAI.","1036":"To Serve AI (It's a Cookbook).","1037":"Welcome to AI matters 4(4).","1038":"Welcome to AI Matters issue 3.","1039":"Create Captivating Content with the Help of a Free Long Form Writing Assistant","1040":"The Case for Explicit Ethical Agents","1041":"A Standard Model of the Mind: Toward a Common Computational Framework across Artificial Intelligence, Cognitive Science, Neuroscience, and Robotics","1042":"Google AI algorithm masters ancient game of Go","1043":"On Exploring the Possibilities and the Limits of AI for an Interoperable and Empowering Industry 4.0","1044":"From here to human-level AI.","1045":"Once upon a time in AI.","1046":"Foundations of AI: The Big Issues.","1047":"Ask not what AI can do, but what AI should do: Towards a framework of task delegability.","1048":"What is a Knowledge Representation","1049":"A Novel R\u00f4le for AI?","1050":"What's missing in AI: The Interface Layer","1051":"Computer Go: An AI oriented survey.","1052":"Quantum computation, quantum theory and AI.","1053":"Modelling Social Action for AI Agents.","1054":"AI and Law: A fruitful synergy.","1055":"Functional Transformations in AI Discovery Systems.","1056":"AI & Society special issue on work organisation.","1057":"Commercial AI Trends Seen at AAAI-87.","1058":"Phase Mapper: Accelerating Materials Discovery with AI.","1059":"Interviews on AI and Education: Martial Vivet.","1060":"Whither AI: Identity Challenges of 1993-95.","1061":"Databases in Large AI Systems - Workshop Report.","1062":"AI Growing Up: The Changes and Opportunities.","1063":"Building AI Applications: Yesterday, Today, and Tomorrow.","1064":"The Pyro Toolkit for AI and Robotics.","1065":"AI profiles: an interview with Iolanda Leite.","1066":"AI policy: organizations, resources, and recent symposia.","1067":"A Framework for AI System.","1068":"What is a Knowledge Representation?","1069":"Disentangling Trust in Voice Assistants - A Configurational View on Conversational AI Ecosystems","1070":"Practical Considerations in Deploying AI for Defect Prediction: A Case Study Within the Turkish Telecommunication Industry","1071":"AI Enables Explainable Drug Sensitivity Screenings.","1072":"The AI Black Box Explanation Problem.","1073":"Modern Workspace Based Policy Management with Automated Keyword Extraction and AI Based Records Management using Azure Cognitive Services","1074":"AI education matters: teaching hidden Markov models.","1075":"AI profiles: an interview with Peter Norvig.","1076":"AI profiles: an interview with Thomas Dietterich.","1077":"Posthuman learning: AI from novice to expert?","1078":"A Year in K-12 AI Education.","1079":"Building Bridges Between AI and Cognitive Psychology.","1080":"The Best of AI in Japan - Prologue.","1081":"Genetic programming and AI planning systems","1082":"An AI-Based Methodology for Factory Design.","1083":"AI profiles: an interview with Leslie Kaelbling.","1084":"AI profiles: an interview with Kristian Kersting.","1085":"AI practioners: on the Northeast Ohio ACM.","1086":"AI profiles: an interview with Peter Stone.","1087":"AI education: deep neural network learning resources.","1088":"AI & Society Birthday Issue Volume 21(4).","1089":"The new AI spring: a deflationary view.","1090":"Green Engineering AI Tools Benefit the Environment.","1091":"Emperor AI, Where Is Your New Mind?","1092":"AI and Agents: State of the Art.","1093":"Immobile Robots AI in the New Millennium.","1094":"The posthuman: AI, dronology, and \"becoming alien\".","1095":"Society under threat... but not from AI.","1096":"AI profiles: an interview with Maja Matari\u0107.","1097":"AI education matters: biductive computing with prolog.","1098":"Mediating between AI and highly specialized users.","1099":"Announcing the Digital Edition of AI Magazine.","1100":"Architects of Intelligence","1101":"Intelligent peer networks for collaborative Web search","1102":"A Comparison of the Novamente AI Design with the Human Mind-Brain","1103":"Machine learning and AI-based approaches for bioactive ligand discovery\r\n  and GPCR-ligand recognition","1104":"Scheming with Objects","1105":"Multiagent Systems","1106":"AI Research at Bolt, Beranek & Newman, Inc.","1107":"Announcing the New App for AI Magazine.","1108":"A Perspective on AI Research in India.","1109":"AI buzzwords explained: distributed constraint optimization problems.","1110":"AI profiles: an interview with Jim Kurose.","1111":"Interviews on AI and Education: Jan Elshout.","1112":"Requirements for AI-based Teammates: A Qualitative Inquiry in the Context of Creative Workshops","1113":"Exploiting AI Technologies to Realise Adaptive Workflow Systems","1114":"A Missing Piece in the Puzzle: Considering the Role of Task Complexity in Human-AI Decision Making","1115":"Kurzweil's argument for the success of AI.","1116":"Comparing formal theories of context in AI.","1117":"AM: A Case Study in AI Methodology.","1118":"Quantum Pancomputationalism and Statistical Data Science: From Symbolic to Statistical AI, and to Quantum AI.","1119":"AI Therapist \u2013 Emotion Detection using Facial Detection and Recognition and Showing Content According to Emotions","1120":"AI Principles from the Future of Life Institute","1121":"Recent work in philosophy of interest to AI","1122":"Toward a Theory of Intelligent Complex Systems: From Symbolic AI to Embodied and Evolutionary AI.","1123":"Questioning the AI: Informing Design Practices for Explainable AI User Experiences","1124":"AI4People---An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations","1125":"Practical Introspection as Inspiration for AI.","1126":"Cancer: A Computational Disease that AI Can Cure","1127":"Welcome to AI matters, volume 3, issue 2.","1128":"AAAI-97 Workshop on AI and Knowledge Management.","1129":"Melomics: A Case-Study of AI in Spain.","1130":"Identifying Terrorist Activity with AI Plan Recognition Technology.","1131":"AI Planning through Mixed-Integer Programming.","1132":"The Two (Computational) Faces of AI.","1133":"Generative AI in Customer Support Services: A Framework for Augmenting the Routines of Frontline Service Employees","1134":"The Deskilling of Domain Expertise in AI Development","1135":"Computer Bridge: A Big Win for AI Planning","1136":"The Nature of AI: A Reply to Schank.","1137":"The Current State of AI: One Man's Opinion.","1138":"Reading the Web: A Breakthrough Goal for AI","1139":"Welcome to AI Matters, volume 2, issue 3.","1140":"Welcome to AI Matters, volume 2, issue 2.","1141":"Welcome to AI matters, volume 3, issue 4.","1142":"A tour of machine learning: An AI perspective.","1143":"Teaching Integrated AI through Interdisciplinary Project-Driven Courses.","1144":"Computer Bridge - A Big Win for AI Planning.","1145":"A Virtual Archive for the History of AI.","1146":"AI recognition of differences among book-length texts.","1147":"Programming by Example: The Human Face of AI.","1148":"Solving Mathematical Puzzles: A Challenging Competition for AI.","1149":"Software Social Organisms: Implications for Measuring AI Progress.","1150":"AI and Music: From Composition to Expressive Performance.","1151":"Using 4D\/RCS to Address AI Knowledge Integration.","1152":"Welcome to AI Matter: volume 2, issue 1.","1153":"AI buzzwords explained: multi-agent path finding (MAPF).","1154":"Workshop on the Foundations of AI: Final Report.","1155":"The Gardens of Learning: A Vision for AI.","1156":"Cancer: A Computational Disease that AI Can Cure.","1157":"AI Approaches to Fraud Detection and Risk Management.","1158":"The Mind at AI: Horseless Carriage to Clock.","1159":"Using Natural Language Processing to Assist the Visually Handicapped in Writing Compositions.","1160":"Reasoning about Interaction for Hospital Decision Making.","1161":"Discrete-Event Systems for Modelling Decision-Making in Human Motor Control.","1162":"Salient Object Detection in Noisy Images.","1163":"Using Deep Reinforcement Learning Methods for Autonomous Vessels in 2D Environments.","1164":"STOCS: An Efficient Self-Tuning Multiclass Classification Approach.","1165":"An Attribute Redundancy Measure for Clustering.","1166":"An Empirical Study of Encodings for Group MaxSAT.","1167":"Moving Object Modelling Approach for Lowering Uncertainty in Location Tracking Systems.","1168":"A Low-Scan Incremental Association Rule Maintenance Method Based on the Apriori Property.","1169":"Incremental Case-Based Reasoning for Classification.","1170":"Constraint-Based Vehicle Assembly Line Sequencing.","1171":"Watching You, Watching Me.","1172":"Concept-Learning in the Presence of Between-Class and Within-Class Imbalances.","1173":"Flexible Approximators for Approximating Fixpoint Theory.","1174":"Lexical Data Augmentation for Text Classification in Deep Learning.","1175":"Personalized Multi-Faceted Trust Modeling in Social Networks.","1176":"Personalized Student Attribute Inference.","1177":"The K-Closest Resemblance Classifier for Remote Sensing Data.","1178":"Deep Multi Agent Reinforcement Learning for Autonomous Driving.","1179":"Adversarial Models for Deterministic Finite Automata.","1180":"Selection Driven Query Focused Abstractive Document Summarization.","1181":"Empirical Confidence Models for Supervised Machine Learning.","1182":"Happiness Analysis with Fisher Information of Dirichlet-Multinomial Mixture Model.","1183":"A Deeper Look at Bongard Problems.","1184":"ELEM2: A Learning System for More Accurate Classifications.","1185":"Fault Prediction in the Telephone Access Loop Using a Neural Network.","1186":"Distance Constraint Arrays: A Model for Reasoning on Intervals with Qualitative and Quantitative Distances.","1187":"-: A Semantic Web Based Knowledge Representation and Context-Driven Morphing Framework.","1188":"Generating Satisfiable SAT Instances Using Random Subgraph Isomorphism.","1189":"Situation Event Logic for Early Validation of Multi-Agent Systems.","1190":"Planning Strategy Representation of DoLittle.","1191":"Training Global Linear Models for Chinese Word Segmentation.","1192":"Utility Theory-Based User Models for Intelligent Interface Agents.","1193":"An Object Indexing Methodology as Support to Object Recognition.","1194":"AUC: A Better Measure than Accuracy in Comparing Learning Algorithms.","1195":"A Procedural Planning System for Goal Oriented Agents in Games.","1196":"An Ontology-Based Spatial Clustering Selection System.","1197":"A Neural Network Based Approach to the Artificial Aging of Facial Images.","1198":"Negotiating Exchanges of Private Information for Web Service Eligibility.","1199":"Learning by Discovering Conflicts.","1200":"A Genetic and Social Computational Model for the Emergence of Skill-Based Agent Specialization.","1201":"Role Assignment for an Agent Group in Consideration of Conflicts among Agents.","1202":"A Formal Study on the Dualities in Temporal Projection Problems.","1203":"Macro Learning in Planning as Parameter Configuration.","1204":"Image Morphing: Transfer Learning between Tasks That Have Multiple Outputs.","1205":"Dsharp: Fast d-DNNF Compilation with sharpSAT.","1206":"Text Similarity Using Google Tri-grams.","1207":"Classification Automaton and Its Construction Using Learning.","1208":"A Genetic K-means Clustering Algorithm Applied to Gene Expression Data.","1209":"Fuzzy C-Means Clustering of Web Users for Educational Sites.","1210":"Getting Emotional about News Summarization.","1211":"Multiagent Decision by Partial Evaluation.","1212":"Exploiting Semantic Roles for Asynchronous Question Answering in an Educational Setting.","1213":"Managing Concurrent Negotiations in Multi-agent Systems.","1214":"Session Boundary Detection for Association Rule Learning Using n-Gram Language Models.","1215":"A Study of Recommending Locations on Location-Based Social Network by Collaborative Filtering.","1216":"Model-Based Least-Squares Policy Evaluation.","1217":"On the Applicability of L-systems and Iterated Function Systems for Grammatical Synthesis of 3D Models.","1218":"Post-supervised Template Induction for Dynamic Web Sources.","1219":"Compact Features for Sentiment Analysis.","1220":"Search Bound Strategies for Rule Mining by Iterative Deepening.","1221":"Probabilistic Reasoning for Meal Planning in Intelligent Fridges.","1222":"Semi-supervised Self-training for Sentence Subjectivity Classification.","1223":"Multi-agent Framework for a Virtual Enterprise of Demand-Responsive Transportation.","1224":"Unsupervised Relation Extraction Using Dependency Trees for Automatic Generation of Multiple-Choice Questions.","1225":"Inductive Improvement of Part-of-Speech Tagging and Its Effect on a Terminology of Molecular Biology.","1226":"Simulating Cognitive Phenomena with a Symbolic Dynamical System.","1227":"Answer Set Programming for Stream Reasoning.","1228":"Characterizing a Brain-Based Value-Function Approximator.","1229":"Determining an Optimal Seismic Network Configuration Using Self-Organizing Maps.","1230":"A Logic of Inductive Implication or Artificial Intelligence Meets Philosophy of Science II.","1231":"Partial Evaluation for Planning in Multiagent Expedition.","1232":"A Supervised Method of Feature Weighting for Measuring Semantic Relatedness.","1233":"Comparison of Semantic Similarity for Different Languages Using the Google n-gram Corpus and Second-Order Co-occurrence Measures.","1234":"A Statistical Model for Topic Segmentation and Clustering.","1235":"Full Border Identification for Reduction of Training Sets.","1236":"A Context-Aware Reputation-Based Model of Trust for Open Multi-agent Environments.","1237":"Parallelizing a Convergent Approximate Inference Method.","1238":"Multi Class Adult Image Classification Using Neural Networks.","1239":"English to Chinese Translation of Prepositions.","1240":"Topic Detection and Document Similarity on Financial News.","1241":"Adjectives: A Uniform Semantic Approach.","1242":"Compressing Bayesian Networks: Swarm-Based Descent, Efficiency, and Posterior Accuracy.","1243":"A Sentence-Level Sparse Gamma Topic Model for Sentiment Analysis.","1244":"Constrained Bayesian Optimization for Problems with Piece-wise Smooth Constraints.","1245":"Matching R\u00e9sum\u00e9s to Job Descriptions with Stacked Models.","1246":"QUANTUM: A Function-Based Question Answering System.","1247":"Supporting the Needs of Mobile Home Care Workers: A Case Study for Saskatoon District Health System.","1248":"Performance Evaluation of an Agent Based Distributed Data Mining System.","1249":"A Comparative Study of Two Density-Based Spatial Clustering Algorithms for Very Large Datasets.","1250":"Using Inter-agent Trust Relationships for Efficient Coalition Formation.","1251":"Rule Mining and Prediction Using the Flek Machine - A New Machine Learning Engine.","1252":"The Task Rehearsal Method of Life-Long Learning: Overcoming Impoverished Data.","1253":"Deborah Ann Stacey, Application of Bayesian Networks to Shopping Assistance.","1254":"Construction of a Non-redundant Cover for Conditional Independencies.","1255":"An Enhanced Genetic Algorithm Approach to the Channel Assignment Problem in Mobile Cellular Networks.","1256":"Generalized Arc Consistency with Application to MaxCSP.","1257":"Using Agent Replication to Enhance Reliability and Availability of Multi-agent Systems.","1258":"Improving Conversation Engagement Through Data-Driven Agent Behavior Modification.","1259":"Reranking Candidate Lists for Improved Lexical Induction.","1260":"Towards a Comprehensive Evaluation of Recommenders: A Cognition-Based Approach.","1261":"Harnessing Open Information Extraction for Entity Classification in a French Corpus.","1262":"Grounding Social Interaction with Affective Intelligence.","1263":"Advice-Based Exploration in Model-Based Reinforcement Learning.","1264":"Logic-Based Benders Decomposition for Two-Stage Flexible Flow Shop Scheduling with Unrelated Parallel Machines.","1265":"Electronic Contract Framework for Contractual Agents.","1266":"Learning with Prior Domain Knowledge and Insufficient Annotated Data.","1267":"An Approach to Improving Single Sample Face Recognition Using High Confident Tracking Trajectories.","1268":"Neural Network-POMDP-Based Traffic Sign Classification Under Weather Conditions.","1269":"Mining Biomedical Literature: An Open Source and Modular Approach.","1270":"Reasoning About Operations on Sets.","1271":"A Smart Home Agent for Plan Recognition.","1272":"A Novel Genetic Algorithm for the Word Sense Disambiguation Problem.","1273":"Time-Sensitive Topic-Based Communities on Twitter.","1274":"Simple Support-Based Distributed Search.","1275":"Learning the Semantic Meaning of a Concept from the Web.","1276":"Multi-state Directed Acyclic Graphs.","1277":"Bayesian Learning for Feed-Forward Neural Network with Application to Proteomic Data: The Glycosylation Sites Detection of the Epidermal Growth Factor-Like Proteins Associated with Cancer as a Case Study.","1278":"MITS: A Mixed-Initiative Intelligent Tutoring System for Sudoku.","1279":"Belief Selection in Point-Based Planning Algorithms for POMDPs.","1280":"Progressive Defeat Paths in Abstract Argumentation Frameworks.","1281":"Integrating Information Gathering Interaction into Transfer of Control Strategies in Adjustable Autonomy Multiagent Systems.","1282":"Path Propagation for Inference in Bayesian Networks.","1283":"Recurrent Boosting for Classification of Natural and Synthetic Time-Series Data.","1284":"On the Use of Possibilistic Bases for Local Computations in Product-Based Possibilistic Networks.","1285":"A Novel Approach for Automatic Palmprint Recognition.","1286":"Learning Na\u00efve Bayes Tree for Conditional Probability Estimation.","1287":"Using Noun Phrase Heads to Extract Document Keyphrases.","1288":"A Comparison of Association Rule Discovery and Bayesian Network Causal Inference Algorithms to Discover Relationships in Discrete Data.","1289":"Sparse Representation for Machine Learning.","1290":"Using Object Influence Areas to Quantitatively Deal with Neighborhood and Perception in Route Descriptions.","1291":"Quantitative Aspects of Behaviour Network Verification.","1292":"A Causal Approach for Mining Interesting Anomalies.","1293":"Detecting and Categorizing Indices in Lecture Video Using Supervised Machine Learning.","1294":"d-Separation: Strong Completeness of Semantics in Bayesian Network Inference.","1295":"Local and Global Influence on Twitter.","1296":"Extracting Information-Rich Part of Texts Using Text Denoising.","1297":"Metaheuristics for Score-and-Search Bayesian Network Structure Learning.","1298":"An Improved Data Sanitization Algorithm for Privacy Preserving Medical Data Publishing.","1299":"Reinforcement Learning in Nonstationary Environment Navigation Tasks.","1300":"Intelligent Tutoring Systems Measuring Student's Effort During Assessment.","1301":"Selective Retrieval for Categorization of Semi-structured Web Resources.","1302":"Pathfinding by Demand Sensitive Map Abstraction.","1303":"Convex Cardinality Restricted Boltzmann Machine and Its Application to Pattern Recognition.","1304":"Weka-SAT: A Hierarchical Context-Based Inference Engine to Enrich Trajectories with Semantics.","1305":"Incremental Cluster Updating Using Gaussian Mixture Model.","1306":"Partial Satisfaction Planning under Time Uncertainty with Control on When Objectives Can Be Aborted.","1307":"Toward a Computational Model for Collective Emotion Regulation Based on Emotion Contagion Phenomenon.","1308":"Combining Textual Pre-game Reports and Statistical Data for Predicting Success in the National Hockey League.","1309":"Rule Extraction from Random Forest: the RF+HC Methods.","1310":"Inferring Road Maps from Sparsely-Sampled GPS Traces.","1311":"Visual Predictions of Traffic Conditions.","1312":"Consolidation Using Sweep Task Rehearsal: Overcoming the Stability-Plasticity Problem.","1313":"Learning Paired-Associate Images with an Unsupervised Deep Learning Architecture.","1314":"Combinatorial Reverse Electricity Auctions.","1315":"Abstractive Meeting Summarization as a Markov Decision Process.","1316":"Robustness of Classifiers to Changing Environments.","1317":"Comparative Study of Dimensionality Reduction Methods Using Reliable Features for Multiple Datasets Obtained by rs-fMRI in ADHD Prediction.","1318":"SmartHome Energy Saving Using a Multi-objective Approach Based on Appliances Usage Profiles.","1319":"Design and Implementation of a Smart Quotation System.","1320":"Supervised Machine Learning for Summarizing Legal Documents.","1321":"Using Learned PSR Model for Planning under Uncertainty.","1322":"A Computer Approach for Face Aging Problems.","1323":"Exploring Case-Based Bayesian Networks and Bayesian Multi-nets for Classification.","1324":"Performance Evaluation of Agent Toolkits.","1325":"Annotation Concept Synthesis and Enrichment Analysis.","1326":"Planning Algorithms and Planning Problems (Abstract).","1327":"Using Language to Determine Success in Negotiations: A Preliminary Study.","1328":"Constraint Satisfaction Methods for Information Personalization.","1329":"Modelling Singularity in Vision to Learn Rotation Invariance toward Recognition.","1330":"A Novel Approach for Recommending Ranked User-Generated Reviews.","1331":"Reference Constraints and Individual Level Inheritance.","1332":"Peer-Based Intelligent Tutoring Systems: A Corpus-Oriented Approach.","1333":"Corpus-Based Term Relatedness Graphs in Tag Recommendation.","1334":"A Chaotic Neural Network for the Maximum Clique Problem.","1335":"Term-Based Clustering and Summarization of Web Page Collections.","1336":"Feature Extraction of Handwritten Symbols Using Fuzzy Logic.","1337":"The Reconstruction of the Interleaved Sessions from a Server Log.","1338":"Genetic Algorithm-Induced Optimal Blackjack Strategies in Noisy Settings.","1339":"Wavelet Network with OLS Optimization for Speech Signal Processing.","1340":"Histogram Arc Consistency as a Value Ordering Heuristic.","1341":"Preliminary Study of Attention Control Modeling in Complex Skill Training Environments.","1342":"Finding Interesting Summaries in GenSpace Graphs Efficiently.","1343":"An Algorithm for Anaphora Resolution in Aviation Safety Reports.","1344":"Decision Mining with User Preference.","1345":"Machine Learning in a Quantum World.","1346":"A Fast Computation of Inter-class Overlap Measures Using Prototype Reduction Schemes.","1347":"Predicting Sparse Clients' Actions with CPOPT-Net in the Banking Environment.","1348":"Lexicographic Preference Trees with Hard Constraints.","1349":"Categorizing Emails Using Machine Learning with Textual Features.","1350":"Neural Prediction of Patient Needs in an Ovarian Cancer Online Discussion Forum.","1351":"Measuring Human Emotion in Short Documents to Improve Social Robot and Agent Interactions.","1352":"An Experiment for Background Subtraction in a Dynamic Scene.","1353":"Crowd Prediction Under Uncertainty.","1354":"Exploiting Symmetry of Independence in d-Separation.","1355":"Machine Translation on a Parallel Code-Switched Corpus.","1356":"Mitigating Overfitting Using Regularization to Defend Networks Against Adversarial Examples.","1357":"Automatic Generation of Video Game Character Images Using Augmented Structure-and-Style Networks.","1358":"Multi-class Ensemble Learning of Imbalanced Bidding Fraud Data.","1359":"Principal Sample Analysis for Data Ranking.","1360":"ChatGPT is not all you need. A State of the Art Review of large\r\n  Generative AI models","1361":"Organic and dynamic tool for use with knowledge base of AI ethics for promoting engineers' practice of ethical AI design.","1362":"Considerations for AI fairness for people with disabilities.","1363":"AI education matters: building a fake news detector.","1364":"CiteSeerX: AI in a Digital Library Search Engine.","1365":"Using Educational Robotics to Motivate Complete AI Solutions.","1366":"A Review of Real-Time Strategy Game AI.","1367":"The Value of AI Tools: Some Lessons Learned.","1368":"AI Research and Applications in Digital's Service Organization.","1369":"Introduction to the Special Issue on \u00dcsable AI\".","1370":"Guest Editorial AI & Society: Enterprise, Innovations and Society.","1371":"Welcome to AI matters, volume 3, issue 3.","1372":"AI Research in the People's Republic of China.","1373":"Special issue: AI and next-generation supply networks.","1374":"Novel Study on AI Based Chatbot ChatGPT Impacts on the Traditional Library Management","1375":"A Behavior-Based Proactive User Authentication Model Utilizing Mobile Application Usage Patterns.","1376":"Weakly Supervised, Data-Driven Acquisition of Rules for Open Information Extraction.","1377":"Detecting Depression from Voice.","1378":"Identifying Misaligned Spans in Parallel Corpora Using Change Point Detection.","1379":"A Framework for Determining Effective Team Members Using Evolutionary Computation in Dynamic Social Networks.","1380":"Name2Vec: Personal Names Embeddings.","1381":"Weighting Words Using Bi-Normal Separation for Text Classification Tasks with Multiple Classes.","1382":"Towards a Novel Data Representation for Classifying Acoustic Signals.","1383":"A Generic Evolutionary Algorithm for Efficient Multi-Robot Task Allocations.","1384":"Artificial Intelligence-Based Latency Estimation for Distributed Systems.","1385":"Enhancing Unsupervised Pretraining with External Knowledge for Natural Language Inference.","1386":"Learning Latent Factor Models of Travel Data for Travel Prediction and Analysis.","1387":"Task-Structure Based Mediation: The Travel-Planning Assistant Example.","1388":"Genome-Wide Canonical Correlation Analysis-Based Computational Methods for Mining Information from Microbiome and Gene Expression Data.","1389":"User Interface Aspects of a Translation Typing System.","1390":"A Hybrid Approach to Making Recommendations and Its Application to the Movie Domain.","1391":"Imprecise and Uncertain Engineering Information Modeling in Databases.","1392":"A Statistical Corpus-Based Term Extractor.","1393":"Constraint Programming Lessons Learned from Crossword Puzzles.","1394":"Towards a Temporal Extension of Formal Concept Analysis.","1395":"A Survey on Statistical Relational Learning.","1396":"The WordNet Weaver: Multi-criteria Voting for Semi-automatic Extension of a Wordnet.","1397":"Sentiment and Factual Transitions in Online Medical Forums.","1398":"A Markov Decision Process Model for Strategic Decision Making in Sailboat Racing.","1399":"Evaluation and Application of Scenario Based Design on Thunderbird.","1400":"Granular State Space Search.","1401":"Grounding Formulas with Complex Terms.","1402":"Selective Sampling for Classification.","1403":"Use of Fuzzy Histograms to Model the Spatial Distribution of Objects in Case-Based Reasoning.","1404":"Word Clustering with Validity Indices.","1405":"MML-Based Approach for Determining the Number of Topics in EDCM Mixture Models.","1406":"Prediction of Container Damage Insurance Claims for Optimized Maritime Port Operations.","1407":"A Unified Evaluation Framework for Recommenders.","1408":"Generic and Query-Based Text Summarization Using Lexical Cohesion.","1409":"Using Communicative Acts to Plan the Cinematographic Structure of Animations.","1410":"An Incremental Machine Learning Algorithm for Nuclear Forensics.","1411":"Two-Literal Logic Programs and Satisfiability Representation of Stable Models: A Comparison.","1412":"A Structural Characterization of DAG-Isomorphic Dependency Models.","1413":"Mobile App for Detection of Counterfeit Banknotes.","1414":"Multi-agent System Architecture for Computer-Based Tutoring Systems.","1415":"RFCT: An Association-Based Causality Miner.","1416":"An Efficient Compositional Semantics for Natural-Language Database Queries with Arbitrarily-Nested Quantification and Negation.","1417":"Grid-Based Path-Finding.","1418":"Modeling Organizational Rules in the Multi-agent Systems Engineering Methodology.","1419":"Modelling an Academic Curriculum Plan as a Mixed-Initiative Constraint Satisfaction Problem.","1420":"Visual Perception Similarities to Improve the Quality of User Cold Start Recommendations.","1421":"Infusing Domain Knowledge to Improve the Detection of Alzheimer's Disease from Everyday Motion Behaviour.","1422":"Automated Scheduling: Reinforcement Learning Approach to Algorithm Policy Learning.","1423":"A Tool for Defining and Simulating Storage Strategies on the Smart Grid.","1424":"Poetry Chronological Classification: Hafez.","1425":"Fuzzy Computational Model for Emotions Originated in Workplace Events.","1426":"Hierarchical Shortest Pathfinding Applied to Route-Planning for Wheelchair Users.","1427":"Distributed Data Mining in a Ubiquitous Healthcare Framework.","1428":"A Simple Method for Testing Independencies in Bayesian Networks.","1429":"Learning Statistically Significant Contrast Sets.","1430":"A Pruning-Based Algorithm for Computing Optimal Coalition Structures in Linear Production Domains.","1431":"Text Compression by Syntactic Pruning.","1432":"Classification Based on Logical Concept Analysis.","1433":"A Reorganization Strategy to Build Fault-Tolerant Multi-Agent Systems.","1434":"A Multi-site Subcellular Localizer for Fungal Proteins.","1435":"Multiagent Constraint Satisfaction with Multiply Sectioned Constraint Networks.","1436":"On the Quality and Quantity of Random Decisions in Stochastic Local Search for SAT.","1437":"ICS: An Interactive Classification System.","1438":"Unsupervised Labeling of Noun Clusters.","1439":"Probabilistic Melodic Harmonization.","1440":"Scheduling Methods for Parallel Automated Theorem Proving.","1441":"Simulating Competing Alife Organisms by Constructive Compound Neural Networks.","1442":"Move Pruning and Duplicate Detection.","1443":"An Extendable Natural Language Interface to a Consumer Service Database.","1444":"Typical Example Selection for Learning Classifiers.","1445":"Interact: A Staged Approach to Customer Service Automation.","1446":"Collocation Discovery for Optimal Bilingual Lexicon Development.","1447":"A Sparse Probabilistic Model of User Preference Data.","1448":"Confused and Thankful: Multi-label Sentiment Classification of Health Forums.","1449":"Improvements to Boosting with Data Streams.","1450":"Revisiting the Epistemics of Protocol Correctness.","1451":"Shape-Based Analysis for Automatic Segmentation of Arabic Handwritten Text.","1452":"Collaborative Filtering with Users' Qualitative and Conditional Preferences.","1453":"Sensory Updates to Combat Path-Integration Drift.","1454":"Maintaining Preference Networks That Adapt to Changing Preferences.","1455":"Modeling Role-Based Agent Team.","1456":"Accelerated Backpropagation Learning: Extended Dynamic Parallel Tangent Optimization Algorithm.","1457":"Performance Measures in Classification of Human Communications.","1458":"Question Answering Summarization of Multiple Biomedical Documents.","1459":"Quantitatively Evaluating Formula-Variable Relevance by Forgetting.","1460":"A Novel Content Based Methodology for a Large Scale Multimodal Biometric System.","1461":"Incremental Neighborhood Graphs Construction for Multidimensional Databases Indexing.","1462":"Creating a Fuzzy Believer to Model Human Newspaper Readers.","1463":"A Probabilistic Framework for Detecting Unusual Events in Mobile Sensor Networks.","1464":"Learning to Measure Influence in a Scientific Social Network.","1465":"Real-Time Sentiment-Based Anomaly Detection in Twitter Data Streams.","1466":"Probabilistic TCP-net.","1467":"Rhetorical Figuration as a Metric in Text Summarization.","1468":"Resolving Elections with Partial Preferences Using Imputation.","1469":"The Use of NLP Techniques in Static Code Analysis to Detect Weaknesses and Vulnerabilities.","1470":"Unsupervised Multi-modal Learning.","1471":"Policies, Conversations, and Conversation Composition.","1472":"Automatic Text Segmentation for Movie Subtitles.","1473":"Who Is the Artificial Author?","1474":"On the Usage of Discourse Relations Across Texts with Different Readability Levels.","1475":"Dynamic Budget-Constrained Pricing in the Cloud.","1476":"Complete Axiomatization and Complexity of Coalition Logic of Temporal Knowledge for Multi-agent Systems.","1477":"Novel Game Playing Strategies Using Adaptive Data Structures.","1478":"On the Role of Possibility in Action Execution and Knowledge in the Situation Calculus.","1479":"Bayesian Networks to Model Pseudomonas aeruginosa Survival Mechanism and Identify Low Nutrient Response Genes in Water.","1480":"Stoic Ethics for Artificial Agents.","1481":"Evaluation of Rare Event Detection.","1482":"Parity: The Problem that Won't Go Away.","1483":"Reinforcement Learning for Real-World Control Applications.","1484":"Efficient Induction of Recursive Prolog Definitons.","1485":"Predicting Optimal Constraint Satisfaction Methods.","1486":"Reasoning about Unknown, Counterfactual, and Nondeterministic Actions in First-Order Logic.","1487":"LPMEME: A Statistical Method for Inductive Logic Programming.","1488":"Evaluating a Smart Recommender for an Evolving E-learning System: A Simulation-Based Study.","1489":"Decision Tree Learning Systems with Switching Evaluators.","1490":"Offensive Language Detection Using Multi-level Classification.","1491":"On Sketch Based Anonymization That Satisfies Differential Privacy Model.","1492":"Score Calibration for Optimal Biometric Identification.","1493":"A Chart Generator for Shake and Bake Machine Translation.","1494":"Overlap versus Imbalance.","1495":"Finding a Single, All, or the Most Probable Solution to a Finite or Non-finite Interval Algebra Network.","1496":"Distributed Data Mining vs. Sampling Techniques: A Comparison.","1497":"Balancing Robotic Teleoperation and Autonomy for Urban Search and Rescue Environments.","1498":"The IMAP Hybrid Method for Learning Gaussian Bayes Nets.","1499":"Planning and Learning in a Natural Resource Information System.","1500":"Notes on Generating Satisfiable SAT Instances Using Random Subgraph Isomorphism.","1501":"Argumentation-Based Reasoning with Inconsistent Knowledge Bases.","1502":"Phrase-Based Statistical Machine Translation for a Low-Density Language Pair.","1503":"Using Classifier Performance Visualization to Improve Collective Ranking Techniques for Biomedical Abstracts Classification.","1504":"Artificial Aging of Faces by Support Vector Machines.","1505":"Binary Decision Tree Using Genetic Algorithm for Recognizing Defect Patterns of Cold Mill Strip.","1506":"Constraint Directed Dynamic Backtracking.","1507":"The Frequency of Hedging Cues in Citation Contexts in Scientific Writing.","1508":"The Use of Increasingly Specific User Models in the Design of Mixed-Initiative Systems.","1509":"An Investigation of Grammar Design in Natural-Language Speech Recognition.","1510":"A Principled Modular Approach to Construct Flexible Conversation Protocols.","1511":"Resolvent Clause Weighting Local Search.","1512":"Evaluation Methods for Ordinal Classification.","1513":"A Semi-supervised Approach to Bengali-English Phrase-Based Statistical Machine Translation.","1514":"Improving Document Search Using Social Bookmarking.","1515":"Oracles and Assistants: Machine Learning Applied to Network Supervision.","1516":"Automatic Extraction of Lexical Relations from Analytical Definitions Using a Constraint Grammar.","1517":"Living with Constraints.","1518":"Modeling and Inference with Relational Dynamic Bayesian Networks.","1519":"Dueling CSP Representations: Local Search in the Primal versus Dual Constraint Graph.","1520":"Learning English Syllabification Rules.","1521":"Strings Clustering and Statistical Validation of Clusters.","1522":"Enumerating Unlabeled and Root Labeled Trees for Causal Model Acquisition.","1523":"Novice-Friendly Natural Language Generation Template Authoring Environment.","1524":"Opinion Learning without Emotional Words.","1525":"A Trainable Bracketer for Noun Modifiers.","1526":"Lessons Learned in the Development and Implementation of a Bilingual Nationally Accessible Knowledge-Based System.","1527":"Belief Rough Set Classifier.","1528":"Control of Constraint Weights for a 2D Autonomous Camera.","1529":"Back to the Future: Changing the Direction of Time to Discover Causality.","1530":"Artificial Intelligence and Human Brain Imaging.","1531":"Cycle-Cutset Sampling for Bayesian Networks.","1532":"On the Structure Model Interpretation of Wright's NESS Test.","1533":"Answer Formulation for Question-Answering.","1534":"Searching Solutions in the Crypto-arithmetic Problems: An Adaptive Parallel Genetic Algorithm Approach.","1535":"A Three-Level Cognitive Architecture for the Simulation of Human Behaviour.","1536":"Modeling Local Belief Revision in a Dynamic Reasoning System.","1537":"Populating a Knowledge Base from a Dictionary.","1538":"Searching for Poor Quality Machine Translated Text: Learning the Difference between Human Writing and Machine Translations.","1539":"Risk Neutral Calibration of Classifiers.","1540":"Learning Sentiments from Tweets with Personal Health Information.","1541":"Domain Adaptation Techniques for Machine Translation and Their Evaluation in a Real-World Setting.","1542":"A Strategic Reputation-Based Mechanism for Mobile Ad Hoc Networks.","1543":"Predicting Good Propagation Methods for Constraint Satisfaction.","1544":"Feature Selection Strategies for Text Categorization.","1545":"Experiences Building a Distributed Sensor Network.","1546":"Automatic Acquisition of Gender Information for Anaphora Resolution.","1547":"Privacy Compliance Enforcement in Email.","1548":"Image Transformation: Inductive Transfer between Multiple Tasks Having Multiple Outputs.","1549":"Verbs Speak Loud: Verb Categories in Learning Polarity and Strength of Opinions.","1550":"The Role of Nominalizations in Prepositional Phrase Attachment in GENIA.","1551":"A Reputation Model Framework for Artificial Societies: A Case Study in Child Vehicle Safety Simulation.","1552":"Towards an Ontology-Based Spatial Clustering Framework.","1553":"Automatic Identification of Parallel Documents With Light or Without Linguistic Resources.","1554":"Quick Spatial Outliers Detecting with Random Sampling.","1555":"Fast Protein Superfamily Classification Using Principal Component Null Space Analysis.","1556":"Reducing Position-Sensitive Subset Ranking to Classification.","1557":"Comparing Humans and Automatic Speech Recognition Systems in Recognizing Dysarthric Speech.","1558":"Improving Phenotype Name Recognition.","1559":"Problem-Solving Knowledge Mining from Users' Actions in an Intelligent Tutoring System.","1560":"An Efficient Resource Allocation Approach in Real-Time Stochastic Environment.","1561":"Partial Local FriendQ Multiagent Learning: Application to Team Automobile Coordination Problem.","1562":"Learning Dialogue POMDP Models from Data.","1563":"Wise Sliding Window Segmentation: A Classification-Aided Approach for Trajectory Segmentation.","1564":"Graph-Based Domain-Specific Semantic Relatedness from Wikipedia.","1565":"Automatic Semantic Web Annotation of Named Entities.","1566":"An Ensemble Method Based on AdaBoost and Meta-Learning.","1567":"Noun Sense Disambiguation with WordNet for Software Design Retrieval.","1568":"Speech Intention Classification with Multimodal Deep Learning.","1569":"Unsupervised Extraction of Diagnosis Codes from EMRs Using Knowledge-Based and Extractive Text Summarization Techniques.","1570":"RideSafe: Detecting Sexual Harassment in Rideshares.","1571":"Predicting Aggressive Responsive Behaviour Among People with Dementia.","1572":"Amalgamated Models for Detecting Duplicate Bug Reports.","1573":"Predicting the Number of Reported Bugs in a Software Repository.","1574":"Locating Influential Agents in Social Networks: Budget-Constrained Seed Set Selection.","1575":"Mixing ICI and CSI Models for More Efficient Probabilistic Inference.","1576":"Evaluation of a Failure Prediction Model for Large Scale Cloud Applications.","1577":"VecHGrad for Solving Accurately Tensor Decomposition.","1578":"Forecasting Seat Counts in the 2019 Canadian Federal Election Using Twitter.","1579":"Vehicle Traffic Estimation Using Weather and Calendar Data.","1580":"Question-Worthy Sentence Selection for Question Generation.","1581":"Towards Analyzing the Sentiments in the Fields of Automobiles and Real-Estates with Specific Focus on Arabic Online Reviews.","1582":"From Explicit to Implicit Entity Linking: A Learn to Rank Framework.","1583":"Unsupervised Monocular Depth Estimation CNN Robust to Training Data Diversity.","1584":"Quantified Coalition Logic of Knowledge, Belief and Certainty.","1585":"A Three-Way Decision Approach to Email Spam Filtering.","1586":"AI Consciousness.","1587":"What AI Can Do for Battle Management: A Report of the First AAAI Workshop on AI Applications to Battle Management.","1588":"A Learning Method for Developing PROAFTN Classifiers and a Comparative Study with Decision Trees.","1589":"Anomaly Detection and Prototype Selection Using Polyhedron Curvature.","1590":"Classification of Rare Recipes Requires Linguistic Features as Special Ingredients.","1591":"Automatic Polyp Segmentation Using Convolutional Neural Networks.","1592":"A Deep Neural Network for Counting Vessels in Sonar Signals.","1593":"Augmented Out-of-Sample Comparison Method for Time Series Forecasting Techniques.","1594":"Customer Segmentation and Churn Prediction in Online Retail.","1595":"Big Players: Emotion in Twitter Communities Tweeting About Global Warming.","1596":"Reinforcement Learning in a Physics-Inspired Semi-Markov Environment.","1597":"Partial Label Learning by Entropy Minimization.","1598":"Improving Classification Using Topic Correlation and Expectation Propagation.","1599":"Attending Knowledge Facts with BERT-like Models in Question-Answering: Disappointing Results and Some Explanations.","1600":"3D Depthwise Convolution: Reducing Model Parameters in 3D Vision Tasks.","1601":"Ensemble of Multiple Kernel SVM Classifiers.","1602":"Automatic Discovery of Network Applications: A Hybrid Approach.","1603":"Coordination of Subject Markers in Arabic and Typed Categorial Logic.","1604":"Dimensionality Reduction and Visualization by Doubly Kernelized Unit Ball Embedding.","1605":"The Importance of Being Discrete: Learning Classes of Actions and Outcomes through Interaction.","1606":"Agents with Genders for Inventory Planning in E-Management.","1607":"Search Techniques for Non-linear Constraint Satisfaction Problems with Inequalities.","1608":"\u00c9valuation d'un Syst\u00e8me pour le R\u00e9sum\u00e9 Automatique de Documents \u00c9lectroniques.","1609":"Planning Animations Using Cinematography Knowledge.","1610":"Stratified Partial-Order Logic Programming.","1611":"Combinatorial Auctions, Knapsack Problems, and Hill-Climbing Search.","1612":"Knowledge and Planning in an Action-Based Multi-agent Framework: A Case Study.","1613":"Imitation and Reinforcement Learning in Agents with Heterogeneous Actions.","1614":"Using Non Boolean Similarity Functions for Frequent Similar Pattern Mining.","1615":"Trajectory Generation with Player Modeling.","1616":"A Hybrid Schema for Systematic Local Search.","1617":"Subspace Mapping of Noisy Text Documents.","1618":"A Model for Reasoning about Interaction with Users in Dynamic, Time Critical Environments for the Application of Hospital Decision Making.","1619":"Somatic Copy Number Alteration-Based Prediction of Molecular Subtypes of Breast Cancer Using Deep Learning Model.","1620":"Enhanced Collaborative Filtering Through User-Item Subgroups, Particle Swarm Optimization and Fuzzy C-Means.","1621":"Generating Accurate Virtual Examples for Lifelong Machine Learning.","1622":"CB-DBSCAN: A Novel Clustering Algorithm for Adjacent Clusters with Different Densities.","1623":"Trace Equivalence Characterization Through Reinforcement Learning.","1624":"Safe Policy Learning with Constrained Return Variance.","1625":"A Shallow Learning - Reduced Data Approach for Image Classification.","1626":"Efficient Transformer-Based Sentence Encoding for Sentence Pair Modelling.","1627":"Learning Career Progression by Mining Social Media Profiles.","1628":"Inter and Intra Document Attention for Depression Risk Assessment.","1629":"DeepAnom: An Ensemble Deep Framework for Anomaly Detection in System Processes.","1630":"Predicting Commentaries on a Financial Report with Recurrent Neural Networks.","1631":"Instance Ranking and Numerosity Reduction Using Matrix Decomposition and Subspace Learning.","1632":"Options in Multi-task Reinforcement Learning - Transfer via Reflection.","1633":"Event Prediction in Social Graphs Using 1-Dimensional Convolutional Neural Network.","1634":"Logo Recognition Based on the Dempster-Shafer Fusion of Multiple Classifiers.","1635":"Statistical Parsing with Context-Free Filtering Grammar.","1636":"Exploratory Analysis of Co-Change Graphs for Code Refactoring.","1637":"Classifying Biomedical Abstracts Using Committees of Classifiers and Collective Ranking Techniques.","1638":"Relational Concepts and the Fourier Transform: An Empirical Study.","1639":"Sequential Instance-Based Learning.","1640":"A Hybrid Convergent Method for Learning Probabilistic Networks.","1641":"Grapheme Generation in Learning to Read English Words.","1642":"A Heuristic Incremental Modeling Approach to Course Timetabling.","1643":"The Effect of Genetic Operator Probabilities and Selection Strategies on the Performance of a Genetic Algorithm.","1644":"Automatic Frame Extraction from Sentences.","1645":"Accent Classification Using Support Vector Machine and Hidden Markov Model.","1646":"Finding Partitions for Learning Control of Dynamic Systems.","1647":"Establishing Logical Connectivity between Query Keywords and Database Contents.","1648":"Multi-attribute Exchange Market: Theory and Experiments.","1649":"An Iterative Hybrid Filter-Wrapper Approach to Feature Selection for Document Clustering.","1650":"Rank-Based Transformation in Measuring Semantic Relatedness.","1651":"Compiling the Lexicographic Inference Using Boolean Cardinality Constraints.","1652":"Maintaining Genetic Diversity in Genetic Algorithms through Co-evolution.","1653":"A Hybrid Genetic Algorithm for the Vehicle Routing Problem with Time Windows.","1654":"On the Complexity of VLSI-Friendly Neural Networks for Classification Problems.","1655":"Predicate Invention from a Few Examples.","1656":"Machine\/Deep Learning : wie lernen k\u00fcnstliche neuronale Netze?","1657":"Semantic Integration Is What You Do Before The Deep Learning","1658":"Mathematics for Machine Learning","1659":"Machine learning : a probabilistic perspective","1660":"Data mining : practical machine learning tools and techniques","1661":"Introduction to Machine Learning","1662":"Machine Learning in Material Characterization","1663":"Machine learning","1664":"Foundations of Machine Learning","1665":"Automatic Differentiation in Machine Learning: a Survey","1666":"Machine \/ Deep Learning : wie lernen k\u00fcnstliche neuronale Netze?","1667":"Machine Learning: Hands-On for Developers and Technical Professionals","1668":"Machine Learning and Cosmology","1669":"A Survey on Distributed Machine Learning","1670":"Cultivation of Crops using Machine Learning and Deep Learning","1671":"Detecting Spam Blogs: A Machine Learning Approach","1672":"Tikhonov, Ivanov and Morozov regularization for support vector machine learning","1673":"Einf\u00fchrung in Machine Learning mit Python","1674":"Efficient and Robust Automated Machine Learning","1675":"Generating Case-Oriented Training From Diagnostic Expert Systems","1676":"Machine learning for hackers","1677":"Machine Learning in Medicine A Primer","1678":"Designing for the Long Tail of Machine Learning","1679":"Detecting Phishing using Machine Learning","1680":"Machine Learning for Information Extraction in Informal Domains","1681":"Machine Learning for Observational Cosmology","1682":"Machine Learning: A Probabilistic Perspective","1683":"Comparative Study on Machine Learning Algorithms for Network Intrusion Detection System","1684":"Fake News Detection using Machine Learning","1685":"Music Genre Classification using Machine Learning","1686":"Machine learning and the physical sciences","1687":"Segmentation White Blood Cells by Machine Learning Algorithms","1688":"Modern Machine Learning and Particle Physics","1689":"josephmisiti\/awesome-machine-learning","1690":"Seven Myths in Machine Learning Research","1691":"TherML: Thermodynamics of Machine Learning","1692":"A novel approach for federated machine learning using Raspberry Pi","1693":"Prediction of Cervical Cancer using Machine Learning and Deep Learning Algorithms","1694":"Sentimental Emotion Analysis using Python and Machine Learning","1695":"Machine learning in epidemiology and health outcomes research","1696":"Collaborative filtering: A machine learning perspective","1697":"Automatic differentiation in machine learning: a survey","1698":"Covid 19 Prediction in India using Machine Learning","1699":"An Experimental Evaluation of Integrating Machine Learning with Knowledge Acquisition","1700":"Machine Learning Approach to Classify Twitter Hate Speech","1701":"Image Captioning Generator using Deep Machine Learning","1702":"Lecture Notes: Optimization for Machine Learning","1703":"Inferring Web QoE with Machine Learning from Encrypted Network Traffic","1704":"Estimating Evaporation using Machine Learning Based Ensemble Technique","1705":"Transfer Automatic Machine Learning","1706":"The Pervasiveness of Data Mining and Machine Learning","1707":"Analyzing Titanic Disaster using Machine Learning Algorithms","1708":"Foundations of Machine Learning.","1709":"Machine Learning Approach for Employee Attrition Analysis","1710":"Causality for Machine Learning","1711":"Amazon Product Review Sentiment Analysis with Machine Learning","1712":"Gold Price Prediction using Machine Learning","1713":"Human Emotion Recognition using Machine Learning","1714":"Heart Disease Prediction using Machine Learning Algorithm","1715":"Detection of Fake News using Machine Learning","1716":"Stock Market Prediction using Machine Learning","1717":"Broadcasting Forensics Using Machine Learning Approaches","1718":"Machine Learning Explainability for External Stakeholders","1719":"A first course in machine learning","1720":"Lung Cancer Detection using Machine Learning","1721":"Forecasting onion armyworm using tree-based machine learning models","1722":"Infrastructure for Usable Machine Learning: The Stanford DAWN Project","1723":"Tackling Climate Change with Machine Learning","1724":"Integrating Machine Learning with Human Knowledge","1725":"BPMN4sML: A BPMN Extension for Serverless Machine Learning. Technology\r\n  Independent and Interoperable Modeling of Machine Learning Workflows and\r\n  their Serverless Deployment Orchestration","1726":"A Survey on Prediction Techniques of Heart Disease using Machine Learning","1727":"A Few Useful Things to Know About Machine Learning","1728":"Quantum machine learning for data scientists","1729":"Credit Card Fraud Detection Using Hybrid Machine Learning Algorithm","1730":"Non-convex Optimization for Machine Learning","1731":"Machine Learning in Simulation-Based Analysis","1732":"Melanoma Skin Cancer Detection using Image Processing and Machine Learning","1733":"Machine Learning in Astronomy: a practical overview","1734":"Comparative Study of Machine Learning Algorithms for Rainfall Prediction","1735":"Basic Concepts in Machine Learning","1736":"Developing Bug-Free Machine Learning Systems With Formal Mathematics","1737":"Dynamic Control Flow in Large-Scale Machine Learning","1738":"Troubling Trends in Machine Learning Scholarship","1739":"Machine Learning in the Field of Optical Character Recognition OCR","1740":"High energy nuclear physics meets Machine Learning","1741":"Face Recognition Based Attendance System using Machine Learning","1742":"A Machine Learning Approach for Instance Matching Based on Similarity Metrics","1743":"Anticipation of Forged Video Evidence using Machine Learning","1744":"Survey on Key Phrase Extraction using Machine Learning Approaches","1745":"The Marginal Value of Adaptive Gradient Methods in Machine Learning","1746":"A high-bias, low-variance introduction to Machine Learning for\r\n  physicists","1747":"MXNet: A Flexible and Efficient Machine Learning Library for\r\n  Heterogeneous Distributed Systems","1748":"User Personality Prediction on Facebook Social Media using Machine Learning","1749":"Optimization Methods for Large-Scale Machine Learning","1750":"Introduction to Tensor Decompositions and their Applications in Machine\r\n  Learning","1751":"A Machine Learning Perspective for the Semantic Web","1752":"Adversarial Machine Learning","1753":"Machine learning of temporal relations","1754":"A Study on Credit Card Fraud Detection using Machine Learning","1755":"Machine learning in automated text categorization","1756":"Towards A Rigorous Science of Interpretable Machine Learning","1757":"Active learning machine learns to create new quantum experiments","1758":"Thumbs up? Sentiment Classification using Machine Learning Techniques","1759":"Machine-Learning Research: Four Current Directions","1760":"Compression and Machine Learning: A New Perspective on Feature Space Vectors","1761":"Machine Learning in Automated Text Categorization","1762":"Machine Learning-Friendly Biomedical Datasets for Equivalence and\r\n  Subsumption Ontology Matching","1763":"Introduction to Machine Learning (Adaptive Computation and Machine Learning)","1764":"A Deep Analysis on Prevailing Spam Mail Filteration Machine Learning Approaches","1765":"Applying machine learning techniques to predict the properties of energetic materials","1766":"Enabling Air Pollution Prediction through IoT and Machine Learning","1767":"A Comparative Study on Mushroom Classification using Supervised Machine Learning Algorithms","1768":"Reconciling modern machine learning practice and the bias-variance\r\n  trade-off","1769":"Machine Learning","1770":"Data-driven Advice for Applying Machine Learning to Bioinformatics Problems.","1771":"Machine Learning as an Experimental Science","1772":"Machine Learning Methods for Histopathological Image Analysis","1773":"Asynchronous Byzantine Machine Learning (the case of SGD)","1774":"3Q: Machine learning and climate modeling","1775":"Asynchronous Byzantine Machine Learning","1776":"Stop Explaining Black Box Machine Learning Models for High Stakes\r\n  Decisions and Use Interpretable Models Instead","1777":"Machine Learning in Virtualization: Estimate a Virtual Machine's Working Set Size.","1778":"Informed Machine Learning -- A Taxonomy and Survey of Integrating Knowledge into Learning Systems","1779":"Informed Machine Learning -- A Taxonomy and Survey of Integrating\r\n  Knowledge into Learning Systems","1780":"VQE-generated Quantum Circuit Dataset for Machine Learning","1781":"SparCML: High-Performance Sparse Communication for Machine Learning","1782":"TensorNetwork: A Library for Physics and Machine Learning","1783":"A New Perspective on Machine Learning: How to do Perfect Supervised\r\n  Learning","1784":"Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation.","1785":"Model Evaluation, Model Selection, and Algorithm Selection in Machine\r\n  Learning","1786":"Structured machine learning: the next ten years.","1787":"Fairness-Aware Machine Learning: Practical Challenges and Lessons Learned","1788":"Pen and Paper Exercises in Machine Learning","1789":"Intelligent Fall Detection Using Statistical Features and Machine Learning","1790":"Machine learning of neuroimaging for assisted diagnosis of cognitive impairment and dementia: A systematic review","1791":"Galaxy Zoo: Reproducing Galaxy Morphologies Via Machine Learning","1792":"Interpretable Machine Learning: Fundamental Principles and 10 Grand\r\n  Challenges","1793":"How to avoid machine learning pitfalls: a guide for academic researchers","1794":"Detection of URL Based Phishing Websites using Machine Learning","1795":"Pattern recognition and machine learning","1796":"A Study on Prediction of Share Price by Using Machine Learning LSTM Model","1797":"Predicting the Level of Crowdfunding Outcome in Africa A Supervised Machine Learning Approach","1798":"Machine learning quantum states in the NISQ era","1799":"Reconciling modern machine learning and the bias-variance trade-off","1800":"Lifelong Machine Learning, Second Edition","1801":"A Comprehensive Survey of Loss Functions in Machine Learning","1802":"Causal Machine Learning: A Survey and Open Problems","1803":"Programming by demonstration using version space algebra","1804":"Comparative Study of Cyberbullying Detection using Different Machine Learning Algorithms","1805":"Discover the GellMann-Okubo formula with machine learning","1806":"Credit Cards Frauds and Cybersecurity Threats Machine Learning Detection Algorithms as Countermeasures","1807":"A Separability-Entanglement Classifier via Machine Learning","1808":"What you see is what you can change: Human-centered machine learning by interactive visualization","1809":"Clinical Depression Detection Using Speech Feature With Machine Learning Approach","1810":"Autonomous Machine Learning.","1811":"Machine and Deep Learning Applications in Particle Physics","1812":"Monte Carlo Gradient Estimation in Machine Learning","1813":"Machine Learning on Human Connectome Data from MRI","1814":"A review on Machine Learning Techniques for Neurological disorders estimation by Analyzing EEG Waves","1815":"Role of Advanced Machine Learning Techniques and Deep Learning Approach Based Decision Support System for Accurate Diagnosis of Severe Respiratory Diseases","1816":"Proof-of-Learning: A Blockchain Consensus Mechanism Based on Machine Learning Competitions","1817":"Using Experts' Opinions in Machine Learning Tasks","1818":"PrivPy: Enabling Scalable and General Privacy-Preserving Machine\r\n  Learning","1819":"TF-Replicator: Distributed Machine Learning for Researchers","1820":"Model-based machine learning","1821":"Speech Emotion Recognition Using Deep Neural Network and Extreme Learning Machine","1822":"COVID 19 Outbreak Prediction and Forecasting in Bangladesh using Machine Learning Algorithm","1823":"Incremental learning from noisy data","1824":"Making large-scale support vector machine learning practical","1825":"Machine Learning: Genetic Algorithms Part 1 (Javascript)","1826":"PDEBENCH: An Extensive Benchmark for Scientific Machine Learning","1827":"Credit card fraud detection and classification by deep learning and machine learning","1828":"Interpretable machine learning: definitions, methods, and applications","1829":"Automated License Plate detection and Speed estimation of Vehicle Using Machine Learning Haar Classifier Algorithm","1830":"Exploration of Parameter Spaces Assisted by Machine Learning","1831":"Drowsiness and Alcohol Detection for Accident Prevention using Machine Learning","1832":"Era of Sociology News Rumors News Detection using Machine Learning","1833":"Machine Learning applications in Expert Systems and\n                 Information Retrieval","1834":"Hidden Technical Debt in Machine Learning Systems.","1835":"Practical Bayesian Optimization of Machine Learning Algorithms.","1836":"Machine Learning Models in Stock Market Prediction","1837":"Machine Learning and Cosmological Simulations II: Hydrodynamical\r\n  Simulations","1838":"TargetSpy: a supervised machine learning approach for microRNA target prediction","1839":"Therapeutics Data Commons: Machine Learning Datasets and Tasks for\r\n  Therapeutics","1840":"Machine Learning Operations (MLOps): Overview, Definition, and\r\n  Architecture","1841":"Auto-sklearn: Efficient and Robust Automated Machine Learning.","1842":"A Frame Study on Sentiment Analysis of Hindi Language Using Machine Learning","1843":"Extreme Learning Machine for Real Time Recognition of Brazilian Sign Language","1844":"E-Mail Security Algorithm to Filter Out Spam E-mails using Machine Learning","1845":"Theoretical Impediments to Machine Learning With Seven Sparks from the\r\n  Causal Revolution","1846":"Machine Learning in Systems Biology. Proc. 3rd International Workshop","1847":"Building attack detection system base on machine learning","1848":"Dynamical and Machine Learning Hybrid Seasonal Prediction of Summer Rainfall in China","1849":"AttentionFire_v1.0: interpretable machine learning fire model for burned area predictions over tropics","1850":"Model-Agnostic Interpretability of Machine Learning","1851":"UCI Repository of machine learning databases","1852":"Machine Learning for the Detection of Oil Spills in Satellite Radar Images.","1853":"C4.5: programs for machine learning","1854":"Similarity-based Classification: Concepts and Algorithms","1855":"Machine learning and feature selection for drug response prediction in precision oncology applications","1856":"Supervised Machine Learning Techniques: An Overview with Applications to\r\n  Banking","1857":"Solver Recommendation For Transport Problems in Slabs Using Machine\r\n  Learning","1858":"Analysis of Machine Learning and Statistics Tool Box (Matlab R2016) over Novel Benchmark Cervical Cancer Database","1859":"NetKet: A Machine Learning Toolkit for Many-Body Quantum Systems","1860":"Classes of Kernels for Machine Learning: A Statistics Perspective.","1861":"Machine learning and microsimulation techniques on the prognosis of dementia: A systematic literature review","1862":"Learning and Revising User Profiles: The Identification of Interesting Web Sites","1863":"Machine learning confronts the elephant in the room","1864":"The Ladder: A Reliable Leaderboard for Machine Learning Competitions","1865":"The zoo of Fairness metrics in Machine Learning","1866":"OpenLUR: Off-the-shelf air pollution modeling with open features and machine learning","1867":"Machine Learning meets Data-Driven Journalism: Boosting International\r\n  Understanding and Transparency in News Coverage","1868":"Machine learning and bibliographic data universe : assessing efficacy of backend algorithms in Annif through retrieval metrics","1869":"Towards Machine Learning on the Semantic Web","1870":"Putting research-based machine learning solutions for subject indexing into practice","1871":"Paradoxes in Fair Machine Learning.","1872":"Bundle Methods for Machine Learning.","1873":"Effective Parallelisation for Machine Learning.","1874":"Socially guided machine learning","1875":"Smart Manufacturing: Monitoring of Tool Wear using Machine Learning Methods","1876":"Dual Learning for Machine Translation.","1877":"Predict the Covid 19 using Machine Learning Model from a Symptoms of the Body","1878":"Instance spaces for machine learning classification","1879":"Real-Time Machine Learning: The Missing Pieces","1880":"Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning","1881":"Second-Order Stochastic Optimization for Machine Learning in Linear Time","1882":"Supervised Machine Learning: A Review of Classification Techniques","1883":"Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent","1884":"Machine learning models for lung cancer classification using array comparative genomic hybridization.","1885":"Unsupervised Neural Machine Translation for Low-Resource Domains via Meta-Learning","1886":"Motion Learning Toolbox \u2013 A Python library for preprocessing of XR motion tracking data for machine learning applications","1887":"Rethinking statistical learning theory: learning using statistical invariants","1888":"TPOT: A Tree-Based Pipeline Optimization Tool for Automating Machine Learning.","1889":"A machine learning approach to coreference resolution of noun phrases","1890":"A machine learning classifier trained on cancer transcriptomes detects NF1 inactivation signal in glioblastoma","1891":"Using machine learning to compress the matter transfer function $T(k)$","1892":"OpenML: An R Package to Connect to the Networked Machine Learning\r\n  Platform OpenML","1893":"Gaussian Processes for Machine Learning","1894":"Bridging Machine Learning and Logical Reasoning by Abductive Learning.","1895":"Machine Learning Techniques","1896":"MLtuner: System Support for Automatic Machine Learning Tuning","1897":"Crop Prediction System using Machine Learning","1898":"Learning Generalizable Device Placement Algorithms for Distributed Machine Learning.","1899":"Scikit-learn: Machine learning in Python","1900":"In Pursuit of Interpretable, Fair and Accurate Machine Learning for\r\n  Criminal Recidivism Prediction","1901":"Machine learning as a new horizon for colorectal cancer risk prediction? A systematic review","1902":"An Introduction to MCMC for Machine Learning","1903":"Machine Learning and Software Engineering","1904":"Scaling Up Machine Learning For Quantum Field Theory with Equivariant\r\n  Continuous Flows","1905":"Pattern Recognition and Machine Learning","1906":"Machine Learning Classifiers as an Aid for Geographical Survey","1907":"An Introduction to MCMC for Machine Learning.","1908":"Data-Driven Machine Learning Techniques for Self-healing in Cellular\r\n  Wireless Networks: Challenges and Solutions","1909":"Experimental kernel-based quantum machine learning in finite feature\r\n  space","1910":"Do no harm: a roadmap for responsible machine learning for health care","1911":"A survey on datasets for fairness\u2010aware machine learning","1912":"Dumb Meaning: Machine Learning and Artificial Semantics","1913":"The CAMELS project: Cosmology and Astrophysics with MachinE Learning\r\n  Simulations","1914":"A Study of the Learnability of Relational Properties (Model Counting\r\n  Meets Machine Learning)","1915":"Learning Logical Definitions from Relations","1916":"MAUL: Machine Agent User Learning","1917":"Applying Inductive Machine Learning to Soil Salinity Analysis","1918":"Machine Learning for Sequential Data: A Review","1919":"Expressive Ontology Learning as Neural Machine Translation","1920":"Survey on Formal Concept Analysis Based Supervised Classification Techniques","1921":"How do visual explanations foster end users\\textquotesingle appropriate trust in machine learning?","1922":"Evolutionary Machine Learning: A Survey","1923":"Multiagent Systems: A Survey from a Machine Learning Perspective","1924":"Learning a Multi-View Stereo Machine.","1925":"Machine Learning-based Real-time Estimation of Quality of Experience from Encrypted Video Streaming Traffic","1926":"Informed Machine Learning for Industry.","1927":"A few useful things to know about machine learning","1928":"Machine Learning Classification over Encrypted Data.","1929":"Safe Exploration for Interactive Machine Learning.","1930":"Invariant kernel functions for pattern analysis and machine learning","1931":"Interpretable Machine Learning for Science with PySR and\r\n  SymbolicRegression.jl","1932":"Machine Learning for Video-Based Rendering.","1933":"Efficient and Robust Automated Machine Learning.","1934":"Measures of distortion for machine learning.","1935":"Combining Machine Learning and Human Judgment in Author Disambiguation","1936":"Software effort estimation using machine learning methods","1937":"Machine-Learning media bias","1938":"Towards Automated Machine Learning: Evaluation and Comparison of AutoML\r\n  Approaches and Tools","1939":"Machine learning as an experimental science (revisited)","1940":"A user-oriented splog filtering based on a machine learning","1941":"The use of machine learning in the study of suicidal and non-suicidal self-injurious thoughts and behaviors: A systematic review","1942":"Foolbox: A Python toolbox to benchmark the robustness of machine\r\n  learning models","1943":"Tunability: Importance of Hyperparameters of Machine Learning Algorithms.","1944":"Learned multiphysics inversion with differentiable programming and\r\n  machine learning","1945":"Understanding Machine Learning - From Theory to Algorithms.","1946":"Density Ratio Estimation in Machine Learning.","1947":"Enhancing Technical Simulations with Machine Learning.","1948":"A new approach of clustering based machine-learning algorithm","1949":"Real or Fake? Learning to Discriminate Machine from Human Generated Text","1950":"Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics","1951":"Machine Learning for Aerodynamic Uncertainty Quantification.","1952":"Introduction to Statistical Machine Learning","1953":"Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies","1954":"Developing Open Source Educational Resources for Machine Learning and Data Science","1955":"TFX: A TensorFlow-Based Production-Scale Machine Learning Platform","1956":"Data Mining: Practical Machine Learning Tools and Techniques","1957":"Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series).","1958":"Disease Prediction using Machine Learning","1959":"Machine Learning in Electronic Quantum Matter Imaging Experiments","1960":"Malware Analysis","1961":"Faster and Better: A Machine Learning Approach to Corner Detection","1962":"Critical Tools for Machine Learning: Situating, Figuring, Diffracting, Fabulating Machine Learning Systems Design","1963":"Machine Learning Anwendungen in der betrieblichen Praxis : Praktische Empfehlungen zur betrieblichen Mitbestimmung","1964":"A Performance Survey of Public Domain Machine Learning Algorithms","1965":"Strategies and Principles of Distributed Machine Learning on Big Data","1966":"Machine learning: Trends, perspectives, and prospects","1967":"Bayesian Reasoning and Machine Learning","1968":"Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)","1969":"A Case Study on Machine Learning for Synthesizing Benchmarks","1970":"Machine learning for user modeling","1971":"Dealing with Nuisance Parameters using Machine Learning in High Energy\r\n  Physics: a Review","1972":"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\r\n  Algorithms","1973":"Balancing Competing Objectives with Noisy Data: Score-Based Classifiers\r\n  for Welfare-Aware Machine Learning","1974":"Learning by Abstraction: The Neural State Machine.","1975":"GENO - GENeric Optimization for Classical Machine Learning.","1976":"A Machine Learning Approach to Conjoint Analysis.","1977":"Incremental and Decremental Support Vector Machine Learning.","1978":"API design for machine learning software: experiences from the\r\n  scikit-learn project","1979":"Probabilistic Matrix Factorization for Automated Machine Learning.","1980":"ML-Schema: Exposing the Semantics of Machine Learning with Schemas and Ontologies","1981":"Machine Learning for Programming","1982":"Causal Learning and Machine Learning.","1983":"Machine Learning as a universal tool for quantitative investigations of\r\n  phase transition","1984":"Human Expectations and Perceptions of Learning in Machine Teaching","1985":"Unprovability comes to machine learning","1986":"Implementation of a Heart Disease Risk Prediction Model Using Machine Learning","1987":"Machine Learning : kurz & gut","1988":"Machine Learning Inverse Problem for Topological Photonics","1989":"Rank-based univariate feature selection methods on machine learning classifiers for code smell detection","1990":"Development of a Hybrid Dynamic Expert System for the Diagnosis of Peripheral Diabetes and Remedies using a Rule Based Machine Learning Technique","1991":"Using machine learning to create high-efficiency freeform illumination\r\n  design tools","1992":"Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)","1993":"Multi-dimensional Discrimination in Law and Machine Learning - A Comparative Overview","1994":"Effective Information Extraction Framework for Heterogeneous Clinical Reports Using Online Machine Learning and Controlled Vocabularies","1995":"Machine Learning into Metaheuristics","1996":"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting","1997":"Interactive Machine Learning (iML)","1998":"Learning to learn with the informative vector machine","1999":"Topic Space Trajectories: A case study on machine learning literature","2000":"Machine Learning and Chaos Theory in Agriculture.","2001":"Data-driven Advice for Applying Machine Learning to Bioinformatics Problems","2002":"The Future of Machine Learning: Supervised, Unsupervised and Reinforcement Learning","2003":"A learning machine: I","2004":"ECG multi-class classification using neural network as machine learning model","2005":"Learning from correlation with extreme learning machine.","2006":"Selective data acquisition for machine learning","2007":"Machine Learning Guided 3D Image Recognition for Carbonate Pore and Mineral Volumes Determination","2008":"Convolutional LSTM Network: A Machine Learning Approach for\r\n  Precipitation Nowcasting","2009":"Machine learning models in electronic health records can outperform conventional survival models for predicting patient mortality in coronary artery disease.","2010":"Integrating Different Machine Learning Methods to Support Search in Cross-domain\r\n    Information Sources - the Project Awake","2011":"Performance Evaluation of Machine Learning Models to Predict Heart Attack","2012":"Effective injury prediction in professional soccer with GPS data and machine learning","2013":"Second-Order Optimization for Non-Convex Machine Learning: An Empirical\r\n  Study","2014":"A Machine Learning Approach to Pedestrian Detection for Autonomous Vehicles Using High-Definition 3D Range Data","2015":"A Comparison of Machine Learning Algorithms for the Surveillance of\r\n  Autism Spectrum Disorder","2016":"Introduction to machine learning.","2017":"Unveiling two types of local order in liquid water using machine\r\n  learning","2018":"Predicting Pre-Owned Car Prices Using Machine Learning","2019":"A Meta-Analysis of Overfitting in Machine Learning.","2020":"Machine Learning for Bioclimatic Modelling","2021":"A Machine Learning Approach to Predict Chemical Reactions.","2022":"Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent.","2023":"Snap ML: A Hierarchical Framework for Machine Learning.","2024":"Feminist epistemology for machine learning systems design","2025":"Thumbs up? Sentiment Classification using Machine Learning \n  Techniques","2026":"Understanding machine learning : from theory to algorithms","2027":"Machine Learning Paradigms - Advances in Learning Analytics.","2028":"Relative loss bounds for multidimensional regression problems","2029":"Multi-wavelength properties of radio and machine-learning identified\r\n  counterparts to submillimeter sources in S2COSMOS","2030":"Waffles: A Machine Learning Toolkit","2031":"Machine Learning Anwendungen in der betrieblichen Praxis.","2032":"An Integrated Machine Learning Framework for Effective Prediction of Cardiovascular Diseases","2033":"Machine Learning in Natural Language Processing","2034":"Learning Curves in Machine Learning.","2035":"Rapid Prediction of Bacterial Heterotrophic Fluxomics Using Machine Learning and Constraint Programming.","2036":"Querying the Web with Statistical Machine Learning","2037":"Machine Learning-Friendly Biomedical Datasets for\u00a0Equivalence and\u00a0Subsumption Ontology Matching","2038":"Predicting the risk of suffering chronic social exclusion with machine learning","2039":"Machine Learning Paradigms: Artificial Immune Systems and their Applications in Software Personalization","2040":"Machine Learning: A Dark Side of Cancer Computing","2041":"A Few Useful Things to Know about Machine Learning","2042":"A Machine Learning Approach to Student Modelling","2043":"Machine Learning for Health ( ML4H ) 2019 : What Makes Machine Learning in Medicine Different?","2044":"Intervening or associated? Machine learning classification of redshifted\r\n  H I 21-cm absorption","2045":"Physics-informed machine learning","2046":"Adaptive Extreme Learning Machine for Recurrent Beta-basis Function\r\n  Neural Network Training","2047":"Improving fairness in machine learning systems: What do industry\r\n  practitioners need?","2048":"A Review of the Gumbel-max Trick and its Extensions for Discrete\r\n  Stochasticity in Machine Learning","2049":"Machine learning of poorly predictable ecological\n                 data","2050":"Kernel methods in machine learning","2051":"Predictive Analytics with Microsoft Azure Machine Learning","2052":"Making Learning Physical - Machine Intelligence and Quantum Resources.","2053":"Curiosity and Intrinsic Motivation for Autonomous Machine Learning.","2054":"MEX Interfaces: Automating Machine Learning Metadata Generation","2055":"Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations","2056":"SIMLR: Machine Learning inside the SIR model for COVID-19 Forecasting","2057":"Jet Substructure at the Large Hadron Collider: A Review of Recent\r\n  Advances in Theory and Machine Learning","2058":"Discovering user communities on the Internet using unsupervised machine learning techniques","2059":"Improving reproducibility in machine learning research: a report from the NeurIPS 2019 reproducibility program","2060":"Glossary of Terms","2061":"Gaussian processes for machine learning.","2062":"An Accelerated Communication-Efficient Primal-Dual Optimization\r\n  Framework for Structured Machine Learning","2063":"Adversarial Approach to Persona Based Dialogue Agents","2064":"Scikit-learn: Machine Learning in Python","2065":"Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligentsystems","2066":"Machine Learning to Decipher the Astrophysical Processes at Cosmic Dawn","2067":"Automated Analysis of Reflection in Writing: Validating Machine Learning Approaches","2068":"Map-Reduce for Machine Learning on Multicore.","2069":"Lifelong Machine Learning","2070":"Classifier systems and the animat problem","2071":"Timeliness online regularized extreme learning machine.","2072":"Prediction of Synergism from Chemical-Genetic Interactions by Machine Learning","2073":"Meta-Learning for Low-Resource Neural Machine Translation","2074":"A Scalable Machine Learning Approach to Go.","2075":"Tensor Comprehensions: Framework-Agnostic High-Performance Machine\r\n  Learning Abstractions","2076":"A Winnow based approach to Context-Sensitive Spelling Correction","2077":"RG inspired Machine Learning for lattice field theory","2078":"Machine Learning in IoT for Autonomous, Adaptive Sensing.","2079":"Critical Tools for Machine Learning: Working with Intersectional Critical Concepts in Machine Learning Systems Design","2080":"Learning at the Knowledge Level","2081":"Machine Learning for Neuroimaging with Scikit-Learn","2082":"Common Innovation in e-learning, Machine Learning and Humanoid","2083":"Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments.","2084":"Contamination Attacks and Mitigation in Multi-Party Machine Learning.","2085":"An Empirical Bayes Approach to Optimizing Machine Learning Algorithms.","2086":"Machine Learning that Matters","2087":"Machine Learning Based Audio Synthesis: Blessing and Curse?","2088":"A Multi-Batch L-BFGS Method for Machine Learning.","2089":"A Step Toward Quantifying Independently Reproducible Machine Learning Research.","2090":"Making AI Forget You: Data Deletion in Machine Learning.","2091":"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation","2092":"q-means: A quantum algorithm for unsupervised machine learning.","2093":"Communication Efficient Distributed Machine Learning with the Parameter Server.","2094":"Feature Selection for Machine Learning","2095":"Principles and theory for data mining and machine learning","2096":"Machine learning assisted readout of trapped-ion qubits","2097":"The logic of learning: a brief introduction to Inductive Logic Programming","2098":"Effectively using unsupervised machine learning in next generation\r\n  astronomical surveys","2099":"Speech Analytics Based on Machine Learning.","2100":"A Stratigraphic Prediction Method Based on Machine Learning","2101":"Hidden Physics Models: Machine Learning of Nonlinear Partial\r\n  Differential Equations","2102":"BLAZE: Blazing Fast Privacy-Preserving Machine Learning.","2103":"Beyond federated learning: On confidentiality-critical machine learning applications in industry","2104":"An Overview of Recent Machine Learning Strategies in Data Mining","2105":"Scaling Machine Learning as a Service.","2106":"Detecting communities and their evolutions in dynamic social networks\u2014a\u00a0Bayesian approach","2107":"Computational Machine Learning in Theory and Praxis","2108":"A Topology Layer for Machine Learning.","2109":"Reproducibility standards for machine learning in the life sciences","2110":"Diagnostic of pathology on the vertebral column machine learning - Cluster K-nearest Neighbor (CKNN) part (I)","2111":"Machine learning - a probabilistic perspective.","2112":"Statistical and machine-learning data mining techniques for better predictive modeling and analysis of big data, second edition","2113":"Delayed Impact of Fair Machine Learning.","2114":"Orthogonal Machine Learning: Power and Limitations.","2115":"Privacy Risks of Securing Machine Learning Models against Adversarial\r\n  Examples","2116":"Machine learning methods without tears: a primer for ecologists.","2117":"International journal of machine learning and cybernetics.","2118":"Homo-ELM: fully homomorphic extreme learning machine.","2119":"Unsupervised extreme learning machine with representational features.","2120":"Automated classification of synaptic vesicles in electron tomograms of C. elegans using machine learning","2121":"Review: machine learning techniques applied to cybersecurity.","2122":"SystemML: Declarative machine learning on MapReduce","2123":"Machine learning for medical imaging: methodological failures and recommendations for the future","2124":"PersoNews: A Personalized News Reader Enhanced by Machine Learning and Semantic Filtering","2125":"Evolving a Learning Machine by Genetic Programming","2126":"Modern Machine Learning: More with Less, Cheaper and Better.","2127":"Keynote: Machine Learning in Engineering - A View from Industry.","2128":"Statistical Machine Learning Makes Automatic Control Practical for Internet Datacenters.","2129":"The current and future uses of machine learning in ecosystem service research","2130":"Applied Data Science: Using Machine Learning for Alarm Verification.","2131":"Associating Fundamental Features with Technical Indicators for Analyzing Quarterly Stock Market Trends Using Machine Learning Algorithms","2132":"An Attribute-Value Machine Learning Approach To Student Modelling","2133":"Practical Machine Learning for Software Engineering and Knowledge Engineering","2134":"Learning New Physics from a Machine","2135":"Matching Networks for One Shot Learning","2136":"Semi-supervised low rank kernel learning algorithm via extreme learning machine.","2137":"Intrusion Detection Using Machine Learning in Databases","2138":"Asynchronous Parallel Stochastic Gradient Descent - A Numeric Core for\r\n  Scalable Distributed Machine Learning Algorithms","2139":"Development and validation of a machine learning algorithm and hybrid system to predict the need for life-saving interventions in trauma patients","2140":"Determinantal Point Processes for Machine Learning.","2141":"Non-convex Optimization for Machine Learning.","2142":"Experimental identification of the second-order non-Hermitian skin effect with physics-graph-informed machine learning","2143":"A Machine Learning Approach to Predict Missing Flux Densities in\r\n  Multi-band Galaxy Surveys","2144":"Phase Transitions in Machine Learning.","2145":"Trust in AutoML: Exploring Information Needs for Establishing\r\nTrust in Automated Machine Learning Systems","2146":"Java-ML: A Machine Learning Library","2147":"Machine Learning Scan and Application in SUSY","2148":"Machine Learning and Game Playing.","2149":"Machine Learning Paradigms: Advances in Data Analytics.","2150":"Medicine: Applications of Machine Learning.","2151":"Machine Learning for IT Security.","2152":"Learning Weighted Top-$k$ Support Vector Machine.","2153":"Sentiment Analysis of Restaurant Reviews Using Machine Learning Techniques","2154":"Data Mining and Machine Learning in Astronomy","2155":"Uncovering social spammers: social honeypots + machine learning","2156":"Fault Tolerance in Iterative-Convergent Machine Learning.","2157":"Evaluation of Synthetic Data for Privacy-Preserving Machine Learning.","2158":"Adaptive serious educational games using machine learning.","2159":"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting.","2160":"Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning.","2161":"Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning.","2162":"Multimodal Machine Learning for Automated ICD Coding.","2163":"Sensitive time series prediction using extreme learning machine.","2164":"NBWELM: naive Bayesian based weighted extreme learning machine.","2165":"Network embedding based on deep extreme learning machine.","2166":"An experimental study on symbolic extreme learning machine.","2167":"Enhancing Machine Learning based QoE Prediction by Ensemble Models","2168":"Sparse Bayesian Learning and the Relevance Vector Machine","2169":"Using Machine Learning to Break Visual Human Interaction Proofs (HIPs).","2170":"Machine Learning Applied to Perception: Decision Images for Gender Classification.","2171":"A Stability-based Validation Procedure for Differentially Private Machine Learning.","2172":"On Model Parallelization and Scheduling Strategies for Distributed Machine Learning.","2173":"Discrepancy, Coresets, and Sketches in Machine Learning.","2174":"Unsupervised feature learning with sparse Bayesian auto-encoding based extreme learning machine.","2175":"Sparse Bayesian learning and the relevance vector machine","2176":"Exploring the value of machine learning for weighted multi-model combination of an ensemble of global hydrological models","2177":"Large scale image annotation: learning\u00a0to\u00a0rank with\u00a0joint word-image embeddings","2178":"The Marginal Value of Adaptive Gradient Methods in Machine Learning.","2179":"Estimation of matrix trace using machine learning","2180":"Machine Learning for Data Streams with Practical Examples in MOA","2181":"HyParSVM - A New Hybrid Parallel Software for Support Vector Machine Learning on SMP Clusters","2182":"Conceptual Foundations on Debiasing for Machine Learning-Based Software","2183":"Data Mining: Practical Machine Learning Tools and Techniques with\tJava Implementations","2184":"Simulative Evaluation of (In-)Confident Machine Learning for User-based Active Learning","2185":"Using classification tree analysis to generate propensity score weights.","2186":"Machine learning classifiers and fMRI: A tutorial\n                 overview","2187":"Genetic Programming Applied to Othello: Introducing\n                 Students to Machine Learning Research","2188":"Pattern Recognition and Machine Learning (Information Science and Statistics)","2189":"Universal Differential Equations for Scientific Machine Learning","2190":"Machine Learning - The Art and Science of Algorithms that Make Sense of Data.","2191":"Selection of relevant features in machine learning","2192":"Support vector machine active learning with applications to text classification","2193":"Integrating Machine Learning With Knowledge Acquisition Through Direct Interaction With Domain Experts","2194":"Guest Editors Introduction: Machine Learning and Natural Language","2195":"Classification of Astrophysics Journal Articles with Machine Learning to\r\n  Identify Data for NED","2196":"Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation","2197":"How I failed machine learning in medical imaging -- shortcomings and\r\n  recommendations","2198":"Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics","2199":"Taming Non-Linear Dynamics and Turbulence with Machine Learning Control.","2200":"Privacy Aware Machine Learning and the \"Right to be Forgotten\".","2201":"Machine Learning Classifier Performance as an Indicator for Data Acquisition Regimes in Geographical Field Surveys","2202":"Unsupervised and supervised machine learning in user modeling for intelligent learning environments","2203":"Can 5G and Machine Learning Replace the Global Positioning System?","2204":"Asynchronous Byzantine Machine Learning (the case of SGD).","2205":"Identifying and Correcting Label Bias in Machine Learning.","2206":"Python (deep learning and machine learning) for EEG signal processing on\r\n  the example of recognizing the disease of alcoholism","2207":"Optimal classification trees","2208":"User-Centred Evaluation for Machine Learning.","2209":"Evaluation of Interactive Machine Learning Systems.","2210":"Incremental extreme learning machine based on deep feature embedded.","2211":"An optimization algorithm guided by a machine learning approach.","2212":"Two swarm intelligence approaches for tuning extreme learning machine.","2213":"Fairness in Machine Learning: Lessons from Political Philosophy.","2214":"A nonlinear kernel support matrix machine for matrix learning.","2215":"Two-stage extreme learning machine for high-dimensional data.","2216":"Person detector for different overhead views using machine learning.","2217":"Personalized and Private Peer-to-Peer Machine Learning.","2218":"Transfer learning for the probabilistic classification vector machine.","2219":"Quantum Machine Learning.","2220":"ADVISOR: A Machine Learning Architecture for Intelligent Tutor\n\tConstruction","2221":"Quantum Machine Learning Applied to the Classification of Diabetes","2222":"An experimental comparison of machine learning for adaptive sketch\r\n\trecognition","2223":"Insights from Machine Learning Applied to Human Visual Classification.","2224":"Elements of Causal Inference: Foundations and Learning Algorithms","2225":"C 4.5 : programs for machine learning","2226":"Genetic Algorithms in Machine Learning.","2227":"Function Decomposition in Machine Learning.","2228":"On user-centric modular QoE prediction for voip based on machine-learning algorithms","2229":"MEX Vocabulary: A Lightweight Interchange Format for Machine Learning Experiments","2230":"A Machine Learning-based Workflow for Automatic Detection of Anomalies in Machine Tools","2231":"Machine Learning in User Modeling.","2232":"Machine Learning and Intelligent Agents.","2233":"Integrated Architectures for Machine Learning.","2234":"Machine Learning for Health Informatics.","2235":"Gaussian Processes in Machine Learning.","2236":"A machine-learning approach to coding book reviews as quality indicators: Toward a theory of megacitation","2237":"A novel approach to predict Water Quality Index using machine learning models: A review of the methods employed and future possibilities","2238":"Machine Learning in Medical Applications.","2239":"Machine learning methods for metabolic pathway prediction.","2240":"Deep learning's shallow gains: a comparative evaluation of algorithms for automatic music generation","2241":"Evolution of a heterogeneous hybrid extreme learning machine.","2242":"Machine learning and word sense disambiguation in the biomedical domain: design and evaluation issues.","2243":"Experiments in socially guided machine learning: understanding how\n\thumans teach","2244":"A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks.","2245":"Learning to combine foveal glimpses with a third-order Boltzmann machine.","2246":"Interpolating between Optimal Transport and MMD using Sinkhorn Divergences","2247":"The Interplay of Optimization and Machine Learning Research","2248":"Machine Learning Methods for the Protein Fold Recognition Problem.","2249":"Map-Reduce for Machine Learning on Multicore","2250":"Statistical Reinforcement Learning - Modern Machine Learning Approaches.","2251":"Newly Proposed Technique for Autism Spectrum Disorder based Machine Learning","2252":"Data Shapley: Equitable Valuation of Data for Machine Learning.","2253":"UCI Machine Learning Repository","2254":"Extreme Learning Machines: A Survey","2255":"Support vector machine\nactive learning with applications to text classification","2256":"Handwritten character recognition using wavelet energy and extreme learning machine.","2257":"Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning.","2258":"Decoupled Classifiers for Group-Fair and Efficient Machine Learning.","2259":"Support Vector Machine Active Learning with Applications to Text Classification.","2260":"The Enron Corpus: A New Dataset for Email Classification Research","2261":"A Progressive Batching L-BFGS Method for Machine Learning.","2262":"Robust Benchmarking for Machine Learning of Clinical Entity Extraction.","2263":"Improving machine learning approaches to coreference resolution","2264":"Connections Between Inductive Inference and Machine Learning.","2265":"Predicting habitat suitability with machine learning models: The potential area of Pinus sylvestris L. in the Iberian Peninsula","2266":"Developing Bug-Free Machine Learning Systems With Formal Mathematics.","2267":"Residual Unfairness in Fair Machine Learning from Prejudiced Data.","2268":"An Improved Method to Detect Intrusion Using Machine Learning Algorithms","2269":"A Framework for Combining Symbolic and Neural Learning","2270":"Lifelong Machine Learning Systems: Beyond Learning Algorithms.","2271":"Machine learning approaches for the discovery of gene\u2013gene interactions in disease data","2272":"Principles of Explanatory Debugging to Personalize Interactive Machine Learning","2273":"Extreme learning machine with hybrid cost function of G-mean and probability for imbalance learning.","2274":"Toward optimal probabilistic active learning using a Bayesian approach","2275":"Optimal Choice : New Machine Learning Problem and Its Solution","2276":"Using \u201cAnnotator Rationales\u201d to Improve Machine Learning for Text Categorization","2277":"Machine-Learning-Based Reduced Order Model for Macro-Scale Stochastic Plasticity.","2278":"All-optical machine learning using diffractive deep neural networks","2279":"A survey of robust optimization based machine learning with special reference to support vector machines.","2280":"Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations (The Morgan Kaufmann Series in Data Management Systems)","2281":"A machine learning approach to mapping baryons onto dark matter halos\r\n  using the EAGLE and C-EAGLE simulations","2282":"Machine Learning - An Algorithmic Perspective.","2283":"Machine Learning for High-Speed Corner Detection","2284":"Solving Engineering Problems with Machine Learning - Introduction to the Special Theme.","2285":"Machine Learning Identifies Robust Matrisome Markers and Regulatory Mechanisms in Cancer","2286":"A Bayesian\/Information Theoretic Model of Learning to Learn via Multiple Task Sampling","2287":"The Effects of Data Quality on Machine Learning Algorithms.","2288":"Extreme Learning Machine for Microarray Cancer Classification","2289":"A Machine Learning Model for Stock Market Prediction","2290":"Deep Belief Network-Based Machine Learning for Medical Imputation","2291":"Machine Learning and Rule-based Approaches to Assertion Classification","2292":"Machine Learning for Precipitation Nowcasting from Radar Images","2293":"Deep learning, machine vision in agriculture in 2021","2294":"Unleashing Machine Learning onto Big Data: Issues, Challenges and Trends.","2295":"Machine Learning for Enhancement Land Cover and Crop Types Classification.","2296":"Toward Machine Learning Through Genetic Code-like\n                 Transformations","2297":"Deep One-Class Classification","2298":"Comparison of Data Encodings and Machine Learning Architectures for User Identification on Arbitrary Motion Sequences","2299":"Hyperspectral image classification based on multiple reduced kernel extreme learning machine.","2300":"Machine Learning for Intelligent Information Access.","2301":"Design of custom-made stacked patch antennas: a machine learning approach.","2302":"Stochastic trust region inexact Newton method for large-scale machine learning.","2303":"Binary Classification of Arousal in Built Environments using Machine Learning.","2304":"Natural language processing through different classes of machine learning","2305":"An Overview of Machine Learning","2306":"Machine Learning Techniques for Face Analysis.","2307":"Machine Learning Applications to Power Systems.","2308":"Machine Learning in Human Language Technology.","2309":"Haptic recognition using hierarchical extreme learning machine with local-receptive-field.","2310":"A survey on application of machine learning for Internet of Things.","2311":"A Machine Learning Based Framework for Verification and Validation of Massive Scale Image Data","2312":"Machine Learning for Classification of Streaming Data","2313":"Strategic Directions in Machine Learning.","2314":"Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets.","2315":"Structured adaptive and random spinners for fast machine learning computations.","2316":"AUC\u03bc: A Performance Metric for Multi-Class Machine Learning Models.","2317":"Automating the Construction of Compiler Heuristics\n                 Using Machine Learning","2318":"DistRDF2ML - Scalable Distributed In-Memory Machine Learning Pipelines for RDF Knowledge Graphs","2319":"DECEPT: Detecting Cyber-Physical Attacks using Machine Learning on Log Data.","2320":"PREDICTION AND DIAGNOSIS OF BREAST CANCER USING MACHINE LEARNING AND ENSEMBLE CLASSIFIERS","2321":"Explanatory Interactive Machine Learning","2322":"Computer implemented machine learning method and\n                 system","2323":"Interactive Machine Learning for Applications in Food Science.","2324":"Machine Learning Methods That Economists Should Know About","2325":"Classifying under Computational Resource Constraints: Anytime Classification Using Probabilistic Estimators","2326":"Machine Learning Challenges Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment","2327":"Chemically intuited, large-scale screening of MOFs by machine learning techniques","2328":"Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies.","2329":"Rule based systems for classification in machine learning context.","2330":"Machine learning-based characterization of hydrochar from biomass:\r\n  Implications for sustainable energy and material production","2331":"On neurobiological, neuro-fuzzy, machine learning, and statistical\r\n\tpattern recognition techniques","2332":"Machine Learning for Fluid Mechanics","2333":"Machine-learning paradigms for selecting ecologically\n                 significant input variables","2334":"Application of Machine Learning to the Process of Crop Selection Based on Land Dataset","2335":"DISEASE PREDICTION USING MACHINE LEARNING OVER BIG DATA","2336":"Regroup: interactive machine learning for on-demand group creation in social networks","2337":"Machine Learning Paradigms for Speech Recognition: An\n                 Overview","2338":"Analysing the Epoch of Reionization with three-point correlation\r\n  functions and machine learning techniques","2339":"The evolution of boosting algorithms: from machine learning to statistical modelling","2340":"Mining the Semantic Web: Requirements for machine learning","2341":"Privacy Risk in Machine Learning: Analyzing the Connection to\r\n  Overfitting","2342":"Accelerated discovery of stable lead-free hybrid organic-inorganic perovskites via machine learning","2343":"In Search of New Product Ideas: Identifying Ideas in Online Communities by Machine Learning and Text Mining","2344":"F-WSS \\(^++\\) : incremental wrapper subset selection algorithm for fuzzy extreme learning machine.","2345":"A robust multilayer extreme learning machine using kernel risk-sensitive loss criterion.","2346":"Separating theorem of samples in Banach space for support vector machine learning.","2347":"Classifying imbalanced data using ensemble of reduced kernelized weighted extreme learning machine.","2348":"Fast and unsupervised outlier removal by recurrent adaptive reconstruction extreme learning machine.","2349":"Positive and negative fuzzy rule system, extreme learning machine and image classification.","2350":"Breast cancer detection based on Gabor-wavelet transform and machine learning methods.","2351":"Automating Spoken Dialogue Management Design Using Machine Learning: An Industry Perspective","2352":"S\/HIC: Robust Identification of Soft and Hard Sweeps Using Machine Learning","2353":"Finding Density Functionals with Machine Learning","2354":"Kernels and Distances for Structured Data","2355":"Arousal Detection for Biometric Data in Built Environments using Machine Learning.","2356":"Machine Learning: The High Interest Credit Card of Technical Debt","2357":"Multi-Engine Machine Translation as a Lifelong Machine Learning Problem.","2358":"Mechanism Design via Machine Learning (short version)","2359":"Introduction to Machine Learning in Healthcare Informatics.","2360":"EFFICIENT PREDICTION OF DNA-BINDING PROTEINS USING MACHINE LEARNING","2361":"What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use.","2362":"Classifying relevant video tutorials for the school\u2019s learning management system using support vector machine algorithm","2363":"Adopting a Machine Learning Approach in the Design of Smart Transportation Systems.","2364":"A First Course in Machine Learning.","2365":"Python Machine Learning","2366":"Machine Learning in Early Genetic Detection of Multiple Sclerosis Disease: A Survey","2367":"Improving Reproducibility in Machine Learning Research (A Report from\r\n  the NeurIPS 2019 Reproducibility Program)","2368":"Machine Learning Methods in Automatic Image Annotation.","2369":"Revealing User Confidence in Machine Learning-Based Decision Making.","2370":"Estimating the ``wrong'' graphical model: benefits in the computation-limited setting","2371":"Machine Learning for Signature Verification.","2372":"Numerical Algorithms - Methods for Computer Vision, Machine Learning, and Graphics.","2373":"Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning","2374":"Transparency Communication for Machine Learning in Human-Automation Interaction.","2375":"The use of machine learning methodologies to analyse antibiotic and biocide susceptibility in Staphylococcus aureus","2376":"Mitigating Neural Network Overconfidence with Logit Normalization","2377":"Optical inspection of tool wear using machine learning methods","2378":"Introduction to Machine Learning and Statistics","2379":"Sample Complexity of Sinkhorn Divergences","2380":"An Optimal Machine Learning Classification Model for Flash Memory Bit Error Prediction.","2381":"Predicting QoE in Cellular Networks using Machine Learning and in-Smartphone Measurements","2382":"Advances in the Application of Machine Learning\n                 Techniques in Drug Discovery, Design and Development","2383":"Data Management in Machine Learning Systems","2384":"Rapid Prediction of Bacterial Heterotrophic Fluxomics Using Machine Learning and Constraint Programming","2385":"Ovid: A Machine Learning Approach for Automated Vandalism Detection in OpenStreetMap","2386":"An initial study on the rank of input matrix for extreme learning machine.","2387":"Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks","2388":"Boltzmann Machine Learning Using Mean Field Theory and Linear Response Correction.","2389":"Sustainable thermoelectric materials predicted by machine learning","2390":"A semantic matching energy function for learning with multi-relational data: Application to word-sense disambiguation","2391":"Studying Up Machine Learning Data: Why Talk About Bias When We Mean Power?","2392":"Kernel extreme learning machine based on fuzzy set theory for multi-label classification.","2393":"Reconciling schemas of disparate data sources: a machine-learning approach","2394":"Machine Learning in Drug Discovery A Comprehensive Analysis of Applications, Challenges and Future Directions","2395":"How to develop machine learning models for healthcare","2396":"Learning from the experts: From expert systems to machine-learned diagnosis models.","2397":"HOList: An Environment for Machine Learning of Higher Order Logic Theorem Proving.","2398":"SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient.","2399":"Predicting flow stress behavior of an AA7075 alloy using machine learning methods","2400":"Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale.","2401":"Neural Additive Models: Interpretable Machine Learning with Neural Nets","2402":"Targeted Adversarial Attacks on Wind Power Forecasts","2403":"APPLYING MACHINE LEARNING TECHNIQUES TO FIND\r\nIMPORTANT ATTRIBUTES FOR HEART FAILURE SEVERITY ASSESSMENT","2404":"Deep symbolic regression for recurrence prediction","2405":"A survey on bias and fairness in machine learning","2406":"Applying Machine Learning Techniques to Find Important Attributes for Heart Failure Severity Assessment","2407":"A Machine Learning Approach to Building Domain-Specific Search Engines","2408":"Machine Learning Models for YouTube QoE and User Engagement Prediction in Smartphones","2409":"APPLYING MACHINE LEARNING TECHNIQUES TO FIND IMPORTANT ATTRIBUTES FOR HEART FAILURE SEVERITY ASSESSMENT","2410":"Attention in Psychology, Neuroscience, and Machine Learning","2411":"Automating the Construction of Internet Portals with Machine Learning","2412":"Deep Learning of Semisupervised Process Data With Hierarchical Extreme Learning Machine and Soft Sensor Application","2413":"Improving the Accuracy of Subseasonal Forecasting of China Precipitation With a Machine Learning Approach","2414":"Identifying essential genes in bacterial metabolic networks with machine learning methods","2415":"Machine Learning and Data Optimization using BPNN and GA in DOC","2416":"Combining Humans and Machine Learning: A Novel Approach for Evaluating Crowdsourcing Contributions in Idea Contests","2417":"Machine Learning Techniques for Prostate Ultrasound Image Diagnosis.","2418":"Machine Learning in Vector Models of Neural Networks.","2419":"Machine Learning Solutions in Computer-Aided Medical Diagnosis.","2420":"Efficient surrogate modeling methods for large-scale Earth system models\r\n  based on machine learning techniques","2421":"Programming by Demonstration Using Version Space Algebra","2422":"Parallel implementation of the machine learning\/statistical method random forest (R package randomForest)","2423":"Automatic optic disc detection using low-rank representation based semi-supervised extreme learning machine.","2424":"A fast decision making method for mandatory lane change using kernel extreme learning machine.","2425":"Some Notes on Applied Mathematics for Machine Learning.","2426":"Local receptive field based extreme learning machine with three channels for histopathological image classification.","2427":"A systematic review of the research trends of machine learning in supply chain management.","2428":"Reasoning Under Uncertainty: Towards Collaborative Interactive Machine Learning.","2429":"Ontology Matching: A Machine Learning Approach","2430":"Comparison of Machine Learning for Autonomous Robot Discovery.","2431":"The Application of Machine Learning to the Prediction of Heart Attack","2432":"Trust and Transparency in Machine Learning-Based Clinical Decision Support.","2433":"Effective Design in Human and Machine Learning: A Cognitive Perspective.","2434":"A Machine Learning Approach to Software Requirements Prioritization","2435":"House Price Estimates Based on Machine Learning Algorithm","2436":"Coupling feature selection and machine learning methods for navigational query identification","2437":"A New Security Boundary of Component Differentially Challenged XOR Pufs Against Machine Learning Modeling Attacks","2438":"AXIS: Generating Explanations at Scale with Learnersourcing and Machine Learning","2439":"Machine Learning Applied to Ultrasound Imaging ? The Next Step in Democratising Medical Imaging.","2440":"Graph Learning with a Nearest Neighbor Approach","2441":"SMART: Automated Support for Ontology Merging and Alignment","2442":"Composing Modeling and Simulation with Machine Learning in Julia","2443":"Toward fusion plasma scenario planning for NSTX-U using machine-learning-accelerated models.","2444":"Classifying Lung Cancer Severity with Ensemble Machine Learning in Health Care Claims Data.","2445":"Multiagent Systems: A Survey from a Machine Learning\r\n                  Perspective","2446":"Learning Bayesian Networks","2447":"Differentially Private Empirical Risk Minimization","2448":"Exact and Robust Conformal Inference Methods for Predictive Machine Learning with Dependent Data.","2449":"The Structure-Mapping Engine: Algorithm and Examples","2450":"Integrating Machine Learning and Optimization Methods for Imaging of Patients with Prostate Cancer.","2451":"Machine Learning-based Query Augmentation for SPARQL Endpoints","2452":"Using Machine Learning to Maintain Rule-based Named - Entity Recognition and Classification Systems","2453":"Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things.","2454":"Random Forests","2455":"Guest Editorial: Special Issue on Structured Prediction","2456":"Virtual Calibration of Cosmic Ray Sensor: Using Supervised Ensemble Machine Learning","2457":"Fairness and Machine Learning","2458":"Prediction of the hot metal silicon content in blast furnace based on extreme learning machine.","2459":"Initial-training-free online sequential extreme learning machine based adaptive engine air-fuel ratio control.","2460":"Computer vision and machine learning for archaeology","2461":"A bibliometric overview of International Journal of Machine Learning and Cybernetics between 2010 and 2017.","2462":"Segmentation of the left ventricle in cardiac MRI using a hierarchical extreme learning machine model.","2463":"A survey on datasets for fairness-aware machine learning","2464":"VDES J2325-5229 a z=2.7 gravitationally lensed quasar discovered using\r\n  morphology independent supervised machine learning","2465":"Machine Learning for In Silico Modeling of Tumor Growth.","2466":"Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems.","2467":"2D Transparency Space - Bring Domain Users and Machine Learning Experts Together.","2468":"Machine learning approach to predict protein phosphorylation sites by incorporating evolutionary information.","2469":"Machine Learning in DNA Microarray Analysis for Cancer Classification","2470":"Machine learning architectures for scalable and reliable subject indexing","2471":"The committee machine: Computational to statistical gaps in learning a two-layers neural network.","2472":"Classification of dog barks: a machine learning approach","2473":"Wavelet-based Machine Learning Techniques for ECG Signal Analysis.","2474":"Clinical Utility of Machine Learning and Longitudinal EHR Data.","2475":"Getting It Right at the Very Start -- Building Project\n                 Models where Data Is Expensive by Combining Human\n                 Expertise, Machine Learning and Information Theory","2476":"An implementation of genetic algorithms for rule based\n                 machine learning","2477":"Distributed Representations of Sentences and Documents","2478":"Machine Learning Techniques for AD\/MCI Diagnosis and Prognosis.","2479":"High Fidelity Approximation of Slow Simulators Using\n                 Machine Learning for Real-time\n                 Simulation\/Optimization","2480":"Hierarchical Multi-Label Classification Networks","2481":"The Use of Machine Learning and Neural Networks in the Digital Economy and International Digital Integration","2482":"The Application Of Machine Learning To A Renal Biopsy Data-Base","2483":"German's Next Language Model","2484":"Language Model Cascades","2485":"Cross-lingual Language Model Pretraining","2486":"CTRL: A Conditional Transformer Language Model for Controllable\r\n  Generation","2487":"A Tool for Model-Based Language Specification","2488":"A Neural Probabilistic Language Model","2489":"Language model-based document clustering using random walks","2490":"Teaching the Uzbek Language is a Topical Issue An Interactive Learning Model","2491":"How Much Knowledge Can You Pack Into the Parameters of a Language Model?","2492":"Language Model Analysis for Ontology Subsumption Inference","2493":"Universal Language Model Fine-tuning for Text Classification","2494":"Medical Semantic Similarity with a Neural Language Model","2495":"Symphony Generation with Permutation Invariant Language Model","2496":"Language Model Integration for the Recognition of Handwritten Medieval Documents","2497":"Cross-lingual Language Model Pretraining.","2498":"A Neural Probabilistic Language Model.","2499":"A Simple Language Model for Task-Oriented Dialogue","2500":"A general language model for information retrieval","2501":"Teaching the Uzbek Language is a Topical Issue On the Example of an Interactive Model of Education","2502":"Unified Language Model Pre-training for Natural Language Understanding and Generation.","2503":"A Scalable Hierarchical Distributed Language Model.","2504":"Beyond Scotton's Matrix Language Frame Model A Look into Some Bassa French Code Switched Discourse Samples","2505":"Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning.","2506":"Language and Tool Support for Model Checking of Fault-Tolerant Distributed Algorithms","2507":"Action Language: a specification language for model checking reactive systems","2508":"Dealing with written language semantics by a connectionist model of cognitive reading","2509":"Adaptive Test Generation Using a Large Language Model","2510":"CamemBERT: a Tasty French Language Model","2511":"PALM: Pre-training an Autoencoding&Autoregressive Language Model for\r\n  Context-conditioned Generation","2512":"Using Random Forests in the Structured Language Model.","2513":"Correlated Bigram LSA for Unsupervised Language Model Adaptation.","2514":"Rethinking with Retrieval: Faithful Large Language Model Inference","2515":"Better language models with model merging","2516":"Sophia: A Scalable Stochastic Second-order Optimizer for Language Model\r\n  Pre-training","2517":"Character-Aware Neural Language Models","2518":"Question Prediction Language Model","2519":"Language learning as language use: A cross-linguistic model of child language development.","2520":"Network model of human language","2521":"Using Role-Based Modeling Language (RBML) to Characterize Model Families","2522":"The model transformation language of the VIATRA2 framework","2523":"COVID-Twitter-BERT: A Natural Language Processing Model to Analyse\r\n  COVID-19 Content on Twitter","2524":"Megatron-LM: Training Multi-Billion Parameter Language Models Using\r\n  Model Parallelism","2525":"Unbounded cache model for online language modeling with open vocabulary.","2526":"Gated Word-Character Recurrent Language Model","2527":"Conceptual Model Generation from Requirements Model: A Natural Language Processing Approach.","2528":"A cross-language personalized recommendation model in digital libraries","2529":"GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking.","2530":"Pseudo Relevance Feedback Using Semantic Clustering in Relevance Language Model","2531":"LLaVA-$\\phi$: Efficient Multi-Modal Assistant with Small Language Model","2532":"Multidimensional Data Model and Query Language for Informetrics","2533":"Language Models are Few-Shot Learners","2534":"On language and connectionism: Analysis of a parallel distributed processing model of language acquisition","2535":"An Object Storage Model for the Truffle Language Implementation Framework","2536":"A Conceptual Model and Predicate Language for Data Selection and Projection Based on Provenance.","2537":"Cross Language Information Retrieval Model for Discovering WSDL Documents Using Arabic Language Query","2538":"Fast Exact Inference with a Factored Model for Natural Language Parsing","2539":"A Translate-Edit Model for Natural Language Question to SQL Query\r\n  Generation on Multi-relational Healthcare Data","2540":"Fast Exact Inference with a Factored Model for Natural Language Parsing.","2541":"Toolformer: Language Models Can Teach Themselves to Use Tools","2542":"SecureUML: A UML-based modeling language for model-driven security","2543":"A neural probabilistic language model","2544":"Inter-language reflection: A conceptual model and its implementation","2545":"A Method for Validating a Conceptual Model by Natural Language Discourse\r\n\tGeneration","2546":"Language models are unsupervised multitask learners","2547":"Petri nets model of unified modeling language statecharts","2548":"Recurrent neural network based language model.","2549":"Participatory design in open education: a workshop model for developing a pattern language","2550":"OWL 1.1 Web Ontology Language: Model-Theoretic Semantics","2551":"Integrating word relationships into language models","2552":"Thematic text clustering for domain specific language model adaptation","2553":"State of the Art of QVT: A Model Transformation Language Standard","2554":"Automated Transformation of Descartes Modeling Language to Palladio Component Model","2555":"Sign Language Recognition Using Long Short-Term Memory Deep Learning Model","2556":"KLAPER : An Intermediate Language for Model-Driven Predictive Analysis of Performance and Reliability","2557":"Integrating Meta-modelling Aspects with Graph Transformation for Efficient Visual Language Definition and Model Manipulation","2558":"04101 Summary - Language Engineering for Model-driven Software Development.","2559":"Blocking Blog Spam with Language Model Disagreement.","2560":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","2561":"Language production and serial order: A functional analysis and a model","2562":"A case based reasoning model for multilingual language generation in dialogues","2563":"Systems Biology Markup Language (SBML) Level 2: Structures and Facilities for Model Definitions","2564":"A Case Based Reasoning Model for Multilingual Language Generation in Dialogues","2565":"A discriminative language model with pseudo-negative samples","2566":"Generating Diverse Code Explanations using the GPT-3 Large Language Model","2567":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining","2568":"04101 Abstracts Collection - Language Engineering for Model-Driven Software Development.","2569":"The Programmer\u2019s Assistant: Conversational Interaction with a Large Language Model for Software Development","2570":"A Computational Model of Natural Language Communication: Interpretation, Inference, and Production in Database Semantics","2571":"Language Models are Unsupervised Multitask Learners","2572":"A Term Weighted Neural Language Model and Stacked Bidirectional LSTM Based Framework for Sarcasm Identification","2573":"Knowledge Distillation of Large Language Models","2574":"An effective approach to verbose queries using a limited dependencies language model","2575":"A domain-specific visual language for domain model evolution","2576":"A Systematic Evaluation of Large Language Models of Code","2577":"Towards Model Transformation in Generated\r\nEclipse Editor Plug-Ins","2578":"A Real Dynamic Cyber Trust Model","2579":"REPLUG: Retrieval-Augmented Black-Box Language Models","2580":"Training Compute-Optimal Large Language Models","2581":"Merging Models with the Epsilon Merging Language (EML)","2582":"A language model approach to keyphrase extraction","2583":"Model-Based Self-Aware Performance and Resource Management Using the Descartes Modeling Language","2584":"A Survey of Large Language Models","2585":"GDT4MAS: a formal model and language to specify and verify agent-based complex systems.","2586":"ContextUML: A UML-Based Modeling Language for Model-Driven Development of Context-Aware Web Services Development","2587":"Time, Language and Action - A Unified Long-Term Memory Model for Sensory-Motor Chains and Word Schemata.","2588":"Translating EXPRESS language model into C language model.","2589":"Modeling a Model Transformation Language.","2590":"A procedural model of language understanding","2591":"Improving Language Model Predictions via Prompts Enriched with Knowledge Graphs","2592":"Calibration, Entropy Rates, and Memory in Language Models","2593":"How Many Languages Can a Language Model Model?","2594":"Improving Neural Language Models with a Continuous Cache","2595":"TinyBERT: Distilling BERT for Natural Language Understanding","2596":"Three fundamental dimensions of scientific workflow interoperability: Model of computation, language, and execution environment","2597":"Factored language models and generalized parallel backoff","2598":"Language model capitalization.","2599":"Similarity Language Model.","2600":"Modeling Value Evaluation of Semantics Aided Secondary Language Acquisition as Model Driven Knowledge Management","2601":"Training language models to follow instructions with human feedback","2602":"Language Model Transformers as Evaluators for Open-domain Dialogues","2603":"Do Massively Pretrained Language Models Make Better Storytellers?","2604":"Language model adaptation for spoken language systems.","2605":"Cluster-based retrieval using language models","2606":"Scientific-Pedagogical Content and Model of Formation of Media Culture on the Basis Of Interdisciplinary Cooperation for Future Foreign Language Teachers","2607":"Emergent Analogical Reasoning in Large Language Models","2608":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","2609":"Better Language Models with Model Merging.","2610":"Model Shrinkage for Discriminative Language Models.","2611":"Better Language Models with Model Merging","2612":"Enriching Medcial Terminology Knowledge Bases via Pre-trained Language\r\n  Model and Graph Convolutional Network","2613":"Using Syntactic Dependency and Language Model X-IOTA IR System for CLIPS Mono and Bilingual Experiments in CLEF 2005","2614":"Masked Language Model Scoring.","2615":"Latent Tree Language Model.","2616":"German's Next Language Model.","2617":"Language Model Grammar Conversion.","2618":"Character-based Language Model.","2619":"The Referent Model Language","2620":"Model Driven Language Engineering.","2621":"Language model speaker adaptation.","2622":"Tensor network language model.","2623":"Question Prediction Language Model.","2624":"Unsupervised language model adaptation.","2625":"Frame Augmented Language Model.","2626":"Word Sense Disambiguation with Neural Language Models","2627":"Language - a biological model.","2628":"Statistical feature language model.","2629":"Binarized LSTM Language Model.","2630":"A Structured Language Model","2631":"Model Transformation Language MOLA.","2632":"The Language Model LMNtal.","2633":"Model driven language engineering.","2634":"A Structured Language Model.","2635":"In-Context Retrieval-Augmented Language Models","2636":"Specification of modelling languages in a flexible meta-model architecture","2637":"LoRA: Low-Rank Adaptation of Large Language Models","2638":"Pre-trained Language Model Representations for Language Generation.","2639":"Syllable-level Neural Language Model for Agglutinative Language.","2640":"Conceptual language model design for spoken language understanding.","2641":"Improved Phonotactic Language Recognition Using Collaborated Language Model.","2642":"Pre-trained language model representations for language generation.","2643":"A Phonotactic Language Model for Spoken Language Identification.","2644":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages","2645":"A Subword Level Language Model for Bangla Language.","2646":"FinGPT: Open-Source Financial Large Language Models","2647":"PageRank without hyperlinks: structural re-ranking using links induced by language models","2648":"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome","2649":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training.","2650":"Model-driven Development of Environmental Modeling Languages: Language and Model Coupling.","2651":"Are All Languages Equally Hard to Language-Model?","2652":"Composition Techniques for Rule-Based Model Transformation Languages","2653":"A redefinition of the syndrome of Broca\u00e2\u0080\u0099s aphasia: Implications for a neuropsychological model of language","2654":"A heuristic Hidden Markov Model to recognize inflectional words in sign system for Indonesian language known as SIBI (Sistem Isyarat Bahasa Indonesia)","2655":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","2656":"Improvements to N-gram Language Model Using Text Generated from Neural Language Model.","2657":"The role of inhibition in a spreading-activation model of language production II: The simulation perspective","2658":"The role of inhibition in a spreading-activation model of language production I: The psycholinguistic perspective","2659":"Healthcare NER Models Using Language Model Pretraining.","2660":"A Generalized Language Model as the Combination of Skipped n-grams\r\n               and Modified Kneser Ney Smoothing","2661":"Patterns of Language - A Population Model for Language Structure","2662":"What Kind of Language Is Hard to Language-Model?","2663":"Iterative Language Model Adaptation for Indo-Aryan Language Identification.","2664":"Improved language modelling through better language model evaluation measures.","2665":"Language Architecture: An Architecture Language for Model-Driven Engineering.","2666":"Cross-language Acoustic Model Refinement for the Indonesian Language.","2667":"A Continuous Space Neural Language Model for Bengali Language.","2668":"A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser Ney Smoothing","2669":"Class-Based Language Model Adaptation.","2670":"Discriminative training on language model.","2671":"Quantization-based language model compression.","2672":"Topically Driven Neural Language Model.","2673":"Inter-document Contextual Language model.","2674":"The LOGIDATA+ Model and Language.","2675":"Continuous Experience-aware Language Model.","2676":"The latent words language model.","2677":"A Natural Language Processing Model.","2678":"Language integration for model formalization.","2679":"Similar n-gram language model.","2680":"Context dependent language model adaptation.","2681":"Model Checking Regular Language Constraints.","2682":"A Neural Knowledge Language Model.","2683":"Topic Compositional Neural Language Model.","2684":"Cooperative model-based language understanding.","2685":"Unsupervised discriminative language model training.","2686":"Large Margin Neural Language Model.","2687":"A hierarchical Dirichlet language model.","2688":"Language use: A performance model.","2689":"Data-driven executable language model.","2690":"Back-off language model compression.","2691":"A Hierarchical Dirichlet Language Model","2692":"Language Discrimination via PPM Model.","2693":"The Architecture Description Language MoDeL.","2694":"Language-Independent Model Transformation Verification.","2695":"Detection of Language (Model) Errors.","2696":"Towards model and language composition.","2697":"Metamodel based model transformation language.","2698":"Connectionist Language Model for Polish.","2699":"Entity based translation language model.","2700":"Knowledge-Aware Language Model Pretraining.","2701":"On Language-Independent Model Modularisation.","2702":"The MT model transformation language.","2703":"Language Model Based Query Classification.","2704":"Teaching Model Driven Language Handling.","2705":"modelNLgeneration: natural language model extraction.","2706":"A Neural Syntactic Language Model.","2707":"Multilingual Machine Translation with Large Language Models: Empirical\r\n  Results and Analysis","2708":"Atlas: Few-shot Learning with Retrieval Augmented Language Models","2709":"Temporal Provenance Model (TPM): Model and Query Language","2710":"Language Model Supervision for Handwriting Recognition Model Adaptation.","2711":"Blank Language Models","2712":"A language model for highly inflective non-agglutinative languages.","2713":"Evaluating Language Model Finetuning Techniques for Low-resource Languages.","2714":"LSA-based language model adaptation for highly inflected languages.","2715":"Component Models for Semantic Web Languages","2716":"Language without words: A pointillist model for natural language processing.","2717":"An Improved Recurrent Neural Network Language Model for Programming Language.","2718":"Two novel language model estimation techniques for statistical language identification.","2719":"Language Model Adaptation for Language and Dialect Identification of Text.","2720":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.","2721":"On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining","2722":"Action Language: a specification language for model checking reactive systems.","2723":"Language Without Words: A Pointillist Model for Natural Language Processing","2724":"Natural language processing model compiling natural language into byte code.","2725":"Language model adaptation for language and dialect identification of text.","2726":"Towards Cross Language Process Model Reuse - A Language Independent Representation of Process Models.","2727":"LM4KG: Improving Common Sense Knowledge Graphs with Language Models","2728":"TruthfulQA: Measuring How Models Mimic Human Falsehoods","2729":"Wodel: a domain-specific language for model mutation.","2730":"Training Connectionist Models for the Structured Language Model.","2731":"CausaLM: Causal Model Explanation Through Counterfactual Language Models.","2732":"Integrating natural language oriented models with feature model.","2733":"Language-specific model checking of UML-RT models.","2734":"Combination of random indexing based language model and n-gram language model for speech recognition.","2735":"Compositional matrix-space models of language: Definitions, properties, and learning methods","2736":"Efficient Language Model Construction for Spoken Dialog Systems by Inducting Language Resources of Different Languages.","2737":"On the interoperability of model-to-model transformation languages","2738":"Language Models, Smoothing, and IDF Weighting","2739":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models","2740":"Paragraph vector based topic model for language model adaptation.","2741":"Using a Model Merging Language for Reconciling Model Versions.","2742":"Using a Model Merging Language for Reconciling Model Versions","2743":"A Visual Specification Language for Model-to-Model Transformations.","2744":"Identifying and Reducing Gender Bias in Word-Level Language Models","2745":"CamemBERT: a Tasty French Language Model.","2746":"Language model adaptation using Random Forests.","2747":"Communicative language model: structure and functioning.","2748":"Anchor-model fusion for language recognition.","2749":"Bayesian recurrent neural network language model.","2750":"Bidirectional Language Model for Handwriting Recognition.","2751":"Data augmentation and language model adaptation.","2752":"Language Model Based on Word Clustering.","2753":"Recurrent Neural Network Based Language Model","2754":"Toward the Ultimate ASR Language Model.","2755":"Concentration Bounds for Unigrams Language Model.","2756":"Smoothing a Tera-word Language Model.","2757":"Hierarchical Pitman-Yor-Dirichlet Language Model.","2758":"A change propagating model transformation Language.","2759":"Morphological Verb-Aware Tibetan Language Model.","2760":"Language-based model for requirements engineering.","2761":"Event Modeling with the MODEL Language.","2762":"Growing an n-gram language model.","2763":"Recurrent neural network based language model","2764":"Towards Model-Driven Software Language Modernization.","2765":"Smoothlm: A language model compression library.","2766":"Antimony: a modular model definition language.","2767":"A Succinct N-gram Language Model.","2768":"Feather: A Feature Model Transformation Language.","2769":"APQL: A Process-Model Query Language.","2770":"Model Adaptation For Spoken Language Understanding.","2771":"A Language-Independent Model Query Tool.","2772":"Natural language in model world interfaces.","2773":"Neural Network Language Model with Cache.","2774":"Gated Word-Character Recurrent Language Model.","2775":"Tailoring an Interpretable Neural Language Model.","2776":"Refinement of a Structured Language Model","2777":"Markov Recurrent Neural Network Language Model.","2778":"Constructive Model of the Natural Language.","2779":"Automatic Learning of Language Model Structure.","2780":"Quantum Language Model-based Query Expansion.","2781":"Model Driven Language Engineering with Kermeta.","2782":"Language Model Based Arabic Word Segmentation.","2783":"SentenceMIM: A Latent Variable Language Model.","2784":"Wiki-40B: Multilingual Language Model Dataset.","2785":"Meta-Learning a Dynamical Language Model.","2786":"Language-agnostic simulation model management platform.","2787":"Speech enhanced multi-Span language model.","2788":"Language model adaptation using word clustering.","2789":"Discriminative training of language model classifiers.","2790":"A hierarchical language model for CSR.","2791":"Language Model Based Temporal Information Indexing.","2792":"\u0098The\u009c generic model query language GMQL","2793":"Model Fusion in Conceptual Language Modeling.","2794":"Curriculum based discriminative language model training.","2795":"An XQuery-Based Model Transformation Language.","2796":"Uyghur Language Model with Graphic Structure.","2797":"Epsilon Flock: a model migration language.","2798":"Applying Language Model into IR Task.","2799":"Hierarchical probabilistic neural network language model.","2800":"Overview of partial model query language.","2801":"Language model adaptation using dynamic marginals.","2802":"Actor model of Anemone functional language.","2803":"An associative memory model of language.","2804":"Topical Language Model for Snippet Retrieval.","2805":"Title language model for information retrieval.","2806":"Dependence language model for information retrieval.","2807":"Concept-Oriented Model and Query Language","2808":"BioMegatron: Larger Biomedical Domain Language Model.","2809":"A Melody-Conditioned Lyrics Language Model.","2810":"A scalable hierarchical distributed language model","2811":"Temporal kernel neural network language model.","2812":"StreamAPAS: Query Language and Data Model.","2813":"Utterance selection model of language change","2814":"Umple: a model-oriented programming language.","2815":"Language Model for Mongolian Polyphone Proofreading.","2816":"Online LDA-Based Language Model Adaptation.","2817":"A Model Repository Description Language - MRDL.","2818":"A Hierarchical Word Sequence Language Model.","2819":"The bag model in language statistics.","2820":"Towards improved language model evaluation measures.","2821":"Language Model based on POS Tagger.","2822":"Multi-domain neural network language model.","2823":"Hierarchical Probabilistic Neural Network Language Model.","2824":"The hidden vector state language model.","2825":"The generic model query language GMQL.","2826":"Model-Based Language Engineering with EMFText.","2827":"GottBERT: a pure German Language Model","2828":"Semantic Language Model for Tunisian Dialect.","2829":"Chinese language model adaptation based on document classification and multiple domain-specific language models.","2830":"Language Model and Sentence Structure Manipulations for Natural Language Application Systems.","2831":"Conditioned Natural Language Generation using only Unconditioned Language Model: An Exploration.","2832":"Language Segmentation of Twitter Tweets using Weakly Supervised Language Model Induction.","2833":"The model weaving description language (MWDL)-Towards a formal aspect oriented language for MDA model transformations","2834":"A tree-based statistical language model for natural language speech recognition.","2835":"A new language model adaptation framework using modification of structures of background corpus and language model.","2836":"Bigram-based natural language model and statistical motion symbol model for scalable language of humanoid robots.","2837":"Graphic Language Model for Agglutinative Languages: Uyghur as Study Case.","2838":"Language model adaptation for resource deficient languages using translated data.","2839":"Model Theoretic Semantics For Many-Purpose Languages And Language Hierarchies.","2840":"Automatic Symbol Processing for Language Model Building in Slavic Languages.","2841":"Large Language Models as Optimizers","2842":"Blackbox meets blackbox: Representational Similarity and Stability\r\n  Analysis of Neural Language Models and Brains","2843":"Are Emergent Abilities of Large Language Models a Mirage?","2844":"Nonlinear interpolation of topic models for language model adaptation.","2845":"Nonlinear interpolation of topic models for language model\n  adaptation","2846":"Word Topical Mixture Models for Dynamic Language Model Adaptation.","2847":"mel- model extractor language for extracting facts from models.","2848":"Multi-modal Discriminative Model for Vision-and-Language Navigation.","2849":"Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models.","2850":"Improved mixed language speech recognition using asymmetric acoustic model and language model with code-switch inversion constraints.","2851":"Language model and acoustic model information in probabilistic speech recognition.","2852":"The Functional Data Model and the Data Model Language DAPLEX","2853":"Using a connectionist model in a syntactical based language model.","2854":"Evaluation of a language model using a clustered model backoff.","2855":"Semi-supervised learning of language model using unsupervised topic model.","2856":"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context","2857":"Acoustic and lexical resource constrained ASR using language-independent acoustic model and language-dependent probabilistic lexical model.","2858":"A new robust relevance model in the language model framework.","2859":"ReAct: Synergizing Reasoning and Acting in Language Models","2860":"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked\r\n  Language Models","2861":"ST-BERT: Cross-modal Language Model Pre-training For End-to-end Spoken Language Understanding.","2862":"Phrase-based data selection for language model adaptation in spoken language translation.","2863":"Integrating Prosodics into a Language Model for Spoken Language Understanding of Thai.","2864":"A word language model based contextual language processing on Chinese character recognition.","2865":"A quantitative model of first language influence in second language consonant learning.","2866":"Practical Text Classification With Large Pre-Trained Language Models","2867":"Structural Parsing of Natural Language Text in Tamil Language Using Dependency Model.","2868":"Combining lexical and morphological knowledge in language model for inflectional (czech) language.","2869":"MEGATRON-CNTRL: Controllable Story Generation with External Knowledge\r\n  Using Large-Scale Language Models","2870":"Code-Switch Language Model with Inversion Constraints for Mixed Language Speech Recognition.","2871":"BERT: Pre-training of Deep Bidirectional Transformers for Language\r\n  Understanding","2872":"Language Model is All You Need: Natural Language Understanding as Question Answering.","2873":"Instance-based on-line language model adaptation.","2874":"A Formal Language for Model Transformation Specification.","2875":"A Proposed Language Model Based on LSTM.","2876":"A general language model for information retrieval.","2877":"Incorporating probabilities into the dualgram language model.","2878":"Using story topics for language model adaptation.","2879":"Language model adaptation for tiny adaptation corpora.","2880":"N-gram distribution based language model adaptation.","2881":"Smoothing issues in the structured language model.","2882":"One Model for the Learning of Language.","2883":"Context dependent recurrent neural network language model.","2884":"Affective language model adaptation via corpus selection.","2885":"Subword Language Model for Query Auto-Completion.","2886":"PaLM: A Hybrid Parser and Language Model.","2887":"A language model for compromised user analysis.","2888":"Information Extraction Using the Structured Language Model","2889":"Putting it all together: language model combination.","2890":"Language model representations for beam-search decoding.","2891":"Web spam identification through language model analysis.","2892":"Simple Fusion: Return of the Language Model.","2893":"KenLM: Faster and Smaller Language Model Queries.","2894":"A Bidirectional Model For Natural Language Processing.","2895":"Minimally Supervised Model of Early Language Acquisition.","2896":"Scalable Modified Kneser-Ney Language Model Estimation.","2897":"Word Sense Language Model for Information Retrieval.","2898":"Language Model Representations for the GOPOLIS Database.","2899":"slimIPL: Language-Model-Free Iterative Pseudo-Labeling.","2900":"Delta Operation Language for Model Difference Representation.","2901":"A neuro-propositional model of language processing.","2902":"Language identification using Gaussian mixture model tokenization.","2903":"A Pattern Language Model for Framework Development.","2904":"An approach to cross-language model versioning.","2905":"A Low-Resourced Peruvian Language Identification Model.","2906":"A Formal Language Model for Parsing SGML.","2907":"Topic tracking language model for speech recognition.","2908":"A Data-Oriented Model of Literary Language.","2909":"GePpeTto Carves Italian into a Language Model.","2910":"A dynamic language model for speech recognition","2911":"Multiscale recurrent neural network based language model.","2912":"Implicit Language Model in LSTM for OCR.","2913":"A language model approach for tag recommendation.","2914":"A Language Model for Dynamic Code Updating.","2915":"Statistical Translation Language Model for Twitter Search.","2916":"Improvements in tree-based language model representation.","2917":"Techniques for approximating a trigram language model.","2918":"Training data optimization for language model adaptation.","2919":"Hjelmslev's semiotic model of language: An exegesis","2920":"A Computer Model of Child Language Learning.","2921":"Reliable feature selection for language model adaptation.","2922":"MOF Model to Text Transformation Language 1.0","2923":"Declarative data cleaning: Language, model, and\r\n                   algorithms","2924":"A statistical model for lost language decipherment","2925":"A Dynamical Systems Model for Language Change","2926":"Graph-Based Statistical Language Model for Code.","2927":"Multi-cell LSTM Based Neural Language Model.","2928":"A Generalized Language Model in Tensor Space.","2929":"Accurate language model estimation with document expansion.","2930":"Detecting nepotistic links by language model disagreement.","2931":"Cynical Selection of Language Model Training Data.","2932":"Aspects of a Language Model for Designing.","2933":"A posteriori multiple word-domain language model.","2934":"Scalable language model look-ahead for LVCSR.","2935":"VMQL: A generic visual model query language.","2936":"An optimization model of global language complexity.","2937":"A production system model of language processing","2938":"Language planning and practice: the Lithuanian model","2939":"A Probabilistic Model for Natural Language Understanding.","2940":"NLOMJ-Natural Language Object Model in Java.","2941":"Multi-Classification Model for Spoken Language Understanding.","2942":"Model-100: Specification language for interacting processes.","2943":"Biases in Predicting the Human Language Model","2944":"Language model parameter estimation using user transcriptions.","2945":"Evolving Algebra Model of Programming Language Semantics.","2946":"Linguistic Profiling of a Neural Language Model.","2947":"Modeling Language Evolution for Model Family Support.","2948":"Temporal Data Model and Query Language Concepts.","2949":"Use and Acquisition of Semantic Language Model.","2950":"Federated Operation Model for the Language Grid.","2951":"Language Model-Driven Unsupervised Neural Machine Translation.","2952":"Linguistically Inspired Language Model Augmentation for MT.","2953":"LTSmin: High-Performance Language-Independent Model Checking.","2954":"A Probabilistic Language Model for Hand Drawings.","2955":"A domain-specific language for model coupling.","2956":"Latent dirichlet language model for speech recognition.","2957":"Regular Patterns - Probably Approximately Correct Language Model.","2958":"A Dynamic Language Model for Speech Recognition.","2959":"Language Model Information Retrieval with Document Expansion.","2960":"Information Extraction Using the Structured Language Model.","2961":"Counterfactual Language Model Adaptation for Suggesting Phrases.","2962":"Model-Driven Engineering Meets Generic Language Technology.","2963":"A Role Sharing Model of Language Areas.","2964":"User language model for collaborative personalized search.","2965":"RobBERT: a Dutch RoBERTa-based Language Model.","2966":"Biases in Predicting the Human Language Model.","2967":"Language recognition in the sliding window model.","2968":"Towards a Formal Model of Language Networks.","2969":"REALM: Retrieval-Augmented Language Model Pre-Training.","2970":"CharBERT: Character-aware Pre-trained Language Model.","2971":"Microscopic Abrams-Strogatz model of language competition","2972":"A Language Model Sensitive to Discourse Context.","2973":"Statistical language model adaptation: review and perspectives.","2974":"Multi-class composite N-gram language model.","2975":"Connecting and Comparing Language Model Interpolation Techniques.","2976":"Phonetic Temporal Neural Model for Language Identification.","2977":"Extensions of recurrent neural network language model.","2978":"An Efficiently Focusing Large Vocabulary Language Model.","2979":"Bridging Model-Based and Language-Based Security.","2980":"An Analytical Model of Language Resource Sustainability.","2981":"Prosodic attribute model for spoken language identification.","2982":"Contextual Language Model Adaptation for Conversational Agents.","2983":"LINQ as Model Tansformation Language for MDD.","2984":"Hierarchical discriminative model for spoken language understanding.","2985":"Strategies for Language Model Web-Data Collection.","2986":"Language model adaptation for automatic call transcription.","2987":"Language-model optimization by mapping of corpora.","2988":"CLMAD: A Chinese Language Model Adaptation Dataset.","2989":"Contextual language model adaptation using dynamic classes.","2990":"KLMR: a knowledge language for model realization.","2991":"Effective topic-tree based language model adaptation.","2992":"Language model adaptation using cross-lingual information.","2993":"Within and across sentence boundary language model.","2994":"Efficient language model adaptation through MDI estimation.","2995":"Recognition performance of a structured language model.","2996":"ATC: A Low-Level Model Transformation Language.","2997":"XSIM: A corporate financial model development language.","2998":"Language support for model-driven software development.","2999":"A Meta-Model for Language-Independent Refactoring","3000":"Blocking Blog Spam with Language Model Disagreement","3001":"Detecting nepotistic links by language model disagreement","3002":"Model-Based Generation of Natural Language Specifications.","3003":"A Denotational Semantical Model for Orc Language.","3004":"A Component Model for the ABS Language.","3005":"A Task Oriented Natural Language Understanding Model.","3006":"Process Model Generation from Natural Language Text.","3007":"An empirical study on language model adaptation.","3008":"Universal Query Language for Unified State Model.","3009":"A Dynamical Systems Model for Language Change.","3010":"A consolidated language model for speech recognition.","3011":"An interactive activation model of language production","3012":"Selecting Informative Contexts Improves Language Model Finetuning.","3013":"Toupie: a Constraint Language for Model Checking","3014":"LSM: Language Sense Model for Information Retrieval.","3015":"Stochastic Optimization of a Probabilistic Language Model.","3016":"Studying Model Ambiguity in a Language ITS.","3017":"Digging Language Model - Maximum Entropy Phrase Extraction.","3018":"A proximity language model for information retrieval.","3019":"Language model for IR using collection information.","3020":"Natural language semantic model for arithmetic sentences.","3021":"Proposed Model for Natural Language ABAC Authoring.","3022":"A General Language Model for Information Retrieval.","3023":"Personalized Language Model for Query Auto-Completion.","3024":"Intelligent Selection of Language Model Training Data.","3025":"Slovak Language Model from Internet Text Data.","3026":"Tabula: A Language to Model Spreadsheet Tables.","3027":"The challenges of a model transformation language.","3028":"A Statistical Model for Lost Language Decipherment.","3029":"A neuronal model of the language cortex.","3030":"Language model adaptation for video lectures transcription.","3031":"Integrated natural language dialogues: A computational model.","3032":"Emerging Sentiment Language Model for Emotion Detection.","3033":"Declarative Data Cleaning: Language, Model, and Algorithms.","3034":"MOF Model to Text Transformation Language, v1.0","3035":"Learning a language model from continuous speech.","3036":"Using Story Topics for Language Model Adaptation","3037":"Language Model-based Retieval for Farsi Documents.","3038":"A model-based approach to language integration.","3039":"Finnish resources for evaluating language model semantics.","3040":"Language-Specific Model Versioning Based on Signifiers.","3041":"Passage Retrieval Based on Language Models","3042":"A deep language model for software code.","3043":"Supervised learning model for parsing Arabic language.","3044":"Reusing a Statistical Language Model for Generation.","3045":"Search Result Clustering Using Label Language Model.","3046":"Unigram Language Model for Chinese Word Segmentation.","3047":"Supporting a parallel functional language computational model.","3048":"Declarative Data Cleaning: Language, Model, and Algorithms","3049":"A Model for a Spacecraft Operations Language","3050":"Adaptive language model in automatic online subtitling.","3051":"Topic cache language model for speech recognition.","3052":"Automated Model Transformations Using the C.C Language.","3053":"Improving a connectionist based syntactical language model.","3054":"An online incremental language model adaptation method.","3055":"Language model adaptation using minimum discrimination information.","3056":"Automatic control markup language - protocol and model.","3057":"Structured Output Layer neural network language model.","3058":"Natural Language Query Processing for Model Management.","3059":"Language Model and Clustering based Information Retrieval.","3060":"A Computational Model of First Language Acquisition","3061":"Flage: field-oriented language for agents model.","3062":"Search and classification based language model adaptation.","3063":"Big Data Language Model of Contemporary Polish.","3064":"Language model verbalization for automatic speech recognition.","3065":"Unsupervised language model adaptation for broadcast news.","3066":"Integrating detailed information into a language model.","3067":"Language model adaptation via minimum discrimination information.","3068":"Contextual Text Denoising with Masked Language Model.","3069":"hULMonA: The Universal Language Model in Arabic.","3070":"Recognition Performance of a Structured Language Model","3071":"An Article Language Model for BBS Search.","3072":"A Computer Model of Child Language Acquisition.","3073":"Approaches for Neural-Network Language Model Adaptation.","3074":"Pattern Language Verification in Model Driven Design.","3075":"Language-Based Process Model Discovery and Enhancement.","3076":"A new representation of statistical language model.","3077":"Lip-reading based on fuzzy language model.","3078":"Noun-Phrase Model and Natural Query Language.","3079":"A hybrid model to investigate language change.","3080":"Melody Track Selection Using Discriminative Language Model.","3081":"A Trigger Language Model-based IR System.","3082":"A spatiotemporal database model and query language.","3083":"KnowSemLM: A Knowledge Infused Semantic Language Model.","3084":"Using the web for fast language model construction in minority languages.","3085":"Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model.","3086":"A New Word Language Model Evaluation Metric for Character Based Languages.","3087":"Distant Co-occurrence Language Model for ASR in LooseWord Order Languages.","3088":"Language Model Adaptation Using Machine-Translated Text for Resource-Deficient Languages.","3089":"A best-first language processing model integrating the unification grammar and Markov language model for speech recognition applications.","3090":"LF-PPL: A Low-Level First Order Probabilistic Programming Language for\r\n  Non-Differentiable Models","3091":"Efficient Guided Generation for Large Language Models","3092":"MulCode: A Multiplicative Multi-way Model for Compressing Neural Language Model.","3093":"Method driven model: a unified model for an object composition language.","3094":"Construction of spoken language model including fillers using filler prediction model.","3095":"Using TAGs, a Tree Model, and a Language Model for Generation.","3096":"Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model.","3097":"A two phase arabic language model for speech recognition and other language applications.","3098":"Transfer learning of language-independent end-to-end ASR with language model fusion.","3099":"Model-to-Code-Transformation mit der Model to Text Transformation Language.","3100":"A language model based approach towards large scale and lightweight language identification systems.","3101":"Temporal Relationship Extraction for Natural Language Texts by Using Deep Bidirectional Language Model.","3102":"Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion.","3103":"A model contract and model integration language for integrating geography models in distributed environment.","3104":"Using Data-Driven Subword Units in Language Model of Highly Inflective Slovenian Language.","3105":"End-To-End Spoken Language Understanding Without Matched Language Speech Model Pretraining Data.","3106":"Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining.","3107":"ReMoLa: Responsibility model language to align access rights\r\n               with business process requirements","3108":"Topic Structure-Aware Neural Language Model: Unified language model that maintains word and topic ordering by their embedded representations.","3109":"The Object Constraint Language: Getting your models ready for MDA","3110":"VisiCola, a model and a language for visibility control in programming languages.","3111":"Comparison of Different Modeling Units for Language Model Adaptation for Inflected Languages.","3112":"Looking for topic similarities of highly inflected languages for language model adaptation.","3113":"Language modeling via stochastic processes","3114":"Causal Discovery with Language Models as Imperfect Experts","3115":"A Policy Deployment Model for the Ponder Language.","3116":"Abstract Data Types in the Model Programming Language.","3117":"Model Management Based on a Visual Transformation Language.","3118":"Training Code-Switching Language Model with Monolingual Data.","3119":"FinBERT: A Pretrained Language Model for Financial Communications.","3120":"Natural language in the Common Model of Cognition.","3121":"Keyword and phrase spotting with heuristic language model.","3122":"Ecological language acquisition via incremental model-based clustering.","3123":"Prosodic features for a maximum entropy language model.","3124":"Semi-automatic language model acquisition without large corpora.","3125":"Dynamic language model adaptation using keyword category classification.","3126":"Language model cross adaptation for LVCSR system combination.","3127":"The Distributed Ontology, Model and Specification Language \u2013 DOL","3128":"Gram: a graph data model and query language.","3129":"Using Machine Language Model for Mimimorphic Malware Detection.","3130":"A test model for domain-specific language development.","3131":"A Model Language for Describing Spatio-temporal Changes.","3132":"Unsupervised acoustic model training for the Korean language.","3133":"Just Add Functions: A Neural-Symbolic Language Model.","3134":"oLMpics - On what Language Model Pre-training Captures.","3135":"Bayesian phonotactic Language Model for Acoustic Unit Discovery.","3136":"Multiple Model Text Normalization for the Polish Language.","3137":"A Model-Based Combination Language for Scheduling Verification.","3138":"Language Model Rest Costs and Space-Efficient Storage.","3139":"A Language Model based Evaluator for Sentence Compression.","3140":"Using LSTMs to Model the Java Programming Language.","3141":"Toward a Psycholinguistically-Motivated Model of Language Processing.","3142":"Sequencing in a connectionist model of language processing.","3143":"Toward A Multiple Environments Model Of Natural Language.","3144":"System Model-Based Definition of Modeling Language Semantics.","3145":"Deep Attentive Structured Language Model Based on LSTM.","3146":"Pre-trained Language Model for Biomedical Question Answering.","3147":"Representation of Extended RBAC Model Using UML Language.","3148":"MultiFiT: Efficient Multi-lingual Language Model Fine-tuning.","3149":"Experiments with Linguistic Categories for Language Model Optimization.","3150":"The Catchment Feature Model for Multimodal Language Analysis.","3151":"Traffic Session Identification Based on Statistical Language Model.","3152":"Actor Model of a New Functional Language - Anemone.","3153":"A Model for Language Annotations on the Web.","3154":"A Temporal Query Language For A Conceptual Model.","3155":"The design of a language for model transformations.","3156":"VMTL: a language for end-user model transformation.","3157":"Language model based interactive estimation of distribution algorithm.","3158":"Neutral Evolution: a Null Model for Language Dynamics.","3159":"A Geometrically Enhanced Conceptual Model and Query Language.","3160":"Pretrained Language Model Embryology: The Birth of ALBERT.","3161":"Left language model state for syntactic machine translation.","3162":"A rule-based language model for reading recognition.","3163":"The design of a language for model transformations","3164":"Using Constraints with Action Language for Model Evolution.","3165":"Encoding Probabilistic Causal Model in Probabilistic Action Language.","3166":"AraBERT: Transformer-based Model for Arabic Language Understanding.","3167":"Table Search Using a Deep Contextualized Language Model.","3168":"A Concept Language Model for Ad-hoc Retrieval.","3169":"FlauBERT: Unsupervised Language Model Pre-training for French.","3170":"Interpretive Language Implementation from a Layered Operational Model.","3171":"A Model for a Reflective Object-Oriented Language","3172":"A Data Model and Query Language for EXODUS","3173":"Designing a controlled language for interactive model checking","3174":"Towards a Synergetic Statistical Model of Language Phonology.","3175":"Integrating denotational meaning into a DBN language model.","3176":"Model-based semantic dictionaries for medical language understanding.","3177":"Unsupervised language model adaptation using n-gram weighting.","3178":"Temporal Action Detection Using a Statistical Language Model.","3179":"Language Model Pre-training for Hierarchical Document Representations.","3180":"Revisiting the dependence language model for information retrieval.","3181":"Language model based Chinese financial news sentiment classification.","3182":"Neutral evolution: A null model for language dynamics","3183":"Traditional Japanese Haiku Generator using RNN Language Model.","3184":"Scalable language model adaptation for spoken dialogue systems.","3185":"Supporting Process Model Validation through Natural Language Generation.","3186":"Model generation for natural language interpretation and analysis.","3187":"Distributional Tensor Space Model of Natural Language Semantics.","3188":"Bacterial Named Entity Recognition Based on Language Model.","3189":"A Natural Language Interface Using A World Model.","3190":"A Production System Model Of First Language Acquisition.","3191":"Implicitly Supervised Language Model Adaptation for Meeting Transcription.","3192":"Shielding Google's language toxicity model against adversarial attacks.","3193":"A language-model-based approach for subjectivity detection.","3194":"TALE: A Temporal Active Language and Execution Model.","3195":"LMNtal: A Language Model with Links and Membranes.","3196":"Entity-Aware Language Model as an Unsupervised Reranker.","3197":"Build Chinese Language Model with Recurrent Neural Network.","3198":"Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model","3199":"Continuous-speech recognition using a stochastic language model.","3200":"Random Language Model: a path to principled complexity.","3201":"Hierarchical pitman-yor language model for information retrieval.","3202":"A Model of Revision in Natural Language Generation.","3203":"DeltaEcore - A Model-Based Delta Language Generation Framework.","3204":"SciBERT: A Pretrained Language Model for Scientific Text","3205":"Empower Entity Set Expansion via Language Model Probing.","3206":"Pipelined language model construction for Polish speech recognition.","3207":"Topic-Dependent-Class-Based $n$-Gram Language Model.","3208":"Exact training of a neural syntactic language model.","3209":"Selecting articles from the language model training corpus.","3210":"SLAMPA: Recommending Code Snippets with Statistical Language Model.","3211":"Specifying Usage Control Model with Object Constraint Language.","3212":"Model Processing Operations for the Unified Modeling Language","3213":"Vocabulary and language model adaptation using information retrieval.","3214":"Language model size reduction by pruning and clustering.","3215":"A Model for Time Granularity in Natural Language.","3216":"Online language model adaptation for spoken dialog translation.","3217":"Contextual Concept Language Model for Answering Biomedical Questions.","3218":"Graph-Based Language Model of Long-Distance Dependency.","3219":"Integrating Language Model in Handwritten Chinese Text Recognition.","3220":"The Lexicon in a Model of Language Production","3221":"Natural Language Interface on a Video Data Model.","3222":"Perceptually Grounded Language Acquisition: A Neural\/Procedural Model","3223":"AFPL, an Abstract Language Model for Firewall ACLs.","3224":"Research on knowledge elements in exponential language model.","3225":"Latent Topic Visual Language Model for Object Categorization.","3226":"Few-shot NLG with Pre-trained Language Model.","3227":"Bayesian Language Model Interpolation for Mobile Speech Input.","3228":"A specification language for the WIDE workflow model.","3229":"A Model Transformation Language Based on Logic Programming.","3230":"Variable Length Language Model for Chinese Character Recognition.","3231":"Natural Language Generation Model for Mammography Reports Simulation.","3232":"Fine-grained Language Identification with Multilingual CapsNet Model.","3233":"Audio-Attention Discriminative Language Model for ASR Rescoring.","3234":"Language-Level Symmetry Reduction for Probabilistic Model Checking.","3235":"A Public Chinese Dataset for Language Model Adaptation.","3236":"Norm-ML - A Modeling Language to Model Norms.","3237":"A new metric for stochastic language model evaluation.","3238":"A Computational Psycholinguistic Model of Natural Language Processing.","3239":"Style & topic language model adaptation using HMM-LDA","3240":"ModelML: a Markup Language for Automatic Model Synthesis.","3241":"A Model and an Hypothesis for Language Structure","3242":"Structure and performance of a dependency language model","3243":"Automatic language identification using discrete hidden Markov model.","3244":"Unsupervised language model adaptation methods for spontaneous speech.","3245":"Unsupervised estimation of the language model scaling factor.","3246":"On integrating the lexicon with the language model.","3247":"Using information retrieval methods for language model adaptation.","3248":"Pedagogical Model Based on Semantic Web Rule Language.","3249":"A Pseudo-Deterministic Model of Human Language Processing.","3250":"VEHICCLE: a model of web-based language learning.","3251":"Uncertainty detection in natural language: a probabilistic model.","3252":"Leveraging Social Annotation for Topic Language Model Adaptation.","3253":"Iterative language model estimation: efficient data structure & algorithms.","3254":"Automatic Language Model Adaptation for Spoken Document Retrieval.","3255":"Model generation for natural language interpretation and analysis","3256":"Adaptable Multi-Domain Language Model for Transformer ASR.","3257":"Smoothing document language model with local word graph.","3258":"Change Propagation in an Internal Model Transformation Language.","3259":"Few-Shot NLG with Pre-Trained Language Model.","3260":"Future Vector Enhanced LSTM Language Model for LVCSR.","3261":"Hyper-Textual Language Model for Web Information Retrieval.","3262":"The ARC Programming Model - Language Constructs for Coordination.","3263":"Acoustically discriminative language model training with pseudo-hypothesis.","3264":"Towards a Structured Workflow Language for Model Management.","3265":"Two-parameter Model of Word Length \"Language - Genre\"","3266":"Improving Indonesian Text Classification Using Multilingual Language Model.","3267":"Application of reflection in a model transformation language.","3268":"Hyper-textual language model for web information retrieval.","3269":"Matching Reviews to Objects using a Language Model.","3270":"Relevance-Promoting Language Model for Short-Text Conversation.","3271":"A language model for statements of software code.","3272":"A Simple Language Model for Task-Oriented Dialogue.","3273":"Statistical Model Based Approach to Spoken Language Acquisition.","3274":"A discriminative language model with pseudo-negative samples.","3275":"Unsupervised Language Model Adaptation Incorporating Named Entity Information.","3276":"An Active, Object-Oriented, Model-Equivalent Programming Language.","3277":"Learning Semantic Representations in a Bigram Language Model.","3278":"Domain-Specific Language Facilitates Scheduling in Model Checking.","3279":"A Language Specification Tool for Model-Based Parsing.","3280":"A UTP Refinement Model of the STeC Language.","3281":"Voice search language model adaptation using contextual information.","3282":"Universal Language Model Fine-tuning for Patent Classification.","3283":"Feature Model Synthesis from Language-Independent Functional Descriptions.","3284":"Language Model-Based Document Clustering Using Random Walks.","3285":"An exponential translation model for target language morphology.","3286":"Baby-Steps Towards Building a Spanglish Language Model.","3287":"A New Language Model Based on Possibility Theory.","3288":"fUML as an Assembly Language for Model Transformation.","3289":"The hypernode model and its associated query language.","3290":"A Study of Language Model for Image Retrieval.","3291":"A General Parsing Model for Music and Language.","3292":"A Trainable Spaced Repetition Model for Language Learning.","3293":"The Data Model and Query Language of LauRel.","3294":"Clinical Decision Support with the SPUD Language Model.","3295":"Multidimensional Data Model and Query Language for Informetrics.","3296":"Images to syntax: a neuropropositional model of language.","3297":"Syntax-based Attention Model for Natural Language Inference.","3298":"Semantic Labeling Using a Deep Contextualized Language Model.","3299":"Enhancing model transformation synthesis using natural language processing.","3300":"Implementing the DDI4 Model in the R Language","3301":"Motion-Language Association Model for Human-Robot Communication.","3302":"Model-Based Query Language for Analyzing Clinical Processes.","3303":"Statistical Databases: Their Model, Query Language and Security.","3304":"Model of language acquisition. Inductive and deductive approaches","3305":"A Temporal Relational Model and a Query Language.","3306":"EBLA: A Perceptually Grounded Model of Language Acquisition","3307":"A language learning model for finite parameter spaces","3308":"A trellis-based language model for speech recognition.","3309":"Structure and performance of a dependency language model.","3310":"Improved methods for language model based question classification.","3311":"Integrating MAP, marginals, and unsupervised language model adaptation.","3312":"SciBERT: A Pretrained Language Model for Scientific Text.","3313":"Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model.","3314":"Audio-attention discriminative language model for ASR rescoring.","3315":"A Time-Aware Language Model for Microblog Retrieval.","3316":"Multi-Engine Machine Translation with Voted Language Model.","3317":"Phrase Based Language Model For Statistical Machine Translation.","3318":"Synthetic Language Generation and Model Validation in BEAST2.","3319":"A Model of Spatial Reference Frames in Language.","3320":"Model Regularity of Legal Language in Active Modifications.","3321":"A Grammatical Inference Model for Measuring Language Complexity.","3322":"Irony Detection Based on Character Language Model Classifiers.","3323":"A Corpus Balancing Method for Language Model Construction.","3324":"The Distributed Ontology, Model and Specification Language - DOL.","3325":"A statistical semantic language model for source code.","3326":"A Data Model and Query Language for EXODUS.","3327":"A Decomposable Attention Model for Natural Language Inference.","3328":"Multi-Task Neural Model for Agglutinative Language Translation.","3329":"Crossmodal Language Grounding in an Embodied Neurocognitive Model.","3330":"Bayesian molecular design with a chemical language model.","3331":"Heterogeneous recurrent neural networks for natural language model.","3332":"A Neural Model for Predicting Dementia from Language.","3333":"Medical Semantic Similarity with a Neural Language Model.","3334":"An Information-Based Cross-Language Information Retrieval Model.","3335":"Acquisition of a Language Computational Model for NLP.","3336":"Factored Language Model based on Recurrent Neural Network.","3337":"A Language-Independent Unsupervised Model for Morphological Segmentation.","3338":"Language and Translation Model Adaptation using Comparable Corpora.","3339":"Future vector enhanced LSTM language model for LVCSR.","3340":"Syllable-based Myanmar language model for speech recognition.","3341":"Model-Driven Language Engineering: The ASMETA Case Study.","3342":"A hybrid computational model for spoken language understanding.","3343":"A class based language model for speech recognition.","3344":"Agent-Based Model Characterization Using Natural Language Processing.","3345":"Gram: A Graph Data Model and Query Language.","3346":"Abstract property language for MDG model checking methodology.","3347":"Brain-Operated Typewriter using the Language Prediction Model.","3348":"Mental Model Ascription by Language-Enabled Intelligent Agents.","3349":"Stochastic language model for analyzing document physical layout.","3350":"Unsupervised language model adaptation for lecture speech transcription.","3351":"Online Biomedical Concept Annotation Using Language Model Mapping.","3352":"Affix-augmented stem-based language model for persian.","3353":"Universal Language Model Fine-tuning for Text Classification.","3354":"F-Alloy: An Alloy Based Model Transformation Language.","3355":"Language Model Adaptation in Machine Translation from Speech.","3356":"ParsBERT: Transformer-based Model for Persian Language Understanding.","3357":"Language-model-based ranking in entity-relation graphs.","3358":"Structured Indexing Model for Cross-Language Information Retrieval.","3359":"Hierarchical Pitman-Yor Language Model for Machine Translation.","3360":"Service Language Model: New Ecology for Service Development.","3361":"An Efficient Language Model Using Double-Array Structures.","3362":"Style & Topic Language Model Adaptation Using HMM-LDA.","3363":"Towards Model Driven Architectures for Human Language Technologies.","3364":"Explainable Artificial Intelligence (XAI)","3365":"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","3366":"High-performance medicine: the convergence of human and artificial intelligence","3367":"Artificial Intelligence in Education","3368":"A Conversation on Artificial Intelligence, Chatbots, and Plagiarism in Higher Education","3369":"Foundation models for generalist medical artificial intelligence","3370":"Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education","3371":"Can artificial intelligence help for scientific writing?","3372":"Explanation in Artificial Intelligence: Insights from the Social Sciences","3373":"ARTIFICIAL INTELLIGENCE FOR THE REAL WORLD","3374":"Managing artificial intelligence","3375":"Trustworthy artificial intelligence","3376":"Appropriateness of Cardiovascular Disease Prevention Recommendations Obtained From a Popular Online Chat-Based Artificial Intelligence Model.","3377":"Scientific discovery in the age of artificial intelligence","3378":"Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT","3379":"Artificial Intelligence A Modern Approach 3rd Edition","3380":"Artificial intelligence in disease diagnosis: a systematic literature review, synthesizing framework and future research agenda","3381":"From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where","3382":"Trustworthy Artificial Intelligence: A Review","3383":"Additional Comments on the \u201cWhite Paper: On Artificial Intelligence - A European approach to excellence and trust\u201d","3384":"Artificial Intelligence for the Metaverse: A Survey","3385":"Definition, roles, and potential research issues of the metaverse in education: An artificial intelligence perspective","3386":"Dual use of artificial-intelligence-powered drug discovery","3387":"Human activity recognition in artificial intelligence framework: a narrative review","3388":"Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation, and Diagnosis for COVID-19","3389":"Artificial Intelligence in Agriculture: A Literature Survey","3390":"Artificial Intelligence A Modern Approach Global Edition","3391":"Has the Future Started? The Current Growth of Artificial Intelligence, Machine Learning, and Deep Learning","3392":"On scientific understanding with artificial intelligence","3393":"Artificial Intelligence in Healthcare","3394":"Artificial Intelligence Review","3395":"Quo vadis artificial intelligence?","3396":"The potential for artificial intelligence in healthcare","3397":"Artificial intelligence to deep learning: machine intelligence approach for drug discovery","3398":"Artificial intelligence, robotics, advanced technologies and human resource management: a systematic review","3399":"Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing","3400":"Edge Artificial Intelligence for 6G: Vision, Enabling Technologies, and Applications","3401":"The Clinician and Dataset Shift in Artificial Intelligence.","3402":"Artificial intelligence-enhanced electrocardiography in cardiovascular disease management","3403":"Explainable artificial intelligence: an analytical review","3404":"The role of artificial intelligence in healthcare: a structured literature review","3405":"Human Trust in Artificial Intelligence: Review of Empirical Research","3406":"COVID-19 Artificial Intelligence Diagnosis Using Only Cough Recordings","3407":"Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey","3408":"Roles of artificial intelligence in construction engineering and management: A critical review and future trends","3409":"Study on artificial intelligence: The state of the art and future prospects","3410":"Artificial intelligence: A powerful paradigm for scientific research","3411":"Artificial intelligence in radiology","3412":"Artificial Intelligence in Service","3413":"Artificial intelligence and computational pathology","3414":"DARPA's Explainable Artificial Intelligence (XAI) Program","3415":"Predicting cancer outcomes with radiomics and artificial intelligence in radiology","3416":"Artificial intelligence and machine learning in design of mechanical materials.","3417":"Checklist for Artificial Intelligence in Medical Imaging (CLAIM): A Guide for Authors and Reviewers.","3418":"Explainable artificial intelligence: a comprehensive review","3419":"Artificial Intelligence in Cancer Research and Precision Medicine.","3420":"Artificial Intelligence Applied to Battery Research: Hype or Reality?","3421":"Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence","3422":"Joseph E. Aoun: Robot-proof: higher education in the age of artificial intelligence","3423":"Artificial Intelligence and Business Value: a Literature Review","3424":"The practical implementation of artificial intelligence technologies in medicine","3425":"Artificial Intelligence & Accounting Artificial Intelligence & Accounting","3426":"RESEARCH ON ARTIFICIAL INTELLIGENCE FOR CITIZEN SERVICES AND GOVERNMENT","3427":"A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI","3428":"Artificial Intelligence (AI) applications for COVID-19 pandemic","3429":"Human-Centered Artificial Intelligence: Reliable, Safe & Trustworthy","3430":"Abstraction and analogy\u2010making in artificial intelligence","3431":"Artificial intelligence\u2013enabled rapid diagnosis of patients with COVID-19","3432":"Photonics for artificial intelligence and neuromorphic computing","3433":"Artificial Intelligence: the global landscape of ethics guidelines","3434":"Artificial Intelligence and Management: The Automation\u2013Augmentation Paradox","3435":"Inference in artificial intelligence with deep optics and photonics","3436":"Nanotechnology and artificial intelligence to enable sustainable and precision agriculture","3437":"Mapping the Landscape of Artificial Intelligence Applications against COVID-19","3438":"Drug discovery with explainable artificial intelligence","3439":"Explainability for artificial intelligence in healthcare: a multidisciplinary perspective","3440":"Bias in data\u2010driven artificial intelligence systems\u2014An introductory survey","3441":"Application of Artificial Intelligence in Healthcare: Chances and Challenges","3442":"Artificial Intelligence Forecasting of Covid-19 in China","3443":"Towards an Artificial Intelligence Framework for Data-Driven Prediction of Coronavirus Clinical Severity","3444":"Empowering Things With Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things","3445":"The role of artificial intelligence in achieving the Sustainable Development Goals","3446":"Human- versus Artificial Intelligence","3447":"Efficient and Privacy-Enhanced Federated Learning for Industrial Artificial Intelligence","3448":"Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension","3449":"A survey of clinicians on the use of artificial intelligence in ophthalmology, dermatology, radiology and radiation oncology","3450":"An Overview of Artificial Intelligence Applications for Power Electronics","3451":"Artificial intelligence in healthcare: past, present and future","3452":"A strategic framework for artificial intelligence in marketing","3453":"Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension","3454":"Siri, Siri, in my hand: Who\u2019s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence","3455":"In AI we trust? Perceptions about automated decision-making by artificial intelligence","3456":"Application of artificial intelligence in physical education","3457":"How artificial intelligence will change the future of marketing","3458":"The promise of artificial intelligence: a review of the opportunities and challenges of artificial intelligence in healthcare.","3459":"Application of artificial intelligence to the electrocardiogram","3460":"Governance of artificial intelligence","3461":"Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence","3462":"Systematic review of research on artificial intelligence applications in higher education \u2013 where are the educators?","3463":"Artificial intelligence in cyber security: research advances, challenges, and opportunities","3464":"Recommendation of the Council on Artificial Intelligence (OECD)","3465":"Consumers and Artificial Intelligence: An Experiential Perspective","3466":"Artificial intelligence in cancer research, diagnosis and therapy","3467":"Artificial Intelligence and Human Trust in Healthcare: Focus on Clinicians","3468":"Artificial intelligence vs COVID-19: limitations, constraints and pitfalls","3469":"Artificial Intelligence in Medicine: Today and Tomorrow","3470":"Artificial Intelligence in Dentistry: Chances and Challenges","3471":"Artificial Intelligence and COVID-19: Deep Learning Approaches for Diagnosis and Treatment","3472":"Key challenges for delivering clinical impact with artificial intelligence","3473":"A Literature Review of Artificial Intelligence","3474":"Aims for cultivating students\u2019 key competencies based on artificial intelligence education in China","3475":"Business Artificial Intelligence","3476":"Artificial Intelligence and Law","3477":"Notions of explainability and evaluation approaches for explainable artificial intelligence","3478":"Change Detection Based on Artificial Intelligence: State-of-the-Art and Challenges","3479":"Artificial Intelligence and the Future of Work","3480":"Development and evaluation of an artificial intelligence system for COVID-19 diagnosis","3481":"Artificial intelligence in cancer imaging: Clinical challenges and applications","3482":"Human Compatible: Artificial Intelligence and the Problem of Control","3483":"On the Interpretability of Artificial Intelligence in Radiology: Challenges and Opportunities.","3484":"Attachment and trust in artificial intelligence","3485":"Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy","3486":"Artificial intelligence for clinical oncology.","3487":"Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way","3488":"Artificial intelligence, systemic risks, and sustainability","3489":"Applications of Artificial Intelligence and Machine learning in smart cities","3490":"Resistance To Medical Artificial Intelligence","3491":"Artificial Intelligence in Human Resources Management: Challenges and a Path Forward","3492":"BlockIoTIntelligence: A Blockchain-enabled Intelligent IoT Architecture with Artificial Intelligence","3493":"Causability and explainability of artificial intelligence in medicine","3494":"Artificial intelligence in digital pathology \u2014 new tools for diagnosis and precision oncology","3495":"Artificial Intelligence in Medicine: 17th Conference on Artificial Intelligence in Medicine, AIME 2019, Poznan, Poland, June 26\u201329, 2019, Proceedings","3496":"Artificial intelligence, bias and clinical safety","3497":"Artificial Intelligence, Values, and Alignment","3498":"Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness","3499":"Implementation of artificial intelligence in agriculture for optimisation of irrigation and application of pesticides and herbicides","3500":"Artificial intelligence in COVID-19 drug repurposing","3501":"Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence","3502":"Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again","3503":"The rise of artificial intelligence in healthcare applications","3504":"Life 3.0: being human in the age of artificial intelligence","3505":"Artificial intelligence in drug discovery and development","3506":"Overview of artificial intelligence in medicine","3507":"Toward understanding the impact of artificial intelligence on labor","3508":"The impact of artificial intelligence in medicine on the future role of the physician","3509":"Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.","3510":"Ethical and legal challenges of artificial intelligence-driven healthcare","3511":"Rethinking drug design in the artificial intelligence era","3512":"IoT, Big Data, and Artificial Intelligence in Agriculture and Food Industry","3513":"Explainable Artificial Intelligence: a Systematic Review","3514":"Detection of Breast Cancer with Mammography: Effect of an Artificial Intelligence Support System.","3515":"How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence","3516":"Ethics of artificial intelligence and robotics","3517":"Artificial intelligence applications in the development of autonomous vehicles: a survey","3518":"Artificial Intelligence Against Covid-19: An Early Review","3519":"In AI We Trust: Ethics, Artificial Intelligence, and Reliability","3520":"Evaluation of Combined Artificial Intelligence and Radiologist Assessment to Interpret Screening Mammograms","3521":"Innovation and Design in the Age of Artificial Intelligence","3522":"Arming the public with artificial intelligence to counter social bots","3523":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","3524":"Vision, challenges, roles and research issues of Artificial Intelligence in Education","3525":"The history of artificial intelligence in medicine.","3526":"The carbon impact of artificial intelligence","3527":"Artificial Intelligence in Health Care: Bibliometric Analysis","3528":"Artificial Intelligence and Machine Learning Applications in Smart Production: Progress, Trends, and Directions","3529":"Big Data and Artificial Intelligence Modeling for Drug Discovery.","3530":"The Impact of Artificial Intelligence on the Labor Market","3531":"Water Quality Prediction Using Artificial Intelligence Algorithms","3532":"A historical perspective of explainable Artificial Intelligence","3533":"A Research Agenda for Hybrid Intelligence: Augmenting Human Intellect With Collaborative, Adaptive, Responsible, and Explainable Artificial Intelligence","3534":"A short guide for medical professionals in the era of artificial intelligence","3535":"Artificial intelligence in oncology","3536":"The Ethical Implications of Using Artificial Intelligence in Auditing","3537":"Why general artificial intelligence will not be realized","3538":"The unreasonable effectiveness of deep learning in artificial intelligence","3539":"On big data, artificial intelligence and smart cities","3540":"Digital pathology and artificial intelligence.","3541":"A comprehensive review on automation in agriculture using artificial intelligence","3542":"Transparency and reproducibility in artificial intelligence","3543":"Application of Artificial Intelligence in Dentistry","3544":"Emotional intelligence or artificial intelligence\u2013 an employee perspective","3545":"Urban Artificial Intelligence: From Automation to Autonomy in the Smart City","3546":"Four Principles of Explainable Artificial Intelligence","3547":"Artificial intelligence: Who is responsible for the diagnosis?","3548":"The promise of artificial intelligence in chemical engineering: Is it here, finally?","3549":"Role of artificial intelligence in operations environment: a review and bibliometric analysis","3550":"Artificial intelligence in recommender systems","3551":"Design of online intelligent English teaching platform based on artificial intelligence techniques","3552":"Applications of artificial intelligence for disaster management","3553":"Artificial Intelligence and Evolutionary Computations in Engineering Systems","3554":"Mapping the challenges of Artificial Intelligence in the public sector: Evidence from public healthcare","3555":"Artificial intelligence in drug development: present status and future prospects.","3556":"Artificial intelligence in radiation oncology","3557":"Use of CT and artificial intelligence in suspected or COVID-19 positive patients: statement of the Italian Society of Medical and Interventional Radiology","3558":"Artificial intelligence in the creative industries: a review","3559":"Artificial Intelligence in Anesthesiology","3560":"Explore success factors that impact artificial intelligence adoption on telecom industry in China","3561":"Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda","3562":"A Review of Further Directions for Artificial Intelligence, Machine Learning, and Deep Learning in Smart Logistics","3563":"Artificial intelligence for education: Knowledge and its assessment in AI-enabled learning ecologies","3564":"Towards Explainable Artificial Intelligence","3565":"Transparency and trust in artificial intelligence systems","3566":"Criminal justice, artificial intelligence systems, and human rights","3567":"State-of-the-Art Artificial Intelligence Techniques for Distributed Smart Grids: A Review","3568":"Reporting of artificial intelligence prediction models","3569":"Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review.","3570":"Health Care Employees\u2019 Perceptions of the Use of Artificial Intelligence Applications: Survey Study","3571":"Application and theory gaps during the rise of Artificial Intelligence in Education","3572":"Towards Transparency by Design for Artificial Intelligence","3573":"Improving public services using artificial intelligence: possibilities, pitfalls, governance","3574":"The Use of Artificial Intelligence in Tribology\u2014A Perspective","3575":"Artificial Intelligence and Acute Stroke Imaging","3576":"Transparency in artificial intelligence","3577":"Artificial intelligence in health care: accountability and safety","3578":"Artificial intelligence and the future of global health","3579":"Artificial intelligence and communication: A Human\u2013Machine Communication research agenda","3580":"Rulers of the world, unite! The challenges and opportunities of artificial intelligence","3581":"Towards a new generation of artificial intelligence in China","3582":"The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care","3583":"Data governance: Organizing data for trustworthy Artificial Intelligence","3584":"Artificial Intelligence: A guide for Thinking Humans","3585":"Application of Artificial Intelligence to Gastroenterology and Hepatology.","3586":"Artificial Intelligence in the Public Sector","3587":"Artificial intelligence: a survey on evolution, models, applications and future trends","3588":"A new era: artificial intelligence and machine learning in prostate cancer","3589":"Artificial Intelligence in FinTech: understanding robo-advisors adoption among customers","3590":"Artificial Intelligence for Mental Health and Mental Illnesses: an Overview","3591":"Designing Futuristic Telemedicine Using Artificial Intelligence and Robotics in the COVID-19 Era","3592":"What can the brain teach us about building artificial intelligence?","3593":"Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning","3594":"Understanding the Role of Artificial Intelligence in Personalized Engagement Marketing","3595":"Organizational Decision-Making Structures in the Age of Artificial Intelligence","3596":"Artificial-Intelligence-Enabled Intelligent 6G Networks","3597":"Artificial intelligence in education : challenges and opportunities for sustainable development","3598":"Artificial Intelligence: The Ambiguous Labor Market Impact of Automating Prediction","3599":"Customer experiences in the age of artificial intelligence","3600":"Application of artificial intelligence in surgery","3601":"Artificial intelligence in cancer therapy","3602":"Artificial intelligence for diabetic retinopathy screening, prediction and management.","3603":"From Statistical Relational to Neuro-Symbolic Artificial Intelligence","3604":"THE EMERGING ROLE OF ARTIFICIAL INTELLIGENCE IN MODERN SOCIETY","3605":"Artificial intelligence, machine learning and process automation: existing knowledge frontier and way forward for mining sector","3606":"Artificial Intelligence Empowered Edge Computing and Caching for Internet of Vehicles","3607":"What Is Artificial Intelligence?","3608":"Artificial Intelligence in Healthcare: Review and Prediction Case Studies","3609":"Artificial intelligence as the next step towards precision pathology","3610":"Artificial intelligence and sustainable development","3611":"DARPA's explainable artificial intelligence (XAI) program","3612":"Global Evolution of Research in Artificial Intelligence in Health and Medicine: A Bibliometric Study","3613":"Radiomics with artificial intelligence: a practical guide for beginners.","3614":"Introducing Artificial Intelligence Training in Medical Education","3615":"Artificial intelligence and machine learning in clinical development: a translational perspective","3616":"Multimedia Intelligence: When Multimedia Meets Artificial Intelligence","3617":"Artificial Intelligence: American Attitudes and Trends","3618":"Skills for physical artificial intelligence","3619":"Introduction to artificial intelligence in medicine","3620":"The EU Approach to Ethics Guidelines for Trustworthy Artificial Intelligence","3621":"Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns","3622":"Artificial intelligence for diabetic retinopathy screening: a review","3623":"Review of Artificial Intelligence Adversarial Attack and Defense Technologies","3624":"Questions for Artificial Intelligence in Health Care.","3625":"Artificial intelligence in clinical and genomic diagnostics","3626":"Artificial Intelligence and Surgical Decision-Making.","3627":"Artificial Intelligence in Medicine: Where Are We Now?","3628":"Evolutionary Fuzzy Systems for Explainable Artificial Intelligence: Why, When, What for, and Where to?","3629":"Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability","3630":"Artificial intelligence (AI) and its implications for market knowledge in B2B marketing","3631":"Explainable Artificial Intelligence","3632":"Artificial Intelligence (AI)","3633":"The NOMAD laboratory: from data sharing to artificial intelligence","3634":"Application of artificial intelligence in gastroenterology","3635":"Explainable artificial intelligence: A survey","3636":"Artificial intelligence and machine learning in spine research","3637":"Artificial Intelligence and the Implementation Challenge","3638":"Sales profession and professionals in the age of digitization and artificial intelligence technologies: concepts, priorities, and questions","3639":"Artificial Intelligence for COVID-19 Drug Discovery and Vaccine Development","3640":"GeoAI: spatially explicit artificial intelligence techniques for geographic knowledge discovery and beyond","3641":"Artificial Intelligence\u2014The Revolution Hasn\u2019t Happened Yet","3642":"Artificial intelligence and deep learning in ophthalmology","3643":"Artificial intelligence: Implications for the future of work.","3644":"Addressing Bias in Artificial Intelligence in Health Care.","3645":"Artificial Intelligence in Surgery: Promises and Perils","3646":"Methodologic Guide for Evaluating Clinical Performance and Effect of Artificial Intelligence Technology for Medical Diagnosis and Prediction.","3647":"Artificial Intelligence in Cardiology: Present and Future.","3648":"Artificial intelligence biosensors: Challenges and prospects.","3649":"Artificial Intelligence and Marketing: Pitfalls and Opportunities","3650":"Ethical Issues in Advanced Artificial Intelligence","3651":"Legal, regulatory, and ethical frameworks for development of standards in artificial intelligence (AI) and autonomous robotic surgery","3652":"On Defining Artificial Intelligence","3653":"Artificial intelligence faces reproducibility crisis.","3654":"Robustness and Explainability of Artificial Intelligence","3655":"Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models","3656":"Artificial Intelligence in Medical Imaging","3657":"Use of artificial intelligence in infectious diseases","3658":"Using neuroscience to develop artificial intelligence","3659":"Artificial Intelligence, Automation and Work","3660":"Artificial Intelligence for Vehicle-to-Everything: A Survey","3661":"Artificial intelligence in healthcare: An essential guide for health leaders","3662":"Human-Centered Artificial Intelligence and Machine Learning","3663":"Artificial Intelligence, Discretion, and Bureaucracy","3664":"Integrating Artificial Intelligence and Nanotechnology for Precision Cancer Medicine","3665":"Artificial Intelligence in Diabetes Care.","3666":"Machine Learning and Artificial Intelligence","3667":"The Artificial Intelligence of the Ethics of Artificial Intelligence","3668":"Physician perspectives on integration of artificial intelligence into diagnostic pathology","3669":"Artificial Intelligence in Lung Cancer Pathology Image Analysis","3670":"Should Health Care Demand Interpretable Artificial Intelligence or Accept \u201cBlack Box\u201d Medicine?","3671":"Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making","3672":"Synthetic organic chemistry driven by artificial intelligence","3673":"Artificial intelligence in orthodontics","3674":"Emerging Applications of Artificial Intelligence in Neuro-Oncology.","3675":"The Judicial Demand for Explainable Artificial Intelligence","3676":"Applications of Artificial Intelligence in Agriculture: A Review","3677":"Artificial intelligence in medical imaging of the liver","3678":"Applications of Artificial Intelligence Techniques in Engineering","3679":"Understanding artificial intelligence ethics and safety","3680":"Artificial Intelligence and Personalized Medicine.","3681":"The evolution of citation graphs in artificial intelligence research","3682":"Artificial Intelligence for Medical Image Analysis: A Guide for Authors and Reviewers.","3683":"Improving Accuracy and Efficiency with Concurrent Use of Artificial Intelligence for Digital Breast Tomosynthesis.","3684":"The Promise of Artificial Intelligence.","3685":"A is for Artificial Intelligence: The Impact of Artificial Intelligence Activities on Young Children's Perceptions of Robots","3686":"Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application","3687":"Artificial Intelligence for Clinical Trial Design.","3688":"Artificial Intelligence in Higher Education: A Bibliometric Study on its Impact in the Scientific Literature","3689":"Assessment of Accuracy of an Artificial Intelligence Algorithm to Detect Melanoma in Images of Skin Lesions","3690":"Artificial Intelligence in Radiotherapy Treatment Planning: Present and Future","3691":"Sustainable development of organizations based on the combinatorial model of artificial intelligence","3692":"Artificial intelligence for precision medicine in neurodevelopmental disorders","3693":"Artificial intelligence in medical education","3694":"Potential Liability for Physicians Using Artificial Intelligence.","3695":"A review of the artificial intelligence methods in groundwater level modeling","3696":"Advancing Drug Discovery via Artificial Intelligence.","3697":"Marketing and Artificial Intelligence","3698":"The Promise of Artificial Intelligence","3699":"Social and juristic challenges of artificial intelligence","3700":"Towards Intelligent Regulation of Artificial Intelligence","3701":"Artificial Intelligence in Dermatology\u2014Where We Are and the Way to the Future: A Review","3702":"Brain Intelligence: Go beyond Artificial Intelligence","3703":"Artificial intelligence\u2014the third revolution in pathology","3704":"Artificial Intelligence and Machine Learning in Anesthesiology.","3705":"Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed","3706":"DeepStack: Expert-level artificial intelligence in heads-up no-limit poker","3707":"Application of artificial intelligence techniques in the petroleum industry: a review","3708":"Towards artificial general intelligence with hybrid Tianjic chip architecture","3709":"Artificial intelligence in reproductive medicine","3710":"Artificial intelligence in the intensive care unit","3711":"Artificial Intelligence and Law: An Overview","3712":"Artificial Intelligence in Nephrology: Core Concepts, Clinical Applications, and Perspectives.","3713":"ARTIFICIAL INTELLIGENCE APPLICATION IN SMART WAREHOUSING ENVIRONMENT FOR AUTOMATED LOGISTICS","3714":"Artificial Intelligence in Nuclear Medicine","3715":"Primer on artificial intelligence and robotics","3716":"Artificial Intelligence and the Modern Productivity Paradox: A Clash of Expectations and Statistics","3717":"Artificial intelligence in breast ultrasound","3718":"Prediction Machines: The Simple Economics of Artificial Intelligence","3719":"Trusting artificial intelligence in cybersecurity is a double-edged sword","3720":"Artificial intelligence in cancer diagnosis and prognosis: Opportunities and challenges.","3721":"Exploring the impact of artificial intelligence on teaching and learning in higher education","3722":"Understanding Edge Computing: Engineering Evolution With Artificial Intelligence","3723":"Role of Artificial Intelligence within the Telehealth Domain","3724":"The Economics of Artificial Intelligence","3725":"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation","3726":"Artificial intelligence and ambient intelligence","3727":"Artificial Intelligence for Engineering Design , Analysis and Manufacturing","3728":"Trust Me, I\u2019m a Chatbot: How Artificial Intelligence in Health Care Fails the Turing Test","3729":"Artificial Intelligence Transforms the Future of Health Care.","3730":"Machine learning \\& artificial intelligence in the quantum domain","3731":"Review on the Application of Artificial Intelligence in Smart Homes","3732":"Computer vision and artificial intelligence in precision agriculture for grain crops: A systematic review","3733":"Artificial intelligence in glaucoma","3734":"Research on Application of Artificial Intelligence in Computer Network Technology","3735":"Artificial intelligence in retina","3736":"Emerging artificial intelligence methods in structural engineering","3737":"Industrial Artificial Intelligence for industry 4.0-based manufacturing systems","3738":"Applications of artificial intelligence in intelligent manufacturing: a review","3739":"Artificial Intelligence in Cardiology.","3740":"Artificial intelligence in breast imaging.","3741":"Artificial Intelligence for infectious disease Big Data Analytics.","3742":"A Review of Artificial Intelligence in the Internet of Things","3743":"Artificial Intelligence: A Guide to Intelligent Systems","3744":"Art, Creativity, and the Potential of Artificial Intelligence","3745":"Engineering a Less Artificial Intelligence","3746":"Clinical Decision Support in the Era of Artificial Intelligence.","3747":"Intelligent 5G: When Cellular Networks Meet Artificial Intelligence","3748":"Artificial Intelligence, Algorithmic Pricing and Collusion","3749":"Artificial Intelligence for Drug Toxicity and Safety.","3750":"Medical students' attitude towards artificial intelligence: a multicentre survey","3751":"[Artificial intelligence and machine learning].","3752":"Artificial intelligence for the electrocardiogram","3753":"The Impact of Artificial Intelligence on Innovation","3754":"Arti\ufb01cial Intelligence","3755":"Artificial Intelligence and Digital Pathology: Challenges and Opportunities","3756":"Will Artificial Intelligence Replace Radiologists?","3757":"The Forthcoming Artificial Intelligence (AI) Revolution: Its Impact on Society and Firms","3758":"Computer-aided detection in chest radiography based on artificial intelligence: a survey","3759":"Artificial Intelligence and Games","3760":"Artificial Intelligence for Diabetes Management and Decision Support: Literature Review","3761":"Artificial Intelligence and Journalism","3762":"Governing artificial intelligence: ethical, legal and technical opportunities and challenges","3763":"Artificial intelligence, machine learning and health systems","3764":"Ethical governance is essential to building trust in robotics and artificial intelligence systems","3765":"Ethics in artificial intelligence: introduction to the special issue","3766":"State of the Art: Reproducibility in Artificial Intelligence","3767":"Artificial Intelligence in Precision\u00a0Cardiovascular Medicine.","3768":"Artificial Intelligence in the 21st Century","3769":"De Novo Design of Bioactive Small Molecules by Artificial Intelligence","3770":"Artificial intelligence powers digital medicine","3771":"Artificial Intelligence in Medical Practice: The Question to the Answer?","3772":"Intelligent nanophotonics: merging photonics and artificial intelligence at the nanoscale","3773":"Building Ethics into Artificial Intelligence","3774":"Artificial intelligence in medicine: current trends and future possibilities.","3775":"Artificial intelligence-enabled healthcare delivery","3776":"Machine learning: applications of artificial intelligence to imaging and diagnosis","3777":"Physician Confidence in Artificial Intelligence: An Online Mobile Survey","3778":"Gender Bias in Artificial Intelligence: The Need for Diversity and Gender Theory in Machine Learning","3779":"Artificial Intelligence: A European Perspective","3780":"Preparing for the future of Artificial Intelligence","3781":"From analytics to artificial intelligence","3782":"Natural and Artificial Intelligence in Neurosurgery: A Systematic Review","3783":"A survey on artificial intelligence trends in spacecraft guidance dynamics and control","3784":"Artificial Intelligence in Pathology","3785":"Clinical Implications and Challenges of Artificial Intelligence and Deep Learning.","3786":"Real-Time Use of Artificial Intelligence in Identification of Diminutive Polyps During Colonoscopy","3787":"Artificial Intelligence and Economic Growth","3788":"Cyber security meets artificial intelligence: a survey","3789":"Artificial intelligence for analyzing orthopedic trauma radiographs","3790":"Artificial Intelligence and Big Data in Public Health","3791":"Artificial intelligence in Internet of things","3792":"Building Trust in Artificial Intelligence, Machine Learning, and Robotics","3793":"Artificial Intelligence in Drug Discovery and Development","3794":"Artificial Intelligence in Drug Design","3795":"Artificial Intelligence for Long-Term Robot Autonomy: A Survey","3796":"Exploring the Impact of Artificial Intelligence: Prediction Versus Judgment","3797":"THE ETHICS OF ARTIFICIAL INTELLIGENCE","3798":"Applying artificial intelligence: implications for recruitment","3799":"A novel hybrid artificial intelligence approach for flood susceptibility assessment","3800":"Artificial Intelligence, Economics, and Industrial Organization","3801":"Artificial intelligence in healthcare","3802":"Artificial intelligence and machine learning in emergency medicine","3803":"Quantitative Phase Imaging and Artificial Intelligence: A Review","3804":"Artificial intelligence and echocardiography","3805":"Will artificial intelligence solve the human resource crisis in healthcare?","3806":"How artificial intelligence is changing drug discovery","3807":"Artificial Intelligence and its Role in Near Future","3808":"Health intelligence: how artificial intelligence transforms population and personalized health","3809":"Chatbots, Humbots, and the Quest for Artificial General Intelligence","3810":"The Emergence of Artificial Intelligence: How Automation is Changing Auditing","3811":"Framing the challenges of artificial intelligence in medicine","3812":"Artificial Intelligence and its Implications for Income Distribution and Unemployment","3813":"Artificial Intelligence trends in education: a narrative overview","3814":"Artificial intelligence in gastrointestinal endoscopy: The future is almost here","3815":"Hello marketing, what can artificial intelligence help you with?","3816":"Discrimination, artificial intelligence, and algorithmic decision-making","3817":"Artificial Intelligence: A Very Short Introduction","3818":"Artificial intelligence in drug design","3819":"Artificial intelligence test: a case study of intelligent vehicles","3820":"What This Computer Needs Is a Physician: Humanism and Artificial Intelligence.","3821":"An artificial intelligence platform for the multihospital collaborative management of congenital cataracts","3822":"Has artificial intelligence become alchemy?","3823":"DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker","3824":"Artificial Intelligence Policy: A Primer and Roadmap","3825":"Application of artificial intelligence in ophthalmology.","3826":"Linking Artificial Intelligence Principles","3827":"Managing the Global Financial System on the Basis of Artificial Intelligence: Possibilities and Limitations","3828":"Progress in Artificial Intelligence: 18th EPIA Conference on Artificial Intelligence, EPIA 2017, Porto, Portugal, September 5-8, 2017, Proceedings","3829":"Advances of flexible pressure sensors toward artificial intelligence and health care applications","3830":"Artificial intelligence in cardiology","3831":"Artificial intelligence and statistics","3832":"Life 3.0: Being Human in the Age of Artificial Intelligence","3833":"The Technological Elements of Artificial Intelligence","3834":"A Review on Application of Artificial Intelligence in Teaching and Learning in Educational Contexts","3835":"A reference framework and overall planning of industrial artificial intelligence (I-AI) for new application scenarios","3836":"Analysis of the Impact of Artificial Intelligence Application on the Development of Accounting Industry","3837":"The future of radiology augmented with Artificial Intelligence: A strategy for success.","3838":"Why Artificial Intelligence Will Not Outsmart Complex Knowledge Work","3839":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face","3840":"From local explanations to global understanding with explainable AI for trees","3841":"Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning","3842":"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT","3843":"Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education","3844":"Constitutional AI: Harmlessness from AI Feedback","3845":"International evaluation of an AI system for breast cancer screening","3846":"AI did not write this manuscript, or did it? Can we trick the AI text detector into generated texts? The potential future of ChatGPT and AI in Sports & Exercise Medicine manuscript generation","3847":"Artificial Intelligence Risk Management Framework (AI RMF 1.0)","3848":"Ethics Guidelines for Trustworthy AI","3849":"Fusing Blockchain and AI With Metaverse: A Survey","3850":"AI-Based Modeling: Techniques, Applications and Research Issues Towards Automation, Intelligent and Smart Systems","3851":"SenticNet 7: A Commonsense-based Neurosymbolic AI Framework for Explainable Sentiment Analysis","3852":"Multimodal biomedical AI","3853":"Can AI Help in Screening Viral and COVID-19 Pneumonia?","3854":"Habitat: A Platform for Embodied AI Research","3855":"\u201cEveryone wants to do the model work, not the data work\u201d: Data Cascades in High-Stakes AI","3856":"Atlas of AI","3857":"Advances, challenges and opportunities in creating data for trustworthy AI","3858":"What is AI Literacy? Competencies and Design Considerations","3859":"Concrete Problems in AI Safety","3860":"Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making","3861":"Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance","3862":"Artificial Intelligence (AI) and Internet of Medical Things (IoMT) Assisted Biomedical Systems for Intelligent Healthcare","3863":"Reporting guideline for the early stage clinical evaluation of decision support systems driven by artificial intelligence: DECIDE-AI","3864":"The Roadmap to 6G - AI Empowered Wireless Networks","3865":"Explainable AI: A Review of Machine Learning Interpretability Methods","3866":"Prevention of Phishing Attacks Using AI-Based Cybersecurity Awareness Training","3867":"The Future of AI in Medicine: A Perspective from a Chatbot","3868":"Ray: A Distributed Framework for Emerging AI Applications","3869":"Guidelines for Human-AI Interaction","3870":"Reporting guideline for the early-stage clinical evaluation of decision support systems driven by artificial intelligence: DECIDE-AI","3871":"Towards Long Lifetime Battery: AI-Based Manufacturing and Management","3872":"Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence","3873":"The Ethics of AI Ethics: An Evaluation of Guidelines","3874":"Rapid AI Development Cycle for the Coronavirus (COVID-19) Pandemic: Initial Results for Automated Detection & Patient Monitoring using Deep Learning CT Image Analysis","3875":"AI Feynman: A physics-inspired method for symbolic regression","3876":"Thinking responsibly about responsible AI and \u2018the dark side\u2019 of AI","3877":"Explainable AI","3878":"AI2-THOR: An Interactive 3D Environment for Visual AI","3879":"Principles alone cannot guarantee ethical AI","3880":"A Survey on the Convergence of Edge Computing and AI for UAVs: Opportunities and Challenges","3881":"Advancing mathematics by guiding human intuition with AI","3882":"Trust in AI and Its Role in the Acceptance of AI Technologies","3883":"AI4People\u2014An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations","3884":"The uselessness of AI ethics","3885":"Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI","3886":"The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies","3887":"How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals","3888":"Green AI","3889":"AI for radiographic COVID-19 detection selects shortcuts over signal","3890":"The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture","3891":"Superhuman AI for multiplayer poker","3892":"Explainable AI: Interpreting, Explaining and Visualizing Deep Learning","3893":"AI-based chatbots in customer service and their effects on user compliance","3894":"Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI","3895":"Engaged to a Robot? The Role of AI in Service","3896":"AI-Driven Tools for Coronavirus Outbreak: Need of Active Learning and Cross-Population Train\/Test Models on Multitudinal\/Multimodal Data","3897":"Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems","3898":"Aligning AI With Shared Human Values","3899":"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks","3900":"Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design","3901":"Blockchain for AI: Review and Open Research Challenges","3902":"Explainable AI: from black box to glass box","3903":"A Unified Framework of Five Principles for AI in Society","3904":"AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system","3905":"Expanding Explainability: Towards Social Transparency in AI systems","3906":"How to cheat on your final paper: Assigning AI for student writing","3907":"From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices","3908":"Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices","3909":"Defining organizational AI governance","3910":"Planning chemical syntheses with deep neural networks and symbolic AI","3911":"Sustainable AI: AI for sustainability and the sustainability of AI","3912":"AI and the Everything in the Whole Wide World Benchmark","3913":"Superhuman AI for heads-up no-limit poker: Libratus beats top professionals","3914":"Do as AI say: susceptibility in deployment of clinical decision-aids","3915":"Explaining Explanations in AI","3916":"AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias","3917":"Doctor AI: Predicting Clinical Events via Recurrent Neural Networks","3918":"Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products","3919":"Sustainable AI: Environmental Implications, Challenges and Opportunities","3920":"In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning","3921":"AI-Driven Cybersecurity: An Overview, Security Intelligence Modeling and Research Directions","3922":"Machines as teammates: A research agenda on AI in team collaboration","3923":"ViZDoom: A Doom-based AI research platform for visual reinforcement learning","3924":"Human-Centered AI","3925":"Towards a Roadmap on Software Engineering for Responsible AI","3926":"A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems","3927":"A robotic platform for flow synthesis of organic compounds informed by AI planning","3928":"The Role of AI, Machine Learning, and Big Data in Digital Twinning: A Systematic Literature Review, Challenges, and Opportunities","3929":"AI in medicine must be explainable","3930":"What about investors? ESG analyses as tools for ethics-based AI auditing","3931":"Adoption and use of AI tools: a research agenda grounded in UTAUT","3932":"When Will AI Exceed Human Performance? Evidence from AI Experts","3933":"What do we need to build explainable AI systems for the medical domain?","3934":"Ready or Not, AI Comes\u2014 An Interview Study of Organizational AI Readiness Factors","3935":"HUMAN + MACHINE: REIMAGINING WORK IN THE AGE OF AI","3936":"Trustworthy AI: From Principles to Practices","3937":"Software Engineering for AI-Based Systems: A Survey","3938":"Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies","3939":"The AI Index 2021 Annual Report","3940":"A Survey of Embodied AI: From Simulators to Research Tasks","3941":"A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective","3942":"A Review of Artificial Intelligence (AI) in Education from 2010 to 2020","3943":"Understanding and Creating Art with AI: Review and Outlook","3944":"Trustworthy AI: A Computational Perspective","3945":"Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI","3946":"Acceptability of artificial intelligence (AI)-led chatbot services in healthcare: A mixed-methods study","3947":"Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork","3948":"AI can be sexist and racist \u2014 it\u2019s time to make it fair","3949":"Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims","3950":"Communication-Efficient Edge AI: Algorithms and Systems","3951":"A Survey on AI-Driven Digital Twins in Industry 4.0: Smart Manufacturing and Advanced Robotics","3952":"Talking AI into Being: The Narratives and Imaginaries of National AI Strategies and Their Performative Politics","3953":"Mapping value sensitive design onto AI for social good principles","3954":"The Oxford Handbook of Ethics of AI","3955":"Excavating AI: the politics of images in machine learning training sets","3956":"AI-Native Network Slicing for 6G Networks","3957":"Artificial Intelligence (AI) and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts","3958":"An international survey on AI in radiology in 1,041 radiologists and radiology residents part 1: fear of replacement, knowledge, and attitude","3959":"Can we open the black box of AI?","3960":"Understanding dark side of artificial intelligence (AI) integrated business analytics: assessing firm\u2019s operational inefficiency and competitiveness","3961":"RoboTHOR: An Open Simulation-to-Real Embodied AI Platform","3962":"The AI gambit: leveraging artificial intelligence to combat climate change\u2014opportunities, challenges, and recommendations","3963":"Rise of Machine Agency: A Framework for Studying the Psychology of Human-AI Interaction (HAII)","3964":"Adoption of AI-based chatbots for hospitality and tourism","3965":"\"Hello AI\": Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making","3966":"Ethics as a Service: A Pragmatic Operationalisation of AI Ethics","3967":"Intention to use analytical artificial intelligence (AI) in services \u2013 the effect of technology readiness and awareness","3968":"How AI Developers Overcome Communication Challenges in a Multidisciplinary Team","3969":"Socially Responsible AI Algorithms: Issues, Purposes, and Challenges","3970":"Assessing the Fairness of AI Systems: AI Practitioners' Processes, Challenges, and Needs for Support","3971":"Precision Medicine, AI, and the Future of Personalized Health Care","3972":"Content moderation, AI, and the question of scale","3973":"Influence of artificial intelligence (AI) on firm performance: the business value of AI-based transformation projects","3974":"Neurosymbolic AI: the 3rd wave","3975":"Predicting COVID-19 in China Using Hybrid AI Model","3976":"Collaborative Intelligence: Humans and AI Are Joining Forces","3977":"Developing Middle School Students' AI Literacy","3978":"Truthful AI: Developing and governing AI that does not lie","3979":"Contributions and Risks of Artificial Intelligence (AI) in Building Smarter Cities: Insights from a Systematic Review of the Literature","3980":"Envisioning AI for K-12: What Should Every Child Know about AI?","3981":"\u201cBrilliant AI Doctor\u201d in Rural Clinics: Challenges in AI-Powered Clinical Decision Support System Deployment","3982":"On assessing trustworthy AI in healthcare Best practice for machine learning as a supportive tool to recognize cardiac arrest in emergency calls","3983":"AI-Mediated Communication: Definition, Research Agenda, and Ethical Considerations","3984":"Rearrangement: A Challenge for Embodied AI","3985":"One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques","3986":"AI ethics","3987":"AI-Driven Zero Touch Network and Service Management in 5G and Beyond: Challenges and Research Directions","3988":"A governance model for the application of AI in health care","3989":"Human capital and AI in industry 4.0. Convergence and divergence in social entrepreneurship in Russia","3990":"Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance","3991":"The Hanabi Challenge: A New Frontier for AI Research","3992":"Digital servitization value co-creation framework for AI services: a research agenda for digital transformation in financial service ecosystems","3993":"Influences of artificial intelligence (AI) awareness on career competency and job burnout","3994":"Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges","3995":"MINIMAR (MINimum Information for Medical AI Reporting): Developing reporting standards for artificial intelligence in health care","3996":"The Who in Explainable AI: How AI Background Shapes Perceptions of AI Explanations","3997":"Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI","3998":"Human-AI Collaboration in Data Science","3999":"Putting AI ethics to work: are the tools fit for purpose?","4000":"Transitioning to Human Interaction with AI Systems: New Challenges and Opportunities for HCI Professionals to Enable Human-Centered AI","4001":"Responsible AI for Digital Health: a Synthesis and a Research Agenda","4002":"The Feeling Economy: Managing in the Next Generation of Artificial Intelligence (AI)","4003":"The Wrong Kind of Ai? Artificial Intelligence and the Future of Labor Demand","4004":"Blockchain and AI-Based Solutions to Combat Coronavirus (COVID-19)-Like Epidemics: A Survey","4005":"How to Design AI for Social Good: Seven Essential Factors","4006":"Use of AI-based tools for healthcare purposes: a survey study from consumers\u2019 perspectives","4007":"AI and Jobs: Evidence from Online Vacancies","4008":"On the Utility of Learning about Humans for Human-AI Coordination","4009":"Establishing the rules for building trustworthy AI","4010":"From a \u2018race to AI\u2019 to a \u2018race to AI regulation\u2019: regulatory competition for artificial intelligence","4011":"Operationalising AI ethics: barriers, enablers and next steps","4012":"Open Problems in Cooperative AI","4013":"Industry 4.0 in Finance: The Impact of Artificial Intelligence (AI) on Digital Financial Inclusion","4014":"Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models","4015":"Artificial Intelligence (AI) Ethics: Ethics of AI and Ethical AI","4016":"Developing specific reporting guidelines for diagnostic accuracy studies assessing AI interventions: The STARD-AI Steering Group","4017":"Looking Back, Looking Ahead: Symbolic versus Connectionist AI","4018":"Why Are We Using Black Box Models in AI When We Don\u2019t Need To? A Lesson From An Explainable AI Competition","4019":"AI-based computer-aided diagnosis (AI-CAD): the latest review to read first","4020":"The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies","4021":"How AI can be a force for good","4022":"AI for social good: unlocking the opportunity for positive impact","4023":"Behavior Trees in Robotics and AI: An Introduction","4024":"Embedding Values in Artificial Intelligence (AI) Systems","4025":"All the News That\u2019s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation","4026":"Updates in Human-AI Teams: Understanding and Addressing the Performance\/Compatibility Tradeoff","4027":"Next generation digital platforms : toward human-AI hybrids","4028":"The Dark Sides of Artificial Intelligence: An Integrated AI Governance Framework for Public Administration","4029":"AI Benchmark: Running Deep Neural Networks on Android Smartphones","4030":"On The Role of Knowledge Graphs in Explainable AI","4031":"Will You Accept an Imperfect AI?: Exploring Designs for Adjusting End-user Expectations of AI Systems","4032":"The relationship between trust in AI and trustworthy machine learning technologies","4033":"Conversational AI: The Science Behind the Alexa Prize","4034":"AI Benchmark: All About Deep Learning on Smartphones in 2019","4035":"A 65nm 1Mb nonvolatile computing-in-memory ReRAM macro with sub-16ns multiply-and-accumulate for binary DNN AI edge processors","4036":"Trustworthy AI","4037":"AI ethics should not remain toothless! A call to bring back the teeth of ethics","4038":"The AI-Based Cyber Threat Landscape","4039":"2 SUPERINTELLIGENCE, MONSTERS, AND THE AI APOCALYPSE","4040":"Blind spots in AI ethics","4041":"What Does Explainable AI Really Mean? A New Conceptualization of Perspectives","4042":"AI-Based Digital Assistants","4043":"Blockchain and AI Meet in the Metaverse","4044":"Envisioning Communities: A Participatory Approach Towards AI for Social Good","4045":"Emerging challenges in AI and the need for AI ethics education","4046":"Designing for human-AI complementarity in K-12 education","4047":"The \u201cinconvenient truth\u201d about AI in healthcare","4048":"Artificial intelligence (AI) and global health: how can AI contribute to health in resource-poor settings?","4049":"Implementation of artificial intelligence (AI) applications in radiology: hindering and facilitating factors","4050":"AI Techniques for COVID-19","4051":"The dark side of AI-powered service interactions: exploring the process of co-destruction from the customer perspective","4052":"Continuous Learning AI in Radiology: Implementation Principles and Early Applications.","4053":"Demystifying AI: What Digital Transformation Leaders Can Teach You about Realistic Artificial Intelligence","4054":"Adoption of artificial intelligence (AI) for talent acquisition in IT\/ITeS organizations","4055":"Accountability of AI Under the Law: The Role of Explanation","4056":"CAPTCHA: Using Hard AI Problems for Security","4057":"Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences","4058":"Close Encounters of the AI Kind: Use of AI Influencers As Brand Endorsers","4059":"Improving AI System Awareness of Geoscience Knowledge: Symbiotic Integration of Physical Approaches and Deep Learning","4060":"The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions","4061":"From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People","4062":"Explainable AI in Healthcare","4063":"Do Humans Trust Advice More if it Comes from AI?: An Analysis of Human-AI Interactions","4064":"Increasing Trust in AI Services through Supplier's Declarations of Conformity","4065":"Designing fair AI for managing employees in organizations: a review, critique, and design agenda","4066":"Conversational AI: Dialogue Systems, Conversational Agents, and Chatbots","4067":"Translational AI and Deep Learning in Diagnostic Pathology","4068":"A new generation of AI: A review and perspective on machine learning technologies applied to smart energy and electric power systems","4069":"Beyond the promise: implementing ethical AI","4070":"Impact of AI and robotics in the tourism sector: a critical insight","4071":"Can we do better explanations? A proposal of user-centered explainable AI","4072":"CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis","4073":"AI in the headlines: the portrayal of the ethical issues of artificial intelligence in the media","4074":"The Whiteness of AI","4075":"Artificial Intelligence (AI) or Intelligence Augmentation (IA): What Is the Future?","4076":"Mental Models of AI Agents in a Cooperative Game Setting","4077":"My Teacher Is a Machine: Understanding Students\u2019 Perceptions of AI Teaching Assistants in Online Education","4078":"Anthropomorphism in AI","4079":"Limits of trust in medical AI","4080":"Gender , Race , and Power in AI","4081":"Tourists\u2019 Attitudes toward the Use of Artificially Intelligent (AI) Devices in Tourism Service Delivery: Moderating Role of Service Value Seeking","4082":"Patients\u2019 views of wearable devices and AI in healthcare: findings from the ComPaRe e-cohort","4083":"From Machine Learning to Explainable AI","4084":"AI in Health: State of the Art, Challenges, and Future Directions","4085":"Disability, Bias, and AI","4086":"More Than \"If Time Allows\": The Role of Ethics in AI Education","4087":"An embedded ethics approach for AI development","4088":"Enabling AI in Future Wireless Networks: A Data Life Cycle Perspective","4089":"FactSheets: Increasing trust in AI services through supplier's declarations of conformity","4090":"Artificial Intelligence in Clinical Decision Support: Challenges for Evaluating AI and Practical Implications","4091":"AI and Blockchain: A Disruptive Integration","4092":"Artificial intelligence (AI) in strategic marketing decision-making: a research agenda","4093":"Trustworthy artificial intelligence (AI) in education","4094":"Intelligence Unleashed: An argument for AI in Education","4095":"AI-IMU Dead-Reckoning","4096":"Inclusive AI literacy for kids around the world","4097":"AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence","4098":"AI-Assisted Decision-making in Healthcare","4099":"Artificial Intelligence (AI) for the early detection of breast cancer: a scoping review to assess AI\u2019s potential in breast screening practice","4100":"Explainable AI in Industry","4101":"Designing educational technologies in the age of AI: A learning sciences-driven approach","4102":"Lessons learned from AI ethics principles for future actions","4103":"AI Safety Gridworlds","4104":"Four ethical priorities for neurotechnologies and AI","4105":"Clinical AI: opacity, accountability, responsibility and liability","4106":"Hybrid collective intelligence in a human\u2013AI society","4107":"The European Legal Framework for Medical AI","4108":"Current Advances, Trends and Challenges of Machine Learning and Knowledge Extraction: From Machine Learning to Explainable AI","4109":"Anatomy of an AI System","4110":"AI for climate: freedom, justice, and other ethical and political challenges","4111":"Explainable AI for Designers: A Human-Centered Perspective on Mixed-Initiative Co-Creation","4112":"The Nooscope manifested: AI as instrument of knowledge extractivism","4113":"With an eye to AI and autonomous diagnosis","4114":"AI-Mediated Communication: How the Perception that Profile Text was Written by AI Affects Trustworthiness","4115":"AI for 5G: research directions and paradigms","4116":"AI for medical imaging goes deep","4117":"Toward human-centered AI","4118":"Friend, Collaborator, Student, Manager: How Design of an AI-Driven Game Level Editor Affects Creators","4119":"You cannot have AI ethics without ethics","4120":"A Misdirected Principle with a Catch: Explicability for AI","4121":"A Berkeley View of Systems Challenges for AI","4122":"Transparent, explainable, and accountable AI for robotics","4123":"No AI Is an Island: The Case for Teaming Intelligence","4124":"Activism by the AI Community: Analysing Recent Achievements and Future Prospects","4125":"Are Current Tort Liability Doctrines Adequate for Addressing Injury Caused by AI?","4126":"\"Scary Robots\": Examining Public Responses to AI","4127":"A Layered Model for AI Governance","4128":"Toward fairness in AI for people with disabilities SBG@a research roadmap","4129":"The Demand for AI Skills in the Labor Market","4130":"What's Next for AI Ethics, Policy, and Governance? A Global Overview","4131":"AI^2: Training a Big Data Machine to Defend","4132":"Managing Bias in AI","4133":"Designing for Complementarity: Teacher and Student Needs for Orchestration Support in AI-Enhanced Classrooms","4134":"Letting the Computers Take Over: Using AI to Solve Marketing Problems","4135":"AI safety via debate","4136":"There is a blind spot in AI research","4137":"Computer knows best? The need for value-flexibility in medical AI","4138":"Asking 'Why' in AI: Explainability of intelligent systems - perspectives and challenges","4139":"The AI Advantage: How to Put the Artificial Intelligence Revolution to Work","4140":"AI Ethics - Too Principled to Fail?","4141":"Job candidates\u2019 reactions to AI-Enabled job application processes","4142":"AI in talent acquisition: a review of AI-applications used in recruitment and selection","4143":"Stakeholders in Explainable AI","4144":"A Review of Future and Ethical Perspectives of Robotics and AI","4145":"General Video Game AI: Competition, Challenges and Opportunities","4146":"Artificial Intelligence Adoption: AI-readiness at Firm-Level","4147":"Demystification of AI-driven medical image interpretation: past, present and future","4148":"Towards a Serverless Platform for Edge AI","4149":"AI in Education needs interpretable machine learning: Lessons from Open Learner Modelling","4150":"Defining AI in Policy versus Practice","4151":"AI Extenders: The Ethical and Societal Implications of Humans Cognitively Extended by AI","4152":"15 challenges for AI: or what AI (currently) can\u2019t do","4153":"A Year in K-12 AI Education","4154":"Challenges and opportunities: from big data to knowledge in AI 2.0","4155":"Ai and Jobs: The Role of Demand","4156":"Challenges of Human-Aware AI Systems","4157":"AI and the path to envelopment: knowledge as a first step towards the responsible regulation and use of AI-powered machines","4158":"Responsible Artificial Intelligence: Designing Ai for Human Values","4159":"TED: Teaching AI to Explain its Decisions","4160":"Responsible AI: requirements and challenges","4161":"Directions in Hybrid Intelligence: Complementing AI Systems with Human Intelligence","4162":"THE AI ADVANTAGE","4163":"Customer Acceptance of AI in Service Encounters: Understanding Antecedents and Consequences","4164":"Scaling learning algorithms towards AI","4165":"AI, agency and responsibility: the VW fraud case and beyond","4166":"Methods of AI for Multimodal Sensing and Action for Complex Situations","4167":"On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications","4168":"Building Ethically Bounded AI","4169":"Techology trend of edge AI","4170":"Libratus: The Superhuman AI for No-Limit Poker","4171":"AI Now 2017 Report","4172":"Evaluating Visual Conversational Agents via Cooperative Human-AI Games","4173":"An AI Race for Strategic Advantage: Rhetoric and Risks","4174":"Modelling and Influencing the AI Bidding War: A Research Agenda","4175":"The AI detectives.","4176":"AI Anxiety","4177":"Incomplete Contracting and AI Alignment","4178":"Reframing AI Discourse","4179":"Impacts on Trust of Healthcare AI","4180":"Towards an \"Ethics by Design\" Methodology for AI Research Projects","4181":"AI: Its Nature and Future","4182":"My Computer Is an Honor Student - but How Intelligent Is It? Standardized Tests as a Measure of AI","4183":"Using AI to Teach AI: Lessons from an Online AI Class","4184":"Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!","4185":"A New AI Evaluation Cosmos: Ready to Play the Game?","4186":"AI and education: the importance of teacher and student relations","4187":"Artificial intelligence. Fears of an AI pioneer.","4188":"AI-based Game Design Patterns","4189":"Introduction to AI Robotics","4190":"A Review of Real-Time Strategy Game AI","4191":"Computational Notebooks for AI Education","4192":"AI Methods in Algorithmic Composition: A Comprehensive Survey","4193":"The errors, insights and lessons of famous AI predictions \u2013 and what they mean for the future","4194":"Ethics in Machine Learning and Other Domain-Specific AI Algorithms","4195":"The Machine Question: Critical Perspectives on AI, Robots, and Ethics","4196":"AI Grand Challenges for Education","4197":"VEGA-QSAR: AI Inside a Platform for Predictive Toxicology","4198":"Turing Test as a Defining Feature of AI-Completeness","4199":"Planning with Markov Decision Processes: An AI Perspective","4200":"Game AI Pro 2: Collected Wisdom of Game AI Professionals","4201":"The Mario AI Championship 2009-2012","4202":"Game AI revisited","4203":"The Mario AI Benchmark and Competitions","4204":"A history of AI and Law in 50 papers: 25\u00a0years of the international conference on AI and Law","4205":"Thinking Inside the Box: Controlling and Using an Oracle AI","4206":"Melomics: A Case-Study of AI in Spain","4207":"Tools such as ChatGPT threaten transparent science; here are our ground rules for their use","4208":"ChatGPT: five priorities for research","4209":"AI and Opinion Mining","4210":"ChatGPT is fun, but not an author","4211":"Preferences in AI: An overview","4212":"Monte-Carlo Tree Search: A New Framework for Game AI","4213":"AI-Complete, AI-Hard, or AI-Easy - Classification of Problems in AI","4214":"Intelligent Technology for an Aging Population: The Use of AI to Assist Elders with Cognitive Impairment","4215":"Chatting and cheating: Ensuring academic integrity in the era of ChatGPT","4216":"ChatGPT listed as author on research papers: many scientists disapprove","4217":"Explanation and trust: what to tell the user in security and AI?","4218":"Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios","4219":"The 2010 Mario AI Championship: Level Generation Track","4220":"Abstracts written by ChatGPT fool scientists","4221":"Outlier Detection in High Dimensional Data","4222":"ChatGPT Goes to Law School","4223":"Examining Science Education in ChatGPT: An Exploratory Study of Generative Artificial Intelligence","4224":"Human-Level AI's Killer Application: Interactive Computer Games","4225":"Holistic Network Virtualization and Pervasive Network Intelligence for 6G","4226":"The Rise and Potential of Large Language Model Based Agents: A Survey","4227":"Why Heideggerian AI Failed and How Fixing it Would Require Making it More Heideggerian","4228":"Representation Learning: A Review and New Perspectives","4229":"A New Direction in AI: Toward a Computational Theory of Perceptions","4230":"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)","4231":"AI's War on Manipulation: Are We Winning?","4232":"Dynamic Game Difficulty Scaling Using Adaptive Behavior-Based AI","4233":"The Alchemy System for Statistical Relational AI: User Manual","4234":"A Survey on Bias and Fairness in Machine Learning","4235":"Word sense disambiguation: A survey","4236":"Generative Adversarial Text to Image Synthesis","4237":"Efficient Processing of Deep Neural Networks: A Tutorial and Survey","4238":"A Survey of Automated Web Service Composition Methods","4239":"Relational inductive biases, deep learning, and graph networks","4240":"Fog and IoT: An Overview of Research Opportunities","4241":"Ensemble-based classifiers","4242":"Designing games with a purpose","4243":"About the authors","4244":"Artificial intelligence for edge service optimization in Internet of Vehicles: A survey","4245":"Multiagent Systems: A Modern Approach to Dis- tributed Artificial Intelligence A Review","4246":"Pyro: Deep Universal Probabilistic Programming","4247":"The 2009 Mario AI Competition","4248":"Dota 2 with Large Scale Deep Reinforcement Learning","4249":"Brave new world: service robots in the frontline","4250":"Health Care, Capabilities, and AI Assistive Technologies","4251":"Software Engineering for Machine Learning: A Case Study","4252":"The Basic AI Drives","4253":"COVID-CT-Dataset: A CT Scan Dataset about COVID-19","4254":"Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems","4255":"A Brief History of Artificial Intelligence: On the Past, Present, and Future of Artificial Intelligence","4256":"The 1998 AI Planning Systems Competition","4257":"Embodied Question Answering","4258":"Auditory Scene Analysis: The Perceptual Organization of Sound by Albert Bregman (review)","4259":"Understanding intelligence","4260":"Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers","4261":"A survey of deep learning techniques for autonomous driving","4262":"Using Machine Learning","4263":"Human-level play in the game of Diplomacy by combining language models with strategic reasoning","4264":"Towards Personalized Federated Learning","4265":"Artificial intelligence in histopathology: enhancing cancer research and clinical oncology","4266":"Artificial Intelligence for Remote Sensing Data Analysis: A review of challenges and opportunities","4267":"Adaptive game AI with dynamic scripting","4268":"Federated Learning for Internet of Things: A Comprehensive Survey","4269":"What Do You Mean by \"AI\"?","4270":"AI Game Programming Wisdom","4271":"Rapid and Reliable Adaptation of Video Game AI","4272":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis","4273":"Galactica: A Large Language Model for Science","4274":"BloombergGPT: A Large Language Model for Finance","4275":"LISA: Reasoning Segmentation via Large Language Model","4276":"TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation","4277":"OpenAssistant Conversations - Democratizing Large Language Model Alignment","4278":"ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge","4279":"Gorilla: Large Language Model Connected with Massive APIs","4280":"Efficient Memory Management for Large Language Model Serving with PagedAttention","4281":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest","4282":"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning","4283":"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases","4284":"Accelerating Large Language Model Decoding with Speculative Sampling","4285":"A Survey on Large Language Model based Autonomous Agents","4286":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","4287":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction","4288":"AudioPaLM: A Large Language Model That Can Speak and Listen","4289":"Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach","4290":"Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners","4291":"Valley: Video Assistant with Large Language model Enhanced abilitY","4292":"Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation","4293":"Large Language Model Guided Tree-of-Thought","4294":"Self-planning Code Generation with Large Language Model","4295":"StructGPT: A General Framework for Large Language Model to Reason over Structured Data","4296":"Prompting Large Language Model for Machine Translation: A Case Study","4297":"mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration","4298":"Radiology-GPT: A Large Language Model for Radiology","4299":"Studying Large Language Model Generalization with Influence Functions","4300":"Analysis of large-language model versus human performance for genetics questions","4301":"ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model","4302":"Could a Large Language Model be Conscious?","4303":"Creating a Large Language Model of a Philosopher","4304":"A large language model for electronic health records","4305":"PromptChainer: Chaining Large Language Model Prompts through Visual Programming","4306":"An empirical analysis of compute-optimal large language model training","4307":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts","4308":"FinBERT\n : A Large Language Model for Extracting Information from Financial Text\u2020","4309":"Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning","4310":"Large Language Model Unlearning","4311":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","4312":"Evaluating Large Language Models Trained on Code","4313":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","4314":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models","4315":"WizardLM: Empowering Large Language Models to Follow Complex Instructions","4316":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities","4317":"CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society","4318":"Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models","4319":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","4320":"A Watermark for Large Language Models","4321":"Flamingo: a Visual Language Model for Few-Shot Learning","4322":"Kosmos-2: Grounding Multimodal Large Language Models to the World","4323":"PaLM-E: An Embodied Multimodal Language Model","4324":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM","4325":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","4326":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","4327":"How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment","4328":"Teaching Large Language Models to Self-Debug","4329":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models","4330":"Large Language Models Can Be Easily Distracted by Irrelevant Context","4331":"Large language models generate functional protein sequences across diverse families","4332":"Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback","4333":"WizardCoder: Empowering Code Large Language Models with Evol-Instruct","4334":"Towards Expert-Level Medical Question Answering with Large Language Models","4335":"Benchmarking Large Language Models for News Summarization","4336":"Large Language Models are not Fair Evaluators","4337":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs","4338":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","4339":"ChatGPT and a new academic reality: Artificial Intelligence\u2010written research papers and the ethics of the large language models in scholarly publishing","4340":"High-throughput Generative Inference of Large Language Models with a Single GPU","4341":"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities","4342":"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning","4343":"Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent","4344":"Graph of Thoughts: Solving Elaborate Problems with Large Language Models","4345":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day","4346":"Large Language Models are Zero-Shot Rankers for Recommender Systems","4347":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages","4348":"ART: Automatic multi-step reasoning and tool-use for large language models","4349":"LaMP: When Large Language Models Meet Personalization","4350":"ExpertPrompting: Instructing Large Language Models to be Distinguished Experts","4351":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models","4352":"Voyager: An Open-Ended Embodied Agent with Large Language Models","4353":"Planning with Large Language Models for Code Generation","4354":"Extending Context Window of Large Language Models via Positional Interpolation","4355":"Guiding Pretraining in Reinforcement Learning with Large Language Models","4356":"LLM-Pruner: On the Structural Pruning of Large Language Models","4357":"Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases","4358":"When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain","4359":"A Survey on Multimodal Large Language Models","4360":"Safety Assessment of Chinese Large Language Models","4361":"Evaluating the Performance of Large Language Models on GAOKAO Benchmark","4362":"The Capacity for Moral Self-Correction in Large Language Models","4363":"Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks","4364":"Large language models encode clinical knowledge","4365":"Query2doc: Query Expansion with Large Language Models","4366":"Auditing large language models: a three-layered approach","4367":"Editing Large Language Models: Problems, Methods, and Opportunities","4368":"Fundamental Limitations of Alignment in Large Language Models","4369":"Role play with large language models","4370":"Do Large Language Models Understand Chemistry? A Conversation with ChatGPT","4371":"Chain-of-Verification Reduces Hallucination in Large Language Models","4372":"Emergent and Predictable Memorization in Large Language Models","4373":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding","4374":"Automatic Evaluation of Attribution by Large Language Models","4375":"Aligning Large Language Models through Synthetic Feedback","4376":"Memory Augmented Large Language Models are Computationally Universal","4377":"Explainability for Large Language Models: A Survey","4378":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models","4379":"Program Synthesis with Large Language Models","4380":"On the Risk of Misinformation Pollution with Large Language Models","4381":"Evolutionary-scale prediction of atomic level protein structure with a language model","4382":"A systematic evaluation of large language models of code","4383":"Extracting Training Data from Large Language Models","4384":"Reasoning with Language Model is Planning with World Model","4385":"Large Language Models Are Human-Level Prompt Engineers","4386":"Efficient Large-Scale Language Model Training on GPU Clusters","4387":"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning","4388":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models","4389":"Wordcraft: Story Writing With Large Language Models","4390":"Generate rather than Retrieve: Large Language Models are Strong Context Generators","4391":"Emergent analogical reasoning in large language models","4392":"Large Language Models Struggle to Learn Long-Tail Knowledge","4393":"TabLLM: Few-shot Classification of Tabular Data with Large Language Models","4394":"Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies","4395":"Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models","4396":"Exploring Length Generalization in Large Language Models","4397":"Faithful Reasoning Using Large Language Models","4398":"Large Language Models are reasoners with Self-Verification","4399":"Code as Policies: Language Model Programs for Embodied Control","4400":"Large Language Models Can Be Strong Differentially Private Learners","4401":"Prompting Is Programming: A Query Language for Large Language Models","4402":"Leveraging Large Language Models for Multiple Choice Question Answering","4403":"Learning Video Representations from Large Language Models","4404":"Evaluating the Text-to-SQL Capabilities of Large Language Models","4405":"Blockchain Large Language Models","4406":"Large Language Models with Controllable Working Memory","4407":"Large Language Models Are Reasoning Teachers","4408":"Visual Classification via Description from Large Language Models","4409":"Measuring Progress on Scalable Oversight for Large Language Models","4410":"Large Language Models and the Reverse Turing Test","4411":"Meaning without reference in large language models","4412":"The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models","4413":"Do Large Language Models know what humans know?","4414":"CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model","4415":"Using Large Language Models to Simulate Multiple Humans","4416":"ProtGPT2 is a deep unsupervised language model for protein design","4417":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling","4418":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision","4419":"Typhoon: Thai Large Language Models","4420":"CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities","4421":"CPM: A Large-scale Generative Chinese Pre-trained Language Model","4422":"Black-Box Tuning for Language-Model-as-a-Service","4423":"REALM: Retrieval-Augmented Language Model Pre-Training","4424":"What Language Model to Train if You Have One Million GPU Hours?","4425":"Persistent Anti-Muslim Bias in Large Language Models","4426":"Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models","4427":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm","4428":"Jigsaw: Large Language Models meet Program Synthesis","4429":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing","4430":"A Recipe for Arbitrary Text Style Transfer with Large Language Models","4431":"BERTweet: A pre-trained language model for English Tweets","4432":"Examining Zero-Shot Vulnerability Repair with Large Language Models","4433":"Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning","4434":"Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval","4435":"Large Margin Neural Language Model","4436":"CTRL: A Conditional Transformer Language Model for Controllable Generation","4437":"Structured Pruning of Large Language Models","4438":"Baichuan 2: Open Large-scale Language Models","4439":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining","4440":"Paraphrasing with Large Language Models","4441":"FlauBERT: Unsupervised Language Model Pre-training for French","4442":"German\u2019s Next Language Model","4443":"FinBERT: A Pretrained Language Model for Financial Communications","4444":"Parameter-efficient fine-tuning of large-scale pre-trained language models","4445":"Analyzing the Structure of Attention in a Transformer Language Model","4446":"Contrastive Distillation on Intermediate Representations for Language Model Compression","4447":"Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning","4448":"A large annotated corpus for learning natural language inference","4449":"Generalizing Question Answering System with Pre-trained Language Model Fine-tuning","4450":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes","4451":"Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models","4452":"PaLI: A Jointly-Scaled Multilingual Language-Image Model","4453":"Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation","4454":"A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition","4455":"Predictive power of word surprisal for reading times is a linear function of language model quality","4456":"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation","4457":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding","4458":"PaLM: Scaling Language Modeling with Pathways","4459":"Improving Language Understanding by Generative Pre-Training","4460":"Universal and Transferable Adversarial Attacks on Aligned Language Models","4461":"Temporal Action Detection Using a Statistical Language Model","4462":"OPT: Open Pre-trained Transformer Language Models","4463":"Language Is Not All You Need: Aligning Perception with Language Models","4464":"Graph-Based Statistical Language Model for Code","4465":"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks","4466":"FLAVA: A Foundational Language And Vision Alignment Model","4467":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?","4468":"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research","4469":"Improving Factuality and Reasoning in Language Models through Multiagent Debate","4470":"Multimodal Chain-of-Thought Reasoning in Language Models","4471":"Scaling Instruction-Finetuned Language Models","4472":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism","4473":"PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation","4474":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","4475":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","4476":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control","4477":"AraBERT: Transformer-based Model for Arabic Language Understanding","4478":"Larger language models do in-context learning differently","4479":"Language Models can Solve Computer Tasks","4480":"WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training","4481":"Efficient Large Scale Language Modeling with Mixtures of Experts","4482":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models","4483":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","4484":"Self-Instruct: Aligning Language Models with Self-Generated Instructions","4485":"Large-scale chemical language representations capture molecular structure and properties","4486":"VideoBERT: A Joint Model for Video and Language Representation Learning","4487":"Large Language Models in Machine Translation","4488":"Finetuned Language Models Are Zero-Shot Learners","4489":"TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models","4490":"Controllable Story Generation with External Knowledge Using Large-Scale Language Models","4491":"Reward Design with Language Models","4492":"Few-shot Learning with Retrieval Augmented Language Models","4493":"Solving Quantitative Reasoning Problems with Language Models","4494":"Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models","4495":"COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter","4496":"KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation","4497":"Adversarial Training for Large Neural Language Models","4498":"GIT: A Generative Image-to-text Transformer for Vision and Language","4499":"Quantifying Memorization Across Neural Language Models","4500":"AudioLM: A Language Modeling Approach to Audio Generation","4501":"Scalable Modified Kneser-Ney Language Model Estimation","4502":"Scaling Laws for Neural Language Models","4503":"Otter: A Multi-Modal Model with In-Context Instruction Tuning","4504":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","4505":"Speech Model Pre-training for End-to-End Spoken Language Understanding","4506":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","4507":"Reproducible Scaling Laws for Contrastive Language-Image Learning","4508":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher","4509":"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data","4510":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation","4511":"RankGen: Improving Text Generation with Large Ranking Models","4512":"Reweighted Proximal Pruning for Large-Scale Language Representation","4513":"Ignore Previous Prompt: Attack Techniques For Language Models","4514":"On the Advance of Making Language Models Better Reasoners","4515":"Teaching language models to support answers with verified quotes","4516":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor","4517":"Language Models are Multilingual Chain-of-Thought Reasoners","4518":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks","4519":"Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study","4520":"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition","4521":"PandaGPT: One Model To Instruction-Follow Them All","4522":"Teaching Small Language Models to Reason","4523":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference","4524":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts","4525":"Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health","4526":"QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering","4527":"A General Language Assistant as a Laboratory for Alignment","4528":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer","4529":"FILIP: Fine-grained Interactive Language-Image Pre-Training","4530":"BASE Layers: Simplifying Training of Large, Sparse Models","4531":"Training Language Models with Memory Augmentation","4532":"A Large-Scale Study of MySpace: Observations and Implications for Online Social Networks","4533":"Multi-Task Deep Neural Networks for Natural Language Understanding","4534":"Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model","4535":"Should You Mask 15% in Masked Language Modeling?","4536":"ProGen2: Exploring the Boundaries of Protein Language Models","4537":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections","4538":"Leveraging Large Amounts of Weakly Supervised Data for Multi-Language Sentiment Classification","4539":"DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting","4540":"StereoSet: Measuring stereotypical bias in pretrained language models","4541":"SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network","4542":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts","4543":"Large-Scale Differentially Private BERT","4544":"ERNIE: Enhanced Language Representation with Informative Entities","4545":"Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition","4546":"WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia","4547":"Plug and Play Language Models: A Simple Approach to Controlled Text Generation","4548":"Software Framework for Topic Modelling with Large Corpora","4549":"End-to-end attention-based large vocabulary speech recognition","4550":"MPNet: Masked and Permuted Pre-training for Language Understanding","4551":"Hero: Hierarchical Encoder for Video+Language Omni-representation Pre-training","4552":"Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention","4553":"Multimodal Transformer for Unaligned Multimodal Language Sequences","4554":"Release Strategies and the Social Impacts of Language Models","4555":"Adversarial NLI: A New Benchmark for Natural Language Understanding","4556":"Deeper Text Understanding for IR with Contextual Neural Language Modeling","4557":"Unified Vision-Language Pre-Training for Image Captioning and VQA","4558":"K-BERT: Enabling Language Representation with Knowledge Graph","4559":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners","4560":"Annotation Artifacts in Natural Language Inference Data","4561":"Prompting Visual-Language Models for Efficient Video Understanding","4562":"ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data","4563":"Challenges in Detoxifying Language Models","4564":"Visual Relationship Detection with Language Priors","4565":"Enhanced LSTM for Natural Language Inference","4566":"Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition","4567":"CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models","4568":"Scaling End-to-End Models for Large-Scale Multilingual ASR","4569":"HateBERT: Retraining BERT for Abusive Language Detection in English","4570":"PhoBERT: Pre-trained language models for Vietnamese","4571":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization","4572":"12-in-1: Multi-Task Vision and Language Representation Learning","4573":"Compressing Large-Scale Transformer-Based Models: A Case Study on BERT","4574":"FreeLB: Enhanced Adversarial Training for Natural Language Understanding","4575":"Exploring the Limits of Language Modeling","4576":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections","4577":"Number without a language model","4578":"Adaptive Semiparametric Language Models","4579":"Few-shot Learning with Multilingual Generative Language Models","4580":"Larger-Scale Transformers for Multilingual Masked Language Modeling","4581":"Transfer Learning in Natural Language Processing","4582":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models","4583":"Efficient Nearest Neighbor Language Models","4584":"Improving Biomedical Pretrained Language Models with Knowledge","4585":"Grammar as a Foreign Language","4586":"How Context Affects Language Models' Factual Predictions","4587":"ProteinBERT: a universal deep-learning model of protein sequence and function","4588":"Targeted Syntactic Evaluation of Language Models","4589":"Decoding with Large-Scale Neural Language Models Improves Translation","4590":"Injecting Numerical Reasoning Skills into Language Models","4591":"Multi-Task Learning for Multiple Language Translation","4592":"Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision","4593":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models","4594":"Patient Knowledge Distillation for BERT Model Compression","4595":"DIET: Lightweight Language Understanding for Dialogue Systems","4596":"Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art","4597":"A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding","4598":"Pig latin: a not-so-foreign language for data processing","4599":"A Neural Conversational Model","4600":"OpenSubtitles2018: Statistical Rescoring of Sentence Alignments in Large, Noisy Parallel Corpora","4601":"Character-Level Language Modeling with Deeper Self-Attention","4602":"Indri : A language-model based search engine for complex queries ( extended version )","4603":"Semi-supervised Semantic Role Labeling Using the Latent Words Language Model","4604":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers","4605":"A Joint Language Model With Fine-grain Syntactic Tags","4606":"Code completion with statistical language models","4607":"Reducing Sentiment Bias in Language Models via Counterfactual Evaluation","4608":"Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines","4609":"Federated Learning of Deep Networks using Model Averaging","4610":"From Feedforward to Recurrent LSTM Neural Networks for Language Modeling","4611":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts","4612":"AraGPT2: Pre-Trained Transformer for Arabic Language Generation","4613":"Using a Statistical Language Model to Improve the Performance of an HMM-Based Cursive Handwriting Recognition System","4614":"An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models","4615":"The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities","4616":"Neural language models as psycholinguistic subjects: Representations of syntactic state","4617":"Language Model Based Arabic Word Segmentation","4618":"Language Model Adaptation for Statistical Machine Translation via Structured Query Models","4619":"A Neural Syntactic Language Model","4620":"Natural Language Inference by Tree-Based Convolution and Heuristic Matching","4621":"Strategies for training large scale neural network language models","4622":"AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding","4623":"Language Models with Transformers","4624":"Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information","4625":"Language as a Latent Variable: Discrete Generative Models for Sentence Compression","4626":"The nesC language: A holistic approach to networked embedded systems","4627":"Video (language) modeling: a baseline for generative models of natural videos","4628":"BERTje: A Dutch BERT Model","4629":"Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry","4630":"Federated Learning of N-Gram Language Models","4631":"Large-scale Cloze Test Dataset Created by Teachers","4632":"Visual Instruction Tuning","4633":"Large Scale Language Modeling in Automatic Speech Recognition","4634":"Large-scale Semantic Parsing without Question-Answer Pairs","4635":"Jaql: A Scripting Language for Large Scale Semistructured Data Analysis","4636":"Large-Scale Syntactic Language Modeling with Treelets","4637":"Compositional Morphology for Word Representations and Language Modelling","4638":"Bayesian Recurrent Neural Network for Language Modeling","4639":"Deep Contextualized Word Representations","4640":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?","4641":"Visual Prompt Tuning","4642":"N-gram Counts and Language Models from the Common Crawl","4643":"Generative Agents: Interactive Simulacra of Human Behavior","4644":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","4645":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation","4646":"Large vocabulary automatic speech recognition for children","4647":"Code Llama: Open Foundation Models for Code","4648":"Deep Visual-Semantic Alignments for Generating Image Descriptions","4649":"G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment","4650":"Multitask Prompted Training Enables Zero-Shot Task Generalization","4651":"InstructPix2Pix: Learning to Follow Image Editing Instructions","4652":"Crosslingual Generalization through Multitask Finetuning","4653":"StarCoder: may the source be with you!","4654":"LIMA: Less Is More for Alignment","4655":"The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources","4656":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT","4657":"On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling","4658":"Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies","4659":"The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset","4660":"Perturbation Sensitivity Analysis to Detect Unintended Model Biases","4661":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows","4662":"Qwen Technical Report","4663":"Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge","4664":"Probabilistic model for code with decision trees","4665":"Mining source code repositories at massive scale using language modeling","4666":"Large-Scale Bayesian Logistic Regression for Text Categorization","4667":"What Does BERT Look at? An Analysis of BERT\u2019s Attention","4668":"How Multilingual is Multilingual BERT?","4669":"The Power of Scale for Parameter-Efficient Prompt Tuning","4670":"DryadLINQ: A System for General-Purpose Distributed Data-Parallel Computing Using a High-Level Language","4671":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining","4672":"IRSTLM: an open source toolkit for handling large scale language models","4673":"Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects","4674":"Restormer: Efficient Transformer for High-Resolution Image Restoration","4675":"A wafer-scale neuromorphic hardware system for large-scale neural modeling","4676":"Linguistic Knowledge and Transferability of Contextual Representations","4677":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing","4678":"A Generalist Agent","4679":"A Conversational Paradigm for Program Synthesis","4680":"STaR: Bootstrapping Reasoning With Reasoning","4681":"Communication-Efficient Learning of Deep Networks from Decentralized Data","4682":"Enriching Word Vectors with Subword Information","4683":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity","4684":"Mass-Editing Memory in a Transformer","4685":"Using cognitive psychology to understand GPT-3","4686":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","4687":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","4688":"ClipCap: CLIP Prefix for Image Captioning","4689":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers","4690":"Statistical Phrase-Based Translation","4691":"Pointer Sentinel Mixture Models","4692":"Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models","4693":"End-To-End Memory Networks","4694":"A Critical Review of Recurrent Neural Networks for Sequence Learning","4695":"Geometric Deep Learning: Going beyond Euclidean data","4696":"Unsupervised Machine Translation Using Monolingual Corpora Only","4697":"Efficient Transformers: A Survey","4698":"Learning to summarize from human feedback","4699":"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention","4700":"Self-critiquing models for assisting human evaluators","4701":"MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers","4702":"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks","4703":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters","4704":"Matching Words and Pictures","4705":"Beyond English-Centric Multilingual Machine Translation","4706":"An Attentional Model for Speech Translation Without Transcription","4707":"Improving Pre-Trained Multilingual Model with Vocabulary Expansion","4708":"MetaICL: Learning to Learn In Context","4709":"Learned in Translation: Contextualized Word Vectors","4710":"Phrase-Based & Neural Unsupervised Machine Translation","4711":"An Introduction to Conditional Random Fields","4712":"Generating Focused Molecule Libraries for Drug Discovery with Recurrent Neural Networks","4713":"Streaming for large scale NLP: Language Modeling","4714":"Transfer Learning for Low-Resource Neural Machine Translation","4715":"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision","4716":"Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction","4717":"Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences","4718":"XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale","4719":"Neural Text Generation from Structured Data with Application to the Biography Domain","4720":"CLIP2Video: Mastering Video-Text Retrieval via Image CLIP","4721":"Muppet: Massive Multi-task Representations with Pre-Finetuning","4722":"Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation","4723":"Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP","4724":"R-Drop: Regularized Dropout for Neural Networks","4725":"Compacter: Efficient Low-Rank Hypercomplex Adapter Layers","4726":"Accurate and compact large vocabulary speech recognition on mobile devices","4727":"Deep Equilibrium Models","4728":"SCOPE: easy and efficient parallel processing of massive data sets","4729":"An approach based on phonemes to large vocabulary Chinese sign language recognition","4730":"Emoticon Smoothed Language Models for Twitter Sentiment Analysis","4731":"Overview of sciDB: large scale array storage, processing and analysis","4732":"Connectionist language modeling for large vocabulary continuous speech recognition","4733":"Training Neural Network Language Models on Very Large Corpora","4734":"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models","4735":"Generated Knowledge Prompting for Commonsense Reasoning","4736":"AdapterHub: A Framework for Adapting Transformers","4737":"Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings","4738":"Large-Scale Distributed Language Modeling","4739":"BERT for Joint Intent Classification and Slot Filling","4740":"Language and Translation Model Adaptation using Comparable Corpora","4741":"Commonsense Knowledge Mining from Pretrained Models"},"url":{"0":"https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/38513\/artificial-intelligence-in-agriculture\/matthew-n-o-sadiku","1":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/30232\/artificial-intelligence-benefit-and-risks\/seeta-m-chauhan","2":"https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/29784\/artificial-intelligence-in-power-station\/p-naveen","3":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/19180\/artificial-intelligence-based-training-and-placement-management\/krishanu-deb","4":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59801\/the-significance-of-artificial-intelligence-in-digital-marketing\/dr-atul-kumar-mishra","5":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0004370218305988","6":"http:\/\/dx.doi.org\/10.1007\/BF00851350","7":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59910\/artificial-intelligence-role-in-modern-science-aims-merits-risks-and-its-applications\/manish-verma","8":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/49150\/artificial-intelligence-catalyzes-a-revolution-for-21st-century-human-creativity-and-modern-art\/avani-goenka","9":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/30287\/artificial-intelligence-empowering-the-future-of-digital-transformation\/deepak-kumar","10":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31249\/analysis-and-comparative-study-of-the-development-of-technology-with-artificial-intelligence-in-india\/chinmaya-naik","11":"https:\/\/www.ijtsrd.com.com\/computer-science\/artificial-intelligence\/57564\/impact-of-artificial-intelligence-in-the-pharmaceutical-world-a-review\/shaikh-sameer-salim","12":"https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/38516\/artificial-intelligence-in-gaming\/matthew-n-o-sadiku","13":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/47491\/artificial-intelligence-and-humancomputer-interaction\/matthew-n-o-sadiku","14":"http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/18909\/artificial-intelligence-basics-and-terminology\/durgesh-raghuvanshi","15":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/25067\/video-steganography-using-discrete-wavelet-transform-and-artificial-intelligence\/shivani-gupta","16":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/14534\/artificial-intelligence-and-machine-learning's-impact-on-market-design\/mrs-pushpalata-s-patil","17":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352154618301943","18":"http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/17135\/role-of-ethics-in-artificial-intelligence\/rishikesh-r","19":"https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/38514\/artificial-intelligence-in-autonomous-vehicles\/matthew-n-o-sadiku","20":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666920X22000297","21":"https:\/\/gsconlinepress.com\/journals\/gscarr\/content\/artificial-intelligence-dentistry-and-its-future","22":"https:\/\/www.etui.org\/Publications2\/Foresight-briefs\/A-law-on-robotics-and-artificial-intelligence-in-the-EU","23":"https:\/\/www.beck-shop.de\/eubanks-artificial-intelligence-for-hr\/product\/26267226","24":"http:\/\/www.ijtsrd.com\/management\/research-method\/18705\/artificial-intelligence-and-its-role-in-industry\/mrs-jyoti-m-bohra","25":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0735109718344085","26":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/37964\/application-of-artificial-intelligence-in-indian-bankingopportunities-and-challenges\/prof-mohammed-nawaz","27":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/26374\/a-study-on-the-applications-and-impact-of-artificial-intelligence-in-e-commerce-industry\/prof-lakshmi-narayan-n","28":"http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/11482\/intelligent-drone-based-personal-assistant-using-artificial-intelligence-ai\/mr-a-kishorekumar","29":"https:\/\/www.ijtsrd.com\/engineering\/other\/50563\/artificial-intelligence-in-smart-grid\/matthew-n-o-sadiku","30":"http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/11666\/transformational-planning-for-artificial-intelligence\/prachi-shah","31":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/12734\/artificial-intelligence-assisted-weather-based-plant-disease-forecasting-system\/m-juno-isabel-susinthra","32":"https:\/\/www.ijtsrd.com\/other-scientific-research-area\/other\/53850\/impact-of-artificial-intelligence-on-emarketing\/nour-sadeq","33":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0896627317305093","34":"http:\/\/arxiv.org\/abs\/1801.05667","35":"http:\/\/www.ijtsrd.com\/economics\/development-economics\/18312\/artificial-intelligence-in-accordance-to-human-rights\/krishna-ramanujam","36":"https:\/\/www.ijtsrd.com\/humanities-and-the-arts\/education\/59781\/revolutionizing-education-how-artificial-intelligence-is-transforming-the-learning-landscape\/suman-roy","37":"https:\/\/www.ijtsrd.com\/engineering\/information-technology\/33050\/techniques-to-apply-artificial-intelligence-in-power-plants\/anshika-gupta","38":"https:\/\/www.ijtsrd.com\/management\/law-and-management\/33044\/law-and-economic-analysis-of-the-ownership-of-artificial-intelligence-creation\/changjun-wu","39":"https:\/\/gsconlinepress.com\/journals\/gscbps\/content\/application-artificial-intelligence-ai-food-industry","40":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/49182\/stride-for-detecting-suicidal-thoughts-using-artificial-intelligence\/shrivallabh-walkade","41":"https:\/\/www.noemamag.com\/the-exploited-labor-behind-artificial-intelligence\/","42":"http:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-06483-3_8","43":"http:\/\/www.ijtsrd.com\/management\/general-management\/11127\/role-of-artificial-intelligence-in-transforming-human-resource-management\/dr-amol-murgai","44":"http:\/\/arxiv.org\/abs\/1904.08796","45":"https:\/\/www.ijtsrd.com\/engineering\/telecommunications\/31847\/tiddy--an-artificial-intelligence-based-floor-cleaning-robot\/bhumika-t-j","46":"","47":"https:\/\/www.ijtsrd.com\/physics\/engineering-physics\/47866\/review-on-solar-power-system-with-artificial-intelligence\/prof-vijay-aithekar","48":"https:\/\/www.etui.org\/Publications2\/Foresight-briefs\/Artificial-intelligence-a-game-changer-for-the-world-of-work","49":"https:\/\/gjeta.com\/content\/artificial-intelligence-approach-crude-distillation-unit-operation","50":"https:\/\/www.ijtsrd.com.com\/pharmacy\/other\/55095\/impact-of-artificial-intelligence-on-pharm-technology-a-review\/tirumala-durvasula","51":"https:\/\/www.ijtsrd.com.com\/management\/other\/53854\/artificial-intelligence-based-stock-market-prediction-model-using-technical-indicators\/mr-ketan-ashok-bagade","52":"http:\/\/www.amazon.de\/gp\/redirect.html%3FASIN=0137903952%26tag=ws%26lcode=xm2%26cID=2025%26ccmID=165953%26location=\/Artificial-Intelligence-Modern-Approach-Prentice\/dp\/0137903952%253FSubscriptionId=13CT5CVB80YFWJEPWS02","53":"https:\/\/www.ijtsrd.com\/biological-science\/microbiology\/59874\/the-role-of-artificial-intelligence-in-revolutionizing-healthcare-a-comprehensive-review\/kajal-gohane","54":"","55":"","56":"https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/49316\/artificial-intelligence-a-study-of-automation-and-its-impact-on-data-science\/mussaratjahan-korpali","57":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/30227\/an-artificial-intelligence-approach-to-ultra-high-frequency-path-loss-modelling-of-the-suburban-areas-of-abuja-nigeria\/deme-c-abraham","58":"https:\/\/www.ijtsrd.com\/management\/management-development\/31303\/review-on-electricity-consumption-forecasting-in-buildings-using-artificial-intelligence\/aditya-sonar","59":"http:\/\/arxiv.org\/abs\/2303.12712","60":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23850\/advent-of-artificial-intelligence-and-its-impact-on-top-leading-commercial-banks-in-india-%E2%80%93-case-study\/prof-lakshminarayana-n","61":"https:\/\/www.ijtsrd.com.com\/computer-science\/other\/57454\/the-role-of-artificial-intelligence-applications-in-managing-the-employee-performance-evaluation-an-iraqi-case-study\/ali-al-imari","62":"https:\/\/www.theatlantic.com\/technology\/archive\/2021\/04\/artificial-intelligence-misreading-human-emotion\/618696\/","63":"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/26017444\/","64":"","65":"http:\/\/www.ijtsrd.com\/computer-science\/data-miining\/2443\/bayesian-analysis-to-the-experiences-of-corruption-through-artificial-intelligence\/erica-pascual-garcia","66":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31768\/emerging-roles-of-artificial-intelligence-in-ecommerce\/vishal-dineshkumar-soni","67":"http:\/\/www.springerlink.com\/content\/m16u7814852606t3\/","68":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666920X2100014X","69":"https:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/466","70":"","71":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666920X2200042X","72":"http:\/\/www.amazon.com\/Artificial-Intelligence-Engineering-Approach-Computers\/dp\/0070550840","73":"https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/534","74":"https:\/\/www.ijtsrd.com\/humanities-and-the-arts\/education\/27923\/a-study-on-google-is-an-artificial-encyclopedia-affecting-human-intelligence-%E2%80%93-an-empirical-study\/prof-rekha-d-m","75":"https:\/\/journals.bohrpub.com\/index.php\/bijiam\/article\/view\/52","76":"https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/516","77":"https:\/\/ssrn.com\/abstract=3234336","78":"https:\/\/gjeta.com\/content\/appraisal-ethical-issues-and-effect-artificial-intelligence-cryptocurrency-market","79":"","80":"http:\/\/aima.cs.berkeley.edu","81":"https:\/\/doi.org\/10.1145%2F3490100.3516481","82":"https:\/\/www.ijrte.org\/wp-content\/uploads\/papers\/v10i3\/B62920710221.pdf","83":"","84":"http:\/\/arxiv.org\/abs\/2002.06177","85":"","86":"https:\/\/doi.org\/10.21428%2F36973002.83481373","87":"","88":"https:\/\/ec.europa.eu\/info\/files\/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en","89":"http:\/\/www.worldcat.org\/search?qt=worldcat_org_all&q=9780201533774","90":"http:\/\/citeseer.ist.psu.edu\/400591.html","91":"","92":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0888327018300748","93":"","94":"https:\/\/www.ijtsrd.com\/engineering\/electronics-and-communication-engineering\/38107\/design-and-analysis-of-artificial-intelligence-based-approach-for-control-of-wind-turbine\/arshid-mehraj","95":"http:\/\/dx.doi.org\/10.1007\/978-3-642-25832-9","96":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#TjoaBHK20","97":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/31305\/a-review-on-the-impact-of-artificial-intelligence-and-internet-of-things-in-the-transformation-of-ebusiness-sector\/neha-bhujbal","98":"http:\/\/eprints.soton.ac.uk\/252187\/","99":"http:\/\/artint.info\/html\/ArtInt.html","100":"","101":"http:\/\/dx.doi.org\/10.1016\/0954-1810(91)90001-5","102":"https:\/\/gjeta.com\/content\/ethics-artificial-intelligence-issues-and-guidelines-developing-acceptable-ai-systems","103":"https:\/\/www.ijtsrd.commanagement\/management-development\/42380\/the-impact-of-information-and-communication-technologies-on-the-performance-of-human-resources-management-and-the-mediating-role-of-artificial-intelligence\/kourda-hayat","104":"","105":"https:\/\/doi.org\/10.1007\/978-981-19-0351-9_6-1","106":"http:\/\/doi.acm.org\/10.1145\/1083310.1083311","107":"","108":"https:\/\/elib.dlr.de\/194206\/","109":"https:\/\/maliciousaireport.com\/","110":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0004370212000276","111":"https:\/\/doi.org\/10.3233%2Faic-210084","112":"http:\/\/arxiv.org\/abs\/2109.09747","113":"","114":"","115":"","116":"","117":"http:\/\/portal.acm.org\/citation.cfm?id=1045340","118":"","119":"http:\/\/www.amazon.ca\/exec\/obidos\/redirect?tag=citeulike09-20\\&amp;path=ASIN\/0137903952","120":"http:\/\/dblp.uni-trier.de\/db\/reference\/fai\/fai1.html#ReichgeltV05","121":"http:\/\/arxiv.org\/abs\/1908.00369","122":"http:\/\/citeseer.nj.nec.com\/211802.html","123":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai18.html#KrugerM04","124":"https:\/\/doi.org\/10.1177%2F2053951720919776","125":"","126":"https:\/\/doi.org\/10.21428%2F36973002.1ed9ae85","127":"http:\/\/www.amazon.com\/Artificial-Intelligence-Literary-Creativity-Storytelling\/dp\/0805819878%3FSubscriptionId%3D192BW6DQ43CK9FN0ZGG2%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0805819878","128":"","129":"https:\/\/columbialawreview.org\/content\/will-artificial-intelligence-eat-the-law-the-rise-of-hybrid-social-ordering-systems\/","130":"https:\/\/doi.org\/10.1016%2Fj.caeai.2022.100075","131":"http:\/\/arxiv.org\/abs\/1910.10045","132":"https:\/\/doi.org\/10.1093\/nsr\/nwac035","133":"https:\/\/openaccessjournals.eu\/index.php\/ijdias\/article\/view\/1853\/1790","134":"https:\/\/cajotas.centralasianstudies.org\/index.php\/CAJOTAS\/article\/view\/451","135":"","136":"https:\/\/www.mckinsey.com\/industries\/education\/our-insights\/how-artificial-intelligence-will-impact-k-12-teachers","137":"http:\/\/dblp.uni-trier.de\/db\/conf\/digra\/digra2003.html#Mateas03","138":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2012.html#PendleburyHW12","139":"https:\/\/www.teqsa.gov.au\/sites\/default\/files\/2023-09\/assessment-reform-age-artificial-intelligence-discussion-paper.pdf","140":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai5.html#McRobbieS91","141":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai6.html#KumaraOH92","142":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai5.html#Kayser91","143":"","144":"https:\/\/doi.org\/10.1038\/s41591-018-0335-9","145":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S1566253519308103","146":"http:\/\/airccse.org\/journal\/ijscai\/papers\/3114ijscai01.pdf","147":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666920X2200073X","148":"https:\/\/zenodo.org\/record\/7524493","149":"http:\/\/www.youtube.com\/watch?v=_m97_kL4ox0","150":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/7045\/diagnosis-of-hyperglycemia-using--artificial-neural-networks\/abid-sarwar","151":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai18.html#Nissan04","152":"http:\/\/dblp.uni-trier.de\/db\/series\/faia\/faia160.html#GagnonE07","153":"","154":"","155":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai5.html#Vamos91","156":"","157":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai14.html#LuckA00a","158":"https:\/\/www.bohrpub.com\/article\/BIJBNT\/Vol1N1\/BIJBNT_20201102.pdf","159":"http:\/\/www.sciencedirect.com\/science\/article\/B6W86-4RMFNV9-3\/2\/dc086388f1216dbf7f42492751d21543","160":"http:\/\/www.channon.net\/alastair\/msc\/adc_msc.pdf","161":"http:\/\/cajmns.centralasianstudies.org\/index.php\/CAJMNS\/article\/view\/524","162":"https:\/\/ieeexplore.ieee.org\/abstract\/document\/9069875","163":"http:\/\/arxiv.org\/abs\/2202.11264","164":"http:\/\/www.amazon.com\/exec\/obidos\/redirect?tag=citeulike07-20\\&path=ASIN\/1558601910","165":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31629\/a-study-of-deep-learning-applications\/anirban-chakraborty","166":"http:\/\/www.us.oup.com\/us\/catalog\/general\/subject\/LifeSciences\/Neurobiology\/?view=usa\\&\\#38;ci=0195131592","167":"http:\/\/www.aaai.org\/Papers\/IAAI\/2004\/IAAI04-019.pdf","168":"https:\/\/aisel.aisnet.org\/thci\/vol12\/iss3\/1\/","169":"","170":"https:\/\/aircconline.com\/ijsea\/V13N6\/13622ijsea02.pdf","171":"http:\/\/www.ida.his.se\/~tom\/Vyg.AB.sub.web.pdf","172":"https:\/\/bristoluniversitypress.co.uk\/resisting-ai","173":"https:\/\/gitlab2.informatik.uni-wuerzburg.de\/itc-conference\/itc-conference-public\/-\/raw\/master\/itc30\/Mocanu18ITC30.pdf?inline=true","174":"","175":"","176":"","177":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2019.html#BalvertS19","178":"https:\/\/economics.academicjournal.io\/index.php\/economics\/article\/view\/520","179":"http:\/\/dx.doi.org\/10.1007\/978-1-4612-3040-3_1","180":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai17.html#AbbattistaBS03","181":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai5.html#SchankS91","182":"https:\/\/openaccessjournals.eu\/index.php\/ijdias\/article\/view\/855","183":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai21.html#Georgilakis07","184":"https:\/\/pubs.wi-kassel.de\/wp-content\/uploads\/2023\/01\/JML_919.pdf","185":"https:\/\/ieeexplore.ieee.org\/document\/8466590","186":"https:\/\/intelligence.org\/files\/AIPosNegFactor.pdf","187":"http:\/\/arxiv.org\/abs\/1610.07997","188":"","189":"http:\/\/ai.stanford.edu\/~nilsson\/QAI\/qai.pdf","190":"https:\/\/www.jair.org\/index.php\/jair\/article\/view\/10302","191":"http:\/\/dblp.uni-trier.de\/db\/series\/cogtech\/354023733.html#GoertzelP07","192":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai18.html#NissanM04a","193":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai4.html#NardiT90","194":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai7.html#Hilton93","195":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S2095809918301127","196":"http:\/\/scholar.google.de\/scholar.bib?q=info:VjQKeC9pFVoJ:scholar.google.com\/&output=citation&scisig=AAGBfm0AAAAAUcF3M6dQUtHIJRJZHd7Kgpt8Tf2vxuvg&scisf=4&hl=de&as_sdt=0,5&as_vis=1","197":"http:\/\/citeseer.ist.psu.edu\/parunak94applications.html","198":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/60109\/legal-risks-and-preventive-measures-in-chatgpt-applications\/chen-jiaqi","199":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai23.html#NissanGC09","200":"","201":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2018.html#GalloSE18","202":"http:\/\/dblp.uni-trier.de\/db\/conf\/iiasa\/iiasa1986.html#SteinackerTH86","203":"","204":"https:\/\/www.accenture.com\/futureofAI","205":"http:\/\/dblp.uni-trier.de\/db\/reference\/ai\/ai2009.html#SanchezL09","206":"","207":"http:\/\/dblp.uni-trier.de\/db\/conf\/iiasa\/iiasa1986.html#Boden86","208":"https:\/\/weneedtotalk.ai\/","209":"","210":"https:\/\/illej.unibo.it\/article\/view\/14099","211":"","212":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2019.html#Colliot19","213":"https:\/\/doi.org\/10.1098%2Frsta.2018.0089","214":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai7.html#Trappl93","215":"http:\/\/dx.doi.org\/10.2200\/S00206ED1V01Y200907AIM007","216":"http:\/\/dblp.uni-trier.de\/db\/journals\/jagi\/jagi6.html#Veale15","217":"","218":"https:\/\/book.coe.int\/en\/education-policy\/11334-pdf-artificial-intelligence-and-education-a-critical-view-through-the-lens-of-human-rights-democracy-and-the-rule-of-law.html","219":"http:\/\/ftp.iza.org\/dp12292.pdf","220":"https:\/\/journals.researchparks.org\/index.php\/IJOT\/article\/view\/4727\/4401","221":"http:\/\/dblp.uni-trier.de\/db\/series\/cogtech\/354023733.html#Redko07","222":"","223":"https:\/\/cajotas.centralasianstudies.org\/index.php\/CAJOTAS\/article\/view\/1127\/1151","224":"http:\/\/dblp.uni-trier.de\/db\/conf\/dagstuhl\/eai2003.html#PfeiferI03","225":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/GH1989.html#GasserH89a","226":"http:\/\/dblp.uni-trier.de\/db\/series\/cogtech\/354023733.html#PennachinG07","227":"https:\/\/din.de\/go\/roadmap-ai ","228":"http:\/\/dblp.uni-trier.de\/db\/reference\/ai\/ai2009.html#Mandl09","229":"https:\/\/www.ilo.org\/employment\/Whatwedo\/Publications\/working-papers\/WCMS_634157\/lang--en\/index.htm","230":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai8.html#YuH94","231":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai6.html#SticklenKHD92","232":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/29136\/a-study-and-analysis-of-emotional-intelligence-and-its-impacts\/jyoti-shikha","233":"http:\/\/dblp.uni-trier.de\/db\/conf\/iiasa\/iiasa1986.html#Nilsson86","234":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0933365706001126","235":"","236":"","237":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai18.html#NissanM04","238":"https:\/\/doi.org\/10.1177\/09610006221142029","239":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31330\/image-based-facial-recognition\/tovbaev-sirojiddin","240":"http:\/\/sss.sagepub.com\/cgi\/content\/abstract\/19\/2\/301","241":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai33.html#Elizalde-Ramirez19","242":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai3.html#PassinoA89","243":"http:\/\/arxiv.org\/abs\/2111.14874","244":"","245":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia3.html#GoriS06","246":"https:\/\/journals.researchparks.org\/index.php\/IJEFSD\/article\/view\/4463\/4177","247":"","248":"http:\/\/dblp.uni-trier.de\/db\/series\/cogtech\/354023733.html#Voss07","249":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0038092X9900064X","250":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia3.html#AielloGS06","251":"http:\/\/www.amazon.com\/Language-Processing-Prentice-Artificial-Intelligence\/dp\/0131873210%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0131873210","252":"","253":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/46384\/skin-cancer-detection-using-imageprocessing-in-realtime\/sunami-dasgupta","254":"","255":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/7046\/k-nearest-neighbours-based-diagnosis-of-hyperglycemia\/abid-sarwar","256":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia3.html#ChellaIMN06","257":"https:\/\/journals.sagepub.com\/doi\/10.1177\/20319525221114474","258":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#BernijazovoD20","259":"http:\/\/arxiv.org\/abs\/2109.04484","260":"http:\/\/dblp.uni-trier.de\/db\/conf\/dagstuhl\/eai2003.html#SpornsP03","261":"http:\/\/www.genetic-programming.com\/jkpdf\/tai1990.pdf","262":"","263":"","264":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31384\/face-recognition-and-increased-reality-system-for-mobile-devices\/sirojiddin-tavboev","265":"https:\/\/www.worldcat.org\/title\/atlas-of-ai-power-politics-and-the-planetary-costs-of-artificial-intelligence\/oclc\/1242089400&referer=brief_results","266":"http:\/\/arxiv.org\/abs\/1204.4927","267":"","268":"","269":"","270":"http:\/\/dblp.uni-trier.de\/db\/conf\/cepes\/cepes1989.html#Malitza89","271":"http:\/\/arxiv.org\/abs\/2002.04803","272":"http:\/\/dblp.uni-trier.de\/db\/conf\/iiasa\/iiasa1986.html#Trappl86b","273":"http:\/\/dblp.uni-trier.de\/db\/conf\/cepes\/cepes1989.html#LytjeB89","274":"https:\/\/dspace.mit.edu\/handle\/1721.1\/5866","275":"","276":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/B1990.html#Marr90","277":"https:\/\/cajotas.centralasianstudies.org\/index.php\/CAJOTAS\/article\/view\/1146\/1181","278":"http:\/\/www-formal. stanford. edu\/jmc\/whatisai. html","279":"http:\/\/www.informatica.si\/PDF\/37-1\/05_Mladenic-Automatic%20Text%20Analysis%20by%20Artificial%20Intell.pdf","280":"http:\/\/dblp.uni-trier.de\/db\/series\/lncs\/lncs5640.html#AtkinsonS09","281":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai24.html#BogdanovychRSC10","282":"","283":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai33.html#CunneenMM19","284":"http:\/\/urn.kb.se\/resolve?urn=urn:nbn:no:ntnu:diva-1002","285":"","286":"https:\/\/cajlpc.centralasianstudies.org\/index.php\/CAJLPC\/article\/view\/690\/697","287":"https:\/\/aircconline.com\/ijscai\/V11N3\/11322ijscai01.pdf","288":"http:\/\/en.wikipedia.org\/w\/index.php?title=Friendly\\_artificial\\_intelligence\\&oldid=404329929","289":"https:\/\/dspace.mit.edu\/handle\/1721.1\/41487","290":"http:\/\/www.amazon.ca\/exec\/obidos\/redirect?tag=citeulike04-20{\\&}path=ASIN\/0262691337","291":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/AItoday.html#Levy99","292":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/GH1989.html#ShawW89","293":"","294":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaai\/aaai2011.html#DaiMW11","295":"http:\/\/dblp.uni-trier.de\/db\/conf\/dagstuhl\/eai2003.html#Holland03","296":"http:\/\/dblp.uni-trier.de\/db\/journals\/jagi\/jagi5.html#Goertzel14","297":"","298":"http:\/\/www.ias.ac.in\/jess\/apr2008\/d093.pdf","299":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2006.html#WangG06","300":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/Miranda2000.html#Smith00","301":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666920X2200011X","302":"","303":"http:\/\/dblp.uni-trier.de\/db\/conf\/stairs\/stairs2006.html#Doherty06","304":"","305":"","306":"http:\/\/dblp.uni-trier.de\/db\/conf\/bica\/bica2011.html#Downing11","307":"http:\/\/dblp.uni-trier.de\/db\/conf\/bica\/bica2011.html#HerdUMO11","308":"http:\/\/dx.doi.org\/10.2200\/S00091ED1V01Y200705AIM002","309":"http:\/\/dblp.uni-trier.de\/db\/conf\/cepes\/cepes1989.html#KelemenM89","310":"http:\/\/dblp.uni-trier.de\/db\/reference\/ai\/ai2009.html#Ambrosio09","311":"","312":"","313":"http:\/\/dblp.uni-trier.de\/db\/conf\/iiasa\/iiasa1986.html#SchankS86","314":"","315":"http:\/\/dblp.uni-trier.de\/db\/reference\/ai\/ai2009.html#RodriguezPSR09","316":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/LMSR2017.html#0001MR17","317":"http:\/\/dblp.uni-trier.de\/db\/conf\/aia\/aia2005.html#SelvarajahR05","318":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/SR2009.html#Verheij09","319":"","320":"http:\/\/dblp.uni-trier.de\/db\/conf\/cepes\/cepes1989.html#Lees89","321":"http:\/\/cajmtcs.centralasianstudies.org\/index.php\/CAJMTCS\/article\/view\/141","322":"http:\/\/en.wikipedia.org\/w\/index.php?title=Watson\\_(artificial\\_intelligence\\_software)\\&oldid=403807156","323":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/50525\/tracing-of-an-object-in-video-through-mean-shift-protocol\/sahila-fareed","324":"","325":"","326":"https:\/\/patents.google.com\/patent\/US20180018564A1\/en","327":"http:\/\/dblp.uni-trier.de\/db\/journals\/jagi\/jagi3.html#ThorissonNSW12","328":"","329":"https:\/\/doi.org\/10.1007\/978-3-030-32361-5_1","330":"http:\/\/dblp.uni-trier.de\/db\/series\/cogtech\/354023733.html#Kaiser07","331":"https:\/\/lup.lub.lu.se\/student-papers\/search\/publication\/8980218","332":"http:\/\/arxiv.org\/abs\/2308.08708","333":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42444\/handwritten-digit-classification\/souvik-banerjee","334":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia8.html#CarolisG14","335":"http:\/\/dblp.uni-trier.de\/db\/conf\/ccia\/ccia2010.html#Dalmau10","336":"http:\/\/en.wikipedia.org\/w\/index.php?title=Timeline\\_of\\_artificial\\_intelligence\\&oldid=391535807","337":"https:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/1458","338":"http:\/\/en.wikipedia.org\/w\/index.php?title=Ethics\\_of\\_artificial\\_intelligence\\&oldid=399724633","339":"http:\/\/dblp.uni-trier.de\/db\/conf\/aied\/aied2005.html#LaneCLSG05","340":"","341":"","342":"","343":"http:\/\/dblp.uni-trier.de\/db\/conf\/ccia\/ccia2018.html#Merida18","344":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2006.html#Loosemore06","345":"http:\/\/www.diva-portal.org\/smash\/get\/diva2:1217749\/FULLTEXT01.pdf","346":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2006.html#Franklin06","347":"","348":"","349":"","350":"https:\/\/www.boeckler.de\/de\/faust-detail.htm?sync_id=HBS-008498","351":"http:\/\/dblp.uni-trier.de\/db\/journals\/jagi\/jagi2.html#Rohrer10","352":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai30.html#WilliamsM16","353":"","354":"http:\/\/www.sciencedirect.com\/science\/article\/B6TYF-4R1MF4C-1\/2\/36132fd965af5a169b53a197616f4721","355":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0933365709001365","356":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ai1992.html#Havel92","357":"","358":"http:\/\/dblp.uni-trier.de\/db\/journals\/jits\/jits2.html#Karna95","359":"","360":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/41256\/fashion-ai\/ashish-jobson","361":"https:\/\/www.ijtsrd.com.com\/computer-science\/artificial-intelligence\/57493\/utilizing-6g-technology-for-healthcare-monitoring-with-machine-learning\/tapan-golakiya","362":"","363":"","364":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42568\/online-tour-booking-using-fuzzy-decision-making-method\/dr-e-j-thomson-fredrik","365":"http:\/\/www.mpi-sb.mpg.de\/services\/library\/proceedings\/contents\/aaai91.html","366":"","367":"http:\/\/dblp.uni-trier.de\/db\/conf\/jurix\/jurix2019.html#EngersV19","368":"http:\/\/dblp.uni-trier.de\/db\/conf\/scai\/scai2011.html#Nordin11","369":"http:\/\/dblp.uni-trier.de\/db\/conf\/ejc\/ejc2006.html#DuziDDGM06","370":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/24010\/internet-of-things-iot-security-perspective\/sunilkumar-malge","371":"http:\/\/dblp.uni-trier.de\/db\/conf\/birthday\/guarino2019.html#Oltramari19","372":"","373":"","374":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S000437021200077X","375":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0004370212000446","376":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59864\/blockchain-and-ai-convergence-a-new-era-of-possibilities\/manish-verma","377":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2008.html#HwangHH08","378":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecai\/ecai2008.html#Ghahramani08","379":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2006.html#Cassimatis06","380":"http:\/\/www.sciencedirect.com\/science\/article\/B6TYF-4GP1VMJ-1\/2\/4be3aaa78a5e4407ace7425993abafdc","381":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/LMSR2017.html#LawlessS17","382":"http:\/\/www.ascribe.org\/cgi-bin\/spew4th.pl?ascribeid=20040504.114704&time=12%2033%20PDT&year=2004&public=1","383":"","384":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci427.html#Prentzas13","385":"http:\/\/www.amazon.ca\/exec\/obidos\/redirect?tag=citeulike09-20\\&amp;path=ASIN\/0472084607","386":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/107\/web-based-intelligent-inventory-management-system\/madamidola-o-a","387":"http:\/\/portal.acm.org\/citation.cfm?id=1057849.1057866","388":"http:\/\/dblp.uni-trier.de\/db\/conf\/cepes\/cepes1989.html#Suppes89","389":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1610.html#Datta16a","390":"","391":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31653\/image-classification-using-deep-learning\/dr-sachin-k-korde","392":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/30228\/mobile-network-coverage-determination-at-900mhz-for-abuja-rural-areas-using-artificial-neural-networks\/deme-c-abraham","393":"http:\/\/dblp.uni-trier.de\/db\/journals\/computer\/computer6.html#Raphael73","394":"http:\/\/dblp.uni-trier.de\/db\/reference\/bioinf\/ebcb2019-1.html#Scarcello19","395":"citeseer.ist.psu.edu\/dautenhahn95getting.html","396":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2018.html#HolzingerMKH18","397":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42498\/a-review-on-introduction-to-reinforcement-learning\/shreya-khare","398":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/7047\/cervical-smear-analyzer-csa-expert-system-for-identification-of-cervical-cells-in-papanicolaou-smear-test\/abid-sarwar","399":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42378\/fashion-ai-literature\/ashish-jobson","400":"http:\/\/dblp.uni-trier.de\/db\/conf\/aia\/aia2005.html#HoffmanT05","401":"http:\/\/dblp.uni-trier.de\/db\/conf\/iiasa\/iiasa1986.html#Nagao86","402":"","403":"http:\/\/dblp.uni-trier.de\/db\/conf\/aied\/aied2009.html#BlanchardVHL09","404":"http:\/\/dblp.uni-trier.de\/db\/conf\/laptec\/laptec2005.html#NakamatsuS05b","405":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2008.html#GoertzelB08","406":"https:\/\/aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/1904","407":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaaifs\/aaaifs2012-02.html#RaskinT12","408":"http:\/\/dblp.uni-trier.de\/db\/conf\/laptec\/laptec2005.html#NakamatsuS05d","409":"","410":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaaifs\/aaaifs2007-2.html#SchwabacherG07","411":"","412":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2008.html#HartG08","413":"http:\/\/dblp.uni-trier.de\/db\/conf\/laptec\/laptec2005.html#NakamatsuS05c","414":"http:\/\/dblp.uni-trier.de\/db\/conf\/laptec\/laptec2005.html#NakamatsuS05a","415":"https:\/\/onlinelibrary.wiley.com\/doi\/10.1111\/ntwe.12178","416":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2008.html#Milch08","417":"","418":"","419":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/Miranda2000.html#Camurri00","420":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/Miranda2000.html#Holland00","421":"http:\/\/dblp.uni-trier.de\/db\/conf\/ifip12\/icis2018.html#ZhangN18","422":"http:\/\/airccse.com\/JPS\/papers\/1118jps02.pdf","423":"","424":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci427.html#BenderskayaZ13","425":"","426":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci607.html#Arsenio15","427":"http:\/\/dblp.uni-trier.de\/db\/conf\/digra\/digra2003.html#Karlsson03","428":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/acai1987.html#Jorrand87","429":"citeseer.ist.psu.edu\/jamroga99modelling.html","430":"","431":"","432":"","433":"http:\/\/www.amazon.de\/exec\/obidos\/ASIN\/0130803022","434":"","435":"","436":"http:\/\/dblp.uni-trier.de\/db\/conf\/compcon\/compcon1987.html#KaiserF87","437":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2017.html#BrodicPJAD17","438":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/28094\/computational-creativity\/matthew-n-o-sadiku","439":"","440":"http:\/\/www.us.oup.com\/us\/catalog\/general\/?view=usa&ci=9780198250807","441":"http:\/\/dblp.uni-trier.de\/db\/conf\/scai\/scai2015.html#SheulyBBB15","442":"http:\/\/dblp.uni-trier.de\/db\/conf\/ccia\/ccia2015.html#Metta15","443":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#TeixeiraGAF20","444":"http:\/\/dblp.uni-trier.de\/db\/journals\/jaise\/jaise11.html#GamsGHMT19","445":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaai\/hc2011.html#WeldMD11","446":"http:\/\/dblp.uni-trier.de\/db\/journals\/jaciii\/jaciii21.html#Takama17","447":"http:\/\/arxiv.org\/abs\/2306.07899","448":"https:\/\/doi.org\/10.1177%2F20539517221092956","449":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/14331\/face-recognition-technology\/sagar-deshmukh","450":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31888\/traffic-sign-detection-and-recognition-for-automated-driverless-cars-based-on-ssd\/aswathy-madhu","451":"http:\/\/dx.doi.org\/10.1016\/0004-3702(76)90003-5","452":"https:\/\/doi.org\/10.1007\/s10639-023-12405-0","453":"http:\/\/dblp.uni-trier.de\/db\/conf\/ccia\/ccia2019.html#Serra19","454":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2008.html#Smith08","455":"http:\/\/dx.doi.org\/10.1023\/A%3A1022850703159","456":"http:\/\/dblp.uni-trier.de\/db\/conf\/scai\/scai1988.html#SagatunB88","457":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaaifs\/aaaifs2012-02.html#Ivanova12","458":"http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/10936748","459":"http:\/\/www.springerlink.com\/index\/T50XXAMDEE88GCC7","460":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia7.html#LenzeriniS13","461":"http:\/\/doi.searchdl.org\/01.IJCOM.1.3.80","462":"http:\/\/www.aaai.org\/Library\/Workshops\/2004\/ws04-04-001.php","463":"","464":"","465":"http:\/\/www.sciencedirect.com\/science\/article\/B6TYF-4938PVR-4\/2\/047c96a666d64f54e22364adb93c030a","466":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/38287\/traditional-machine-learning-and-nocode-machine-learning-with-its-features-and-application\/hiteshkumar-babubhai-vora","467":"http:\/\/dblp.uni-trier.de\/db\/conf\/cepes\/cepes1989.html#Aiken89","468":"http:\/\/dblp.uni-trier.de\/db\/conf\/dagstuhl\/dfu6.html#CongdonHK13","469":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/60106\/legal-risks-and-preventive-measures-in-chatgpt-applications-in-china\/chen-jiaqi","470":"https:\/\/doi.org\/10.1145%2F3377325.3377538","471":"https:\/\/ieeexplore.ieee.org\/document\/9154906\/","472":"http:\/\/dblp.uni-trier.de\/db\/journals\/technometrics\/technometrics47.html#Zelterman05","473":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1403.html#Greer14h","474":"http:\/\/dblp.uni-trier.de\/db\/conf\/eurospi\/eurospi2019.html#Fehlmann19","475":"http:\/\/dblp.uni-trier.de\/db\/conf\/IEEEicci\/IEEEicci2015.html#Howard15","476":"http:\/\/dblp.uni-trier.de\/db\/series\/sapere\/sapere5.html#ZantKS13","477":"http:\/\/scholar.google.com\/scholar.bib?q=info:LElqzXwJpt8J:scholar.google.com\/&output=citation&hl=de&as_sdt=0,5&as_vis=1&scfhb=1&ct=citation&cd=0","478":"http:\/\/dblp.uni-trier.de\/db\/journals\/sigart\/sigart6.html#Nilsson95","479":"http:\/\/dblp.uni-trier.de\/db\/journals\/scholarpedia\/scholarpedia10.html#Goertzel15","480":"","481":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai50\/ai502006.html#Dautenhahn06","482":"http:\/\/www.aaai.org\/Library\/Workshops\/2004\/ws04-04-019.php","483":"","484":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/38245\/emotion-detector\/ashish-jobson","485":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0378437119317546","486":"https:\/\/www.nature.com\/articles\/s41598-022-21646-x","487":"http:\/\/dblp.uni-trier.de\/db\/conf\/flairs\/flairs2016.html#NeufeldF16","488":"http:\/\/dblp.uni-trier.de\/db\/journals\/jetai\/jetai23.html#Bringsjord11","489":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1908.html#abs-1908-10345","490":"http:\/\/dblp.uni-trier.de\/db\/journals\/widm\/widm9.html#HolzingerLDZM19","491":"http:\/\/dblp.uni-trier.de\/db\/conf\/datalog\/datalog2019.html#Aref19","492":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1803.html#abs-1803-04263","493":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1411.html#Hibbard14","494":"","495":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2009.html#abs-2009-02185","496":"","497":"http:\/\/dblp.uni-trier.de\/db\/conf\/icci\/icci1993.html#Grzymala-Busse93","498":"http:\/\/dblp.uni-trier.de\/db\/conf\/intellisys\/intellisys2019-1.html#RosenbergW19","499":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1908.html#abs-1908-02150","500":"http:\/\/dblp.uni-trier.de\/db\/journals\/cacm\/cacm61.html#Krakovsky18a","501":"http:\/\/portal.acm.org\/citation.cfm?id=166809","502":"http:\/\/dblp.uni-trier.de\/db\/journals\/air\/air1.html#Campbell86","503":"http:\/\/dblp.uni-trier.de\/db\/journals\/tit\/tit9.html#Feigenbaum63","504":"","505":"http:\/\/dblp.uni-trier.de\/db\/journals\/isafm\/isafm13.html#Layne05","506":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl1981.html#Crout81","507":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/14453\/face-recognition-system\/rakeshkumar-h-yadav","508":"https:\/\/www.aaai.org\/ocs\/index.php\/AAAI\/AAAI17\/paper\/view\/14886","509":"http:\/\/dblp.uni-trier.de\/db\/reference\/ap\/is2002.html#Neumann02","510":"http:\/\/dblp.uni-trier.de\/db\/conf\/pricai\/pricai2004.html#Lathrop04","511":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai149.html#Chrisley03","512":"http:\/\/dblp.uni-trier.de\/db\/conf\/chi\/chi2008a.html#SpauldingJGYZ08","513":"http:\/\/dx.doi.org\/10.1016\/j.artint.2006.10.002","514":"https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/552","515":"","516":"citeseer.ist.psu.edu\/103012.html","517":"https:\/\/journals.researchparks.org\/index.php\/IJOT\/article\/view\/4427\/4150","518":"http:\/\/dx.doi.org\/10.1007\/s10462-007-9052-3","519":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42357\/an-extensive-review-on-generative-adversarial-networks-gan\u2019s\/atharva-chitnavis","520":"http:\/\/dblp.uni-trier.de\/db\/conf\/agi\/agi2008.html#LevyG08","521":"http:\/\/dblp.uni-trier.de\/db\/conf\/scai\/scai1988.html#BouteldjaL88","522":"http:\/\/dblp.uni-trier.de\/db\/conf\/scai\/scai1991.html#Nourani91","523":"","524":"","525":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/0004370282900236","526":"https:\/\/www.frontiersin.org\/article\/10.3389\/frvir.2021.627194","527":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/61292\/beyond-ai-the-rise-of-cognitive-computing-as-future-of-computing-chatgpt-analysis\/manish-verma","528":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia7.html#Vico13","529":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/50259\/future-of-robotics\/matthew-n-o-sadiku","530":"https:\/\/www.frontiersin.org\/article\/10.3389\/frvir.2021.686783","531":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/49633\/future-of-digital-natives\/matthew-n-o-sadiku","532":"http:\/\/dx.doi.org\/10.1007\/s10462-010-9166-x","533":"http:\/\/dblp.uni-trier.de\/db\/journals\/monet\/monet23.html#LuLCKS18","534":"http:\/\/dblp.uni-trier.de\/db\/journals\/ieeemm\/ieeemm24.html#Rui17","535":"http:\/\/dblp.uni-trier.de\/db\/journals\/ci\/ci23.html#RemagninoS07","536":"http:\/\/dblp.uni-trier.de\/db\/journals\/expert\/expert29.html#ZengW14","537":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1706.html#LuLCKS17","538":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai50\/ai502006.html#FattoriBMMG06","539":"http:\/\/www.ai.uga.edu\/ftplib\/ai-reports\/ai199405.ps","540":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaai\/aisl2011.html#EdgellV11","541":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1709.html#abs-1709-10242","542":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci166.html#MoschopoulosTBFL09","543":"citeseer.ist.psu.edu\/677739.html","544":"http:\/\/oro.open.ac.uk\/53020\/","545":"http:\/\/www.sciencedirect.com\/science\/article\/B6TYF-4FM01JG-1\/2\/aee68ef2b23906befe476ea32bad5d06","546":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/Miranda2000.html#WigginsS00","547":"http:\/\/www.csc.liv.ac.uk\/~chris\/SE.html","548":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/49142\/deep-learning-applications-and-image-processing\/ahmet-\u00c3\u00b6zcan","549":"","550":"http:\/\/dblp.uni-trier.de\/db\/conf\/ukci\/ukci2019.html#Serrano19","551":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1912.html#abs-1912-05744","552":"http:\/\/dblp.uni-trier.de\/db\/conf\/apascience\/apascience2018.html#HaLK18","553":"","554":"","555":"http:\/\/dblp.uni-trier.de\/db\/conf\/digra\/digra2016.html#LimLH16","556":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23730\/dynamic-question-answer-generator-an-enhanced-approach-to-question-generation\/rahul-bhatia","557":"http:\/\/dblp.uni-trier.de\/db\/conf\/ccia\/ccia2006.html#Sales06","558":"","559":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/37978\/advanced-fire-monitoring-system\/sreejith-s-p","560":"http:\/\/www.sciencedirect.com\/science\/article\/B6TYF-4WHFD7V-1\/2\/b6ecc62544f6057f29d9ea48018df072","561":"","562":"http:\/\/books.google.de\/books?id=QcTuJb7Hi40C&printsec=frontcover&source=gbs_navlinks_s#v=onepage&q=&f=false","563":"","564":"http:\/\/dblp.uni-trier.de\/db\/conf\/robophilosophy\/robophilosophy2018.html#FilzmoserK18","565":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42411\/a-traffic-sign-classifier-model-using-sage-maker\/arpit-seth","566":"https:\/\/www.frontiersin.org\/articles\/10.3389\/frai.2023.1099521\/full","567":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/11512\/arduino-controlled-robotic-arm\/k-aishwarya","568":"http:\/\/dblp.uni-trier.de\/db\/conf\/cepes\/cepes1989.html#Renc89","569":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/38614\/virtual-therapist-for-psychological-healthcare\/tanmay-pachpande","570":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai50\/ai502006.html#ArietaKYY06","571":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23572\/age-invariant-face-recognition\/prathama-v","572":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim5.html#Barstow85","573":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim5.html#Chace85","574":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcnn\/ijcnn2015.html#BlackledgeBT15","575":"http:\/\/dblp.uni-trier.de\/db\/journals\/amai\/amai77.html#PigozziTV16","576":"http:\/\/dblp.uni-trier.de\/db\/conf\/mmb\/mmb87.html#Oren87","577":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1701.html#Hansen17a","578":"http:\/\/dblp.uni-trier.de\/db\/conf\/jcit\/jcit1990.html#Amarel90","579":"http:\/\/dblp.uni-trier.de\/db\/journals\/computer\/computer53.html#0001NN020","580":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2006.html#abs-2006-12362","581":"http:\/\/dblp.uni-trier.de\/db\/journals\/geb\/geb35.html#MondererTV01","582":"","583":"http:\/\/www-formal.stanford.edu\/jmc\/generality.pdf","584":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom7.html#Omar94","585":"http:\/\/dblp.uni-trier.de\/db\/journals\/air\/air1.html#Partridge86","586":"","587":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/A1999.html#Korf99","588":"http:\/\/dblp.uni-trier.de\/db\/journals\/fuin\/fuin15.html#HortyT91","589":"http:\/\/dblp.uni-trier.de\/db\/journals\/computer\/computer53.html#Jimenez-GomezCF20","590":"","591":"","592":"http:\/\/dblp.uni-trier.de\/db\/journals\/jetai\/jetai9.html#Nilsson97","593":"http:\/\/dblp.uni-trier.de\/db\/journals\/jetai\/jetai3.html#Bylander91","594":"http:\/\/dblp.uni-trier.de\/db\/journals\/candc\/candc26.html#CorneM02","595":"","596":"http:\/\/dblp.uni-trier.de\/db\/journals\/alife\/alife1.html#Steels94","597":"http:\/\/dblp.uni-trier.de\/db\/journals\/expert\/expert29.html#ZengM14","598":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1912.html#abs-1912-06485","599":"http:\/\/dblp.uni-trier.de\/db\/journals\/tmm\/tmm22.html#00010020","600":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci427.html#Garcia-AlmanzaAM13","601":"http:\/\/dx.doi.org\/10.2200\/S00372ED1V01Y201107AIM014","602":"http:\/\/dblp.uni-trier.de\/db\/journals\/ets\/ets7.html#Devedzic04","603":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#GiniAGKL18","604":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai170.html#Spector06","605":"","606":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim5.html#Hart84","607":"","608":"http:\/\/dblp.uni-trier.de\/db\/journals\/amai\/amai28.html#Dershowitz00","609":"http:\/\/dblp.uni-trier.de\/db\/journals\/ker\/ker9.html#Parsons94","610":"http:\/\/dblp.uni-trier.de\/db\/journals\/cacm\/cacm30.html#McCarthy87","611":"http:\/\/dblp.uni-trier.de\/db\/conf\/icnc\/icncfskd2019-2.html#ZhangZLCW19","612":"http:\/\/dblp.uni-trier.de\/db\/conf\/pricai\/pricai2000w.html#WilliamsL00","613":"http:\/\/dblp.uni-trier.de\/db\/journals\/information\/information10.html#ImpedovoP19","614":"http:\/\/dblp.uni-trier.de\/db\/conf\/db-workshops\/intervale82.html#Hewitt82","615":"http:\/\/dblp.uni-trier.de\/db\/conf\/eurocast\/eurocast1991.html#MarikL91","616":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/ijcai97.html#Boden97","617":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/ijcai2018.html#KoniecznyL18","618":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/ijcai75.html#GipsS75","619":"http:\/\/dblp.uni-trier.de\/db\/journals\/tc\/tc25.html#Holden76","620":"http:\/\/web.media.mit.edu\/~minsky\/papers\/steps.html","621":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/ijcai77.html#AmarelBBHKMP77","622":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/ijcai77.html#McCorduckMSS77","623":"http:\/\/dblp.uni-trier.de\/db\/journals\/tsmc\/tsmc17.html#KanalLS87","624":"http:\/\/dblp.uni-trier.de\/db\/journals\/scirobotics\/scirobotics4.html#GunningSCMSY19","625":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai171.html#Bench-CaponD07","626":"","627":"http:\/\/dblp.uni-trier.de\/db\/journals\/air\/air29.html#CurranBM08","628":"http:\/\/dblp.uni-trier.de\/db\/journals\/ki\/ki34.html#Lucas20","629":"http:\/\/dblp.uni-trier.de\/db\/journals\/ki\/ki28.html#Klugl14","630":"","631":"","632":"http:\/\/dblp.uni-trier.de\/db\/journals\/advai\/advai2010.html#BerrarSS10","633":"http:\/\/dblp.uni-trier.de\/db\/journals\/ijlit\/ijlit2.html#Martino94","634":"http:\/\/www.mpi-sb.mpg.de\/services\/library\/proceedings\/contents\/aaai97\\_iaai97.html","635":"","636":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/27909\/comparison-of-defuzzification-methods-from-a-real-world-problem\/wai-wai-tun","637":"http:\/\/citeseer.nj.nec.com\/levy00intelligent.html","638":"http:\/\/dblp.uni-trier.de\/db\/journals\/japll\/japll2.html#WheelerP04","639":"http:\/\/dblp.uni-trier.de\/db\/reference\/ecgg\/1.html#HildmannH19c","640":"http:\/\/dblp.uni-trier.de\/db\/journals\/fgcs\/fgcs7.html#Allard92","641":"http:\/\/dblp.uni-trier.de\/db\/journals\/cacm\/cacm61.html#Monroe18","642":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecai\/ecai82.html#Gallaire82","643":"http:\/\/dblp.uni-trier.de\/db\/conf\/ifip\/ifip1962.html#Minsky62","644":"http:\/\/dblp.uni-trier.de\/db\/journals\/cacm\/cacm60.html#Kaplan17","645":"http:\/\/dblp.uni-trier.de\/db\/journals\/csur\/csur28.html#Doyle96","646":"http:\/\/dblp.uni-trier.de\/db\/conf\/ACMse\/ACMse2005-1.html#DawsonH05","647":"http:\/\/dblp.uni-trier.de\/db\/conf\/wsc\/wsc1985.html#Hightower85","648":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1311.html#Laufer13","649":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1802.html#abs-1802-04451","650":"http:\/\/dblp.uni-trier.de\/db\/journals\/cacm\/cacm63.html#DenningD20","651":"http:\/\/dblp.uni-trier.de\/db\/journals\/tsmc\/tsmc16.html#AndrioleS86","652":"http:\/\/dblp.uni-trier.de\/db\/conf\/wsc\/wsc1990.html#Rothenberg90","653":"http:\/\/dblp.uni-trier.de\/db\/journals\/expert\/expert23.html#WangLZ08","654":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai47.html#Nilsson91","655":"","656":"","657":"","658":"","659":"http:\/\/dblp.uni-trier.de\/db\/conf\/aimsa\/aimsa1984.html#Bibel84","660":"http:\/\/dblp.uni-trier.de\/db\/journals\/anor\/anor53.html#Oren94","661":"http:\/\/dblp.uni-trier.de\/db\/reference\/ap\/is2002.html#JainC02","662":"http:\/\/dblp.uni-trier.de\/db\/reference\/ap\/is2002.html#ChenP02","663":"http:\/\/dblp.uni-trier.de\/db\/conf\/bica\/bica2018.html#Targon18","664":"","665":"http:\/\/dblp.uni-trier.de\/db\/series\/asc\/asc44.html#Abraham08","666":"http:\/\/dblp.uni-trier.de\/db\/conf\/itc\/itc1984.html#Robinson84","667":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim2.html#Collins80","668":"http:\/\/dblp.uni-trier.de\/db\/conf\/wsc\/wsc1987.html#Shannon87","669":"http:\/\/dblp.uni-trier.de\/db\/conf\/ictai\/ictai1990.html#Glasgow90","670":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/ijcai91.html#KitanoHHMW91","671":"http:\/\/dblp.uni-trier.de\/db\/conf\/icse\/seams2019.html#LemosG19","672":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1903.html#abs-1903-04442","673":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1909.html#abs-1909-12072","674":"http:\/\/dblp.uni-trier.de\/db\/conf\/IEEEcit\/IEEEcit2004.html#Li04","675":"http:\/\/dblp.uni-trier.de\/db\/reference\/auto\/auto2009.html#Nau09","676":"http:\/\/dblp.uni-trier.de\/db\/conf\/icpr\/icpr1992-1.html#RuoccoVV92","677":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2001.html#abs-2001-00627","678":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr0601.html#abs-cs-0601052","679":"http:\/\/dblp.uni-trier.de\/db\/journals\/ker\/ker3.html#Hern88","680":"http:\/\/dblp.uni-trier.de\/db\/journals\/ras\/ras1.html#Kempf85","681":"http:\/\/web.media.mit.edu\/~minsky\/papers\/steps.html","682":"","683":"","684":"http:\/\/dblp.uni-trier.de\/db\/journals\/aai\/aai5.html#Radermacher91","685":"http:\/\/dblp.uni-trier.de\/db\/journals\/jzusc\/jzusc19.html#YuK18","686":"","687":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaaiss\/tocais2019.html#ZaadnoordijkB19","688":"http:\/\/dblp.uni-trier.de\/db\/journals\/mima\/mima20.html#Jennings10","689":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim6.html#Katz85","690":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai28.html#Rada86","691":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaai\/aaai80.html#Rosenberg80","692":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaai\/safeai2019.html#ZengLH19","693":"","694":"http:\/\/dblp.uni-trier.de\/db\/journals\/computer\/computer52.html#HoleA19","695":"http:\/\/dblp.uni-trier.de\/db\/conf\/dis\/dis2005.html#Bradshaw05","696":"http:\/\/dblp.uni-trier.de\/db\/journals\/aei\/aie1.html#Leff86","697":"http:\/\/dblp.uni-trier.de\/db\/reference\/ap\/is2002.html#TeodorescuK02","698":"http:\/\/dblp.uni-trier.de\/db\/journals\/ail\/ail28.html#Verheij20","699":"http:\/\/dblp.uni-trier.de\/db\/conf\/sisy\/sisy2018.html#Lazanyi18","700":"http:\/\/dblp.uni-trier.de\/db\/conf\/birthday\/Aiello2006.html#NardiI06","701":"http:\/\/dblp.uni-trier.de\/db\/journals\/ubiquity\/ubiquity2005.html#KakDD05","702":"http:\/\/dblp.uni-trier.de\/db\/conf\/hai\/hai2018.html#Jennings18","703":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1712.html#abs-1712-03779","704":"http:\/\/dblp.uni-trier.de\/db\/conf\/vlsic\/vlsic2018.html#DallyGPK0D18","705":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1606.html#SeshiaS16","706":"","707":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais1.html#Ennals87","708":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais2.html#Oliverio88","709":"http:\/\/arxiv.org\/abs\/cs.AI\/0307071","710":"http:\/\/dblp.uni-trier.de\/db\/journals\/cbsn\/cbsn22.html#Wiederhold19f","711":"","712":"","713":"http:\/\/www.mpi-sb.mpg.de\/services\/library\/proceedings\/contents\/aaai94-1.html","714":"http:\/\/www.springerlink.com\/content\/978-3-642-18049-1","715":"","716":"http:\/\/www.mpi-sb.mpg.de\/services\/library\/proceedings\/contents\/aaai93.html","717":"http:\/\/www.mpi-sb.mpg.de\/services\/library\/proceedings\/contents\/aaai96-1.html","718":"","719":"","720":"http:\/\/dblp.uni-trier.de\/db\/conf\/ogai\/ogai1989.html#Krushanov89","721":"","722":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecai\/ecai2014.html#MarquisPP14","723":"","724":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim4.html#Winston83","725":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/14272\/topic-detection-using-machine-learning\/mr-ajmal-rasi","726":"","727":"","728":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0004370200000424","729":"","730":"http:\/\/www.springerlink.com\/content\/dp7567u875240308\/","731":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia4.html#FerrarettiTCPLS10","732":"http:\/\/www.sciencedirect.com\/science\/article\/B6TW2-4BDC59N-5\/2\/debe3eeedadeadcc55d0c01b95b57719","733":"","734":"https:\/\/doi.org\/10.1007\/s13347-022-00498-3","735":"http:\/\/dblp.uni-trier.de\/db\/conf\/cepes\/cepes1989.html#Csonto89","736":"","737":"http:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/09528130802113364","738":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia9.html#Ruggieri15","739":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia7.html#Stock13","740":"https:\/\/journals.researchparks.org\/index.php\/IJHCS\/article\/view\/521","741":"http:\/\/dblp.uni-trier.de\/db\/journals\/ia\/ia13.html#DondioLB19","742":"http:\/\/dblp.uni-trier.de\/db\/conf\/mspn\/mspn2017.html#FonsecaPD17","743":"http:\/\/dblp.uni-trier.de\/db\/conf\/ifip12\/icis2018.html#TaoZR18","744":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42429\/drug-review-sentiment-analysis-using-boosting-algorithms\/sumit-mishra","745":"http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/11281\/student-library-attendance-using-face-recognition\/k-ravikanth-mishra","746":"","747":"","748":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2018.html#BordasNZ18","749":"","750":"https:\/\/doi.org\/10.21428%2F36973002.4a261345","751":"https:\/\/learning.oreilly.com\/library\/view\/pragmatic-ai-an\/9780134863924\/","752":"https:\/\/simplified.com\/ai-logo-maker\/","753":"","754":"","755":"https:\/\/ec.europa.eu\/digital-single-market\/en\/news\/ethics-guidelines-trustworthy-ai","756":"http:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/2709","757":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666920X21000357","758":"http:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/2601","759":"https:\/\/flywly.com\/ai-periodic-table\/","760":"","761":"http:\/\/www.cim.mcgill.ca\/~yon\/ai\/papers\/AIMag20-02-008.pdf","762":"https:\/\/doi.org\/10.1007\/s00287-018-1102-5","763":"http:\/\/www.amazon.fr\/exec\/obidos\/ASIN\/1584502789\/citeulike04-21","764":"","765":"https:\/\/www.etui.org\/publications\/ai-regulation-entering-ai-regulatory-winter","766":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#GuizzardiAGM20","767":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#BillingsPSS98","768":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#McNaughtonRSS03","769":"http:\/\/arxiv.org\/abs\/2112.11471","770":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/60087\/ai-safety-and-regulations-navigating-the-postcovid-era-aims-opportunities-and-challenges-a-chatgpt-analysis\/manish-verma","771":"https:\/\/doi.org\/10.1145\/3170427.3188487","772":"https:\/\/doi.org\/10.1145%2F3290605.3300831","773":"https:\/\/doi.org\/10.21428%2F36973002.c76458f1","774":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#CohenA0PL20","775":"https:\/\/www.accenture.com\/aiboostsprofits","776":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#DahyaM19","777":"","778":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#MenziesS01","779":"","780":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#VerkhogliadO10","781":"https:\/\/www.wiley.com\/en-us\/Is+AI+Good+for+the+Planet%3F-p-9781509547944","782":"","783":"","784":"https:\/\/doi.org\/10.21428%2F36973002.af25d602","785":"https:\/\/onlinelibrary.wiley.com\/doi\/full\/10.1111\/ejed.12533","786":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#ShvoSM18","787":"https:\/\/www.mdpi.com\/2071-1050\/15\/17\/12921","788":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0360131523002440","789":"","790":"http:\/\/arxiv.org\/abs\/1809.08328","791":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom18.html#PeriniS05","792":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom13.html#RaedtPV00","793":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#Gabrilovich09","794":"","795":"https:\/\/doi.org\/10.1145%2F3490099.3511138","796":"https:\/\/doi.org\/10.1145%2F3276742","797":"http:\/\/arxiv.org\/abs\/1812.04608","798":"http:\/\/arxiv.org\/abs\/2105.07825","799":"https:\/\/doi.org\/10.1145%2F3581641.3584052","800":"https:\/\/doi.org\/10.1007%2Fs00146-023-01719-9","801":"https:\/\/medium.com\/@virginiadignum\/the-art-of-ai-accountability-responsibility-transparency-48666ec92ea5","802":"http:\/\/www.computer.org\/csdl\/mags\/ex\/2011\/02\/index.html","803":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666920X21000199","804":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.140.452","805":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666920X21000114","806":"","807":"https:\/\/doi.org\/10.1177\/09637214231217286","808":"https:\/\/doi.org\/10.1145%2F3397481.3450644","809":"https:\/\/downloads.hci.informatik.uni-wuerzburg.de\/2023-chi-lbw-ai-drawing-straka.pdf","810":"http:\/\/dblp.uni-trier.de\/db\/reference\/ai\/ai2009.html#Ingber09","811":"https:\/\/doi.org\/10.1145%2F3464903","812":"https:\/\/doi.org\/10.1145%2F3448250","813":"http:\/\/dblp.uni-trier.de\/db\/reference\/ai\/ai2009.html#DjebbariCAQ09","814":"https:\/\/link.springer.com\/article\/10.1007\/s00146-023-01747-5","815":"https:\/\/www.mckinsey.de\/kuenstliche-intelligenz-wird-zum-wachstumsmotor-fuer-deutsche-industrie","816":"http:\/\/arxiv.org\/abs\/1809.08267","817":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.44.5994","818":"http:\/\/arxiv.org\/abs\/2008.04165","819":"http:\/\/arxiv.org\/abs\/2304.06035","820":"","821":"","822":"","823":"https:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=4543807","824":"","825":"http:\/\/dblp.uni-trier.de\/db\/reference\/ai\/ai2009.html#Martin-GuerreroSLS09","826":"","827":"https:\/\/montrealethics.ai\/wp-content\/uploads\/2020\/06\/State-of-AI-Ethics-June-2020-report.pdf","828":"","829":"","830":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom14.html#Widmer01","831":"https:\/\/www.ijtsrd.com\/engineering\/information-technology\/38002\/smart-verification-of-passenger-using-ai\/leman-kirme","832":"","833":"https:\/\/arxiv.org\/abs\/2312.07086","834":"https:\/\/pubs.wi-kassel.de\/wp-content\/uploads\/2022\/07\/JML_878.pdf","835":"http:\/\/dx.doi.org\/10.17762\/ijritcc2321-8169.150331","836":"http:\/\/arxiv.org\/abs\/1808.07261","837":"https:\/\/www.journals.uchicago.edu\/doi\/10.1086\/699936","838":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/49722\/conversational-ai-powered-chatbot-using-lex-and-aws\/pradyumna-saini","839":"https:\/\/doi.org\/10.1145\/3573051.3596191","840":"https:\/\/doi.org\/10.1145%2F3581641.3584080","841":"http:\/\/arxiv.org\/abs\/1905.11481","842":"https:\/\/doi.org\/10.1080\/21532974.2023.2247480        ","843":"http:\/\/arxiv.org\/abs\/2011.04698","844":"http:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/2716","845":"https:\/\/arxiv.org\/abs\/2302.06975","846":"http:\/\/ieeexplore.ieee.org\/xpl\/articleDetails.jsp?arnumber=7203143","847":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim29.html#LeakeG08","848":"http:\/\/arxiv.org\/abs\/1809.07193","849":"","850":"","851":"https:\/\/www.economist.com\/by-invitation\/2023\/04\/28\/yuval-noah-harari-argues-that-ai-has-hacked-the-operating-system-of-human-civilisation?itm_source=parsely-api","852":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim38.html#GoelJ17","853":"","854":"https:\/\/www.jlis.it\/index.php\/jlis\/article\/view\/437","855":"http:\/\/arxiv.org\/abs\/2211.15021","856":"https:\/\/www.etui.org\/Publications2\/Foresight-briefs\/Labour-in-the-age-of-AI-why-regulation-is-needed-to-protect-workers","857":"http:\/\/dx.doi.org\/10.1109\/mis.2012.92","858":"https:\/\/www.ijtsrd.comengineering\/computer-engineering\/41232\/an-ai-based-atm-intelligent-security-system-using-open-cv-and-yolo\/prem-krishna","859":"","860":"","861":"","862":"","863":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Rovatsos17a","864":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim28.html#Glick07c","865":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Medsker18","866":"http:\/\/dx.doi.org\/10.2139\/ssrn.4475995","867":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#HemmentABMLHRB19","868":"","869":"https:\/\/www.evernote.com\/shard\/s7\/sh\/6174b3c8-fed8-48e2-8daf-bcd36c8ca242\/3b02b05551041df74f418ed1fa73b4ed","870":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim33.html#BarnesBCDKSSSTHW12","871":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Neller17","872":"","873":"https:\/\/doi.org\/10.1145%2F3397481.3450698","874":"http:\/\/www.hutter1.net\/ai\/uaibook.htm","875":"http:\/\/arxiv.org\/abs\/2105.08629","876":"https:\/\/cejsr.academicjournal.io\/index.php\/journal\/article\/view\/945","877":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim13.html#Rich92","878":"https:\/\/anatomyof.ai\/","879":"https:\/\/doi.org\/10.21428%2F36973002.e637d5a8","880":"http:\/\/dx.doi.org\/10.1186\/s40594-023-00418-7","881":"https:\/\/doi.org\/10.5281\/zenodo.8072950","882":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#Botea19c","883":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim25.html#GlasgowJR04","884":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim12.html#Schank91","885":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais1.html#Narayanan87","886":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#Medsker19a","887":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim36.html#DessimozKS15","888":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim33.html#FoxVH12","889":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim39.html#ComanA18","890":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim37.html#Goel16","891":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim18.html#Blanchard97","892":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters1.html#RossiV14","893":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais35.html#Gill20a","894":"https:\/\/wpaicopilot.com\/","895":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#Dignum19","896":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim41.html#BrachmanGB20","897":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Rovatsos18a","898":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais2.html#HallL88","899":"https:\/\/downloads.hci.informatik.uni-wuerzburg.de\/Carolus-and-Kocch-(2023)-MAILS.pdf","900":"http:\/\/arxiv.org\/abs\/1605.02797","901":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais35.html#HagendorffW20","902":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#FureyM18","903":"http:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/issue\/view\/191\/showToc","904":"http:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/2712","905":"https:\/\/doi.org\/10.1145%2F3581641.3584090","906":"","907":"http:\/\/arxiv.org\/abs\/2107.10356","908":"","909":"https:\/\/link.springer.com\/article\/10.1007\/s11192-021-03922-1","910":"https:\/\/doi.org\/10.1038%2Fs42256-019-0088-2","911":"","912":"https:\/\/doi.org\/10.1145%2F3397481.3450650","913":"","914":"","915":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim35.html#Fisher14","916":"http:\/\/www2.imm.dtu.dk\/pubdb\/p.php?3650","917":"http:\/\/dx.doi.org\/10.1007\/s11023-007-9067-1","918":"","919":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#Heer18","920":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais32.html#Gill17d","921":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim26.html#Glick05a","922":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais6.html#Unseld92","923":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais13.html#Gelepithis99","924":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais21.html#Bundy07","925":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais5.html#Sato91","926":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters1.html#Wagstaff14","927":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim27.html#Feigenbaum06","928":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim38.html#Shoham17","929":"https:\/\/pubs.wi-kassel.de\/wp-content\/uploads\/2023\/02\/JML_921.pdf","930":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim14.html#Hedberg93","931":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim27.html#Glick06b","932":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim37.html#FarooqOKK16","933":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Eckroth18","934":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais2.html#Gill88","935":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim19.html#Walsh98","936":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim37.html#Felner16","937":"https:\/\/innovations.bmj.com\/content\/6\/4\/117","938":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim7.html#Waters86","939":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim8.html#Schank87","940":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters2.html#EatonDGGIKLRRSW15","941":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim38.html#Goel17","942":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim33.html#Leake12","943":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim20.html#Nau99","944":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais3.html#Snell89","945":"https:\/\/doi.org\/10.1145%2F3397482.3450721","946":"http:\/\/arxiv.org\/abs\/1909.09577","947":"http:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/2272","948":"","949":"https:\/\/dspace.mit.edu\/handle\/1721.1\/6259","950":"https:\/\/psyarxiv.com\/4cbuv","951":"","952":"","953":"","954":"https:\/\/pubs.wi-kassel.de\/wp-content\/uploads\/2023\/01\/JML_907.pdf","955":"https:\/\/doi.org\/10.1145%2F3377325.3377498","956":"","957":"","958":"","959":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim15.html#Hamscher94","960":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim30.html#Lieberman09","961":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim11.html#HendlerTD90","962":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#HughesH18a","963":"https:\/\/doi.org\/10.1145%2F3397481.3450681","964":"","965":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Su18","966":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim20.html#Weld99","967":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim18.html#Bringsjord97","968":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais14.html#Baggi00","969":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Williams17","970":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Neller17a","971":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais1.html#Smith87","972":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim36.html#RenzGGZ15","973":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Garijo17","974":"http:\/\/www.cs.washington.edu\/homes\/weld\/papers\/pi2.pdf","975":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais20.html#Vesna06a","976":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim27.html#Mackworth06","977":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim32.html#Chun11","978":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim34.html#WoolfLCK13","979":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim39.html#McGregorB18","980":"","981":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim7.html#MarshallOLK86","982":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim15.html#Fisher94","983":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim25.html#KoenigLLF04","984":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#HughesH18","985":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim39.html#GundersenGA18","986":"https:\/\/doi.org\/10.1145%2F3503252.3531311","987":"http:\/\/search.ebscohost.com\/login.aspx?direct=true&db=aph&AN=18144471&site=ehost-live","988":"","989":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai145.html#Furbach03","990":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai175.html#DomshlakHKP11","991":"","992":"","993":"http:\/\/dx.doi.org\/10.1007\/s13347-022-00546-y","994":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim40.html#SofgeLM19","995":"https:\/\/doi.org\/10.1145%2F3377325.3380623","996":"http:\/\/dx.doi.org\/10.2139\/ssrn.4391243","997":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai169.html#Anderson05","998":"http:\/\/arxiv.org\/abs\/2010.00711","999":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59715\/deep-sea-mining-environment-economic-and-hindu-technological-perspectives-with-ai-chatbots-analysis\/manish-verma","1000":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#McGovernE18a","1001":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters1.html#Wagstaff15a","1002":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom1.html#Kobsa88","1003":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais4.html#Furukawa90","1004":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#Zhou19","1005":"","1006":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais33.html#Yajnik18","1007":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais4.html#SatofukaN90","1008":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom28.html#SierraT15","1009":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters1.html#Wagstaff14a","1010":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#McGovern18","1011":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim36.html#FordHGA15","1012":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim18.html#KitanoAKNOM97","1013":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim15.html#Hoffman94","1014":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim39.html#Goel18","1015":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim33.html#EckrothDSB12","1016":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim34.html#TogeliusSKY13","1017":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais2.html#Leith88","1018":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom2.html#Hirschberg89","1019":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#McGovernL19b","1020":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#Sturtevant18","1021":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters2.html#Neller16","1022":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim8.html#Sridharan87","1023":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#McGovernL19","1024":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim37.html#Ventura16","1025":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim21.html#McDermott00","1026":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#McGovern18a","1027":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim26.html#Leake05","1028":"http:\/\/linkinghub.elsevier.com\/retrieve\/pii\/S0004370202003624","1029":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom6.html#SandbergB93","1030":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#McGovernLK19","1031":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#McGovernL19a","1032":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim26.html#McCarthy05","1033":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim38.html#Goel17a","1034":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim29.html#Jacobstein08","1035":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim26.html#Mackworth05","1036":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim39.html#Hendler18","1037":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#McGovernL18","1038":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters1.html#Wagstaff15","1039":"https:\/\/simplified.com\/ai-writer\/long-form-ai-writer","1040":"","1041":"","1042":"http:\/\/www.nature.com\/doifinder\/10.1038\/529445a","1043":"https:\/\/www.preprints.org\/manuscript\/202202.0109\/v1","1044":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai171.html#McCarthy07","1045":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai170.html#Schank06","1046":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai47.html#Kirsh91","1047":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#LubarsT19","1048":"http:\/\/www.aaai.org\/aitopics\/assets\/PDF\/AIMag14-01-002.pdf","1049":"http:\/\/dblp.uni-trier.de\/db\/conf\/icai\/icai2003-2.html#Ferreira03","1050":"","1051":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai132.html#BouzyC01","1052":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai174.html#Ying10","1053":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai103.html#Castelfranchi98","1054":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai150.html#RisslandAL03","1055":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai41.html#Shen90","1056":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais15.html#McEwanE01","1057":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim8.html#Stone87a","1058":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim39.html#BaiXBBRBSDGG18","1059":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom4.html#Sandberg91","1060":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim26.html#Grosz05","1061":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim10.html#FriesenG89","1062":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim19.html#Allen98","1063":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim38.html#SmithE17","1064":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim27.html#BlankKMY06","1065":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#Neumann18a","1066":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Medsker17","1067":"http:\/\/dblp.uni-trier.de\/db\/conf\/icai\/icai2006-2.html#Peng06","1068":"","1069":"","1070":"http:\/\/doi.acm.org\/10.1145\/1540438.1540453","1071":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2019.html#ManicaOB19","1072":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2019.html#GuidottiMP19","1073":"https:\/\/www.ijtsrd.com\/computer-science\/other\/31310\/modern-workspacebased-policy-management-with-automated-keyword-extraction-and-ai-based-records-management-using-azure-cognitive-services\/poornima-s","1074":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#Neller18","1075":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters2.html#McGovernE16","1076":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#Neumann19","1077":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais34.html#Hasse19a","1078":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim40.html#TouretzkyGBMS19","1079":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim40.html#Reed19","1080":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim33.html#Nishida12","1081":"http:\/\/hampshire.edu\/lspector\/pubs\/gp-aaai.pdf","1082":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim7.html#Fisher86","1083":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#Neumann19a","1084":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters4.html#Neumann18","1085":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Hughes17","1086":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#McGovernE17a","1087":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#Neller17b","1088":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais21.html#Cooley07","1089":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais35.html#Maclure20","1090":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim14.html#Hedberg93a","1091":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim18.html#Yeap97","1092":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim23.html#Alonso02","1093":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim17.html#WilliamsN96","1094":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais35.html#Armand20","1095":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais28.html#Narayanan13","1096":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#McGovernE17b","1097":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#Eckroth19","1098":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim30.html#PetrelliDL09","1099":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim34.html#Leake13","1100":"https:\/\/learning.oreilly.com\/library\/view\/architects-of-intelligence\/9781789131512\/","1101":"http:\/\/carl.cs.indiana.edu\/fil\/Papers\/AI-mag-preprint.pdf","1102":"","1103":"http:\/\/arxiv.org\/abs\/2001.06545","1104":"https:\/\/www.cs.cmu.edu\/Groups\/AI\/lang\/scheme\/oop\/yasos\/swob.txt","1105":"http:\/\/www.aaai.org\/ojs\/index.php\/aimagazine\/article\/view\/1370","1106":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim3.html#Bates82","1107":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim34.html#Leake13a","1108":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim33.html#Khemani12","1109":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#FiorettoY18","1110":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#McGovernE17","1111":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom5.html#SandbergB92c","1112":"http:\/\/pubs.wi-kassel.de\/wp-content\/uploads\/2022\/05\/JML_871.pdf","1113":"http:\/\/citeseer.ist.psu.edu\/208880","1114":"https:\/\/doi.org\/10.1145%2F3565472.3592959","1115":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai170.html#McDermott06","1116":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai155.html#SerafiniB04","1117":"http:\/\/dblp.uni-trier.de\/db\/journals\/ai\/ai23.html#RitchieH84","1118":"http:\/\/dblp.uni-trier.de\/db\/conf\/ptai\/ptai2017.html#Maruyama17a","1119":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/33267\/ai-therapist-\u2013-emotion-detection-using-facial-detection-and-recognition-and-showing-content-according-to-emotions\/sanket-godbole","1120":"","1121":"http:\/\/www.sciencedirect.com\/science\/article\/B6TYF-47X2B1P-4M\/2\/e9e59660a64a2ae850defc3e81a15545","1122":"http:\/\/dblp.uni-trier.de\/db\/conf\/ptai\/ptai2013.html#Mainzer13","1123":"https:\/\/doi.org\/10.1145%2F3313831.3376590","1124":"https:\/\/link.springer.com\/article\/10.1007%2Fs11023-018-9482-5","1125":"http:\/\/dblp.uni-trier.de\/db\/series\/sapere\/sapere5.html#Freed13","1126":"","1127":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#EatonM17","1128":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim19.html#Whitehall98","1129":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim34.html#QuintanaAMRV13","1130":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim26.html#JarvisLM05","1131":"http:\/\/dblp.uni-trier.de\/db\/conf\/icai\/icai2007-2.html#BovikSS07","1132":"http:\/\/dblp.uni-trier.de\/db\/series\/sapere\/sapere5.html#Davenport13","1133":"https:\/\/pubs.wi-kassel.de\/wp-content\/uploads\/2023\/12\/JML_956.pdf","1134":"","1135":"","1136":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim4.html#Bundy83","1137":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim4.html#Schank83","1138":"http:\/\/www.cs.cmu.edu\/\\~{}tom\/pubs\/AImagazine-7-2005.html","1139":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters2.html#EatonM16","1140":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters2.html#EatonM15","1141":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#EatonM18","1142":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom27.html#Sebag14","1143":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim38.html#Eaton17","1144":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim19.html#SmithNT98","1145":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim34.html#BuchananES13","1146":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais35.html#DeCanio20","1147":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais6.html#WittenMMH92","1148":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim38.html#ChesaniMM17","1149":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim37.html#Forbus16","1150":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim23.html#MantarasA02","1151":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim27.html#SchlenoffAMBMB06","1152":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters2.html#EatonW15","1153":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#MaK17","1154":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim8.html#Partridge87","1155":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim14.html#Selfridge93","1156":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim32.html#TenenbaumS11","1157":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim19.html#FawcettHPS98","1158":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim10.html#Hill89","1159":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#ChelinKR06","1160":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#Jung10","1161":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Moulton19","1162":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#KumarSGPJ16","1163":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#EtemadZSSMM20","1164":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#QianGC15","1165":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#GoncalvesM98","1166":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#HerasMM12","1167":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#AbdelsalamCCEA11","1168":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#ZhouE01","1169":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#Hashemi01","1170":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#BergenBC01","1171":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#MacInnesBU01","1172":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#Japkowicz01","1173":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#LiuBCYF16","1174":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#XiangCL0H20","1175":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#ParmentierC20","1176":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#AskiaM20","1177":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#BelacelDI20","1178":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#BhallaS020","1179":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#ZhangWG20","1180":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#AryalC20","1181":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#CastroSYV20","1182":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#NajarB20","1183":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#YunBL20","1184":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#AnC98","1185":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#Knudsen98","1186":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#StaabH98","1187":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#Hussain09","1188":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#AntonO09","1189":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#MellouliMM03","1190":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#Baltes98","1191":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#SongS09","1192":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#BrownSB98","1193":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#MineauLB98","1194":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#LingHZ03","1195":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#SheG09","1196":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#GuWZ09","1197":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#Taylor03","1198":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#JiaS03","1199":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#LashkiaA03","1200":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#CockburnK12","1201":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#Zhu12","1202":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#Tan12","1203":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#AlhossainiB12","1204":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#SilverT12","1205":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#MuiseMBH12","1206":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#IslamMK12","1207":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#XiangruiC03","1208":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#WuZK03","1209":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#LingrasYW03","1210":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#KennedyKIS12","1211":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#XiangH12","1212":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#WenCBK12","1213":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#Mansour12","1214":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#HuangPASC03","1215":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#ZhouWRW12","1216":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#LuS03","1217":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#CostaL03","1218":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#ShiMZ03","1219":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#GaudetteJ11","1220":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#Elazmeh03","1221":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#JanzenX03","1222":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#WangSLZ08","1223":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#CabreraC08","1224":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#AfzalMF11","1225":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#AmraniRKM05","1226":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#Larue11","1227":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#DoLL11","1228":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#ConnorT11","1229":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#HigginsWA11","1230":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#SilvestreP05","1231":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#XiangH11","1232":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#KennedyS11","1233":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#JoubarneI11","1234":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#ShafieiM08","1235":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#LiJSU08","1236":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#MokhtariNLN11","1237":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#SuT11","1238":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#KimLPY05","1239":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#LiJB05","1240":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#KakhkiKB18","1241":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#AbdullahF05","1242":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#XiangB18","1243":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#ChenP18","1244":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#DaronkolaeiHC18","1245":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#XuB18","1246":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#PlamondonK02","1247":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#SharifiVD02","1248":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#BaikCB05","1249":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#WangH05","1250":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#BrebanV02","1251":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#Taher18","1252":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#SilverM02","1253":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#XiangY02","1254":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#WongLW02","1255":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#GrewalWN02","1256":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#HorschHG02","1257":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#FedorukD02","1258":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#ProcterLH16","1259":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#JakubinaL18","1260":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#AlslaityT18","1261":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#GottiL16","1262":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#JungHMSW16","1263":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#IcarteKVM18","1264":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#TanT18","1265":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#Salle02","1266":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#Dirks18","1267":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#DewanQLWK16","1268":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#ShahryariH16","1269":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#AlmeidaJM16","1270":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#Heinemann07","1271":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#BouchardGB06","1272":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#AlsaeedanM16","1273":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#FaniZBD16","1274":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#HarveyCG06","1275":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#YuP07","1276":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#WachterH07","1277":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#ShanehB06","1278":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#CaineC06","1279":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#IzadiPA06","1280":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#MartinezGS06","1281":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#ChengC06","1282":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#WuH07","1283":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#VincentPGA07","1284":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#BenferhatS07","1285":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#EkinciA07","1286":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#LiangY06","1287":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#BarkerC00","1288":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#BowesNGC00","1289":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#Li13","1290":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#MoulinKGC00","1291":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#ArmbrustRKB13","1292":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#BabbarSC13","1293":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#BrooksJTG13","1294":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#ButzYM13","1295":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#ZongKB17","1296":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#Shams13","1297":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#LeeB17","1298":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#ZamanOD17","1299":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#LaneRS07","1300":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#Lach13","1301":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#LipczakNKM13","1302":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#BhattacharjeeG13","1303":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#YousefiKS14","1304":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#MorenoJTTM14","1305":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#BigdeliMRM15","1306":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#LabrancheB14","1307":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#SoleimaniK14","1308":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#WeissbockI14","1309":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#MashayekhiG15","1310":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#QiuWW14","1311":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#RhinelanderKL15","1312":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#SilverME15","1313":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#WangS15","1314":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#ShilS17","1315":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#Murray15a","1316":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#AbbasianDJM10","1317":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#GarciaPN17","1318":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#LacerdaFSSC17","1319":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#PatelJ17","1320":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#MonodFL10","1321":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#LiuJY10","1322":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#Luu10","1323":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#HusseinS04","1324":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#JunS04","1325":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#JilineMT10","1326":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#Pollack96","1327":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#SokolovaSN04","1328":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#AbidiC04","1329":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#QiL04","1330":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#ZhangT10","1331":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#Fall96","1332":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#Champaign10","1333":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#Milios10","1334":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#GuY04","1335":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#ZhangZM04","1336":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#FitzgeraldGK04","1337":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#LeiG04","1338":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#ColemanJ04","1339":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#ChenSN04","1340":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#LiuH04","1341":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#LimY04","1342":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#GengH04","1343":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#DilkinaP04","1344":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#Yao04","1345":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#AimeurBG06","1346":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#KimO08","1347":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#CharlierSH19","1348":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#AhmedM19a","1349":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#ZhangRRHRM19","1350":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#JangLCNCH19","1351":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#SkillicornABW19","1352":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#LinYWCD19","1353":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#CostaR19","1354":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#ButzSOM19","1355":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#MenacerLJFMS19","1356":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#KuboT19","1357":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#MannH19","1358":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#AnowarS19","1359":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Ghojogh19","1360":"http:\/\/arxiv.org\/abs\/2301.04655","1361":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais35.html#SekiguchiH20","1362":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#TrewinBMBTGHLM19","1363":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters5.html#GuerzhoyZN19","1364":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim36.html#WuWCKCTOJMG15","1365":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim27.html#GreenwaldAMS06","1366":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim35.html#RobertsonW14","1367":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim38.html#Minton17","1368":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim13.html#RewariAABCGPS92","1369":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim30.html#JamesonSY09","1370":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais21.html#Pancholi07","1371":"http:\/\/dblp.uni-trier.de\/db\/journals\/aimatters\/aimatters3.html#EatonM17a","1372":"http:\/\/dblp.uni-trier.de\/db\/journals\/aicom\/aicom8.html#GuoxingM95","1373":"http:\/\/dblp.uni-trier.de\/db\/journals\/ais\/ais30.html#Srai15","1374":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/52767\/novel-study-on-aibased-chatbot-chatgpt-impacts-on-the-traditional-library-management\/manish-verma","1375":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#AshibaniM19","1376":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#GottiL19","1377":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#TasnimS19","1378":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#PagottoLWG19","1379":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Selvarajah19","1380":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#FoxcroftdA19","1381":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#BaillargeonLM19","1382":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Thomas19","1383":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Arif19","1384":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Mohammed19","1385":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#YangZZZF19","1386":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#GuerzhoyH14","1387":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#SituS00","1388":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#ShikderIH19","1389":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#LanglaisLS01","1390":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#GrantM01","1391":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#MaZM01","1392":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#PantelL01","1393":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#BeachamCSB01","1394":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#NeouchiTF01","1395":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#KhosraviB10","1396":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#PiaseckiBMS09","1397":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#BobicevSO15","1398":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#FergusonE11","1399":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#KhawajaF11","1400":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#LuoY11","1401":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#AavaniWTM11","1402":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#LavioletteMS08","1403":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#DavoustFE08","1404":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#SayedVZ08","1405":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#ZamzamiB18","1406":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#PanchapakesanAF18","1407":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#Alslaity18","1408":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#Chali02","1409":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#KennedyM02","1410":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#Drummond18","1411":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#HuangJLY02","1412":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#WongWL02","1413":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#DittimiS18","1414":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#ShakshukiK02","1415":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#KarimiH02","1416":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#FrostB02","1417":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#Yap02","1418":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2002.html#DeLoach02","1419":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#WuH05","1420":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#FelicioAAPPA16","1421":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#BianKM18","1422":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#Tan18","1423":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#RussellH18","1424":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#RahgozarI16","1425":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#SoleimaniK16","1426":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#YangM07","1427":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#Viswanathan07","1428":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#ButzSOG16","1429":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2016.html#JabbarZ16","1430":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#SombattheeraG06","1431":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#GagnonS06","1432":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#ZhaoY06","1433":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#Mellouli07","1434":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#Nathan07","1435":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#XiangZ07","1436":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#TompkinsH06","1437":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#ZhaoYY07","1438":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#JickelsK06","1439":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#PaiementEB06","1440":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#StenzW00","1441":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#YanTM00","1442":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#Holte13","1443":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#KubonPT00","1444":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#HanC00","1445":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#LallementF00","1446":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#McDonaldTMPT00","1447":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#SmithCP17","1448":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#BobicevS17","1449":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#SouzaM13","1450":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#Hunter13","1451":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#JamalS13","1452":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#Ahmed17","1453":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#JiKO13","1454":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#LeeBF13","1455":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#Zhang07","1456":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2000.html#GhorbaniB00","1457":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#SokolovaL07","1458":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#ShiMWLGKSP07","1459":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#LiangLB13","1460":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#Sultana13","1461":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#HacidY07","1462":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#KrestelWB07","1463":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#Khan13","1464":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#BergsmaMM14","1465":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#PatelHH15","1466":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#AhmedM17","1467":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#AlliheediM14","1468":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#Doucette15","1469":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#MokhovPD14","1470":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#Iqbal15","1471":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#KremerF15","1472":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#ScaianoILR10","1473":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#Lebrun17","1474":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#Davoodi15","1475":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#FriedmanRS15","1476":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#ChenSHH14","1477":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#Polk15","1478":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#VaezianD17","1479":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#SodjahinKLR17","1480":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#Murray17","1481":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#SokolovaECNRJ10","1482":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#Thornton96","1483":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#PendrithR96","1484":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#RiosM96","1485":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#Thompson10","1486":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#Elkan96","1487":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#BhatiaE96","1488":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#TangM04","1489":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#Koshiba96","1490":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#RazaviIUM10","1491":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#Lee10","1492":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#GorodnichyH10","1493":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#Popowich96","1494":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#DenilT10","1495":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#Trudel10","1496":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#AounallahQM04","1497":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#WegnerA04","1498":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#SchulteFGK10","1499":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai96.html#CharleboisGMBB96","1500":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#AntonN10","1501":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#ZhangZXL10","1502":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#RoyP10","1503":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#KouznetsovJ10","1504":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#WangL04","1505":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#KimPSKS04","1506":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#MulvaneyG04","1507":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#MercerDK04","1508":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#Fleming04","1509":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#ShiF04","1510":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#FloresK04","1511":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#PullanZ04","1512":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#GaudetteJ09","1513":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#Roy09","1514":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#BaghiB09","1515":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#NockE98","1516":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#Acosta09","1517":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#Mackworth09","1518":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#Manfredotti09","1519":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#HuangLG03","1520":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#ZhangH98","1521":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#SebbanL98","1522":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#XiangZL09","1523":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#CaropresoIKK09","1524":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#SokolovaL09","1525":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#Barker98","1526":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#HodgesH98","1527":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#TrabelsiEL09","1528":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#AlamG09","1529":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#Karimi03","1530":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#Mitchell03","1531":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#BidyukD03","1532":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#BaldwinN03","1533":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#KosseimPG03","1534":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#LoS03","1535":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#LaruePN12","1536":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#SchwartzU12","1537":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#Scaiano12","1538":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#CarterI12","1539":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#Coleman03","1540":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#BobicevSJS12","1541":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#SankaranRFKPS12","1542":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#NoorianNFM12","1543":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2012.html#ThompsonH12","1544":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#SoucyM03","1545":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#Lesser03","1546":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#Bergsma05","1547":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#ArmourEEJM05","1548":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#SilverT08","1549":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#SokolovaL08","1550":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#SchumanB08","1551":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2008.html#KobtiRSK08","1552":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#WangH05a","1553":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#PatryL05","1554":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#HuangQWC05","1555":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2005.html#FrenchNR05","1556":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#SunJW11","1557":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#MengistuR11","1558":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#KhordadMR11","1559":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2007.html#NkambouNCF07","1560":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#PlamondonCB06","1561":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#LaumonierC06","1562":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#ChinaeiC11","1563":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#EtemadESBMT20","1564":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#Sajadi14","1565":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#ChartonGO11","1566":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#LiuWJM13","1567":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#GomesPPSCFB03","1568":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#GuLCZM17","1569":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#KavuluruHH13","1570":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#SakhujaC20","1571":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#Tajeddin20","1572":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#SehraABS20","1573":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#JahanshahiCB20","1574":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#AgarwalCGT20","1575":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#RoherX20","1576":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#JassasM20","1577":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#CharlierM20","1578":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#DavidsonW20","1579":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#Patel20","1580":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#MahdaviADDG20","1581":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#Yafoz20","1582":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#HosseiniB20","1583":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#AnisimovskiySTK20","1584":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#ChenHSS17","1585":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#ZhouYL10","1586":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaaiss\/tocais2019.html#Boltuc19","1587":"http:\/\/dblp.uni-trier.de\/db\/journals\/aim\/aim9.html#Bonasso88","1588":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#BelacelA11","1589":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#GhojoghK020","1590":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#MohammadiNMQCKM20","1591":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#KassaniKWSD20","1592":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#AghdamBLPW20","1593":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#IlicGC20","1594":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#JhaPMM20","1595":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#DrownVR20","1596":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#BellingerC0T20","1597":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#Han20","1598":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#SumbaB20","1599":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#BerreL20","1600":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#YeLZ19","1601":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2014.html#WangLJM14","1602":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#TavallaeeLBG10","1603":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#JebaliBE10","1604":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2018.html#SoleimaniM18","1605":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#KingO01","1606":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#PhamN01","1607":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#SilaghiSF01","1608":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#NadeauT01","1609":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#KennedyM01","1610":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#OsorioN01","1611":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#Holte01","1612":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#BartDS01","1613":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2001.html#PriceB01","1614":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#GonzalezTCR10","1615":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2015.html#BindewaldPM15","1616":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2004.html#HavensD04","1617":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2011.html#SotoSVM11","1618":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2010.html#JungC10","1619":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2017.html#IslamACDWH17","1620":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#LaishramP19","1621":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Mahfuz19","1622":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2020.html#GhazizadehGM20","1623":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2006.html#DesharnaisLMZ06","1624":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Jain19","1625":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#SmithW19","1626":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#AhmedM19","1627":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#SolimanLB19","1628":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#MaupomeQM19","1629":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#EzemeLMA19","1630":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#MokhtariMB19","1631":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Ghojogh019","1632":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#DenisF19","1633":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2019.html#Molokwu19","1634":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2013.html#BagheriGE13","1635":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#DemkoP09","1636":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#KhosraviC09","1637":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#KouznetsovMIRFSSO09","1638":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#PerezR98","1639":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#EpsteinS98","1640":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#LiuCZ98","1641":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#LingZ98","1642":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#BanksBM98","1643":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#WieseG98","1644":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#ScaianoI09","1645":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#TangG03","1646":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#McGarity98","1647":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#ShinC98","1648":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2003.html#FinkJH03","1649":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#JashkiMBG09","1650":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#BrodaPS09","1651":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai2009.html#YahiB09","1652":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#MorrisonO98","1653":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#BergerSB98","1654":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#Draghici98","1655":"http:\/\/dblp.uni-trier.de\/db\/conf\/ai\/ai1998.html#RiosM98","1656":"https:\/\/jaai.de\/machine-deep-learning-529\/","1657":"https:\/\/dev.bg\/\u0441\u044a\u0431\u0438\u0442\u0438\u0435\/machine-learning-semantic-integration-is-what-you-do-before-the-deep-learning\/","1658":"https:\/\/www.cambridge.org\/au\/universitypress\/subjects\/computer-science\/pattern-recognition-and-machine-learning\/mathematics-machine-learning?format=PB","1659":"https:\/\/www.worldcat.org\/title\/machine-learning-a-probabilistic-perspective\/oclc\/781277861?referer=br&ht=edition","1660":"http:\/\/www.worldcat.org\/title\/data-mining-practical-machine-learning-tools-and-techniques\/oclc\/972378340&referer=brief_results","1661":"","1662":"https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/46392\/machine-learning-in-material-characterization\/matthew-n-o-sadiku","1663":"http:\/\/www.amazon.com\/Machine-Learning-Tom-M-Mitchell\/dp\/0070428077","1664":"","1665":"http:\/\/jmlr.org\/papers\/v18\/17-468.html","1666":"https:\/\/jaai.de\/machine-deep-learning-529\/","1667":"https:\/\/www.safaribooksonline.com\/library\/view\/machine-learning-hands-on\/9781118889497\/","1668":"http:\/\/arxiv.org\/abs\/2203.08056","1669":"http:\/\/arxiv.org\/abs\/1912.09789","1670":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/39891\/cultivation-of-crops-using-machine-learning-and-deep-learning\/ms-a-benazir-begum","1671":"http:\/\/ebiquity.umbc.edu\/paper\/html\/id\/296\/Detecting-Spam-Blogs-A-Machine-Learning-Approach","1672":"https:\/\/doi.org\/10.1007\/s10994-015-5540-x","1673":"https:\/\/www.heise.de\/ratgeber\/scikit-learn-numpy-und-plotly-Einfuehrung-in-Machine-Learning-mit-Python-6156192.html?seite=all","1674":"http:\/\/papers.nips.cc\/paper\/5872-efficient-and-robust-automated-machine-learning","1675":"http:\/\/ki.informatik.uni-wuerzburg.de\/forschung\/publikationen\/lehrstuhl\/Puppe-MML-96\/MML_96.html","1676":"http:\/\/www.amazon.com\/Machine-Learning-Hackers-Drew-Conway\/dp\/1449303714","1677":"https:\/\/www.ijtsrd.com\/computer-science\/other\/20255\/machine-learning-in-medicine-a-primer\/matthew-n-o-sadiku","1678":"http:\/\/arxiv.org\/abs\/2001.07455","1679":"https:\/\/www.ijtsrd.com\/computer-science\/computer-security\/23755\/detecting-phishing-using-machine-learning\/nakkala-srinivas-mudiraj","1680":"http:\/\/dx.doi.org\/10.1023\/A:1007601113994","1681":"http:\/\/arxiv.org\/abs\/2303.15794","1682":"","1683":"https:\/\/www.ijtsrd.com\/computer-science\/computer-network\/38175\/comparative-study-on-machine-learning-algorithms-for-network-intrusion-detection-system\/priya-n","1684":"https:\/\/www.ijtsrd.com\/computer-science\/other\/31148\/fake-news-detection-using-machine-learning\/nikhil-sharma","1685":"https:\/\/www.ijtsrd.comcomputer-science\/data-processing\/41263\/music-genre-classification-using-machine-learning\/seethal-v","1686":"http:\/\/arxiv.org\/abs\/1903.10563","1687":"https:\/\/wjbphs.com\/content\/segmentation-white-blood-cells-machine-learning-algorithms","1688":"http:\/\/arxiv.org\/abs\/2103.12226","1689":"https:\/\/github.com\/josephmisiti\/awesome-machine-learning#javascript-nlp","1690":"http:\/\/arxiv.org\/abs\/1902.06789","1691":"http:\/\/arxiv.org\/abs\/1807.04162","1692":"https:\/\/gjeta.com\/content\/novel-approach-federated-machine-learning-using-raspberry-pi","1693":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/33378\/prediction-of-cervical-cancer-using-machine-learning-and-deep-learning-algorithms\/kayalvizhi-k-r","1694":"https:\/\/www.ijtsrd.comengineering\/computer-engineering\/41198\/sentimental-emotion-analysis-using-python-and-machine-learning\/mohit-chaudhari","1695":"https:\/\/pubmed.ncbi.nlm.nih.gov\/31577910\/","1696":"citeseer.ist.psu.edu\/marlin04collaborative.html","1697":"http:\/\/arxiv.org\/abs\/1502.05767","1698":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/42458\/covid19-prediction-in-india-using-machine-learning\/sarfraj-alam","1699":"","1700":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59873\/machine-learning-approach-to-classify-twitter-hate-speech\/subrata-saha","1701":"https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42344\/image-captioning-generator-using-deep-machine-learning\/sreejith-s-p","1702":"http:\/\/arxiv.org\/abs\/1909.03550","1703":"","1704":"https:\/\/www.ijtsrd.com\/engineering\/agricultural-engineering\/59847\/estimating-evaporation-using-machine-learning-based-ensemble-technique\/r-s-parmar","1705":"http:\/\/arxiv.org\/abs\/1803.02780","1706":"http:\/\/www.mendeley.com\/research\/the-pervasiveness-of-data-mining-asnd-machine-learning\/","1707":"http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/7003\/analyzing-titanic-disaster-using-machine-learning-algorithms\/dr-prabha-shreeraj-nair","1708":"","1709":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23065\/machine-learning-approach-for-employee-attrition-analysis\/dr-r-s-kamath","1710":"http:\/\/arxiv.org\/abs\/1911.10500","1711":"https:\/\/www.ijtsrd.comcomputer-science\/data-processing\/42372\/amazon-product-review-sentiment-analysis-with-machine-learning\/ravi-kumar-singh","1712":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/33143\/gold-price-prediction-using-machine-learning\/dr-abhay-kumar-agarwal","1713":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/25217\/human-emotion-recognition-using-machine-learning\/prof-mrs-dhanamma-jagli","1714":"https:\/\/www.ijtsrd.com\/computer-science\/other\/38358\/heart-disease-prediction-using-machine-learning-algorithm\/ravi-kumar-singh","1715":"https:\/\/www.ijtsrd.com\/computer-science\/computer-security\/33345\/detection-of-fake-news-using-machine-learning\/pujitha-e","1716":"https:\/\/www.ijtsrd.com\/computer-science\/other\/49868\/stock-market-prediction-using-machine-learning\/subham-kumar-gupta","1717":"https:\/\/www.ijtsrd.com.com\/engineering\/computer-engineering\/57545\/broadcasting-forensics-using-machine-learning-approaches\/amit-kapoor","1718":"http:\/\/arxiv.org\/abs\/2007.05408","1719":"https:\/\/www.worldcat.org\/title\/first-course-in-machine-learning\/oclc\/653086269?referer=br&ht=edition","1720":"https:\/\/www.ijtsrd.com\/computer-science\/computer-architecture\/33659\/lung-cancer-detection-using-machine-learning\/harpreet-singh","1721":"https:\/\/gjeta.com\/content\/forecasting-onion-armyworm-using-tree-based-machine-learning-models","1722":"http:\/\/arxiv.org\/abs\/1705.07538","1723":"http:\/\/arxiv.org\/abs\/1906.05433","1724":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2589004220308488","1725":"http:\/\/arxiv.org\/abs\/2208.02030","1726":"https:\/\/www.ijert.org\/a-survey-on-prediction-techniques-of-heart-disease-using-machine-learning","1727":"http:\/\/doi.acm.org\/10.1145\/2347736.2347755","1728":"http:\/\/arxiv.org\/abs\/1804.10068","1729":"https:\/\/www.ijtsrd.com\/computer-science\/other\/60102\/credit-card-fraud-detection-using-hybrid-machine-learning-algorithm\/tripti-gautam","1730":"https:\/\/doi.org\/10.1561%2F2200000058","1731":"http:\/\/doi.acm.org\/10.1145\/2717764.2717786","1732":"https:\/\/www.ijtsrd.com\/engineering\/other\/23936\/melanoma-skin-cancer-detection-using-image-processing-and-machine-learning\/vijayalakshmi-m-m","1733":"http:\/\/arxiv.org\/abs\/1904.07248","1734":"https:\/\/www.ijtsrd.com\/computer-science\/data-miining\/22961\/comparative-study-of-machine-learning-algorithms-for-rainfall-prediction\/mylapalle-yeshwanth","1735":"https:\/\/machinelearningmastery.com\/basic-concepts-in-machine-learning\/","1736":"http:\/\/arxiv.org\/abs\/1706.08605","1737":"http:\/\/arxiv.org\/abs\/1805.01772","1738":"http:\/\/arxiv.org\/abs\/1807.03341","1739":"https:\/\/www.ijtsrd.com\/computer-science\/other\/33233\/machine-learning-in-the-field-of-optical-character-recognition-ocr\/mr-rishabh-dubey","1740":"http:\/\/arxiv.org\/abs\/2303.06752","1741":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/39856\/face-recognition-based-attendance-system-using-machine-learning\/benazir-begum-a","1742":"","1743":"http:\/\/www.ijtsrd.com\/computer-science\/other\/11357\/anticipation-of-forged-video-evidence-using-machine-learning\/g-abinaya","1744":"https:\/\/www.ijtsrd.com\/other-scientific-research-area\/other\/39890\/survey-on-key-phrase-extraction-using-machine-learning-approaches\/preeti-sondhi","1745":"https:\/\/papers.nips.cc\/paper\/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning","1746":"http:\/\/arxiv.org\/abs\/1803.08823","1747":"http:\/\/arxiv.org\/abs\/1512.01274","1748":"https:\/\/www.ijtsrd.com\/computer-science\/data-miining\/33414\/user-personality-prediction-on-facebook-social-media-using-machine-learning\/poonam-l-patil","1749":"http:\/\/arxiv.org\/abs\/1606.04838","1750":"http:\/\/arxiv.org\/abs\/1711.10781","1751":"","1752":"","1753":"http:\/\/portal.acm.org\/citation.cfm?id=1220175.1220270","1754":"https:\/\/www.ijtsrd.com\/computer-science\/other\/30688\/a-study-on-credit-card-fraud-detection-using-machine-learning\/ajayi-kemi-patience","1755":"http:\/\/doi.acm.org\/10.1145\/505282.505283","1756":"http:\/\/arxiv.org\/abs\/1702.08608","1757":"http:\/\/arxiv.org\/abs\/1706.00868","1758":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.17.9562","1759":"citeseer.nj.nec.com\/dietterich97machine.html","1760":"http:\/\/portal.acm.org\/citation.cfm?id=1126054","1761":"http:\/\/doi.acm.org\/10.1145\/505282.505283","1762":"http:\/\/arxiv.org\/abs\/2205.03447","1763":"http:\/\/www.amazon.fr\/exec\/obidos\/ASIN\/0262012111\/citeulike04-21","1764":"https:\/\/www.ijtsrd.com\/computer-science\/data-processing\/33261\/a-deep-analysis-on-prevailing-spam-mail-filteration-machine-learning-approaches\/anu","1765":"http:\/\/arxiv.org\/abs\/1801.04900","1766":"https:\/\/www.ijtsrd.com\/engineering\/information-technology\/30739\/enabling-air-pollution-prediction-through-iot-and-machine-learning\/suraj-kapse","1767":"https:\/\/www.ijtsrd.com\/computer-science\/embedded-system\/42441\/a-comparative-study-on-mushroom-classification-using-supervised-machine-learning-algorithms\/kanchi-tank","1768":"http:\/\/arxiv.org\/abs\/1812.11118","1769":"http:\/\/www.cs.cmu.edu\/~tom\/mlbook.html","1770":"http:\/\/arxiv.org\/abs\/1708.05070","1771":"http:\/\/dx.doi.org\/10.1023\/A:1022623814640","1772":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S2001037017300867","1773":"http:\/\/proceedings.mlr.press\/v80\/damaskinos18a.html","1774":"http:\/\/news.mit.edu\/2019\/mit-3q-paul-o-gorman-machine-learning-for-climate-modeling-0213","1775":"http:\/\/arxiv.org\/abs\/1802.07928","1776":"http:\/\/arxiv.org\/abs\/1811.10154","1777":"https:\/\/www.entechin.com\/machine-learning-basics-types-examples\/","1778":"http:\/\/arxiv.org\/abs\/1903.12394","1779":"http:\/\/arxiv.org\/abs\/1903.12394","1780":"http:\/\/arxiv.org\/abs\/2302.09751","1781":"http:\/\/arxiv.org\/abs\/1802.08021","1782":"http:\/\/arxiv.org\/abs\/1905.01330","1783":"http:\/\/arxiv.org\/abs\/1901.02046","1784":"","1785":"http:\/\/arxiv.org\/abs\/1811.12808","1786":"http:\/\/dblp.uni-trier.de\/db\/journals\/ml\/ml73.html#DietterichDGMT08","1787":"https:\/\/www.semanticscholar.org\/paper\/ac9f3c0b58e3bd17214e52b4b14cd55c1dba2234","1788":"http:\/\/arxiv.org\/abs\/2206.13446","1789":"http:\/\/www.ijtsrd.com\/engineering\/electronics-and-communication-engineering\/19024\/intelligent-fall-detection-using-statistical-features-and-machine-learning\/hephzibah-thomas","1790":"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/30364671","1791":"http:\/\/arxiv.org\/abs\/0908.2033","1792":"http:\/\/arxiv.org\/abs\/2103.11251","1793":"http:\/\/arxiv.org\/abs\/2108.02497","1794":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/35774\/detection-of-url-based-phishing-websites-using-machine-learning\/dr-c-umarani","1795":"https:\/\/www.microsoft.com\/en-us\/research\/publication\/pattern-recognition-machine-learning\/","1796":"https:\/\/www.ijtsrd.com\/other-scientific-research-area\/other\/51976\/a-study-on-prediction-of-share-price-by-using-machine-learning-lstm-model\/manuri-raju","1797":"https:\/\/www.ijtsrd.comeconomics\/finance\/42539\/predicting-the-level-of-crowdfunding-outcome-in-africa-a-supervised-machine-learning-approach\/isaac-okyere-paintsil","1798":"http:\/\/arxiv.org\/abs\/1905.04312","1799":"http:\/\/arxiv.org\/abs\/1812.11118","1800":"","1801":"https:\/\/doi.org\/10.1007\/s40745-020-00253-5","1802":"http:\/\/arxiv.org\/abs\/2206.15475","1803":"","1804":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/30765\/comparative-study-of-cyberbullying-detection-using-different-machine-learning-algorithms\/rohini-k-r","1805":"http:\/\/arxiv.org\/abs\/2208.03165","1806":"https:\/\/www.ijtsrd.com\/computer-science\/computer-security\/52440\/credit-cards-frauds-and-cybersecurity-threats-machine-learning-detection-algorithms-as-countermeasures\/obodoeze-fidelis-c","1807":"http:\/\/arxiv.org\/abs\/1705.01523","1808":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0925231217307609","1809":"http:\/\/www.ijtsrd.com\/engineering\/electronics-and-communication-engineering\/14363\/clinical-depression-detection-using-speech-feature-with-machine-learning-approach\/ms-anjum-shaikh","1810":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2016.html#Alexandre16","1811":"http:\/\/arxiv.org\/abs\/1912.08245","1812":"http:\/\/arxiv.org\/abs\/1906.10652","1813":"http:\/\/arxiv.org\/abs\/1611.08699","1814":"http:\/\/www.ijtsrd.com\/engineering\/information-technology\/7082\/a-review-on-machine-learning-techniques-for-neurological-disorders-estimation-by-analyzing-eeg-waves\/vijaykumar-janga","1815":"https:\/\/www.ijtsrd.com\/engineering\/bio-mechanicaland-biomedical-engineering\/47655\/role-of-advanced-machine-learning-techniques-and-deep-learning-approach-based-decision-support-system-for-accurate-diagnosis-of-severe-respiratory-diseases\/patel-smitkumar-hareshbhai","1816":"https:\/\/ieeexplore.ieee.org\/document\/8783030","1817":"http:\/\/arxiv.org\/abs\/2008.04216","1818":"http:\/\/arxiv.org\/abs\/1801.10117","1819":"http:\/\/arxiv.org\/abs\/1902.00465","1820":"http:\/\/rsta.royalsocietypublishing.org\/content\/371\/1984\/20120222","1821":"https:\/\/www.microsoft.com\/en-us\/research\/publication\/speech-emotion-recognition-using-deep-neural-network-and-extreme-learning-machine\/","1822":"https:\/\/www.ijtsrd.com\/computer-science\/other\/38068\/covid19-outbreak-prediction-and-forecasting-in-bangladesh-using-machine-learning-algorithm\/s-m-abdullah-al-shuaeb","1823":"http:\/\/dx.doi.org\/10.1007\/BF00116895","1824":"citeseer.nj.nec.com\/joachims98making.html","1825":"http:\/\/burakkanber.com\/blog\/machine-learning-genetic-algorithms-part-1-javascript\/","1826":"http:\/\/arxiv.org\/abs\/2210.07182","1827":"https:\/\/gjeta.com\/content\/credit-card-fraud-detection-and-classification-deep-learning-and-machine-learning","1828":"http:\/\/arxiv.org\/abs\/1901.04592","1829":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/33395\/automated-license-plate-detection-and-speed-estimation-of-vehicle-using-machine-learning--haar-classifier-algorithm\/p-devi-mahalakshmi","1830":"http:\/\/arxiv.org\/abs\/2207.09959","1831":"https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/33587\/drowsiness-and-alcohol-detection-for-accident-prevention-using-machine-learning\/shruti-chandrakant-zarekar","1832":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23534\/era-of-sociology-news-rumors-news-detection-using-machine-learning\/chandni-jain","1833":"http:\/\/www.amazon.co.uk\/Machine-Learning-Applications-Information-Retrieval\/dp\/0745800459","1834":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2015.html#SculleyHGDPECYC15","1835":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2012.html#SnoekLA12","1836":"http:\/\/arxiv.org\/abs\/2202.09359","1837":"http:\/\/arxiv.org\/abs\/1510.07659","1838":"http:\/\/www.biomedcentral.com\/1471-2105\/11\/292","1839":"http:\/\/arxiv.org\/abs\/2102.09548","1840":"http:\/\/arxiv.org\/abs\/2205.02302","1841":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/HKV2019.html#FeurerKES0H19","1842":"http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/14397\/a-frame-study-on-sentiment-analysis-of-hindi-language-using-machine-learning\/sheetal-sharma","1843":"https:\/\/ieeexplore.ieee.org\/abstract\/document\/7379391","1844":"http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/10815\/e-mail-security-algorithm-to-filter-out-spam-e-mails-using-machine-learning\/r-aswin","1845":"http:\/\/arxiv.org\/abs\/1801.04016","1846":"http:\/\/eprints.pascal-network.org\/archive\/00005701\/","1847":"https:\/\/gjeta.com\/content\/building-attack-detection-system-base-machine-learning","1848":"https:\/\/doi.org\/10.1007\/s13351-021-0185-0","1849":"https:\/\/gmd.copernicus.org\/preprints\/gmd-2022-195\/","1850":"http:\/\/arxiv.org\/abs\/1606.05386.pdf","1851":"http:\/\/www.ics.uci.edu\/$\\sim$mlearn\/MLRepository.html","1852":"http:\/\/dblp.uni-trier.de\/db\/journals\/ml\/ml30.html#KubatHM98","1853":"http:\/\/portal.acm.org\/citation.cfm?id=152181","1854":"http:\/\/portal.acm.org\/citation.cfm?id=1577069.1577096","1855":"https:\/\/doi.org\/10.1007\/s12551-018-0446-z","1856":"http:\/\/arxiv.org\/abs\/2008.04059","1857":"http:\/\/arxiv.org\/abs\/1906.08259","1858":"http:\/\/www.ijtsrd.com\/computer-science\/other\/7048\/analysis-of-machine-learning-and-statistics-tool-box--matlab-r2016-over-novel-benchmark-cervical-cancer-database\/abid-sarwar","1859":"http:\/\/arxiv.org\/abs\/1904.00031","1860":"http:\/\/www.jmlr.org\/papers\/v2\/genton01a.html","1861":"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/28662070","1862":"http:\/\/dx.doi.org\/10.1023\/a:1007369909943","1863":"https:\/\/www.quantamagazine.org\/machine-learning-confronts-the-elephant-in-the-room-20180920\/","1864":"http:\/\/arxiv.org\/abs\/1502.04585","1865":"http:\/\/arxiv.org\/abs\/2106.00467","1866":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S1352231020302703","1867":"http:\/\/arxiv.org\/abs\/1606.05110","1868":"https:\/\/www.srels.org\/index.php\/sjim\/article\/view\/170891","1869":"","1870":"http:\/\/ceur-ws.org\/Vol-2535\/paper_1.pdf","1871":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#GolzKP19","1872":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2007.html#SmolaVL07","1873":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2017.html#KampBMG17","1874":"http:\/\/hdl.handle.net\/1721.1\/36160","1875":"https:\/\/appliedmldays.org\/","1876":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2016.html#HeXQWYLM16","1877":"https:\/\/www.ijtsrd.com\/computer-science\/other\/33226\/predict-the-covid19-using-machine-learning-model-from-a-symptoms-of-the-body\/mr-umesh-r-maurya","1878":"","1879":"http:\/\/arxiv.org\/abs\/1703.03924","1880":"http:\/\/arxiv.org\/abs\/1712.03141","1881":"http:\/\/arxiv.org\/abs\/1602.03943","1882":"http:\/\/www.informatica.si\/PDF\/31-3\/11_Kotsiantis%20-%20Supervised%20Machine%20Learning%20-%20A%20Review%20of...pdf","1883":"http:\/\/papers.nips.cc\/paper\/6617-machine-learning-with-adversaries-byzantine-tolerant-gradient-descent.pdf","1884":"","1885":"https:\/\/arxiv.org\/abs\/2010.09046","1886":"https:\/\/github.com\/cschell\/Motion-Learning-Toolbox","1887":"https:\/\/doi.org\/10.1007\/s10994-018-5742-0","1888":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/HKV2019.html#OlsonM19","1889":"citeseer.ist.psu.edu\/soon01machine.html","1890":"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/28166733","1891":"http:\/\/arxiv.org\/abs\/2211.06393","1892":"http:\/\/arxiv.org\/abs\/1701.01293","1893":"http:\/\/www.gaussianprocess.org\/gpml\/chapters\/","1894":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#DaiX0Z19","1895":"https:\/\/doi.org\/10.1007\/978-3-031-13818-8_12","1896":"http:\/\/arxiv.org\/abs\/1803.07445","1897":"\/brokenurl#Indias economy is mostly based on agricultural yield growth and linked agro industry products, as it is an agricultural country. Rainwater, which is often unpredictable in India, has a significant impact on agriculture. Agriculture growth is also influenced by a variety of soil parameters, such as nitrogen, phosphorus, and potassium, as well as crop rotation, soil moisture, and surface temperature, as well as climatic factors such as temperature and rainfall. India is quickly advancing in terms of technical advancement. As a result, technology will benefit agriculture by increasing crop productivity, resulting in higher yields for farmers. The suggested project provides a solution for storing temperature, rainfall, and soil characteristics in order to determine which crops are suited for cultivation in a given area. This paper describes a system, implemented as an android application, that employs data analytics techniques to predict the most profitable crop based on current weather and soil conditions. The suggested system will combine data from the repository and the meteorological department to make a prediction of the most suited crops based on current environmental conditions using a machine learning method called Multiple Linear Regression. This gives a farmer a wide range of crops to choose from. As a result, the project creates a system that integrates data from diverse sources, performs data analytics, and conducts predictive analysis in order to improve crop production productivity and boost farmer profit margins over time. Machine learning, crop prediction, and yield estimation are some of the terms used in this paper. Manju D C | Murugan R \"Crop Prediction System using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-3 , April 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49444.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/data-processing\/49444\/crop-prediction-system-using-machine-learning\/manju-d-c","1898":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#AddankiVGMA19","1899":"","1900":"http:\/\/arxiv.org\/abs\/2005.04176","1901":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2772632022000307","1902":"","1903":"","1904":"http:\/\/arxiv.org\/abs\/2110.02673","1905":"","1906":"","1907":"http:\/\/dblp.uni-trier.de\/db\/journals\/ml\/ml50.html#AndrieuFDJ03","1908":"http:\/\/arxiv.org\/abs\/1906.06357","1909":"http:\/\/arxiv.org\/abs\/1906.04137","1910":"https:\/\/doi.org\/10.1038\/s41591-019-0548-6","1911":"https:\/\/www.semanticscholar.org\/paper\/04eab1a82321aa8c4a1e6f66367ce333183f25bc","1912":"https:\/\/doi.org\/10.1453\/1614-0885-1-2023-15452","1913":"http:\/\/arxiv.org\/abs\/2010.00619","1914":"http:\/\/arxiv.org\/abs\/1912.11580","1915":"http:\/\/dx.doi.org\/10.1007\/BF00117105","1916":"http:\/\/cs229.stanford.edu\/projects2010.html","1917":"","1918":"http:\/\/www.springerlink.com\/content\/av8l8hjl6yc2ya3m\n","1919":"","1920":"","1921":"https:\/\/doi.org\/10.1145%2F3377325.3377480","1922":"https:\/\/doi.org\/10.1145%2F3467477","1923":"https:\/\/doi.org\/10.1023\/A:1008942012299","1924":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2017.html#KarHM17","1925":"","1926":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2019.html#BauckhageSH19","1927":"http:\/\/dx.doi.org\/10.1145\/2347736.2347755","1928":"http:\/\/dblp.uni-trier.de\/db\/conf\/ndss\/ndss2015.html#BostPTG15","1929":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#TurchettaB019","1930":"http:\/\/portal.acm.org\/citation.cfm?id=1265328.1265346&coll=GUIDE&dl=GUIDE&CFID=93450342&CFTOKEN=96844240","1931":"http:\/\/arxiv.org\/abs\/2305.01582","1932":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2000.html#SchodlE00","1933":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2015.html#FeurerKESBH15","1934":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2018.html#VankadaraL18","1935":"http:\/\/doi.acm.org\/10.1145\/2063576.2063756","1936":"http:\/\/ieeexplore.ieee.org\/xpl\/articleDetails.jsp?arnumber=4456863","1937":"http:\/\/arxiv.org\/abs\/2109.00024","1938":"http:\/\/arxiv.org\/abs\/1908.05557","1939":"https:\/\/www.aaai.org\/Library\/Workshops\/2006\/ws06-06-002.php","1940":"http:\/\/dl.acm.org\/citation.cfm?id=1929285.1929294","1941":"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/30699872","1942":"http:\/\/arxiv.org\/abs\/1707.04131","1943":"http:\/\/dblp.uni-trier.de\/db\/journals\/jmlr\/jmlr20.html#ProbstBB19","1944":"http:\/\/arxiv.org\/abs\/2304.05592","1945":"","1946":"","1947":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#AsgariKT20","1948":"http:\/\/dx.doi.org\/10.1016\/j.knosys.2005.10.011","1949":"http:\/\/arxiv.org\/abs\/1906.03351","1950":"http:\/\/arxiv.org\/abs\/2001.00089","1951":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#LiuMG20","1952":"http:\/\/www.sciencedirect.com\/science\/book\/9780128021217","1953":"","1954":"http:\/\/arxiv.org\/abs\/2107.14330","1955":"http:\/\/doi.acm.org\/10.1145\/3097983.3098021","1956":"http:\/\/www.amazon.ca\/exec\/obidos\/redirect?tag=citeulike09-20\\&amp;path=ASIN\/0120884070","1957":"","1958":"https:\/\/journals.bohrpub.com\/index.php\/bijcs\/article\/view\/80","1959":"http:\/\/arxiv.org\/abs\/1808.00479","1960":"http:\/\/www.sicherheitsforschung-magdeburg.de\/uploads\/journal\/MJS_060_Chebbi_MachineLearning.pdf","1961":"http:\/\/ieeexplore.ieee.org\/xpl\/login.jsp?tp=&arnumber=4674368&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4674368","1962":"https:\/\/doi.org\/10.1145\/3464385.3467475","1963":"https:\/\/www.boeckler.de\/de\/faust-detail.htm?sync_id=9096","1964":"","1965":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S2095809916309468","1966":"https:\/\/www.science.org\/doi\/abs\/10.1126\/science.aaa8415","1967":"http:\/\/web4.cs.ucl.ac.uk\/staff\/D.Barber\/textbook\/090310.pdf","1968":"http:\/\/www.amazon.com\/Gaussian-Processes-Learning-Adaptive-Computation\/dp\/026218253X","1969":"","1970":"","1971":"http:\/\/arxiv.org\/abs\/2007.09121","1972":"http:\/\/arxiv.org\/abs\/1708.07747","1973":"http:\/\/arxiv.org\/abs\/2003.06740","1974":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#HudsonM19","1975":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#LaueMG19","1976":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2004.html#ChapelleH04","1977":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2000.html#CauwenberghsP00","1978":"http:\/\/arxiv.org\/abs\/1309.0238","1979":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2018.html#FusiSE18","1980":"https:\/\/arxiv.org\/abs\/1807.05351","1981":"http:\/\/dx.doi.org\/10.1145\/2660252.2661744","1982":"http:\/\/dblp.uni-trier.de\/db\/conf\/ambn\/ambn2017.html#Zhang17","1983":"http:\/\/arxiv.org\/abs\/1812.06726","1984":"https:\/\/doi.org\/10.1145%2F3565472.3595612","1985":"http:\/\/www.nature.com\/articles\/d41586-019-00012-4","1986":"https:\/\/www.semanticscholar.org\/paper\/92865c91721a1c274b521f0027d2944fb82f08dd","1987":"","1988":"","1989":"https:\/\/doi.org\/10.1007\/s12065-020-00536-z","1990":"https:\/\/www.ijtsrd.com\/computer-science\/other\/52356\/development-of-a-hybrid-dynamic-expert-system-for-the-diagnosis-of-peripheral-diabetes-and-remedies-using-a-rulebased-machine-learning-technique\/omeye-emmanuel-c","1991":"http:\/\/arxiv.org\/abs\/1903.11166","1992":"http:\/\/arxiv.org\/abs\/1607.02531","1993":"http:\/\/dx.doi.org\/10.1145\/3593013.3593979","1994":"https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5442348\/","1995":"https:\/\/doi.org\/10.1145%2F3459664","1996":"https:\/\/papers.nips.cc\/paper_files\/paper\/2015\/hash\/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html","1997":"","1998":"http:\/\/portal.acm.org\/citation.cfm?id=1015382\\&dl=GUIDE\\&coll=GUIDE\\&CFID=69344422\\&CFTOKEN=94303924","1999":"http:\/\/arxiv.org\/abs\/2010.12294","2000":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#RaubitzekN20","2001":"http:\/\/arxiv.org\/abs\/1708.05070","2002":"https:\/\/journals.researchparks.org\/index.php\/IJOT\/article\/view\/4447\/4167","2003":"http:\/\/www.research.ibm.com\/journal\/rd\/021\/ibmrd0201B.pdf","2004":"https:\/\/ieeexplore.ieee.org\/document\/8379901","2005":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#ZhaoZ19","2006":"","2007":"http:\/\/arxiv.org\/abs\/2111.04612","2008":"http:\/\/arxiv.org\/abs\/1506.04214","2009":"http:\/\/dx.plos.org\/10.1371\/journal.pone.0202344 http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/30169498 http:\/\/www.pubmedcentral.nih.gov\/articlerender.fcgi?artid=PMC6118376","2010":"citeseer.ist.psu.edu\/wurst02integrating.html","2011":"","2012":"https:\/\/arxiv.org\/abs\/1705.08079","2013":"http:\/\/arxiv.org\/abs\/1708.07827","2014":"https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5298591\/","2015":"http:\/\/arxiv.org\/abs\/1804.06223","2016":"","2017":"http:\/\/arxiv.org\/abs\/1707.04593","2018":"https:\/\/cajmns.centralasianstudies.org\/index.php\/CAJMNS\/article\/view\/1686\/1796","2019":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#RoelofsSRFHMS19","2020":"http:\/\/ijacsa.thesai.org\/","2021":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2011.html#KayalaB11","2022":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2017.html#BlanchardMGS17","2023":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2018.html#DunnerPSIARKP18","2024":"https:\/\/arxiv.org\/abs\/2310.13721","2025":"http:\/\/www.cs.cornell.edu\/home\/llee\/papers\/sentiment.pdf","2026":"http:\/\/www.worldcat.org\/search?qt=worldcat_org_all&q=9781107057135","2027":"http:\/\/dblp.uni-trier.de\/db\/series\/isrl\/isrl158.html#VirvouATJ20","2028":"","2029":"http:\/\/arxiv.org\/abs\/1910.03596","2030":"http:\/\/www.jmlr.org\/papers\/volume12\/gashler11a\/gashler11a.pdf","2031":"https:\/\/www.imu-boeckler.de\/download-proxy-for-faust\/download-pdf?url=http%3A%2F%2F217.89.182.78%3A451%2Fabfrage_digi.fau%2Fp_mbf_praxis_2020_33.pdf%3Fprj%3Dhbs-abfrage%26ab_dm%3D1%26ab_zeig%3D9096%26ab_diginr%3D8482","2032":"https:\/\/doi.org\/10.1109%2Faccess.2021.3098688","2033":"http:\/\/www.ellogon.org\/petasis\/bibliography\/Petasis\/Ph.D.Thesis-GeorgiosPetasis.pdf","2034":"http:\/\/dblp.uni-trier.de\/db\/reference\/ml\/ml2010.html#Perlich10","2035":"http:\/\/view.ncbi.nlm.nih.gov\/pubmed\/27092947","2036":"","2037":"","2038":"","2039":"","2040":"http:\/\/arxiv.org\/abs\/1903.07167","2041":"","2042":"","2043":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/ml4h2019.html#DalcaMAFOFCBNB19","2044":"http:\/\/arxiv.org\/abs\/2106.14551","2045":"https:\/\/doi.org\/10.1038\/s42254-021-00314-5","2046":"http:\/\/arxiv.org\/abs\/1810.13135","2047":"http:\/\/arxiv.org\/abs\/1812.05239","2048":"http:\/\/arxiv.org\/abs\/2110.01515","2049":"","2050":"","2051":"","2052":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2016.html#Wittek16","2053":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2016.html#OudeyerLKG16","2054":"https:\/\/www.researchgate.net\/publication\/305143958_MEX_InterfacesAutomating_Machine_Learning_Metadata_Generation","2055":"http:\/\/www.amazon.de\/exec\/obidos\/ASIN\/1558605525","2056":"http:\/\/arxiv.org\/abs\/2106.01590","2057":"http:\/\/arxiv.org\/abs\/1709.04464","2058":"http:\/\/www.sciencedirect.com\/science\/article\/B6V0D-45SRKS8-2\/2\/b191948fb2eff336bae10efe9e3ed820","2059":"","2060":"","2061":"","2062":"http:\/\/arxiv.org\/abs\/1711.05305","2063":"https:\/\/www.cs.tau.ac.il\/~joberant\/teaching\/nlp_spring_2019\/past_projects\/adversarial_dialogue.pdf","2064":"http:\/\/www.jmlr.org\/papers\/volume12\/pedregosa11a\/pedregosa11a.pdf","2065":"http:\/\/www.amazon.com\/exec\/obidos\/redirect?tag=citeulike07-20\\&path=ASIN\/1491962291","2066":"http:\/\/arxiv.org\/abs\/2201.08205","2067":"https:\/\/doi.org\/10.1007\/s40593-019-00174-2","2068":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2006.html#ChuKLYBNO06","2069":"","2070":"http:\/\/dx.doi.org\/10.1007\/BF00058679","2071":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc9.html#LuoYJB18","2072":"http:\/\/dx.doi.org\/10.1016\/j.cels.2015.12.003","2073":"https:\/\/aclanthology.org\/D18-1398\/","2074":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2006.html#WuB06","2075":"http:\/\/arxiv.org\/abs\/1802.04730","2076":"","2077":"http:\/\/dx.doi.org\/10.1051\/epjconf\/201817511025","2078":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2017.html#KraemerTB17","2079":"https:\/\/doi.org\/10.1145\/3531146.3533207","2080":"","2081":"http:\/\/www.frontiersin.org\/neuroinformatics\/10.3389\/fninf.2014.00014\/abstract","2082":"https:\/\/hal-univ-paris8.archives-ouvertes.fr\/hal-01012121","2083":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#SyrgkanisLOHBL19","2084":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2018.html#HayesO18","2085":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2017.html#McInerney17","2086":"","2087":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#Muller20","2088":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2016.html#BerahasNT16","2089":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#Raff19","2090":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#GinartGVZ19","2091":"http:\/\/arxiv.org\/abs\/1406.1078","2092":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#KerenidisLLP19","2093":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2014.html#LiASY14","2094":"","2095":"http:\/\/public.eblib.com\/choice\/publicfullrecord.aspx?p=450516","2096":"http:\/\/arxiv.org\/abs\/1804.07718","2097":"citeseer.nj.nec.com\/flach98logic.html","2098":"http:\/\/arxiv.org\/abs\/1911.06823","2099":"http:\/\/dblp.uni-trier.de\/db\/series\/isrl\/isrl149.html#KorvelKKC19","2100":"https:\/\/www.mdpi.com\/2076-3417\/9\/17\/3553","2101":"http:\/\/arxiv.org\/abs\/1708.00588","2102":"http:\/\/dblp.uni-trier.de\/db\/conf\/ndss\/ndss2020.html#PatraS20","2103":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1877050921003458","2104":"http:\/\/ijacsa.thesai.org\/","2105":"http:\/\/dblp.uni-trier.de\/db\/conf\/papis\/papis2016.html#LiCHZW16","2106":"http:\/\/dx.doi.org\/10.1007\/s10994-010-5214-7","2107":"http:\/\/www.neurocolt.com\/abs\/1995\/..\/..\/tech_reps\/1995\/nc-tr-95-052.ps.gz","2108":"http:\/\/dblp.uni-trier.de\/db\/conf\/aistats\/aistats2020.html#GabrielssonNDS20","2109":"https:\/\/doi.org\/10.1038\/s41592-021-01256-7","2110":"http:\/\/gjeta.com\/content\/diagnostic-pathology-vertebral-column-machine-learning-cluster-k-nearest-neighbor-cknn-part","2111":"","2112":"http:\/\/www.amazon.com\/Statistical-Machine-Learning-Data-Mining-Techniques\/dp\/1439860912\/ref=sr_1_2?s=books&ie=UTF8&qid=1403615181&sr=1-2&keywords=Data+mining+finance","2113":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2018.html#LiuDRSH18","2114":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2018.html#ZadikMS18","2115":"http:\/\/arxiv.org\/abs\/1905.10291","2116":"http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/18605534","2117":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc1.html#Wang10","2118":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc11.html#WangGVC20","2119":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc8.html#DingZZXS17","2120":"https:\/\/doi.org\/10.1371\/journal.pone.0205348","2121":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#MartinezCG19","2122":"http:\/\/dx.doi.org\/10.1109\/ICDE.2011.5767930","2123":"https:\/\/www.nature.com\/articles\/s41746-022-00592-y","2124":"http:\/\/dx.doi.org\/10.1007\/11914853_62","2125":"","2126":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2016.html#BohteN16","2127":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#Ganz20","2128":"http:\/\/dblp.uni-trier.de\/db\/conf\/hotcloud\/hotcloud2009.html#BodikGSFJP09","2129":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0048969721043369","2130":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2016.html#StampfliS16","2131":"https:\/\/journals.bohrpub.com\/index.php\/bijcs\/article\/view\/83","2132":"","2133":"","2134":"http:\/\/arxiv.org\/abs\/1806.02350","2135":"http:\/\/papers.nips.cc\/paper\/6385-matching-networks-for-one-shot-learning","2136":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc8.html#LiuLZWS17","2137":"","2138":"http:\/\/arxiv.org\/abs\/1505.04956","2139":"https:\/\/doi.org\/10.1007\/s11517-013-1130-x","2140":"http:\/\/dblp.uni-trier.de\/db\/journals\/ftml\/ftml5.html#KuleszaT12","2141":"http:\/\/dblp.uni-trier.de\/db\/journals\/ftml\/ftml10.html#JainK17","2142":"https:\/\/onlinelibrary.wiley.com\/doi\/abs\/10.1002\/advs.202202922","2143":"http:\/\/arxiv.org\/abs\/2208.14781","2144":"http:\/\/dblp.uni-trier.de\/db\/reference\/ml\/ml2010.html#SaittaS10a","2145":"https:\/\/doi.org\/10.1145%2F3377325.3377501","2146":"http:\/\/www.jmlr.org\/papers\/volume10\/abeel09a\/abeel09a.pdf","2147":"http:\/\/arxiv.org\/abs\/1708.06615","2148":"http:\/\/dblp.uni-trier.de\/db\/reference\/ml\/ml2010.html#Furnkranz10c","2149":"http:\/\/dblp.uni-trier.de\/db\/series\/isrl\/isrl149.html#TsihrintzisSJ19","2150":"http:\/\/dblp.uni-trier.de\/db\/reference\/ml\/ml2010.html#Morik10","2151":"http:\/\/dblp.uni-trier.de\/db\/reference\/ml\/ml2010.html#Chan10","2152":"http:\/\/dblp.uni-trier.de\/db\/conf\/acml\/acml2019.html#KatoH19","2153":"","2154":"http:\/\/dx.doi.org\/10.1142\/s0218271810017160","2155":"http:\/\/doi.acm.org\/10.1145\/1835449.1835522","2156":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2019.html#QiaoAZX19","2157":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#HittmeirEM20","2158":"","2159":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2015.html#ShiCWYWW15","2160":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2011.html#BachM11","2161":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#KimL19","2162":"http:\/\/dblp.uni-trier.de\/db\/conf\/mlhc\/mlhc2019.html#XuLPGBMPKCMXX19","2163":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#WangLST19","2164":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc9.html#WangZCH18","2165":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#ChuFGW19","2166":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#LiuPST19","2167":"","2168":"http:\/\/jmlr.csail.mit.edu\/papers\/v1\/tipping01a.html","2169":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2004.html#ChellapillaS04","2170":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2004.html#WichmannGSBS04","2171":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2013.html#ChaudhuriV13","2172":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2014.html#LeeKZHGX14","2173":"http:\/\/dblp.uni-trier.de\/db\/conf\/colt\/colt2019.html#KarninL19","2174":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc11.html#ZhangCMH20","2175":"","2176":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S1364815217309817","2177":"http:\/\/dx.doi.org\/10.1007\/s10994-010-5198-3","2178":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2017.html#WilsonRSSR17","2179":"http:\/\/arxiv.org\/abs\/1606.05560","2180":"https:\/\/moa.cms.waikato.ac.nz\/book-html\/","2181":"http:\/\/dx.doi.org\/10.1007\/11823285_36","2182":"https:\/\/pubs.wi-kassel.de\/wp-content\/uploads\/2022\/11\/JML_899.pdf","2183":"http:\/\/www.amazon.de\/Data-Mining-Techniques-Implementations-Management\/dp\/1558605525%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D1558605525","2184":"","2185":"http:\/\/doi.wiley.com\/10.1111\/jep.12744 http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/28371206","2186":"http:\/\/www.sciencedirect.com\/science\/article\/B6WNP-4TYYSKX-5\/2\/1cc423c98274625360b74dd18930fc61","2187":"http:\/\/www.cs.columbia.edu\/~evs\/papers\/sigcse-paper.ps","2188":"http:\/\/www.amazon.com\/Pattern-Recognition-Learning-Information-Statistics\/dp\/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738","2189":"http:\/\/arxiv.org\/abs\/2001.04385","2190":"","2191":"citeseer.ist.psu.edu\/langley94selection.html","2192":"","2193":"","2194":"","2195":"http:\/\/arxiv.org\/abs\/2201.03636","2196":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2018-2.html#ZaremoodiBH18","2197":"http:\/\/arxiv.org\/abs\/2103.10292","2198":"https:\/\/books.google.co.in\/books?id=nAa7PCTIBLkC","2199":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#MacedaLN20","2200":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2016.html#MalleKSH16","2201":"","2202":"http:\/\/dx.doi.org\/10.1145\/1216295.1216315","2203":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#GanteFS20","2204":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2018.html#DamaskinosMGPT18","2205":"http:\/\/dblp.uni-trier.de\/db\/conf\/aistats\/aistats2020.html#JiangN20","2206":"http:\/\/arxiv.org\/abs\/2010.11667","2207":"https:\/\/doi.org\/10.1007\/s10994-017-5633-9","2208":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#CamboG18","2209":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#BoukhelifaBL18","2210":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc7.html#ZhangDZS16","2211":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#CuevasG19","2212":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc9.html#AlshamiriSS18","2213":"http:\/\/dblp.uni-trier.de\/db\/conf\/fat\/fat2018.html#Binns18","2214":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#Ye19a","2215":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc7.html#LiuHMGZ16","2216":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#AhmedAAAK19","2217":"http:\/\/dblp.uni-trier.de\/db\/conf\/aistats\/aistats2018.html#BelletGTT18","2218":"http:\/\/dblp.uni-trier.de\/db\/conf\/copa\/copa2018.html#RaabS18","2219":"http:\/\/dblp.uni-trier.de\/db\/reference\/ml\/ml2017.html#SchuldP17","2220":"https:\/\/www.aaai.org\/Papers\/AAAI\/2000\/AAAI00-084.pdf","2221":"http:\/\/arxiv.org\/abs\/2301.00109","2222":"http:\/\/www.sciencedirect.com\/science\/article\/B6TY8-4KPP42G-C\/2\/01106e7404633aab281bbcc7306ea3e4","2223":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2003.html#GrafW03","2224":"https:\/\/mitpress.mit.edu\/books\/elements-causal-inference","2225":"","2226":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#Shapiro01","2227":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#ZupanBBD01","2228":"","2229":"http:\/\/svn.aksw.org\/papers\/2015\/SEMANTICS_MEX\/public.pdf","2230":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0019057821003773","2231":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#Papatheodorou01","2232":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#PanayiotopoulosZ01","2233":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#Saitta01","2234":"http:\/\/dblp.uni-trier.de\/db\/series\/lncs\/lncs9605.html#Holzinger16","2235":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2003.html#Rasmussen03","2236":"http:\/\/dx.doi.org\/10.1002\/asi.23104","2237":"https:\/\/gjeta.com\/content\/novel-approach-predict-water-quality-index-using-machine-learning-models-review-methods","2238":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#MagoulasP01","2239":"http:\/\/dx.doi.org\/10.1186\/1471-2105-11-15","2240":"https:\/\/doi.org\/10.1007\/s10994-023-06309-w","2241":"","2242":"http:\/\/www.pubmedcentral.nih.gov\/","2243":"","2244":"http:\/\/dblp.uni-trier.de\/db\/conf\/ndss\/ndss2014.html#RasthoferAB14","2245":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2010.html#LarochelleH10","2246":"http:\/\/proceedings.mlr.press\/v89\/feydy19a.html","2247":"","2248":"http:\/\/dblp.uni-trier.de\/db\/series\/isrl\/isrl149.html#StaporRF19","2249":"http:\/\/dblp.uni-trier.de\/rec\/bibtex\/conf\/nips\/ChuKLYBNO06","2250":"","2251":"http:\/\/airccse.org\/journal\/ijcsit2021_curr.html","2252":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2019.html#GhorbaniZ19","2253":"http:\/\/archive.ics.uci.edu\/ml","2254":"","2255":"","2256":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc3.html#ChackoKRA12","2257":"http:\/\/dblp.uni-trier.de\/db\/conf\/ndss\/ndss2020.html#ChaudhariRS20","2258":"http:\/\/dblp.uni-trier.de\/db\/conf\/fat\/fat2018.html#DworkIKL18","2259":"http:\/\/dblp.uni-trier.de\/db\/journals\/jmlr\/jmlr2.html#TongK01","2260":"http:\/\/www.springerlink.com\/content\/q8g7blqvqyxrpvap\/","2261":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2018.html#BollapragadaMNS18","2262":"http:\/\/dblp.uni-trier.de\/db\/conf\/mlhc\/mlhc2020.html#AgrawalOFLS20","2263":"http:\/\/portal.acm.org\/citation.cfm?doid=1073083.1073102","2264":"http:\/\/dblp.uni-trier.de\/db\/reference\/ml\/ml2010.html#CaseJ10","2265":"http:\/\/www.sciencedirect.com\/science\/article\/B6VBS-4JRVBDK-5\/2\/6b75f12e4a096f17439ecf5c766c94c1","2266":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2017.html#SelsamLD17","2267":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2018.html#KallusZ18","2268":"http:\/\/airccse.org\/journal\/ieij\/current.html","2269":"","2270":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaaiss\/aaaiss2013-05.html#SilverYL13","2271":"http:\/\/dx.doi.org\/10.1093\/bib\/bbs024","2272":"http:\/\/dx.doi.org\/10.1145\/2678025.2701399","2273":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc11.html#RiTLXL20","2274":"","2275":"http:\/\/aircconline.com\/ijcsity\/V5N3\/5317ijcsity01.pdf","2276":"","2277":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#KaravelicMI20","2278":"http:\/\/science.sciencemag.org\/content\/361\/6406\/1004","2279":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc11.html#SinglaGS20","2280":"http:\/\/www.amazon.com\/exec\/obidos\/redirect?tag=citeulike07-20\\&path=ASIN\/1558605525","2281":"http:\/\/arxiv.org\/abs\/2106.04980","2282":"","2283":"http:\/\/dx.doi.org\/10.1007\/11744023_34","2284":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#FriedmanL20","2285":"https:\/\/doi.org\/10.3390%2Fijms21228837","2286":"https:\/\/doi.org\/10.1023\/A:1007327622663","2287":"http:\/\/dblp.uni-trier.de\/db\/conf\/iq\/iq2006.html#SessionsV06","2288":"http:\/\/dx.doi.org\/10.17762\/ijritcc2321-8169.150183","2289":"http:\/\/arxiv.org\/abs\/1402.7351","2290":"http:\/\/openaccessjournals.eu\/index.php\/ijiaet\/article\/view\/1495","2291":"http:\/\/jamia.bmj.com\/content\/16\/1\/109.abstract","2292":"http:\/\/arxiv.org\/abs\/1912.12132","2293":"http:\/\/arxiv.org\/abs\/2103.04893","2294":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci801.html#Bhatnagar19","2295":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci801.html#LabanAEST19","2296":"","2297":"http:\/\/proceedings.mlr.press\/v80\/ruff18a.html","2298":"https:\/\/downloads.hci.informatik.uni-wuerzburg.de\/2022-ieeeaivr-schell-comparison-of-data-representations-and-machine-learning-architectures-for-user-identification-on-arbitrary-motion-sequences.pdf","2299":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#LvH19","2300":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#KarakoulasS01","2301":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc4.html#JainPS13","2302":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc11.html#ChauhanSD20","2303":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/affcomp2018.html#YatesCH18","2304":"http:\/\/airccj.org\/CSCP\/vol4\/csit41926.pdf","2305":"","2306":"http:\/\/dblp.uni-trier.de\/db\/series\/cogtech\/354075170.html#ValentiSGC08","2307":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#Hatziargyriou01","2308":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2001.html#FakotakisS01","2309":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#LiLXS19","2310":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc9.html#CuiYCMLQ18","2311":"https:\/\/doi.org\/10.1109%2Ftbdata.2017.2680460","2312":"","2313":"http:\/\/dblp.uni-trier.de\/db\/conf\/siemens\/siemens1993.html#HansonRR93","2314":"http:\/\/dblp.uni-trier.de\/db\/conf\/aistats\/aistats2017.html#KleinFBHH17","2315":"http:\/\/dblp.uni-trier.de\/db\/conf\/aistats\/aistats2017.html#BojarskiCCFGMSS17","2316":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2019.html#KleimanP19","2317":"http:\/\/www.cag.csail.mit.edu\/~mstephen\/stephenson_phdthesis.pdf","2318":"https:\/\/svn.aksw.org\/papers\/2021\/cikm-distrdf2ml\/public.pdf","2319":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2020.html#SkopikWL20","2320":"https:\/\/cajmtcs.centralasianstudies.org\/index.php\/CAJMTCS\/article\/view\/348\/377","2321":"https:\/\/doi.org\/10.1145%2F3306618.3314293","2322":"","2323":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#TondaBCBGLP18","2324":"https:\/\/doi.org\/10.1146\/annurev-economics-080217-053433    ","2325":"","2326":"https:\/\/link.springer.com\/content\/pdf\/10.1007%2F11736790.pdf","2327":"https:\/\/doi.org\/10.1038\/s41524-017-0045-8","2328":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2013.html#JiaAAGD13","2329":"","2330":"http:\/\/arxiv.org\/abs\/2305.16348","2331":"","2332":"https:\/\/www.annualreviews.org\/doi\/10.1146\/annurev-fluid-010719-060214","2333":"","2334":"https:\/\/journals.researchparks.org\/index.php\/IJOT\/article\/view\/4546\/4257","2335":"http:\/\/aircconline.com\/cseij\/V8N1\/8118cseij01.pdf","2336":"https:\/\/doi.org\/10.1145%2F2207676.2207680","2337":"http:\/\/ieeexplore.ieee.org\/xpl\/articleDetails.jsp?arnumber=6423821","2338":"http:\/\/arxiv.org\/abs\/2011.14157","2339":"http:\/\/www.schattauer.de\/index.php?id=1214&doi=10.3414\/ME13-01-0122","2340":"http:\/\/www.smi.ucd.ie\/Dagstuhl-MLSW\/proceedings\/ciravegna-chapman.pdf","2341":"http:\/\/arxiv.org\/abs\/1709.01604","2342":"https:\/\/doi.org\/10.1038\/s41467-018-05761-w","2343":"http:\/\/dx.doi.org\/10.1111\/caim.12202","2344":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#KaleS19","2345":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc11.html#LuoLWBWZ20","2346":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc2.html#HeW11","2347":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#RaghuwanshiS19","2348":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#WangLGZY19","2349":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc2.html#WuWC11","2350":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#GhasemzadehAE19","2351":"","2352":"http:\/\/dx.doi.org\/10.1371%2Fjournal.pgen.1005928","2353":"http:\/\/link.aps.org\/doi\/10.1103\/PhysRevLett.108.253002","2354":"http:\/\/dx.doi.org\/10.1023\/B:MACH.0000039777.23772.30","2355":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/affcomp2017.html#YatesCNH17","2356":"","2357":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaaiss\/aaaiss2013-05.html#Federmann13","2358":"","2359":"http:\/\/dblp.uni-trier.de\/db\/series\/isrl\/isrl56.html#ChowriappaDT14","2360":"https:\/\/wireilla.com\/papers\/ijbb\/V2N2\/2212ijbb01.pdf","2361":"http:\/\/dblp.uni-trier.de\/db\/conf\/mlhc\/mlhc2019.html#TonekaboniJMG19","2362":"https:\/\/gjeta.com\/content\/classifying-relevant-video-tutorials-school%E2%80%99s-learning-management-system-using-support","2363":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2016.html#BacciuCGS16","2364":"","2365":"http:\/\/www.amazon.com\/exec\/obidos\/redirect?tag=citeulike07-20\\&path=ASIN\/1783555130","2366":"https:\/\/aircconline.com\/abstract\/ijcsit\/v12n5\/12520ijcsit01.html","2367":"http:\/\/arxiv.org\/abs\/2003.12206","2368":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci263.html#KwasnickaP10","2369":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#ZhouYC18","2370":"http:\/\/jmlr.csail.mit.edu\/papers\/v7\/wainwright06a.html","2371":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci90.html#SrihariSCB08","2372":"","2373":"http:\/\/arxiv.org\/abs\/2211.04325","2374":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#PynadathBWC18","2375":"http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/23431361","2376":"https:\/\/proceedings.mlr.press\/v162\/wei22d.html","2377":"https:\/\/machine-learning.ama-academy.eu\/","2378":"","2379":"http:\/\/proceedings.mlr.press\/v89\/genevay19a.html","2380":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci801.html#FitzgeraldRS19","2381":"","2382":"http:\/\/isxp1010c.sims.cranfield.ac.uk\/Papers\/paper196.pdf","2383":"","2384":"http:\/\/dx.doi.org\/10.1371\/journal.pcbi.1004838","2385":"https:\/\/doi.org\/10.1145\/3474717.3484204","2386":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc9.html#ZhaoCZMA18","2387":"https:\/\/proceedings.mlr.press\/v97\/lee19d.html","2388":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips1997.html#KappenO97","2389":"https:\/\/onlinelibrary.wiley.com\/doi\/abs\/10.1002\/adts.202200351","2390":"http:\/\/link.springer.com\/10.1007\/s10994-013-5363-6","2391":"https:\/\/dl.acm.org\/doi\/10.1145\/3492853","2392":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#KongsorotHMS19","2393":"http:\/\/portal.acm.org\/citation.cfm?id=375731&dl=GUIDE&coll=GUIDE&CFID=75153142&CFTOKEN=89522229","2394":"https:\/\/journals.researchparks.org\/index.php\/IJOT\/article\/view\/4725\/4399","2395":"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/31000806","2396":"http:\/\/dblp.uni-trier.de\/db\/conf\/mlhc\/mlhc2018.html#RavuriKTA18","2397":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2019.html#BansalLRSW19","2398":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2017.html#NguyenLST17","2399":"https:\/\/www.mdpi.com\/2073-4352\/12\/9\/1281","2400":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2018.html#DolfiASB18","2401":"http:\/\/arxiv.org\/abs\/2004.13912","2402":"","2403":"http:\/\/aircconline.com\/ijcsea\/V7N5\/7517ijcsea01.pdf","2404":"https:\/\/proceedings.mlr.press\/v162\/d-ascoli22a.html","2405":"https:\/\/arxiv.org\/pdf\/1908.09635","2406":"http:\/\/airccse.org\/journal\/ijcsea\/current2017.html","2407":"http:\/\/www-2.cs.cmu.edu\/~knigam\/papers\/cora-ijcai99.pdf","2408":"","2409":"http:\/\/aircconline.com\/ijcsea\/V7N5\/7517ijcsea01.pdf","2410":"https:\/\/doi.org\/10.3389%2Ffncom.2020.00029","2411":"","2412":"https:\/\/ieeexplore.ieee.org\/document\/8002611\/","2413":"https:\/\/doi.org\/10.3389\/feart.2021.659310","2414":"http:\/\/dx.doi.org\/10.1186\/1752-0509-4-56","2415":"http:\/\/ijes.info\/1\/2\/4254126.pdf","2416":"http:\/\/pubs.wi-kassel.de\/wp-content\/uploads\/2018\/03\/JML_679.pdf","2417":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci262.html#HassanienASP10","2418":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci263.html#KryzhanovskyKL10","2419":"http:\/\/dblp.uni-trier.de\/db\/series\/lncs\/lncs9605.html#Belciug16","2420":"http:\/\/arxiv.org\/abs\/1901.05125","2421":"http:\/\/portal.acm.org\/citation.cfm?id=940878","2422":"http:\/\/www.google.com\/search?client=safari&rls=en-us&q=Parallel+implementation+of+the+machine+learning\/statistical+method+random+forest+(R+package+randomForest)&ie=UTF-8&oe=UTF-8","2423":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc11.html#ZhouQYHCL20","2424":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#ChengXZW19","2425":"http:\/\/dblp.uni-trier.de\/db\/conf\/ac\/ml2003.html#Burges03","2426":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#FangXLS19","2427":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc11.html#NiXL20","2428":"http:\/\/dblp.uni-trier.de\/db\/series\/lncs\/lncs9605.html#RobertBRH16","2429":"http:\/\/dx.doi.org\/10.1007\/978-3-540-24750-0_19","2430":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci262.html#Bratko10","2431":"https:\/\/journals.researchparks.org\/index.php\/IJHCS\/article\/view\/4259\/3999","2432":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#Gretton18","2433":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#ZhengG18","2434":"","2435":"","2436":"","2437":"https:\/\/aircconline.com\/ijcnc\/V14N3\/14322cnc01.pdf","2438":"http:\/\/doi.acm.org\/10.1145\/2876034.2876042","2439":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2019.html#Rousseau19","2440":"citeseer.nj.nec.com\/koenig96graph.html","2441":"citeseer.nj.nec.com\/article\/noy99smart.html","2442":"http:\/\/arxiv.org\/abs\/2105.05946","2443":"http:\/\/dblp.uni-trier.de\/db\/conf\/l4dc\/l4dc2020.html#Boyer20","2444":"http:\/\/dblp.uni-trier.de\/db\/conf\/mlhc\/mlhc2017.html#BergquistBKLR17","2445":"http:\/\/jmvidal.cse.sc.edu\/library\/stone00a.pdf","2446":"citeseer.nj.nec.com\/article\/heckerman95learning.html","2447":"http:\/\/jmlr.csail.mit.edu\/papers\/v12\/chaudhuri11a.html","2448":"http:\/\/dblp.uni-trier.de\/db\/conf\/colt\/colt2018.html#ChernozhukovWZ18","2449":"citeseer.nj.nec.com\/falkenhainer89structuremapping.html","2450":"http:\/\/dblp.uni-trier.de\/db\/conf\/mlhc\/mlhc2018.html#MerdanGD18","2451":"http:\/\/dblp.uni-trier.de\/db\/conf\/webist\/webist2018.html#RicoTQP18","2452":"http:\/\/www.ellogon.org\/petasis\/bibliography\/ACL2001\/ACL-2001-CameraReady.pdf","2453":"http:\/\/dblp.uni-trier.de\/db\/conf\/icml\/icml2017.html#KumarGV17","2454":"http:\/\/dx.doi.org\/10.1023\/A:1010933404324","2455":"","2456":"http:\/\/ijacsa.thesai.org\/","2457":"","2458":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc9.html#ZhangZYC18","2459":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#WongGWVY19","2460":"http:\/\/dbiref.uvt.nl\/iPort?request=full_record&db=wo&language=eng&query=194155","2461":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc10.html#XuYW19","2462":"http:\/\/dblp.uni-trier.de\/db\/journals\/mlc\/mlc9.html#LuoYXHLYV18","2463":"https:\/\/doi.org\/10.1002\/widm.1452","2464":"http:\/\/arxiv.org\/abs\/1607.01391","2465":"http:\/\/dblp.uni-trier.de\/db\/series\/lncs\/lncs9605.html#JeanquartierJKTHJH16","2466":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#AbdollahiN18","2467":"http:\/\/dblp.uni-trier.de\/db\/series\/hci\/ZhouC18.html#ZhouC18a","2468":"http:\/\/dx.doi.org\/10.1186\/1471-2105-11-273","2469":"http:\/\/delivery.acm.org.ezproxy.lib.unimelb.edu.au\/10.1145\/830000\/820213\/p189-cho.pdf?key1=820213\\&#38;key2=0643963711\\&#38;coll=GUIDE\\&#38;dl=GUIDE,ACM\\&#38;CFID=13428846\\&#38;CFTOKEN=25868078","2470":"https:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-67008-9_61","2471":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2018.html#AubinMBKMZ18","2472":"http:\/\/dx.doi.org\/10.1007\/s10071-007-0129-9","2473":"http:\/\/dblp.uni-trier.de\/db\/series\/isrl\/isrl56.html#MartisCR14","2474":"http:\/\/dblp.uni-trier.de\/db\/series\/isrl\/isrl56.html#StewartRSE14","2475":"http:\/\/www.scs.org\/docInfo.cfm?get=1720","2476":"http:\/\/www.sciencedirect.com\/science\/article\/B6V2M-40FGBJY-1\/2\/78ce2dab77a99bf15b8dec2aab6ccd61","2477":"https:\/\/proceedings.mlr.press\/v32\/le14.html","2478":"http:\/\/dblp.uni-trier.de\/db\/series\/isrl\/isrl56.html#ShenWZZY14","2479":"http:\/\/www.scs.org\/docInfo.cfm?get=1707","2480":"http:\/\/proceedings.mlr.press\/v80\/wehrmann18a.html","2481":"https:\/\/openaccessjournals.eu\/index.php\/jedic\/article\/view\/1859\/1795","2482":"","2483":"http:\/\/arxiv.org\/abs\/2010.10906","2484":"http:\/\/arxiv.org\/abs\/2207.10342","2485":"http:\/\/arxiv.org\/abs\/1901.07291","2486":"http:\/\/arxiv.org\/abs\/1909.05858","2487":"http:\/\/arxiv.org\/PS_cache\/arxiv\/pdf\/1111\/1111.3970v1.pdf","2488":"http:\/\/dl.acm.org\/citation.cfm?id=944919.944966","2489":"http:\/\/portal.acm.org\/citation.cfm?id=1220896","2490":"https:\/\/www.ijtsrd.com\/humanities-and-the-arts\/philosophy\/47546\/teaching-the-uzbek-language-is-a-topical-issue-an-interactive-learning-model\/d-a-khidoyatova","2491":"http:\/\/arxiv.org\/abs\/2002.08910","2492":"http:\/\/arxiv.org\/abs\/2302.06761","2493":"http:\/\/arxiv.org\/abs\/1801.06146","2494":"http:\/\/doi.acm.org\/10.1145\/2661829.2661974","2495":"http:\/\/arxiv.org\/abs\/2205.05448","2496":"http:\/\/ieeexplore.ieee.org\/document\/5277727\/","2497":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#ConneauL19","2498":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2000.html#BengioDV00","2499":"http:\/\/arxiv.org\/abs\/2005.00796","2500":"http:\/\/dx.doi.org\/10.1145\/319950.320022","2501":"https:\/\/www.ijtsrd.com\/humanities-and-the-arts\/other\/47644\/teaching-the-uzbek-language-is-a-topical-issue-on-the-example-of-an-interactive-model-of-education\/d-a-khidoyatova","2502":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2019.html#00040WWLWGZH19","2503":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2008.html#MnihH08","2504":"https:\/\/www.ijtsrd.com\/other-scientific-research-area\/other\/26561\/beyond-scotton%E2%80%99s-matrix-language-frame-model-a-look-into-some-bassa-french-code-switched-discourse-samples\/mbeng-sampson-tambe","2505":"http:\/\/arxiv.org\/abs\/2011.01403","2506":"http:\/\/portal.acm.org\/citation.cfm?id=1495122","2507":"http:\/\/dx.doi.org\/10.1109\/ICSE.2000.870424","2508":"http:\/\/portal.acm.org\/citation.cfm?id=1496399.1496585&coll=portal&dl=ACM.","2509":"http:\/\/arxiv.org\/abs\/2302.06527","2510":"https:\/\/camembert-model.fr\/","2511":"http:\/\/arxiv.org\/abs\/2004.07159","2512":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2004.html#XuJ04","2513":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2008.html#TamS08","2514":"http:\/\/arxiv.org\/abs\/2301.00303","2515":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.14.5631","2516":"http:\/\/arxiv.org\/abs\/2305.14342","2517":"http:\/\/arxiv.org\/abs\/1508.06615","2518":"http:\/\/www.ics.mq.edu.au\/~diego\/answerfinder\/","2519":"https:\/\/livrepository.liverpool.ac.uk\/3028272\/1\/McCauley_Christiansen_in_press_Psych_Rev.pdf","2520":"internal-pdf:\/\/Markosova2008networkOfHumanLanguage-0397381632\/Markosova2008networkOfHumanLanguage.pdf","2521":"","2522":"http:\/\/dx.doi.org\/10.1016\/j.scico.2007.05.004","2523":"http:\/\/arxiv.org\/abs\/2005.07503","2524":"http:\/\/arxiv.org\/abs\/1909.08053","2525":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2017.html#GraveCJ17","2526":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2016.html#MiyamotoC16","2527":"http:\/\/dblp.uni-trier.de\/db\/conf\/nldb\/nldb2008.html#MontesPEP08","2528":"http:\/\/dx.doi.org\/10.1108\/EL-08-2011-0126","2529":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2018.html#ChenSLCH18","2530":"http:\/\/dx.doi.org\/10.1145\/1645953.1646268","2531":"","2532":"http:\/\/search.ebscohost.com\/login.aspx?direct=true\\&db=lxh\\&AN=10362991\\&site=ehost-live ","2533":"http:\/\/arxiv.org\/abs\/2005.14165","2534":"","2535":"","2536":"http:\/\/dblp.uni-trier.de\/db\/conf\/tapp\/tapp2010.html#ArcherD10","2537":"http:\/\/ijacsa.thesai.org\/","2538":"","2539":"http:\/\/arxiv.org\/abs\/1908.01839","2540":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2002.html#KleinM02","2541":"http:\/\/arxiv.org\/abs\/2302.04761","2542":"citeseer.ist.psu.edu\/lodderstedt02secureuml.html","2543":"","2544":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S1477842405000448","2545":"","2546":"https:\/\/www.semanticscholar.org\/paper\/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu\/9405cc0d6169988371b2755e573cc28650d14dfe","2547":"http:\/\/www.scopus.com\/scopus\/record\/display.url?view=extended&origin=resultslist&eid=2-s2.0-34548317287","2548":"","2549":"http:\/\/jime.open.ac.uk\/2008\/13\/","2550":"","2551":"http:\/\/dx.doi.org\/10.1145\/1076034.1076086","2552":"http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=1318493","2553":"http:\/\/dx.doi.org\/10.1007\/978-3-540-89020-1_26","2554":"","2555":"https:\/\/doi.org\/10.1007\/978-981-99-7093-3_46","2556":"http:\/\/dx.doi.org\/10.1007\/978-3-540-85289-6_13","2557":"http:\/\/dx.doi.org\/10.1007\/978-3-540-24721-0_16","2558":"http:\/\/dblp.uni-trier.de\/db\/conf\/dagstuhl\/P4101.html#BezivinH04","2559":"http:\/\/dblp.uni-trier.de\/db\/conf\/airweb\/airweb2005.html#MishneCL05","2560":"https:\/\/www.semanticscholar.org\/paper\/a54b56af24bb4873ed0163b77df63b92bd018ddc","2561":"","2562":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S095741741200098X","2563":"","2564":"http:\/\/dx.doi.org\/10.1016\/j.eswa.2012.01.085","2565":"http:\/\/www.aclweb.org\/anthology\/P\/P07\/P07-1010","2566":"http:\/\/dx.doi.org\/10.1145\/3501709.3544280","2567":"https:\/\/doi.org\/10.1093\/bioinformatics\/btz682","2568":"http:\/\/dblp.uni-trier.de\/db\/conf\/dagstuhl\/P4101.html#BezivinH04a","2569":"https:\/\/dl.acm.org\/doi\/10.1145\/3581641.3584037","2570":"","2571":"https:\/\/d4mucfpksywv.cloudfront.net\/better-language-models\/language-models.pdf","2572":"https:\/\/www.semanticscholar.org\/paper\/2b269cc89d76bb39fb2120e57768e857b91820be","2573":"http:\/\/arxiv.org\/abs\/2306.08543","2574":"http:\/\/link.springer.com\/10.1007\/978-3-642-04417-5_11","2575":"http:\/\/www.sciencedirect.com\/science\/article\/B6WMM-4CBV2NG-2\/2\/6d330110e2d56220881391d6616b0f32","2576":"http:\/\/arxiv.org\/abs\/2202.13169","2577":"http:\/\/tfs.cs.tu-berlin.de\/gramot\/FinalVersions\/PDF\/EhrigErmelHaensgen.pdf","2578":"https:\/\/www.ijtsrd.com\/computer-science\/programming-language\/26420\/a-real-dynamic-cyber-trust-model\/kuchillapati-chinnari","2579":"http:\/\/arxiv.org\/abs\/2301.12652","2580":"http:\/\/arxiv.org\/abs\/2203.15556","2581":"http:\/\/dx.doi.org\/10.1007\/11880240_16","2582":"http:\/\/dx.doi.org\/10.3115\/1119282.1119287","2583":"http:\/\/dx.doi.org\/10.1109\/TSE.2016.2613863","2584":"http:\/\/arxiv.org\/abs\/2303.18223","2585":"http:\/\/dblp.uni-trier.de\/db\/journals\/siu\/siu10.html#MermetS12","2586":"http:\/\/portal.acm.org\/citation.cfm?id=1084013.1084215","2587":"http:\/\/dblp.uni-trier.de\/db\/journals\/ercim\/ercim2011.html#ChersiFPP11","2588":"http:\/\/dblp.uni-trier.de\/db\/journals\/sigplan\/sigplan38.html#ChunYanMNZP03","2589":"http:\/\/dblp.uni-trier.de\/db\/books\/daglib\/0032299.html#SyrianiGV13","2590":"","2591":"","2592":"http:\/\/arxiv.org\/abs\/1906.05664","2593":"http:\/\/dblp.uni-trier.de\/db\/conf\/vardial\/vardial2016.html#Ostling16","2594":"http:\/\/arxiv.org\/abs\/1612.04426","2595":"https:\/\/www.semanticscholar.org\/paper\/0cbf97173391b0430140117027edcaf1a37968c7","2596":"","2597":"http:\/\/citeseer.ist.psu.edu\/bilmes03factored.html","2598":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2013.html#BeaufaysS13","2599":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2011.html#GillotC11","2600":"","2601":"http:\/\/arxiv.org\/abs\/2203.02155","2602":"https:\/\/www.aclweb.org\/anthology\/2020.coling-main.599","2603":"http:\/\/arxiv.org\/abs\/1909.10705","2604":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/icslp1998.html#RiccardiPN98","2605":"http:\/\/portal.acm.org\/citation.cfm?id=1008992.1009026","2606":"https:\/\/ijcm.academicjournal.io\/index.php\/ijcm\/article\/view\/383","2607":"http:\/\/arxiv.org\/abs\/2212.09196","2608":"http:\/\/arxiv.org\/abs\/2201.11903","2609":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp1996.html#Brants96","2610":"http:\/\/dblp.uni-trier.de\/db\/journals\/ieicet\/ieicet95d.html#ObaHNI12","2611":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr9604.html#cmp-lg-9604005","2612":"http:\/\/arxiv.org\/abs\/1909.00615","2613":"http:\/\/hal.inria.fr\/hal-00953910","2614":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2020.html#SalazarLNK20","2615":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2016.html#Brychcin16","2616":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2010.html#abs-2010-10906","2617":"http:\/\/dblp.uni-trier.de\/db\/conf\/ic\/icomp2006.html#HollandBDP06","2618":"http:\/\/dblp.uni-trier.de\/db\/conf\/raslan\/raslan2014.html#Baisa14","2619":"http:\/\/www.idi.ntnu.no\/~ppp\/referent\/rmlspec.pdf","2620":"http:\/\/dblp.uni-trier.de\/db\/journals\/entcs\/entcs72.html#Kent03","2621":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1995.html#BeslingM95","2622":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1710.html#abs-1710-10248","2623":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl-alta\/acl-alta2007.html#PizzatoM07","2624":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2003-1.html#BacchianiR03","2625":"http:\/\/dblp.uni-trier.de\/db\/conf\/cogsci\/cogsci2012.html#AhnL12","2626":"http:\/\/arxiv.org\/abs\/1603.07012","2627":"","2628":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2004.html#JamoussiLHS04","2629":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2018-1.html#LiuCY18","2630":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr9811.html#cs-CL-9811025","2631":"http:\/\/dblp.uni-trier.de\/db\/conf\/mdafa\/mdafa2004.html#KalninsBC04","2632":"http:\/\/dblp.uni-trier.de\/db\/conf\/iclp\/iclp2003.html#UedaK03","2633":"","2634":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl97.html#Chelba97","2635":"http:\/\/arxiv.org\/abs\/2302.00083","2636":"http:\/\/0-doi.acm.org.innopac.up.ac.za\/10.1145\/1842752.1842807","2637":"http:\/\/arxiv.org\/abs\/2106.09685","2638":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1903.html#abs-1903-09722","2639":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/swcm2017.html#YuKLK17","2640":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#KobusDDM05","2641":"http:\/\/dblp.uni-trier.de\/db\/conf\/ccis\/ccis2018.html#LiuDZWTLWHZZZL18","2642":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2019-1.html#EdunovBA19","2643":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2005.html#LiM05","2644":"http:\/\/arxiv.org\/abs\/2002.08155","2645":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1911.html#abs-1911-07613","2646":"http:\/\/arxiv.org\/abs\/2306.06031","2647":"http:\/\/portal.acm.org\/citation.cfm?id=1076034.1076087","2648":"https:\/\/doi.org\/10.1093\/bioinformatics\/btab083","2649":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2002.html#abs-2002-12804","2650":"http:\/\/dblp.uni-trier.de\/db\/conf\/enviroinfo\/enviroinfo2009-1.html#TheisselmannDF09","2651":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1806.html#abs-1806-03743","2652":"http:\/\/dx.doi.org\/10.1007\/978-3-540-69927-9_11","2653":"","2654":"http:\/\/dx.doi.org\/10.1109\/TIME-E.2015.7389747","2655":"http:\/\/arxiv.org\/abs\/2201.11903","2656":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2019.html#SuzukiINKT19","2657":"","2658":"","2659":"http:\/\/dblp.uni-trier.de\/db\/conf\/wsdm\/hsdm2020.html#TarcarTRDRD20","2660":"http:\/\/aclweb.org\/anthology\/P\/P14\/P14-1108.pdf","2661":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr9608.html#cmp-lg-9608004","2662":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2019-1.html#MielkeCGRE19","2663":"http:\/\/dblp.uni-trier.de\/db\/conf\/vardial\/vardial2018.html#JauhiainenJL18","2664":"http:\/\/dblp.uni-trier.de\/db\/journals\/csl\/csl15.html#ClarksonR01","2665":"http:\/\/dblp.uni-trier.de\/db\/conf\/modelsward\/modelsward2017.html#BrouwersHKL17","2666":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2005-1.html#MartinS05","2667":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2001.html#abs-2001-05315","2668":"","2669":"http:\/\/dblp.uni-trier.de\/db\/series\/cogtech\/54023732.html#EmeleVLG06","2670":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2000.html#ChenLL00","2671":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2001.html#WhittakerR01","2672":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2017-1.html#LauBC17","2673":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2016.html#TranZH16","2674":"http:\/\/dblp.uni-trier.de\/db\/conf\/ewdw\/ewdw90.html#AtzeniT90","2675":"http:\/\/dblp.uni-trier.de\/db\/conf\/kdd\/kdd2016.html#MukherjeeGW16","2676":"http:\/\/dblp.uni-trier.de\/db\/journals\/csl\/csl26.html#DeschachtBM12","2677":"http:\/\/dblp.uni-trier.de\/db\/conf\/gi\/gi80.html#JayezLL80","2678":"http:\/\/dblp.uni-trier.de\/db\/conf\/iri\/iri2011.html#Bouabana-Tebibel11","2679":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2010.html#GillotCLH10","2680":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2008.html#LiuGW08","2681":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1708.html#abs-1708-09073","2682":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1608.html#AhnCPB16","2683":"http:\/\/dblp.uni-trier.de\/db\/conf\/aistats\/aistats2018.html#WangGWSHPSC18","2684":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2003.html#Feng03","2685":"http:\/\/dblp.uni-trier.de\/db\/conf\/siu\/siu2014.html#DikiciS14","2686":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2018.html#HuangLPH18","2687":"http:\/\/dblp.uni-trier.de\/db\/journals\/nle\/nle1.html#MackayP95","2688":"http:\/\/dblp.uni-trier.de\/db\/journals\/ijpp\/ijpp9.html#Moyne80","2689":"http:\/\/dblp.uni-trier.de\/db\/conf\/imcsit\/imcsit2009.html#KollarVW09","2690":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2009.html#HarbCDG09","2691":"","2692":"http:\/\/dblp.uni-trier.de\/db\/conf\/itcc\/itcc2005-1.html#Celikel05","2693":"http:\/\/dblp.uni-trier.de\/db\/conf\/birthday\/nagl2010.html#Klein10","2694":"http:\/\/dblp.uni-trier.de\/db\/conf\/staf\/volt2014.html#LanoRC14","2695":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2000.html#HungLYCS00","2696":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecoop\/globaldsl2013.html#Rumpe13","2697":"http:\/\/dblp.uni-trier.de\/db\/conf\/oopsla\/oopsla2003.html#Agrawal03a","2698":"http:\/\/dblp.uni-trier.de\/db\/series\/sci\/sci390.html#BrockiMK12","2699":"http:\/\/dblp.uni-trier.de\/db\/conf\/www\/www2012c.html#Singh12","2700":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2007.html#abs-2007-00655","2701":"http:\/\/dblp.uni-trier.de\/db\/journals\/taosd\/taosd6.html#HeidenreichHJZ09","2702":"http:\/\/dblp.uni-trier.de\/db\/conf\/sac\/sac2006.html#Tratt06","2703":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecir\/ecir2007.html#MerkelK07","2704":"http:\/\/dblp.uni-trier.de\/db\/journals\/eceasst\/eceasst34.html#GjosaeterP10","2705":"http:\/\/dblp.uni-trier.de\/db\/conf\/oopsla\/dsm2013.html#AckermannV13","2706":"http:\/\/dblp.uni-trier.de\/db\/journals\/ml\/ml60.html#EmamiJ05","2707":"http:\/\/arxiv.org\/abs\/2304.04675","2708":"http:\/\/arxiv.org\/abs\/2208.03299","2709":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1211.html#abs-1211-5009","2710":"http:\/\/dblp.uni-trier.de\/db\/conf\/icfhr\/icfhr2018.html#TensmeyerWDSMB18","2711":"https:\/\/www.aclweb.org\/anthology\/2020.emnlp-main.420","2712":"http:\/\/dblp.uni-trier.de\/db\/conf\/sisy\/sisy2012.html#OstrogonacMSPD12","2713":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1907.html#abs-1907-00409","2714":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2007.html#AlumaeK07","2715":"","2716":"http:\/\/dblp.uni-trier.de\/db\/conf\/scisisis\/scisisis2012.html#SongSPTWCL12","2717":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcnn\/ijcnn2019.html#WuWLZ19","2718":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1995.html#LundG95","2719":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1903.html#abs-1903-10915","2720":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2007.html#abs-2007-15779","2721":"https:\/\/www.biorxiv.org\/content\/early\/2020\/05\/23\/2020.05.20.107003","2722":"http:\/\/dblp.uni-trier.de\/db\/conf\/icse\/icse2000.html#Bultan00","2723":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1212.html#abs-1212-3228","2724":"http:\/\/dblp.uni-trier.de\/db\/conf\/sped\/sped2017.html#TrifanAC17","2725":"http:\/\/dblp.uni-trier.de\/db\/journals\/nle\/nle25.html#JauhiainenLJ19","2726":"http:\/\/dblp.uni-trier.de\/db\/conf\/ifip8-1\/poem2009.html#ShahzadEJ09","2727":"https:\/\/www.informatik.uni-wuerzburg.de\/datascience\/news\/single\/news\/our-paper-lm4kg-improving-common-sense-knowledge-graphs-with-language-models-has-been-presented-a\/","2728":"https:\/\/www.semanticscholar.org\/paper\/77d956cdab4508d569ae5741549b78e715fd0749","2729":"http:\/\/dblp.uni-trier.de\/db\/conf\/sac\/sac2016.html#Gomez-AbajoGL16","2730":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2003.html#XuEJ03","2731":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2005.html#abs-2005-13407","2732":"http:\/\/dblp.uni-trier.de\/db\/conf\/aiccsa\/aiccsa2009.html#LeonardiMFMRD09","2733":"http:\/\/dblp.uni-trier.de\/db\/journals\/sosym\/sosym16.html#ZurowskaD17","2734":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2013.html#FohrM13","2735":"","2736":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/MRGD2014.html#MisuMMKL14","2737":"http:\/\/dx.doi.org\/10.1016\/j.scico.2007.05.005","2738":"http:\/\/www.kde.cs.uni-kassel.de\/conf\/lwa10\/papers\/ir2.pdf","2739":"http:\/\/arxiv.org\/abs\/2206.04615","2740":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2015.html#JinHQY15","2741":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecmdafa\/ecmdafa2006.html#EngelPK06","2742":"http:\/\/dx.doi.org\/10.1007\/11787044_12","2743":"http:\/\/dblp.uni-trier.de\/db\/conf\/vl\/vlhcc2010.html#GuerraLKP10","2744":"https:\/\/www.aclweb.org\/anthology\/N19-3002","2745":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2020.html#MartinMSDRCSS20","2746":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2010.html#DeorasJS10","2747":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1995.html#Kosarev95","2748":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2008.html#Lopez-MorenoRGT08","2749":"http:\/\/dblp.uni-trier.de\/db\/conf\/slt\/slt2014.html#ChienK14","2750":"http:\/\/dblp.uni-trier.de\/db\/conf\/sspr\/sspr2012.html#FrinkenFLO12","2751":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2001.html#JaniszekMB01","2752":"http:\/\/dblp.uni-trier.de\/db\/conf\/paclic\/paclic2006.html#Yuan06","2753":"http:\/\/www.isca-speech.org\/archive\/interspeech_2010\/i10_1045.html","2754":"http:\/\/dblp.uni-trier.de\/db\/conf\/tsd\/tsd2008.html#JelinekP08","2755":"http:\/\/dblp.uni-trier.de\/db\/conf\/colt\/colt2004.html#DrukhM04","2756":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2008s.html#Yuret08","2757":"http:\/\/dblp.uni-trier.de\/db\/journals\/taslp\/taslp23.html#Chien15a","2758":"http:\/\/dblp.uni-trier.de\/db\/journals\/jot\/jot7.html#Tratt08","2759":"http:\/\/dblp.uni-trier.de\/db\/journals\/access\/access7.html#KhysruJD19","2760":"http:\/\/dblp.uni-trier.de\/db\/conf\/iastedSE\/se2004.html#Urrego-Giraldo04","2761":"http:\/\/dblp.uni-trier.de\/db\/conf\/im\/im1997.html#MayerKOY97","2762":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#SiivolaP05","2763":"","2764":"http:\/\/dblp.uni-trier.de\/db\/conf\/staf\/ds2016.html#Neubauer16","2765":"http:\/\/dblp.uni-trier.de\/db\/conf\/siu\/siu2014.html#AkinD14","2766":"http:\/\/dblp.uni-trier.de\/db\/journals\/bioinformatics\/bioinformatics25.html#SmithBCS09","2767":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2009s.html#WatanabeTI09","2768":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1903.html#abs-1903-06965","2769":"http:\/\/dblp.uni-trier.de\/db\/conf\/apbpm\/apbpm2013.html#HofstedeORS0P13","2770":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2005-1.html#Tur05","2771":"http:\/\/dblp.uni-trier.de\/db\/conf\/desrist\/desrist2014.html#DelfmannDHS14","2772":"","2773":"http:\/\/dblp.uni-trier.de\/db\/conf\/tsd\/tsd2012.html#SoutnerLMP12","2774":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2016.html#MiyamotoC16","2775":"http:\/\/dblp.uni-trier.de\/db\/journals\/taslp\/taslp27.html#ZhangZY19","2776":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr0001.html#cs-CL-0001021","2777":"http:\/\/dblp.uni-trier.de\/db\/conf\/asru\/asru2019.html#ChienK19a","2778":"http:\/\/dblp.uni-trier.de\/db\/journals\/actaC\/actaC23.html#ShynkarenkoK18","2779":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling2004.html#DuhK04","2780":"http:\/\/dblp.uni-trier.de\/db\/conf\/ictir\/ictir2018.html#LiMT18","2781":"http:\/\/dblp.uni-trier.de\/db\/conf\/gttse\/gttse2009.html#JezequelBF09","2782":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2003.html#LeePREH03","2783":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2003.html#abs-2003-02645","2784":"http:\/\/dblp.uni-trier.de\/db\/conf\/lrec\/lrec2020.html#GuoDVA20","2785":"http:\/\/dblp.uni-trier.de\/db\/conf\/iclr\/iclr2018w.html#WolfCD18","2786":"http:\/\/dblp.uni-trier.de\/db\/conf\/wsc\/wsc2016.html#BrownCGHLMPRZ16","2787":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2004.html#KhanY04","2788":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2003.html#MoriNI03","2789":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1999.html#OhlerHN99","2790":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/icslp1998.html#Valverde-AlbaceteP98","2791":"http:\/\/dblp.uni-trier.de\/db\/conf\/bis\/bis2008.html#Bassara08","2792":"","2793":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecir\/ecir2009.html#MaisonnasseGC09","2794":"http:\/\/dblp.uni-trier.de\/db\/conf\/siu\/siu2013.html#DikiciS13","2795":"http:\/\/dblp.uni-trier.de\/db\/conf\/medi\/medi2014.html#Almendros-JimenezILS14","2796":"http:\/\/dblp.uni-trier.de\/db\/journals\/jmm2\/jmm9.html#XuehelaitiLJY14","2797":"http:\/\/dblp.uni-trier.de\/db\/journals\/sosym\/sosym13.html#RoseKPPP14","2798":"http:\/\/dblp.uni-trier.de\/db\/conf\/ntcir\/ntcir2004.html#ZhangSZS04","2799":"","2800":"http:\/\/dblp.uni-trier.de\/db\/conf\/ispe\/ispe2003.html#Adachi03","2801":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1997.html#KneserPK97","2802":"http:\/\/dblp.uni-trier.de\/db\/journals\/tjs\/tjs74.html#BatkoK18","2803":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcnn\/ijcnn1990.html#YinT90","2804":"http:\/\/dblp.uni-trier.de\/db\/conf\/inex\/inex2011.html#LiW11","2805":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigir\/sigir2002.html#JinHZ02","2806":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigir\/sigir2004.html#GaoNWC04","2807":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr0901.html#abs-0901-2224","2808":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2010.html#abs-2010-06060","2809":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2018-1.html#WatanabeMFGIN18","2810":"","2811":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2013.html#ShiZCL13","2812":"http:\/\/dblp.uni-trier.de\/db\/conf\/cisis\/cisis2009.html#GorawskiC09","2813":"","2814":"http:\/\/dblp.uni-trier.de\/db\/conf\/icse\/icse2010-2.html#Badreddin10","2815":"http:\/\/dblp.uni-trier.de\/db\/conf\/cncl\/ccl2017.html#LuBG17","2816":"http:\/\/dblp.uni-trier.de\/db\/conf\/tsd\/tsd2018.html#LeheckaP18","2817":"http:\/\/dblp.uni-trier.de\/db\/conf\/icsr\/icsr2016.html#Hamid16","2818":"http:\/\/dblp.uni-trier.de\/db\/conf\/paclic\/paclic2014.html#WuM14","2819":"http:\/\/dblp.uni-trier.de\/db\/journals\/isci\/isci147.html#CriadoGMT02","2820":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1999.html#ClarksonR99","2821":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigmap\/sigmap2008.html#ZiolkoMWZ08","2822":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2013.html#Alumae13","2823":"http:\/\/dblp.uni-trier.de\/db\/conf\/aistats\/aistats2005.html#MorinB05","2824":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#SeneviratneY05","2825":"","2826":"http:\/\/dblp.uni-trier.de\/db\/conf\/gttse\/gttse2011.html#HeidenreichJKSW11","2827":"","2828":"http:\/\/dblp.uni-trier.de\/db\/conf\/ranlp\/ranlp2019.html#MasmoudiLEB19","2829":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1997.html#LinTCCL97","2830":"http:\/\/dblp.uni-trier.de\/db\/conf\/conll\/conll1998.html#KawasakiTT98","2831":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2011.html#abs-2011-07347","2832":"http:\/\/dblp.uni-trier.de\/db\/conf\/sepln\/tweet2015.html#Alfter15","2833":"http:\/\/fparreiras\/papers\/ModelWeavingDescriptionLa.pdf","2834":"http:\/\/dblp.uni-trier.de\/db\/journals\/tsp\/tsp37.html#BahlBSM89","2835":"http:\/\/dblp.uni-trier.de\/db\/conf\/nlpke\/nlpke2009.html#LvLY09","2836":"http:\/\/dblp.uni-trier.de\/db\/conf\/icra\/icra2012.html#TakanoN12","2837":"http:\/\/dblp.uni-trier.de\/db\/conf\/cncl\/ccl2013.html#XuehelaitiLJY13","2838":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#JenssonWIF05","2839":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling1980.html#AndrekaGN80","2840":"http:\/\/dblp.uni-trier.de\/db\/conf\/itat\/itat2016.html#Chaloupka16","2841":"https:\/\/www.semanticscholar.org\/paper\/f8a2dca1e8fe56e698984c077f7ff58d8ca867e9","2842":"http:\/\/arxiv.org\/abs\/1906.01539","2843":"https:\/\/arxiv.org\/abs\/2304.15004","2844":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/icslp1998.html#SeymoreCR98","2845":"","2846":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2007-4.html#ChinC07","2847":"http:\/\/dblp.uni-trier.de\/db\/conf\/models\/models2020.html#HackmanAHG20","2848":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1905.html#abs-1905-13358","2849":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2012.html#LuWFX12","2850":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2013.html#LiF13","2851":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp1989.html#FerrettiMS89","2852":"","2853":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2003-1.html#EmamiXJ03","2854":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/icslp1996.html#MillerA96","2855":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2010.html#BaiHML10","2856":"http:\/\/arxiv.org\/abs\/1901.02860","2857":"http:\/\/dblp.uni-trier.de\/db\/journals\/speech\/speech68.html#RasipuramM15","2858":"http:\/\/dblp.uni-trier.de\/db\/journals\/ipm\/ipm44.html#Li08","2859":"http:\/\/arxiv.org\/abs\/2210.03629","2860":"https:\/\/www.aclweb.org\/anthology\/2020.emnlp-main.154\/","2861":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2010.html#abs-2010-12283","2862":"http:\/\/dblp.uni-trier.de\/db\/conf\/iscslp\/iscslp2012.html#LuWFFX12","2863":"http:\/\/dblp.uni-trier.de\/db\/conf\/paclic\/paclic2008.html#Potisuk08","2864":"http:\/\/dblp.uni-trier.de\/db\/conf\/drr\/drr2010.html#HuangDC10","2865":"http:\/\/dblp.uni-trier.de\/db\/journals\/speech\/speech69.html#GongCL15","2866":"http:\/\/arxiv.org\/abs\/1812.01207","2867":"http:\/\/dblp.uni-trier.de\/db\/journals\/ijcpol\/ijcpol22.html#SelvamNT09","2868":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2002.html#NouzaD02","2869":"http:\/\/arxiv.org\/abs\/2010.00840","2870":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling2012.html#LiF12","2871":"http:\/\/arxiv.org\/abs\/1810.04805","2872":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2011.html#abs-2011-03023","2873":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2013.html#BayerR13","2874":"http:\/\/dblp.uni-trier.de\/db\/conf\/iceis\/iceis2005-3.html#SongHLL05","2875":"http:\/\/dblp.uni-trier.de\/db\/conf\/iotaas\/iotaas2018.html#ZhangLQW18","2876":"","2877":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1991.html#MathesonM91","2878":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1997.html#SeymoreR97","2879":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2006.html#Klakow06","2880":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2000.html#GaoLL00","2881":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2001.html#KimKW01","2882":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1711.html#abs-1711-06301","2883":"http:\/\/dblp.uni-trier.de\/db\/conf\/slt\/slt2012.html#MikolovZ12","2884":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2014.html#MalandrakisPHBFDN14","2885":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2019-1.html#Kim19","2886":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2019-1.html#PengSS19","2887":"http:\/\/dblp.uni-trier.de\/db\/conf\/noms\/noms2018.html#Zincir-HeywoodP18","2888":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr0108.html#cs-CL-0108023","2889":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2000.html#Goodman00","2890":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp1995.html#AntoniolBCF95","2891":"http:\/\/dblp.uni-trier.de\/db\/conf\/airweb\/airweb2009.html#Martinez-RomoA09","2892":"http:\/\/dblp.uni-trier.de\/db\/conf\/wmt\/wmt2018.html#StahlbergCS18","2893":"http:\/\/dblp.uni-trier.de\/db\/conf\/wmt\/wmt2011.html#Heafield11","2894":"http:\/\/dblp.uni-trier.de\/db\/conf\/eacl\/eacl1991.html#Neumann91","2895":"http:\/\/dblp.uni-trier.de\/db\/conf\/conll\/conll2009.html#ConnorGFR09","2896":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2013-2.html#HeafieldPCK13","2897":"http:\/\/dblp.uni-trier.de\/db\/conf\/airs\/airs2006.html#GaoZLL06","2898":"http:\/\/dblp.uni-trier.de\/db\/conf\/tsd\/tsd1999.html#ZibertGDM99","2899":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2010.html#abs-2010-11524","2900":"http:\/\/dblp.uni-trier.de\/db\/conf\/gi\/gi2014.html#Kuryazov14","2901":"http:\/\/dblp.uni-trier.de\/db\/journals\/ijis\/ijis14.html#Buchheit99","2902":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2002.html#Torres-CarrasquilloRD02","2903":"http:\/\/dblp.uni-trier.de\/db\/conf\/compsac\/compsac2003.html#Wu-dongKYHY03","2904":"http:\/\/dblp.uni-trier.de\/db\/journals\/stt\/stt27.html#Storrle07","2905":"http:\/\/dblp.uni-trier.de\/db\/conf\/simbig\/simbig2017.html#Espichan-Linares17","2906":"http:\/\/dblp.uni-trier.de\/db\/journals\/jss\/jss36.html#MatzenGH97","2907":"http:\/\/dblp.uni-trier.de\/db\/journals\/csl\/csl25.html#WatanabeITSA11","2908":"http:\/\/dblp.uni-trier.de\/db\/conf\/eacl\/eacl2017-1.html#BodC17","2909":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2004.html#abs-2004-14253","2910":"","2911":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2015.html#MoriokaIHK15","2912":"http:\/\/dblp.uni-trier.de\/db\/conf\/icdar\/mocr2017.html#SabirRN17","2913":"http:\/\/dblp.uni-trier.de\/db\/journals\/eswa\/eswa38.html#SunWSL11","2914":"http:\/\/dblp.uni-trier.de\/db\/conf\/hotswup\/hotswup2008.html#DuquesneB08","2915":"http:\/\/dblp.uni-trier.de\/db\/conf\/ictir\/ictir2013.html#KarimzadehganZE13","2916":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1995.html#BrugnaraC95","2917":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/icslp1996.html#BrugnaraF96","2918":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2003.html#FangGLS03","2919":"","2920":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaai\/aaai80.html#Selfridge80","2921":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2008.html#ChuehC08","2922":"http:\/\/www.omg.org\/spec\/MOFM2T\/1.0\/PDF","2923":"","2924":"","2925":"http:\/\/www.isrl.uiuc.edu\/~amag\/langev\/paper\/niyogi97aDynamical.html","2926":"http:\/\/dblp.uni-trier.de\/db\/conf\/icse\/icse2015-1.html#NguyenN15","2927":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1811.html#abs-1811-06477","2928":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1901.html#abs-1901-11167","2929":"http:\/\/dblp.uni-trier.de\/db\/conf\/cikm\/cikm2005.html#TaoWMZ05","2930":"http:\/\/dblp.uni-trier.de\/db\/conf\/www\/www2006.html#BenczurBCU06","2931":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1709.html#abs-1709-02279","2932":"http:\/\/dblp.uni-trier.de\/db\/conf\/aid\/aid1998.html#Lansdown98","2933":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#Sicilia-GarciaMS05","2934":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#MassonieNL05","2935":"http:\/\/dblp.uni-trier.de\/db\/conf\/vl\/vlhcc2009.html#Storrle09","2936":"http:\/\/dblp.uni-trier.de\/db\/journals\/glottometrics\/glottometrics35.html#Coloma16","2937":"","2938":"http:\/\/www.linguapax.org\/congres\/taller\/taller2\/Merkiene.html","2939":"http:\/\/dblp.uni-trier.de\/db\/conf\/sac\/sac93.html#AthertonL93","2940":"http:\/\/dblp.uni-trier.de\/db\/conf\/ifip12\/iip2004.html#JiaYM04","2941":"http:\/\/dblp.uni-trier.de\/db\/conf\/icmi\/icmi2019.html#Tanl19","2942":"http:\/\/dblp.uni-trier.de\/db\/journals\/pcs\/pcs33.html#BabichevL07","2943":"http:\/\/www.aclweb.org\/anthology\/P14-2002","2944":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2009.html#HsuG09","2945":"http:\/\/dblp.uni-trier.de\/db\/conf\/ifip\/ifip94-1.html#GlavanR94","2946":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2010.html#abs-2010-01869","2947":"http:\/\/dblp.uni-trier.de\/db\/conf\/models\/models2017s.html#Alwidian17","2948":"http:\/\/dblp.uni-trier.de\/db\/reference\/ap\/is2002.html#BohlenJ02","2949":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2004-d.html#WangWA04","2950":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcnlp\/ijcnlp2011wlr.html#IshidaMKI11","2951":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1911.html#abs-1911-03937","2952":"http:\/\/dblp.uni-trier.de\/db\/conf\/lrec\/lrec2016.html#TambouratzisP16","2953":"http:\/\/dblp.uni-trier.de\/db\/conf\/tacas\/tacas2015.html#KantLMPBD15","2954":"http:\/\/dblp.uni-trier.de\/db\/conf\/icpr\/icpr2010.html#AkceB10","2955":"http:\/\/dblp.uni-trier.de\/db\/conf\/wsc\/wsc2006.html#BulatewiczC06","2956":"http:\/\/dblp.uni-trier.de\/db\/conf\/slt\/slt2008.html#ChienC08","2957":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl-jssp\/acl-jssp2013.html#Popescu13","2958":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl1991.html#JelinekMRS91","2959":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2006.html#TaoWMZ06","2960":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2001.html#ChelbaM01","2961":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcnlp\/ijcnlp2017-2.html#ArnoldCK17","2962":"http:\/\/dblp.uni-trier.de\/db\/conf\/sle\/sle2008.html#Brand08","2963":"http:\/\/dblp.uni-trier.de\/db\/conf\/jsai\/jsai2004.html#ShinozawaS04","2964":"http:\/\/dblp.uni-trier.de\/db\/journals\/tois\/tois27.html#XueHYY09","2965":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2001.html#abs-2001-06286","2966":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2014-2.html#FineFJD14","2967":"","2968":"http:\/\/dblp.uni-trier.de\/db\/conf\/icist\/icist2015.html#KiriginMM15","2969":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2002.html#abs-2002-08909","2970":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2011.html#abs-2011-01513","2971":"http:\/\/www.isrl.uiuc.edu\/~amag\/langev\/paper\/stauffer06languageCompetition.html","2972":"http:\/\/dblp.uni-trier.de\/db\/conf\/konvens\/konvens2014.html#NohP14","2973":"http:\/\/dblp.uni-trier.de\/db\/journals\/speech\/speech42.html#Bellegarda04","2974":"http:\/\/dblp.uni-trier.de\/db\/journals\/speech\/speech41.html#YamamotoIS03","2975":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1908.html#abs-1908-09738","2976":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1705.html#TangWCLA17","2977":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2011.html#MikolovKBCK11","2978":"http:\/\/dblp.uni-trier.de\/db\/conf\/icann\/icann2002.html#KurimoL02","2979":"http:\/\/dblp.uni-trier.de\/db\/conf\/esorics\/esorics2003.html#HeldalH03","2980":"http:\/\/dblp.uni-trier.de\/db\/conf\/lrec\/lrec2012.html#ChoukriA12","2981":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2010.html#NgLLML10","2982":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1806.html#abs-1806-10215","2983":"http:\/\/dblp.uni-trier.de\/db\/journals\/bjmc\/bjmc4.html#KalninsKSCT16","2984":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2013.html#SvecSI13","2985":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2006-1.html#WanH06","2986":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2014.html#HaznedarogluA14","2987":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp1998.html#Klakow98","2988":"http:\/\/dblp.uni-trier.de\/db\/conf\/iscslp\/iscslp2018.html#BaiTYWF18","2989":"http:\/\/dblp.uni-trier.de\/db\/conf\/slt\/slt2016.html#VassermanHA16","2990":"http:\/\/dblp.uni-trier.de\/db\/conf\/compsac\/compsac1989.html#LiangYL89","2991":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#Dieguez-TiradoGL05","2992":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2003.html#KimK03a","2993":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2010.html#MomtaziFK10","2994":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1999.html#Federico99","2995":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1999.html#ChelbaJ99","2996":"http:\/\/dblp.uni-trier.de\/db\/conf\/mdeis\/mdeis2006.html#EstevezPSR06","2997":"http:\/\/dblp.uni-trier.de\/db\/conf\/wsc\/wsc1977.html#Sherden77","2998":"http:\/\/dblp.uni-trier.de\/db\/journals\/scp\/scp73.html#CepaM08","2999":"","3000":"","3001":"","3002":"http:\/\/dblp.uni-trier.de\/db\/conf\/staf\/staf2016w.html#VoS16","3003":"http:\/\/dblp.uni-trier.de\/db\/conf\/ictac\/ictac2010.html#LiZH10","3004":"http:\/\/dblp.uni-trier.de\/db\/conf\/fmco\/fmco2010.html#LienhardtLBSZWSP10","3005":"http:\/\/dblp.uni-trier.de\/db\/conf\/icmi\/icmi2000.html#ZengWW00","3006":"http:\/\/dblp.uni-trier.de\/db\/conf\/caise\/caise2011.html#FriedrichMP11","3007":"http:\/\/dblp.uni-trier.de\/db\/journals\/talip\/talip5.html#GaoSY06","3008":"http:\/\/dblp.uni-trier.de\/db\/journals\/fuin\/fuin129.html#WisniewskiS14","3009":"http:\/\/dblp.uni-trier.de\/db\/journals\/compsys\/compsys11.html#NiyogiB97","3010":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1993.html#WrightJL93","3011":"","3012":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2005.html#abs-2005-00175","3013":"http:\/\/dblp.uni-trier.de\/db\/journals\/lncs\/lncs910.html#Rauzy94","3014":"http:\/\/dblp.uni-trier.de\/db\/conf\/waim\/waim2006.html#BaoZCLLY06","3015":"http:\/\/dblp.uni-trier.de\/db\/conf\/icgi\/icgi1994.html#DennisWS94","3016":"http:\/\/dblp.uni-trier.de\/db\/conf\/um\/um2007.html#MartinN07","3017":"http:\/\/dblp.uni-trier.de\/db\/conf\/tsd\/tsd2016.html#Kanis16","3018":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigir\/sigir2009.html#ZhaoY09","3019":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigir\/sigir2002.html#JinSHC02","3020":"http:\/\/dblp.uni-trier.de\/db\/conf\/iccip\/iccip2017.html#TapsaiMH17","3021":"http:\/\/dblp.uni-trier.de\/db\/conf\/codaspy\/abac2017.html#Turner17","3022":"http:\/\/dblp.uni-trier.de\/db\/conf\/cikm\/cikm99.html#SongC99","3023":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1804.html#abs-1804-09661","3024":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2010s.html#MooreL10","3025":"http:\/\/dblp.uni-trier.de\/db\/conf\/cost\/cost2010s.html#StasHPJ10","3026":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1707.html#MendesS17","3027":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecbs\/ecbs2012.html#FeherL12","3028":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2010.html#SnyderBK10","3029":"http:\/\/dblp.uni-trier.de\/db\/journals\/ijon\/ijon70.html#GaragnaniWP07","3030":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2013.html#Martinez-VillarongaAAJ13","3031":"http:\/\/dblp.uni-trier.de\/db\/journals\/mt\/mt4.html#Calder89","3032":"http:\/\/dblp.uni-trier.de\/db\/conf\/clic-it\/clic-it2017.html#GiachanouPCR17","3033":"http:\/\/dblp.uni-trier.de\/db\/conf\/vldb\/vldb2001.html#GalhardasFSSS01","3034":"http:\/\/www.omg.org\/spec\/MOFM2T\/1.0\/","3035":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2010.html#NeubigMMK10","3036":"","3037":"http:\/\/dblp.uni-trier.de\/db\/conf\/itcc\/itcc2004-2.html#TaghvaCPN04","3038":"http:\/\/dblp.uni-trier.de\/db\/conf\/icse\/mise2013.html#TomassettiVTVK13","3039":"http:\/\/dblp.uni-trier.de\/db\/conf\/nodalida\/nodalida2017.html#VenekoskiV17","3040":"http:\/\/dblp.uni-trier.de\/db\/journals\/jot\/jot11.html#LangerWGKV12","3041":"https:\/\/doi.org\/10.1145\/584792.584854","3042":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1608.html#DamTP16","3043":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1410.html#KhoufiAB14","3044":"http:\/\/dblp.uni-trier.de\/db\/conf\/enlg\/enlg2001.html#HumphreysCW01","3045":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcnlp\/ijcnlp2008.html#LeeNL08","3046":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl-sighan\/acl-sighan2005.html#ChenZZS05","3047":"http:\/\/dblp.uni-trier.de\/db\/conf\/eurosim\/eurosim1996.html#KeaneX96","3048":"","3049":"","3050":"http:\/\/dblp.uni-trier.de\/db\/conf\/iastedCI\/ci2006.html#PrazakPHKMP06","3051":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2010.html#ChuehC10","3052":"http:\/\/dblp.uni-trier.de\/db\/conf\/caise\/ciao2008.html#MerunkaNB08","3053":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2003.html#Emami03","3054":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2001.html#WuZJW01","3055":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1999.html#Reichl99","3056":"http:\/\/dblp.uni-trier.de\/db\/conf\/smc\/smc2004-1.html#GabelL04","3057":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2011.html#LeOAGY11","3058":"http:\/\/dblp.uni-trier.de\/db\/conf\/icis\/icis1983.html#Blanning83","3059":"http:\/\/dblp.uni-trier.de\/db\/conf\/webist\/webist2015.html#GiakoumiMP15","3060":"","3061":"http:\/\/dblp.uni-trier.de\/db\/conf\/icsr\/icsr1994.html#KumenoTOH94","3062":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2008.html#ShiCLKLQ08","3063":"http:\/\/dblp.uni-trier.de\/db\/conf\/fedcsis\/fedcsis2017.html#WolkWM17a","3064":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2013.html#SakBNA13","3065":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2003-1.html#ChenGLA03","3066":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2000.html#ZhangBFS00","3067":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp1995.html#RaoMR95","3068":"http:\/\/dblp.uni-trier.de\/db\/conf\/aclnut\/aclnut2019.html#SunJ19","3069":"http:\/\/dblp.uni-trier.de\/db\/conf\/wanlp\/wanlp2019.html#ElJundiADHES19","3070":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr0001.html#cs-CL-0001022","3071":"http:\/\/dblp.uni-trier.de\/db\/conf\/icwe\/icwe2005.html#XuZL05","3072":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/ijcai81.html#Selfridge81","3073":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2017.html#MaNBK17","3074":"http:\/\/dblp.uni-trier.de\/db\/journals\/isci\/isci237.html#ZamaniB13","3075":"http:\/\/dblp.uni-trier.de\/db\/conf\/euromicro\/euromicro2014.html#Dohrmann14","3076":"http:\/\/dblp.uni-trier.de\/db\/conf\/fskd\/fskd2015.html#YueGRW15","3077":"http:\/\/dblp.uni-trier.de\/db\/conf\/wcsp\/wcsp2015.html#YueRWY15","3078":"http:\/\/dblp.uni-trier.de\/db\/journals\/ibmrd\/ibmrd22.html#SibuyaFT78","3079":"http:\/\/dblp.uni-trier.de\/db\/conf\/icphs\/icphs2015.html#DuranB0D15","3080":"http:\/\/dblp.uni-trier.de\/db\/journals\/ieicet\/ieicet91d.html#WuLSY08","3081":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling2004.html#ZhangSQS04","3082":"http:\/\/dblp.uni-trier.de\/db\/journals\/jss\/jss55.html#KimRK00","3083":"http:\/\/dblp.uni-trier.de\/db\/conf\/conll\/conll2019.html#PengNR19","3084":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2003.html#LeBBC03","3085":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2015.html#MoritaKK15","3086":"http:\/\/dblp.uni-trier.de\/db\/conf\/cncl\/ccl2013.html#WangSZY13","3087":"http:\/\/dblp.uni-trier.de\/db\/series\/asc\/asc95.html#SasZ11","3088":"http:\/\/dblp.uni-trier.de\/db\/journals\/ejasmp\/ejasmp2008.html#JenssonIF08","3089":"http:\/\/dblp.uni-trier.de\/db\/journals\/taslp\/taslp1.html#ChienCL93","3090":"http:\/\/arxiv.org\/abs\/1903.02482","3091":"http:\/\/arxiv.org\/abs\/2307.09702","3092":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2019-1.html#MaCH19","3093":"http:\/\/dblp.uni-trier.de\/db\/journals\/sigplan\/sigplan39.html#BabuR04","3094":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2007.html#OhtaTN07","3095":"http:\/\/dblp.uni-trier.de\/db\/conf\/tag\/tag2000.html#BangaloreR00","3096":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2020.html#TakahashiSN20","3097":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2004.html#Rashwan04","3098":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1811.html#abs-1811-02134","3099":"http:\/\/dblp.uni-trier.de\/db\/conf\/gi\/gi2010-2.html#Petrasch10","3100":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1510.html#SrivastavaVVS15","3101":"http:\/\/dblp.uni-trier.de\/db\/conf\/bigcomp\/bigcomp2020.html#LimC20a","3102":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2019.html#InagumaCBKW19","3103":"http:\/\/dblp.uni-trier.de\/db\/conf\/igarss\/igarss2012.html#WangLL12","3104":"http:\/\/dblp.uni-trier.de\/db\/journals\/ijprai\/ijprai23.html#MaucecRKB09","3105":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2020.html#Price20","3106":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2010.html#abs-2010-13826","3107":"http:\/\/ieeexplore.ieee.org\/xpl\/freeabs_all.jsp?arnumber=6006828","3108":"http:\/\/dblp.uni-trier.de\/db\/conf\/www\/www2019.html#Kawamae19a","3109":"http:\/\/www.amazon.de\/gp\/redirect.html%3FASIN=0321179366%26tag=ws%26lcode=xm2%26cID=2025%26ccmID=165953%26location=\/Object-Constraint-Language-Addison-Wesley-Technology\/dp\/0321179366%253FSubscriptionId=13CT5CVB80YFWJEPWS02","3110":"http:\/\/dblp.uni-trier.de\/db\/journals\/sigplan\/sigplan26.html#Klug91","3111":"http:\/\/dblp.uni-trier.de\/db\/conf\/cicling\/cicling2008.html#Alumae08","3112":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2000.html#MaucecKH00","3113":"http:\/\/arxiv.org\/abs\/2203.11370","3114":"http:\/\/arxiv.org\/abs\/2307.02390","3115":"http:\/\/dblp.uni-trier.de\/db\/conf\/im\/im2001.html#DulayLSD01","3116":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigplan\/sigplan1976.html#JohnsonM76","3117":"http:\/\/dblp.uni-trier.de\/db\/series\/faia\/faia130.html#SmialekK05","3118":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2020.html#ChuangSL20","3119":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2006.html#abs-2006-08097","3120":"http:\/\/dblp.uni-trier.de\/db\/conf\/bica\/bica2018.html#Jackson18","3121":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/icslp1994.html#KawaharaMKD94","3122":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#Salvi05","3123":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2006.html#ChanT06","3124":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2000.html#AkibaI00","3125":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2010.html#YamamotoHMS10","3126":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2010.html#LiuGW10","3127":"https:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-72044-9_2","3128":"http:\/\/dblp.uni-trier.de\/db\/conf\/bda\/bda1992.html#AmannS92","3129":"http:\/\/dblp.uni-trier.de\/db\/conf\/isw\/isc2014.html#YuanZL14","3130":"http:\/\/dblp.uni-trier.de\/db\/conf\/ceec\/ceec2017.html#Kihlman17","3131":"http:\/\/dblp.uni-trier.de\/db\/conf\/qrs\/qrs2015c.html#YaoZ15","3132":"http:\/\/dblp.uni-trier.de\/db\/conf\/iscslp\/iscslp2014.html#LaurentHL14","3133":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1912.html#abs-1912-05421","3134":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1912.html#abs-1912-13283","3135":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2017.html#OndelBCK17","3136":"http:\/\/dblp.uni-trier.de\/db\/conf\/ismis\/ismis2012.html#BrockiMK12","3137":"http:\/\/dblp.uni-trier.de\/db\/conf\/modelsward\/modelsward2019s.html#ZhaoAM19a","3138":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2012.html#HeafieldKL12","3139":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2018-2.html#ZhaoLA18","3140":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1908.html#abs-1908-11685","3141":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling2008.html#SchulerAMS08","3142":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling1988-1.html#GasserD88","3143":"http:\/\/dblp.uni-trier.de\/db\/conf\/ijcai\/ijcai75.html#Bian75","3144":"http:\/\/dblp.uni-trier.de\/db\/conf\/forte\/forte2009.html#GronnigerRR09","3145":"http:\/\/dblp.uni-trier.de\/db\/conf\/iscide\/iscide2017.html#CaoY17","3146":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1909.html#abs-1909-08229","3147":"http:\/\/dblp.uni-trier.de\/db\/conf\/sofsem\/sofsem2005.html#Poniszewska-MarandaGH05","3148":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1909.html#abs-1909-04761","3149":"http:\/\/dblp.uni-trier.de\/db\/conf\/cicling\/cicling2003.html#CasillasVT03","3150":"http:\/\/dblp.uni-trier.de\/db\/conf\/iccv\/iccv2003-1.html#Quek03","3151":"http:\/\/dblp.uni-trier.de\/db\/conf\/adma\/adma2013-2.html#LouLY13","3152":"http:\/\/dblp.uni-trier.de\/db\/conf\/ppam\/ppam2017-2.html#BatkoK17","3153":"http:\/\/dblp.uni-trier.de\/db\/conf\/kgswc\/kgswc2019.html#Gillis-WebberTK19","3154":"http:\/\/dblp.uni-trier.de\/db\/journals\/lncs\/lncs759.html#ElmasriK93","3155":"http:\/\/dblp.uni-trier.de\/db\/journals\/sosym\/sosym5.html#AgrawalKNSV06","3156":"http:\/\/dblp.uni-trier.de\/db\/journals\/sosym\/sosym17.html#AcretoaieSS18","3157":"http:\/\/dblp.uni-trier.de\/db\/journals\/kbs\/kbs200.html#ChenJS20","3158":"http:\/\/dblp.uni-trier.de\/db\/journals\/advcs\/advcs15.html#Blythe12a","3159":"http:\/\/dblp.uni-trier.de\/db\/journals\/jucs\/jucs16.html#Ma10","3160":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2020-1.html#ChiangHL20","3161":"http:\/\/dblp.uni-trier.de\/db\/conf\/iwslt\/iwslt2011.html#HeafieldHKKF11","3162":"http:\/\/dblp.uni-trier.de\/db\/conf\/slte\/slte2009.html#ChengT09","3163":"http:\/\/dx.doi.org\/10.1007\/s10270-006-0027-7","3164":"http:\/\/dblp.uni-trier.de\/db\/conf\/serp\/serp2007-2.html#AlamA07","3165":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaai\/aaai2004.html#TranB04","3166":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2003.html#abs-2003-00104","3167":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2005.html#abs-2005-09207","3168":"http:\/\/dblp.uni-trier.de\/db\/conf\/www\/www2017c.html#ZouLLRYC17","3169":"http:\/\/dblp.uni-trier.de\/db\/conf\/lrec\/lrec2020.html#LeVFSCLACBS20","3170":"http:\/\/dblp.uni-trier.de\/db\/conf\/icci\/icci1993.html#OudshoornM93","3171":"","3172":"","3173":"","3174":"http:\/\/dblp.uni-trier.de\/db\/journals\/jql\/jql21.html#Coloma14","3175":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2005.html#SchulerM05","3176":"http:\/\/dblp.uni-trier.de\/db\/conf\/amia\/amia1999.html#RassinouxBRPR99","3177":"http:\/\/dblp.uni-trier.de\/db\/conf\/ccece\/ccece2011.html#HaidarO11","3178":"http:\/\/dblp.uni-trier.de\/db\/conf\/cvpr\/cvpr2016.html#RichardG16","3179":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1901.html#abs-1901-09128","3180":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigir\/sigir2007.html#MaisonnasseGC07","3181":"http:\/\/dblp.uni-trier.de\/db\/conf\/icmlc\/icmlc2012.html#XuXW12","3182":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1108.html#abs-1108-1275","3183":"http:\/\/dblp.uni-trier.de\/db\/conf\/gcce\/gcce2018.html#ShaoKK18","3184":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1812.html#abs-1812-04647","3185":"http:\/\/dblp.uni-trier.de\/db\/conf\/se\/se2016.html#LeopoldMP16","3186":"","3187":"","3188":"http:\/\/dblp.uni-trier.de\/db\/conf\/bibm\/bibm2019.html#LiFZZHJ19","3189":"http:\/\/dblp.uni-trier.de\/db\/conf\/eacl\/eacl1985.html#IzumidaIYHM85","3190":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling1980.html#Langley80","3191":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2007.html#Huggins-DainesR07","3192":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1801.html#abs-1801-01828","3193":"http:\/\/dblp.uni-trier.de\/db\/journals\/jis\/jis43.html#KarimiS17","3194":"http:\/\/dblp.uni-trier.de\/db\/conf\/caise\/caise96.html#GalES96","3195":"http:\/\/dblp.uni-trier.de\/db\/conf\/membrane\/membrane2004.html#UedaK04","3196":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2018.html#RasooliP18","3197":"http:\/\/dblp.uni-trier.de\/db\/conf\/csa2\/csa2017.html#LinLGZR17","3198":"","3199":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp1989.html#PaeselerN89","3200":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1809.html#abs-1809-01201","3201":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigir\/sigir2010.html#MomtaziK10","3202":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl1986.html#VaughanM86","3203":"http:\/\/dblp.uni-trier.de\/db\/conf\/modellierung\/modellierung2014.html#SeidlSA14","3204":"","3205":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2004.html#abs-2004-13897","3206":"http:\/\/dblp.uni-trier.de\/db\/journals\/amcs\/amcs23.html#SasZ13","3207":"http:\/\/dblp.uni-trier.de\/db\/journals\/taslp\/taslp20.html#NaptaliTN12","3208":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2004-1.html#EmamiJ04","3209":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2000.html#Klakow00","3210":"http:\/\/dblp.uni-trier.de\/db\/conf\/apsec\/apsec2018.html#ZhouZS18","3211":"http:\/\/dblp.uni-trier.de\/db\/conf\/nss\/nss2010.html#LiW10","3212":"https:\/\/practise.cs.tut.fi\/files\/publications\/Art\/petri_selonen_phd_thesis-all.pdf","3213":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2004.html#BigiHM04","3214":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2000.html#GoodmanG00","3215":"http:\/\/dblp.uni-trier.de\/db\/conf\/time\/time1998.html#BecherCE98","3216":"http:\/\/dblp.uni-trier.de\/db\/conf\/iwslt\/iwslt2009.html#Sanchis-TrillesCBF09","3217":"http:\/\/dblp.uni-trier.de\/db\/conf\/csie\/csie2009-7.html#SunYN09","3218":"http:\/\/dblp.uni-trier.de\/db\/conf\/ialp\/ialp2011.html#ZhouY11","3219":"http:\/\/dblp.uni-trier.de\/db\/conf\/icdar\/icdar2009.html#WangYL09","3220":"","3221":"http:\/\/dblp.uni-trier.de\/db\/conf\/dba\/dba2005.html#ErozelCC05","3222":"","3223":"http:\/\/dblp.uni-trier.de\/db\/conf\/iccsa\/iccsa2008-2.html#PozoCG08","3224":"http:\/\/dblp.uni-trier.de\/db\/conf\/nlpke\/nlpke2009.html#JiangW09","3225":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigmap\/sigmap2011.html#WuYLL11","3226":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1904.html#abs-1904-09521","3227":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2011.html#AllauzenR11","3228":"http:\/\/dblp.uni-trier.de\/db\/conf\/sac\/sac1998.html#ChanVSGA98","3229":"http:\/\/dblp.uni-trier.de\/db\/conf\/sofsem\/sofsem2013.html#Almendros-JimenezI13","3230":"http:\/\/dblp.uni-trier.de\/db\/conf\/icmi\/icmi2000.html#ZhangW00","3231":"http:\/\/dblp.uni-trier.de\/db\/journals\/titb\/titb24.html#HoogiMGDR20","3232":"http:\/\/dblp.uni-trier.de\/db\/conf\/bigmm\/bigmm2020.html#VermaB20","3233":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2020.html#GandheR20","3234":"http:\/\/dblp.uni-trier.de\/db\/conf\/qest\/qest2009.html#DonaldsonMP09","3235":"http:\/\/dblp.uni-trier.de\/db\/journals\/vlsisp\/vlsisp92.html#BaiYTWF20","3236":"http:\/\/dblp.uni-trier.de\/db\/conf\/icaart\/icaart2011-2.html#FigueiredoS11","3237":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1999.html#ItoKO99","3238":"http:\/\/dblp.uni-trier.de\/db\/conf\/flairs\/flairs2004.html#Ball04","3239":"","3240":"http:\/\/dblp.uni-trier.de\/db\/conf\/iri\/iri2007.html#ZhangBP07","3241":"","3242":"","3243":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2004.html#WongS04","3244":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2004.html#LussierWF04","3245":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2009.html#WhiteRKJ09","3246":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2001.html#CaseiroT01","3247":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2001.html#ChenGLAA01","3248":"http:\/\/dblp.uni-trier.de\/db\/conf\/iccsa\/iccsa2012-w.html#Silva12","3249":"http:\/\/dblp.uni-trier.de\/db\/conf\/cogsci\/cogsci2011.html#Ball11","3250":"http:\/\/dblp.uni-trier.de\/db\/conf\/compsystech\/compsystech2008.html#Kremenska08","3251":"http:\/\/dblp.uni-trier.de\/db\/conf\/wims\/wims2016.html#JeanHRBM16","3252":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2012.html#WuADHK12","3253":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2008.html#HsuG08","3254":"http:\/\/dblp.uni-trier.de\/db\/conf\/riao\/riao2000.html#AuzanneGFF00","3255":"","3256":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2008.html#abs-2008-06208","3257":"http:\/\/dblp.uni-trier.de\/db\/conf\/cikm\/cikm2009.html#HuangSN09","3258":"http:\/\/dblp.uni-trier.de\/db\/conf\/icmt\/icmt2015.html#Hinkel15","3259":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2020.html#ChenECLW20","3260":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2008.html#abs-2008-01832","3261":"http:\/\/dblp.uni-trier.de\/db\/conf\/grc\/grc2008.html#XieRY08","3262":"http:\/\/dblp.uni-trier.de\/db\/journals\/entcs\/entcs229.html#MarthR09","3263":"http:\/\/dblp.uni-trier.de\/db\/journals\/speech\/speech54.html#KurataSRRIN12","3264":"http:\/\/dblp.uni-trier.de\/db\/conf\/models\/docsym2014.html#Kokaly14","3265":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr0103.html#cs-CL-0103007","3266":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2009.html#abs-2009-05713","3267":"http:\/\/dblp.uni-trier.de\/db\/journals\/sosym\/sosym9.html#Kurtev10","3268":"http:\/\/dblp.uni-trier.de\/db\/journals\/ijgcrsis\/ijgcrsis1.html#XieRY09","3269":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2009.html#DalviKPT09","3270":"http:\/\/dblp.uni-trier.de\/db\/conf\/aaai\/aaai2020.html#LiLBLL20","3271":"http:\/\/dblp.uni-trier.de\/db\/conf\/kbse\/ase2017.html#YangJ0SGL17","3272":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2005.html#abs-2005-00796","3273":"http:\/\/dblp.uni-trier.de\/db\/conf\/pricai\/pricai2000.html#Iwahashi00","3274":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2007.html#OkanoharaT07","3275":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2007.html#LiuL07","3276":"http:\/\/dblp.uni-trier.de\/db\/books\/collections\/Papazoglou2000.html#LiddleEW00","3277":"http:\/\/dblp.uni-trier.de\/db\/conf\/iwcs\/iwcs2013.html#Mitchell13","3278":"http:\/\/dblp.uni-trier.de\/db\/conf\/apsec\/apsec2017.html#TranCA17","3279":"http:\/\/dblp.uni-trier.de\/db\/conf\/ideal\/ideal2011.html#QuesadaBC11","3280":"http:\/\/dblp.uni-trier.de\/db\/conf\/qrs\/qrs2016c.html#ZhangMC16","3281":"http:\/\/dblp.uni-trier.de\/db\/conf\/slt\/slt2016.html#ScheinerWA16","3282":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl-alta\/acl-alta2018.html#Hepburn18","3283":"http:\/\/dblp.uni-trier.de\/db\/conf\/sera\/sera2018.html#MeftehBB18","3284":"http:\/\/dblp.uni-trier.de\/db\/conf\/naacl\/naacl2006.html#Erkan06","3285":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2011.html#Subotin11","3286":"http:\/\/dblp.uni-trier.de\/db\/conf\/cicling\/cicling2007.html#FrancoS07","3287":"http:\/\/dblp.uni-trier.de\/db\/conf\/cicling\/cicling2016-1.html#MenacerBZS16","3288":"http:\/\/dblp.uni-trier.de\/db\/conf\/sle\/sle2014.html#TisiJDSC14","3289":"http:\/\/dblp.uni-trier.de\/db\/conf\/jcit\/jcit1990.html#LeveneP90","3290":"http:\/\/dblp.uni-trier.de\/db\/conf\/icdm\/icdmw2009.html#GengYX09","3291":"http:\/\/dblp.uni-trier.de\/db\/conf\/icmai\/icmai2002.html#Bod02","3292":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2016-1.html#SettlesM16","3293":"http:\/\/dblp.uni-trier.de\/db\/journals\/debu\/debu11.html#Larson88","3294":"http:\/\/dblp.uni-trier.de\/db\/conf\/trec\/trec2015.html#Cummins15","3295":"http:\/\/dblp.uni-trier.de\/db\/journals\/jasis\/jasis54.html#NiemiHJ03","3296":"http:\/\/dblp.uni-trier.de\/db\/journals\/cogsr\/cogsr1.html#Buchheit00","3297":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1607.html#LiuQH16b","3298":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2010.html#abs-2010-16037","3299":"http:\/\/dblp.uni-trier.de\/db\/conf\/models\/models2020c.html#LanoFUY20","3300":"http:\/\/hdl.handle.net\/1808\/27451","3301":"http:\/\/dblp.uni-trier.de\/db\/conf\/iser\/iser2010.html#TakanoKN10","3302":"http:\/\/dblp.uni-trier.de\/db\/conf\/medinfo\/medinfo2013.html#BarzdinsBRS13","3303":"http:\/\/dblp.uni-trier.de\/db\/conf\/ssdbm\/ssdbm83.html#Michalewicz83","3304":"","3305":"http:\/\/dblp.uni-trier.de\/db\/journals\/isci\/isci49.html#NavatheA89","3306":"","3307":"","3308":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/icslp1992.html#WaegnerY92","3309":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/eurospeech1997.html#ChelbaEJJKMPRRSW97","3310":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2007.html#MerkelK07","3311":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2007.html#WangS07","3312":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2019-1.html#BeltagyLC19","3313":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1912.html#abs-1912-09637","3314":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1912.html#abs-1912-03363","3315":"http:\/\/dblp.uni-trier.de\/db\/conf\/trec\/trec2012.html#WeiZLW12","3316":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2004.html#Nomoto04","3317":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1501.html#XuC15","3318":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1607.html#Bradley16a","3319":"http:\/\/dblp.uni-trier.de\/db\/conf\/cosit\/cosit2011.html#TenbrinkK11","3320":"http:\/\/dblp.uni-trier.de\/db\/conf\/jurix\/aicol2009.html#PalmiraniB09","3321":"http:\/\/dblp.uni-trier.de\/db\/conf\/iwann\/iwann2015-1.html#Becerra-Bonache15","3322":"http:\/\/dblp.uni-trier.de\/db\/conf\/iwaipr\/iwaipr2018.html#QuinteroG18","3323":"http:\/\/dblp.uni-trier.de\/db\/conf\/cicling\/cicling2003.html#PinedaMPV03","3324":"http:\/\/dblp.uni-trier.de\/db\/conf\/wadt\/wadt2016.html#Mossakowski16","3325":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigsoft\/fse2013.html#NguyenNNN13","3326":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigmod\/sigmod88.html#CareyDV88","3327":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr1606.html#ParikhT0U16","3328":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2020-s.html#PanLYD20","3329":"http:\/\/dblp.uni-trier.de\/db\/journals\/finr\/finr14.html#HeinrichYHLHKWW20","3330":"http:\/\/dblp.uni-trier.de\/db\/journals\/jcamd\/jcamd31.html#IkebataHIMY17","3331":"http:\/\/dblp.uni-trier.de\/db\/journals\/alr\/alr24.html#TsujiIYMK19","3332":"http:\/\/dblp.uni-trier.de\/db\/conf\/mlhc\/mlhc2019.html#KongJCF19","3333":"http:\/\/dblp.uni-trier.de\/db\/conf\/cikm\/cikm2014.html#VineZKSB14","3334":"http:\/\/dblp.uni-trier.de\/db\/conf\/ecir\/ecir2012.html#LiG12","3335":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling2000.html#SheremetyevaN00","3336":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/coling2012.html#WuLYMHK12","3337":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2007.html#Demberg07","3338":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2008.html#SnoverDS08","3339":"http:\/\/dblp.uni-trier.de\/db\/conf\/asru\/asru2017.html#LiuQ017","3340":"http:\/\/dblp.uni-trier.de\/db\/conf\/ACISicis\/ACISicis2015.html#SoeT15","3341":"http:\/\/dblp.uni-trier.de\/db\/conf\/icsea\/icsea2008.html#GargantiniRS08","3342":"http:\/\/dblp.uni-trier.de\/db\/conf\/icarcv\/icarcv2010.html#HuangE10","3343":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp1996.html#WardI96","3344":"http:\/\/dblp.uni-trier.de\/db\/conf\/wsc\/wsc2019.html#PadillaSO19","3345":"http:\/\/dblp.uni-trier.de\/db\/conf\/echt\/echt92.html#AmannS92","3346":"http:\/\/dblp.uni-trier.de\/db\/journals\/ijcat\/ijcat44.html#AbedHM12","3347":"http:\/\/dblp.uni-trier.de\/db\/journals\/itiis\/itiis5.html#LeeL11","3348":"http:\/\/dblp.uni-trier.de\/db\/conf\/cogsci\/cogsci2013.html#McShane13","3349":"http:\/\/dblp.uni-trier.de\/db\/conf\/drr\/drr2002.html#KanungoM02","3350":"http:\/\/dblp.uni-trier.de\/db\/conf\/interspeech\/interspeech2002.html#NieslerW02","3351":"http:\/\/dblp.uni-trier.de\/db\/conf\/bibm\/bibm2008.html#ReeveHB08","3352":"http:\/\/dblp.uni-trier.de\/db\/conf\/nlpke\/nlpke2010.html#FailiR10","3353":"http:\/\/dblp.uni-trier.de\/db\/conf\/acl\/acl2018-1.html#RuderH18","3354":"http:\/\/dblp.uni-trier.de\/db\/conf\/icmt\/icmt2015.html#GammaitoniK15","3355":"http:\/\/dblp.uni-trier.de\/db\/conf\/icassp\/icassp2007-4.html#BulykoMSNM07","3356":"http:\/\/dblp.uni-trier.de\/db\/journals\/corr\/corr2005.html#abs-2005-12515","3357":"http:\/\/dblp.uni-trier.de\/db\/conf\/sigmod\/keys2009.html#ElbassuoniRW09","3358":"http:\/\/dblp.uni-trier.de\/db\/conf\/coria\/coria2016.html#AliH16","3359":"http:\/\/dblp.uni-trier.de\/db\/conf\/ialp\/ialp2010.html#OkitaW10","3360":"http:\/\/dblp.uni-trier.de\/db\/conf\/seke\/seke2018.html#0001XCY18","3361":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2013.html#YasuharaTNY13","3362":"http:\/\/dblp.uni-trier.de\/db\/conf\/emnlp\/emnlp2006.html#HsuG06","3363":"http:\/\/dblp.uni-trier.de\/db\/conf\/coling\/oiaf4hlt2014.html#BariVT14","3364":"https:\/\/www.semanticscholar.org\/paper\/e1d2f2a717aa03280126f87c8e5fad695f52bf7c","3365":"https:\/\/www.semanticscholar.org\/paper\/530a059cb48477ad1e3d4f8f4b153274c8997332","3366":"https:\/\/www.semanticscholar.org\/paper\/f134abeaf9bfd41f29b97aec675ec31895bf541d","3367":"https:\/\/www.semanticscholar.org\/paper\/6107091b02999f75ac9be7d8d339d92b18b2050e","3368":"https:\/\/www.semanticscholar.org\/paper\/288078127a3078332230442170f6745ed333c700","3369":"https:\/\/www.semanticscholar.org\/paper\/9faa2b0e5cb93f20df0555c3c350fab0b2eccf3a","3370":"https:\/\/www.semanticscholar.org\/paper\/9dafa6c5c609348b46734fc8997b93b3587fec6e","3371":"https:\/\/www.semanticscholar.org\/paper\/8d984ee2eeabb630014f31fc759d4980830c4bdb","3372":"https:\/\/www.semanticscholar.org\/paper\/e89dfa306723e8ef031765e9c44e5f6f94fd8fda","3373":"https:\/\/www.semanticscholar.org\/paper\/7b72711ac2ea7bd7f519cac162a4a6578bbb7d0d","3374":"https:\/\/www.semanticscholar.org\/paper\/31f76619329aba7987394ccb8cac6c9a6dd58a56","3375":"https:\/\/www.semanticscholar.org\/paper\/973650310963f8cdc39c71e79724513004adde2a","3376":"https:\/\/www.semanticscholar.org\/paper\/22ff1f6f4df0497323ac03f446cbc49463128486","3377":"https:\/\/www.semanticscholar.org\/paper\/f08060425aa8a212d74185ee23a08329b89abcd2","3378":"https:\/\/www.semanticscholar.org\/paper\/1daa2c7115ab38b045c8347c698bd1ee9c45e141","3379":"https:\/\/www.semanticscholar.org\/paper\/8603193192a64f0c9943989d209e7492689045c1","3380":"https:\/\/www.semanticscholar.org\/paper\/79d98ae4abec4b13a0447706e44e8d709fe7953c","3381":"https:\/\/www.semanticscholar.org\/paper\/7c1933359a6860fe49d15c6353a241763879e81f","3382":"https:\/\/www.semanticscholar.org\/paper\/2d93d27fb07fc43bb1e430c37f802586bc9aaf00","3383":"https:\/\/www.semanticscholar.org\/paper\/b24b06069392197c5835983f4aba41853fbc1b62","3384":"https:\/\/www.semanticscholar.org\/paper\/96a35bb48ef7c603ffc6c1e8119bca550fa85dfa","3385":"https:\/\/www.semanticscholar.org\/paper\/e2d3b48b46d34fac164ebcad2eb39661712a1d97","3386":"https:\/\/www.semanticscholar.org\/paper\/d7eef9b5bb65feda6647440e7727bbcdf0edaebc","3387":"https:\/\/www.semanticscholar.org\/paper\/ccd561625ae82694965d6cbc724086d5f0e00db9","3388":"https:\/\/www.semanticscholar.org\/paper\/4e95ce827049a350819c5c87caf992f27f9ff792","3389":"https:\/\/www.semanticscholar.org\/paper\/c412efcc493d2a32233c7bfe59c8bbb18fcdc348","3390":"https:\/\/www.semanticscholar.org\/paper\/97b6372d5f4aa3115bbd5251740b95291f29592b","3391":"https:\/\/www.semanticscholar.org\/paper\/5d5829723fb240543ff15ffeda1f63fff47f628d","3392":"https:\/\/www.semanticscholar.org\/paper\/94e32a898cc8c62c5a25fb9fa01e94c21fd41da5","3393":"https:\/\/www.semanticscholar.org\/paper\/9cd7f74f910619d90406464309b3c0e916d453ab","3394":"https:\/\/www.semanticscholar.org\/paper\/b186405889f0ad97eeed10c077c9257c8b2d4353","3395":"https:\/\/www.semanticscholar.org\/paper\/e7ed9a61f6b1df3eab50d4f50dc0f7e1ef2b7705","3396":"https:\/\/www.semanticscholar.org\/paper\/ddf4172cad889f178c2db9b1b6302b3c7d5c0147","3397":"https:\/\/www.semanticscholar.org\/paper\/29409efa04ac99ccf01d2a011d21d5d14e870000","3398":"https:\/\/www.semanticscholar.org\/paper\/bd3d0238549555bd07fd25ff61b3d7e01eb02296","3399":"https:\/\/www.semanticscholar.org\/paper\/928cd808aba140ec298508df87c5579811ff2f41","3400":"https:\/\/www.semanticscholar.org\/paper\/be0bbf06977c4dadbf702287733187884a531b8a","3401":"https:\/\/www.semanticscholar.org\/paper\/129c66d240883c735dbb08c8f025a6573328827b","3402":"https:\/\/www.semanticscholar.org\/paper\/a7a65aec0792126674544fdbdca1aff418de3add","3403":"https:\/\/www.semanticscholar.org\/paper\/0ca9a5ef7695fdaa65325761164c70e56739a902","3404":"https:\/\/www.semanticscholar.org\/paper\/a81434e08ea760cc364c5a9d8aa8cdc09fcbc9f1","3405":"https:\/\/www.semanticscholar.org\/paper\/ef0c62ff070a476f216fe478cc190c773f12a1f6","3406":"https:\/\/www.semanticscholar.org\/paper\/971ee735f8f259b220d35a252ee20f74b82c6371","3407":"https:\/\/www.semanticscholar.org\/paper\/c483beec0afae8d08f011182460095049025b8d1","3408":"https:\/\/www.semanticscholar.org\/paper\/5c7aa19078c07269a00e27a3bc453dcc4d77ce61","3409":"https:\/\/www.semanticscholar.org\/paper\/ea8e180643bd70f49a83e73b3077020652a0faaa","3410":"https:\/\/www.semanticscholar.org\/paper\/6e4d9cd2da1e3667da2cc303c5549a7773f07fa7","3411":"https:\/\/www.semanticscholar.org\/paper\/b9ede5f604668d0b62a306392cd03f47086e245e","3412":"https:\/\/www.semanticscholar.org\/paper\/573e6814c16178186daf537b1e1a5d3c840eef2f","3413":"https:\/\/www.semanticscholar.org\/paper\/b1b0d9452112713efdb163b507a9382938b57554","3414":"https:\/\/www.semanticscholar.org\/paper\/06645d735b59b14479ae1d0392136bbf44227d0f","3415":"https:\/\/www.semanticscholar.org\/paper\/cb0cae7f42d0666d9d90b1ca93416abcccef7c44","3416":"https:\/\/www.semanticscholar.org\/paper\/3b750e38452b3a6a555e3534178984973e69ddf9","3417":"https:\/\/www.semanticscholar.org\/paper\/5b34da942633f3a3afe01386341b8839a985ca0d","3418":"https:\/\/www.semanticscholar.org\/paper\/d1f0f33c197dc9932c4a2e42eb1a8a60635bb401","3419":"https:\/\/www.semanticscholar.org\/paper\/cce1f4f0ffce089ce623f6d132b75fb139302b0f","3420":"https:\/\/www.semanticscholar.org\/paper\/4b89b8238b695b5163ad8a54a2fdf1b429df1cba","3421":"https:\/\/www.semanticscholar.org\/paper\/79086f67c5d7413a05305478b1b38781588ed19d","3422":"https:\/\/www.semanticscholar.org\/paper\/75a292ef4df5be86b3fbb6423b6fd433b28a6d42","3423":"https:\/\/www.semanticscholar.org\/paper\/286a3bf8579deadd9b892bb800614b7c35d5d9a6","3424":"https:\/\/www.semanticscholar.org\/paper\/d55e70a42579c7895938b6c43736163dbaf55145","3425":"https:\/\/www.semanticscholar.org\/paper\/da64e0b776f61c4b59be22f3fe9766d0b42d2475","3426":"https:\/\/www.semanticscholar.org\/paper\/9b21374e3dc537e87f7047cddaf6555a4399119a","3427":"https:\/\/www.semanticscholar.org\/paper\/38f23fe236b152cd4983c8f30d305a568afd0d3e","3428":"https:\/\/www.semanticscholar.org\/paper\/9bc16d048e32c38e6eb545d7579e37e9a4555ffa","3429":"https:\/\/www.semanticscholar.org\/paper\/e49f67fa5c946ad24afcf59699a9cacf1ca53924","3430":"https:\/\/www.semanticscholar.org\/paper\/8c479e81ddaf55aba9044449b5be7b7bf2046b7e","3431":"https:\/\/www.semanticscholar.org\/paper\/1fa6975aaacca107dc69aade93da48829843feb4","3432":"https:\/\/www.semanticscholar.org\/paper\/5e7a7fb69ef7447c2cb8966589e49a5acbee416e","3433":"https:\/\/www.semanticscholar.org\/paper\/7bf47af1f989c1a999d7dab24d86d19f13e8ba55","3434":"https:\/\/www.semanticscholar.org\/paper\/9c145390e6073c96e89cf03d3df3b559f0bb0496","3435":"https:\/\/www.semanticscholar.org\/paper\/5f6b2a12cb4fa2e8509736ca45091f17491ada75","3436":"https:\/\/www.semanticscholar.org\/paper\/6d031ccb880deb70e908518cf9a351026367e8e3","3437":"https:\/\/www.semanticscholar.org\/paper\/3017663daf20f71c3a17637f041de6c5c3507745","3438":"https:\/\/www.semanticscholar.org\/paper\/d3a857a4bccf9a23b2d1e523c89456de728403b3","3439":"https:\/\/www.semanticscholar.org\/paper\/d7f1a885e32faa2194ccd5f85da4c4fb5d788392","3440":"https:\/\/www.semanticscholar.org\/paper\/17f423a5e542a4bd4de0243548e127038dea6ab5","3441":"https:\/\/www.semanticscholar.org\/paper\/e3e5a78b4161a6eeea863c156bee7cf5c67e72ea","3442":"https:\/\/www.semanticscholar.org\/paper\/1a59ca238c64e4bae304240d7df787cf0345d50e","3443":"https:\/\/www.semanticscholar.org\/paper\/703eaf607de6872b13225e053a4e15ad90440a8e","3444":"https:\/\/www.semanticscholar.org\/paper\/3e3df220673388402b6b114eab68a9c5396210b1","3445":"https:\/\/www.semanticscholar.org\/paper\/3ce5a172a96008cbdc5ffedf4572b783301fd468","3446":"https:\/\/www.semanticscholar.org\/paper\/363165e6781dc79829d9e775a4bece3d9639ced6","3447":"https:\/\/www.semanticscholar.org\/paper\/a9bd625a66c8cb1cf6841faaa82862d55debf98f","3448":"https:\/\/www.semanticscholar.org\/paper\/107169ebaa4f979572bebfe56452120440bacb7a","3449":"https:\/\/www.semanticscholar.org\/paper\/57fc2d89cc6c8e578b2767ffacea8d807283e592","3450":"https:\/\/www.semanticscholar.org\/paper\/f2f2026f76ebada185531ab95808515923ea5952","3451":"https:\/\/www.semanticscholar.org\/paper\/10f919b1a5161b560504c225cfb2d1b3a4768f80","3452":"https:\/\/www.semanticscholar.org\/paper\/d020b66a033ab1adc121010e116c594415cb2cea","3453":"https:\/\/www.semanticscholar.org\/paper\/fb98ceb0e4efca62ea57d8dc7eb2787b3feee7b9","3454":"https:\/\/www.semanticscholar.org\/paper\/da75921084a1fe49c43023fe5a2b15cb2b9a36e8","3455":"https:\/\/www.semanticscholar.org\/paper\/2fe4b15267f76c39cf60c0ed8e697d11c7050391","3456":"https:\/\/www.semanticscholar.org\/paper\/3fb5d4ca683f714fe89a565b6ad108f8dcc4bf20","3457":"https:\/\/www.semanticscholar.org\/paper\/1013a3e5b2352d2cbe9d8c6291c6406ac504d8e7","3458":"https:\/\/www.semanticscholar.org\/paper\/34602875dc6c40f8060d517669b80bbed5538da5","3459":"https:\/\/www.semanticscholar.org\/paper\/d142835b642711b18669227ff0d554b1c8a0d481","3460":"https:\/\/www.semanticscholar.org\/paper\/9d9cd469279655f8efc2c0c95e55295036c8ab12","3461":"https:\/\/www.semanticscholar.org\/paper\/d3e8b100038c2bf3983ffae96a56c6af0793a62f","3462":"https:\/\/www.semanticscholar.org\/paper\/04de4d9eb0d53025c8ab6c99d1e743f4c1bc1eb6","3463":"https:\/\/www.semanticscholar.org\/paper\/47862224384097fd472a2221519c2c4f6028ccf3","3464":"https:\/\/www.semanticscholar.org\/paper\/04c902a91806288af4c7646e95cc2c94d9f15d97","3465":"https:\/\/www.semanticscholar.org\/paper\/5ab9776bf67a6470951a932d5f9a1beaf1cec184","3466":"https:\/\/www.semanticscholar.org\/paper\/0508a5488ce2567f04d9f3007c2529dd1d628673","3467":"https:\/\/www.semanticscholar.org\/paper\/77f6a75bba74f36699edceb0edb107dcfb1aaaa1","3468":"https:\/\/www.semanticscholar.org\/paper\/8e404c468452a0d76576f974159170e6c2480c6b","3469":"https:\/\/www.semanticscholar.org\/paper\/a21be1e1774ad4a9eb2bca66de16fab23daa6e04","3470":"https:\/\/www.semanticscholar.org\/paper\/76b1768c4185b4b6e525e797be137964ffd46cd5","3471":"https:\/\/www.semanticscholar.org\/paper\/2ceac7a1587a321eaba63b91607e70b71f8c90a6","3472":"https:\/\/www.semanticscholar.org\/paper\/f1bc43932beb14a00cd47feac4e40951601dd7a9","3473":"https:\/\/www.semanticscholar.org\/paper\/8fde343afa5a2a81a9636491ada2d329928b6177","3474":"https:\/\/www.semanticscholar.org\/paper\/c840f81ebd9563f9fbdce43c3096b9f6512435cb","3475":"https:\/\/www.semanticscholar.org\/paper\/36e6714cec4ad466f527d2e1eef4a0ae52b6d262","3476":"https:\/\/www.semanticscholar.org\/paper\/466a1d93895673720ee63ea779be94de06e5b10c","3477":"https:\/\/www.semanticscholar.org\/paper\/f10ce6ea0b54fc0a71eff8ae4e3d1840f19c4517","3478":"https:\/\/www.semanticscholar.org\/paper\/96cdfbd9440b87a85994ba8c074bd5184ab54dd2","3479":"https:\/\/www.semanticscholar.org\/paper\/2633a948f06a02417a39c9ff4e9c948bbad460d7","3480":"https:\/\/www.semanticscholar.org\/paper\/04258349cf59b55b4b5a65882b5da2dd992a2344","3481":"https:\/\/www.semanticscholar.org\/paper\/4955335096c038f2cf39fdb4cc2bc79edaf363f9","3482":"https:\/\/www.semanticscholar.org\/paper\/6df2126301ab415aed034b0bcd9589b1897fe983","3483":"https:\/\/www.semanticscholar.org\/paper\/4a8692d75d9bb1bbc2b84cddc52521d6a03461d4","3484":"https:\/\/www.semanticscholar.org\/paper\/d7e73699049a73cf90c82a7b5aef589d84dbe794","3485":"https:\/\/www.semanticscholar.org\/paper\/6a132499ca1dc0d23cfe7d5db841b819df63b51b","3486":"https:\/\/www.semanticscholar.org\/paper\/ce882918cd3c92c2fbd3617f8e54243743cdd7d5","3487":"https:\/\/www.semanticscholar.org\/paper\/4885e13f0b0085fffd49dce49b0f608107e60c1b","3488":"https:\/\/www.semanticscholar.org\/paper\/3e702b5cc5a03b8bd462f05feaed6a23f5e9b9da","3489":"https:\/\/www.semanticscholar.org\/paper\/2e8d3c3eaf763246578b67e521f6df3d26b291ca","3490":"https:\/\/www.semanticscholar.org\/paper\/028d026be997ebf1f1c762407d216ff35a81ca73","3491":"https:\/\/www.semanticscholar.org\/paper\/80c5d486c73bc43fcbbfcfbbb1971c1a72a8f27b","3492":"https:\/\/www.semanticscholar.org\/paper\/e8129c016fb48077fedfc23239bb51aa12f70d7f","3493":"https:\/\/www.semanticscholar.org\/paper\/6e80e4b2f2a09bdbc20c63af5d85e6fc333746a0","3494":"https:\/\/www.semanticscholar.org\/paper\/847ada64e867bd2a6ac272b5efb89c914d4b4375","3495":"https:\/\/www.semanticscholar.org\/paper\/8636a08766d585da7035e982cb39869680f5ca45","3496":"https:\/\/www.semanticscholar.org\/paper\/6e23ae3969078ecd6e59260a895c96c360b4921a","3497":"https:\/\/www.semanticscholar.org\/paper\/7aa70e2c12c8ba2dcc828893adb8bb56e3766726","3498":"https:\/\/www.semanticscholar.org\/paper\/7d89b05c7e8d374b39d363075540d3a3fddd5e34","3499":"https:\/\/www.semanticscholar.org\/paper\/fd2c7d7f9c50ef4dbc56b89cf35a0f0b5c11beaa","3500":"https:\/\/www.semanticscholar.org\/paper\/d1a25588ed177489da5570d49489800b6714ae6c","3501":"https:\/\/www.semanticscholar.org\/paper\/40fb2468c3a77c68fe703a6e614f4ad25bd4e3dd","3502":"https:\/\/www.semanticscholar.org\/paper\/e8f834463b7540a3db591a19c1025f89d1a1be1d","3503":"https:\/\/www.semanticscholar.org\/paper\/fd41c526ab65ef21a43dfd2b427f8eaeea6c35ce","3504":"https:\/\/www.semanticscholar.org\/paper\/fc03caf5243e5bed8fa56781eb282c359cfe9c58","3505":"https:\/\/www.semanticscholar.org\/paper\/0b7b22c5d2af53dc85ff264e920dbdfa1ad5e12d","3506":"https:\/\/www.semanticscholar.org\/paper\/efcfb79f4cd92db09dbb21e9e8cd25dbcccd5460","3507":"https:\/\/www.semanticscholar.org\/paper\/16a7b6234e070025a3b6a1e07bd9794d4cdbc2f2","3508":"https:\/\/www.semanticscholar.org\/paper\/205383e5929027989c01f33652b1ed6f344fe993","3509":"https:\/\/www.semanticscholar.org\/paper\/f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10","3510":"https:\/\/www.semanticscholar.org\/paper\/152ef7762980f60ad23aa6fad59aaf7df9b82df0","3511":"https:\/\/www.semanticscholar.org\/paper\/bd15d635a8b4c4435b01045981797fcd4b5671ef","3512":"https:\/\/www.semanticscholar.org\/paper\/b6b7fea1846e85ac1e3c7e3adda6e65b127d0368","3513":"https:\/\/www.semanticscholar.org\/paper\/1fb5cd122affeb0385e2a087d14be4bac103460e","3514":"https:\/\/www.semanticscholar.org\/paper\/3c151694087e7e8dd0dd555c81a3700c31b148e9","3515":"https:\/\/www.semanticscholar.org\/paper\/9b529fe170823f95509585d5aa39fa01a43558fd","3516":"https:\/\/www.semanticscholar.org\/paper\/06b5090c00326183f7b3fe6e891586449e14650e","3517":"https:\/\/www.semanticscholar.org\/paper\/2bd20336dd0b024eff47fd4b1bfd8d57b3553794","3518":"https:\/\/www.semanticscholar.org\/paper\/035b99288bcf90ba24503aac44143516b66b921a","3519":"https:\/\/www.semanticscholar.org\/paper\/b6328d4664aed6b5d7e824f1867b653069d6f252","3520":"https:\/\/www.semanticscholar.org\/paper\/ffa0c918fba3010ccb635c03b4a740bd938a0493","3521":"https:\/\/www.semanticscholar.org\/paper\/77a2f5515b8957163bc64d408af5683f650c3573","3522":"https:\/\/www.semanticscholar.org\/paper\/d385c3ed552661751b436e4cff3e72f7e77ef59f","3523":"https:\/\/www.semanticscholar.org\/paper\/d6425d11904920cf6fafe895e6073e2131978e60","3524":"https:\/\/www.semanticscholar.org\/paper\/26f7201ae06558b8c2e3a2f2ffa5aef7a1b75a0e","3525":"https:\/\/www.semanticscholar.org\/paper\/d584905a9dde8f38e5b4f0a86ecc28c98c4dd4ff","3526":"https:\/\/www.semanticscholar.org\/paper\/805d950d6df9bdabd6b87d06de213909192341db","3527":"https:\/\/www.semanticscholar.org\/paper\/2bbc7e46425d8dabb2c2ebbf28dbbb0462d3b5e3","3528":"https:\/\/www.semanticscholar.org\/paper\/8d2650784b6491fd2b9e73bb212aae229cea01ea","3529":"https:\/\/www.semanticscholar.org\/paper\/82870bc488b57cdf5ea62877109a7278af2926b3","3530":"https:\/\/www.semanticscholar.org\/paper\/b32f7a3d40eb9e95590d69b065b6458fdac0f703","3531":"https:\/\/www.semanticscholar.org\/paper\/e7570c57887c75310e468eace8e3e63ef509ab37","3532":"https:\/\/www.semanticscholar.org\/paper\/967ae25c457d4b7069916b17b46e1f800a3bd662","3533":"https:\/\/www.semanticscholar.org\/paper\/bee3daeccb215157737fa7ad6046f021de514148","3534":"https:\/\/www.semanticscholar.org\/paper\/16bac6335c85b8755ffdef4d050af639405b89b6","3535":"https:\/\/www.semanticscholar.org\/paper\/4e48fe66f895c070aee0e023602cea9975575a05","3536":"https:\/\/www.semanticscholar.org\/paper\/213807c86141298c0bbf82d230d29ebd0be3b136","3537":"https:\/\/www.semanticscholar.org\/paper\/483f6207eee956f04f3ac9186f996a054b5e1449","3538":"https:\/\/www.semanticscholar.org\/paper\/340712391e245e245a76cd74523c1d6443035b62","3539":"https:\/\/www.semanticscholar.org\/paper\/8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801","3540":"https:\/\/www.semanticscholar.org\/paper\/fdf638e7de55475c452e6db954a565f5ead881db","3541":"https:\/\/www.semanticscholar.org\/paper\/2f4f67c7c64bf003b415458c2fabaf08d346ae34","3542":"https:\/\/www.semanticscholar.org\/paper\/3025d34a8439ae92baca3f071d92a14f460ce8c4","3543":"https:\/\/www.semanticscholar.org\/paper\/753d9bb1ffb825cb014568764a16fdaef70ca6e0","3544":"https:\/\/www.semanticscholar.org\/paper\/c066aad4fbdd01c3926f2861bb95af16ece26056","3545":"https:\/\/www.semanticscholar.org\/paper\/2d18bb251f056b24f95eed242d5cd4b50c055f4f","3546":"https:\/\/www.semanticscholar.org\/paper\/d1615484d8af03aef212b2764bca93d70ac9707c","3547":"https:\/\/www.semanticscholar.org\/paper\/bdc574098729f4c4edfde53d5f22a71a76820210","3548":"https:\/\/www.semanticscholar.org\/paper\/bd3ee4fdb807c38261ca5ae9b18e81e95d74458a","3549":"https:\/\/www.semanticscholar.org\/paper\/c97f7d8cc3f8473779c9d65173cc1d4baa79344a","3550":"https:\/\/www.semanticscholar.org\/paper\/0b9e5e206890d1cbb77aa89c1e077cfa3e0d87bb","3551":"https:\/\/www.semanticscholar.org\/paper\/632db2f25b13638609c8e8d1aa313bc7bbdf366d","3552":"https:\/\/www.semanticscholar.org\/paper\/cbc2813c8011edc36b4ed6bf18556f13a202076d","3553":"https:\/\/www.semanticscholar.org\/paper\/75367d4846cbe5700ca8f81de38b1a482ffeb630","3554":"https:\/\/www.semanticscholar.org\/paper\/1913f41dc43f657edc37574583cf50a01940bed8","3555":"https:\/\/www.semanticscholar.org\/paper\/41e11ad81e68ff301eba4ff216c63fc687b095fa","3556":"https:\/\/www.semanticscholar.org\/paper\/05c5718b35549120a982cd464a28c8f95ef3e19c","3557":"https:\/\/www.semanticscholar.org\/paper\/3a60390c005af06b9d7e765abd2c83849ca239fd","3558":"https:\/\/www.semanticscholar.org\/paper\/3670d55a696dea9234660149b490e22fb5c2a3a5","3559":"https:\/\/www.semanticscholar.org\/paper\/94ae261ee01bd6be58c48cce83c1f4d927432c3a","3560":"https:\/\/www.semanticscholar.org\/paper\/196fda4f69add416c43888491290be962383b476","3561":"https:\/\/www.semanticscholar.org\/paper\/1d677a167b92b81eb5671b5461c37be4e22e9610","3562":"https:\/\/www.semanticscholar.org\/paper\/2f858284b3f3e05155cf33c0badc37f3ac90ca85","3563":"https:\/\/www.semanticscholar.org\/paper\/7c663204f4b1f5c0a112bce0d84b21e1c1b7f798","3564":"https:\/\/www.semanticscholar.org\/paper\/21f214165bda8813c95cfa3e471e24aaf8d5776b","3565":"https:\/\/www.semanticscholar.org\/paper\/7a19f30e02c34c4eb7b197d3bcd4fbcb8a4e1602","3566":"https:\/\/www.semanticscholar.org\/paper\/c3397a683640ef38004f0ba0efe1c1665d8b6b52","3567":"https:\/\/www.semanticscholar.org\/paper\/2eaa830db7e6a98f1eb84df9be313debc8332ea8","3568":"https:\/\/www.semanticscholar.org\/paper\/29e66a2113f46ea1d4cede745d46569c997f6b03","3569":"https:\/\/www.semanticscholar.org\/paper\/33a995a3376568f1036b14522db5b783c34ba463","3570":"https:\/\/www.semanticscholar.org\/paper\/19ca6924d782ac57216d254dcc31861cb0b0a89a","3571":"https:\/\/www.semanticscholar.org\/paper\/5434e470c21cde0af431202846c8c6d5d453b4ca","3572":"https:\/\/www.semanticscholar.org\/paper\/03d7cd2475ba417fbc0d5f6b7255f8aa14eb227a","3573":"https:\/\/www.semanticscholar.org\/paper\/8641738a3fec953bc22263be4dbdb923b39213a9","3574":"https:\/\/www.semanticscholar.org\/paper\/be357b6fe7c4fd5f07948947f4bc83bdfbf1d34f","3575":"https:\/\/www.semanticscholar.org\/paper\/0980d299a05499639af63e530aa8b5e133022c8e","3576":"https:\/\/www.semanticscholar.org\/paper\/73395c0a04a86f2c504fc9d02076266620a54e7a","3577":"https:\/\/www.semanticscholar.org\/paper\/a132e66655f9d4f8edba54df053c6656c4c617be","3578":"https:\/\/www.semanticscholar.org\/paper\/6fa84b2082b48baa01d519f9f6573fd5784212e5","3579":"https:\/\/www.semanticscholar.org\/paper\/5e307e074f5a54185834aaf83c5fb4a9bac6f7e6","3580":"https:\/\/www.semanticscholar.org\/paper\/c378d8fe57b2b557352e74d7568381bd42402de4","3581":"https:\/\/www.semanticscholar.org\/paper\/26a690cf81335f7fe35553f3f04c0ef192824e94","3582":"https:\/\/www.semanticscholar.org\/paper\/cc07fc48ce2a381e7f39235cef5fd10b939182c4","3583":"https:\/\/www.semanticscholar.org\/paper\/7bf035d37c55e40941221e00069e32ea0889c157","3584":"https:\/\/www.semanticscholar.org\/paper\/cacab5f8402e7e023413d3c3d3d7188d8e604517","3585":"https:\/\/www.semanticscholar.org\/paper\/4daeaf6a5a4d578d25275e779cf9257fb30fbcfe","3586":"https:\/\/www.semanticscholar.org\/paper\/d562200ac2faa61f14e94b30d8f0d918ba8d4c7e","3587":"https:\/\/www.semanticscholar.org\/paper\/a4da7f5ceeb34615707af2e4f29e27c152590dfc","3588":"https:\/\/www.semanticscholar.org\/paper\/b4837db484f1858709aa0ffc516b4dbd3fdd842d","3589":"https:\/\/www.semanticscholar.org\/paper\/f8576d4e9f0b5caf0218e4866dfa095c01e954f3","3590":"https:\/\/www.semanticscholar.org\/paper\/c041af0e08ccd1afb6b94e17e1da2539f5b34b7a","3591":"https:\/\/www.semanticscholar.org\/paper\/2b75e4c582c267e21845ae04dbc95162fe00c84d","3592":"https:\/\/www.semanticscholar.org\/paper\/5f0625c30014c12f333eb518268647673d18f9f1","3593":"https:\/\/www.semanticscholar.org\/paper\/cccd2cc52b088b930a2dc5829c59c9ae6124cb20","3594":"https:\/\/www.semanticscholar.org\/paper\/442e88a8532dec2e14b6de62df06abc30add32e0","3595":"https:\/\/www.semanticscholar.org\/paper\/9e58e541b8549cd438c9d3e8cfb91dd12760589f","3596":"https:\/\/www.semanticscholar.org\/paper\/4bfdc3910684396e22f6675470002ef162bebdbf","3597":"https:\/\/www.semanticscholar.org\/paper\/697ba06bfcabbbde6292d979b87b2642115f1099","3598":"https:\/\/www.semanticscholar.org\/paper\/5314b05697d58b27a9a16bc2049e9b98999a81f7","3599":"https:\/\/www.semanticscholar.org\/paper\/007a55caac4c46e49fa6f8b09a944b8610f78a81","3600":"https:\/\/www.semanticscholar.org\/paper\/6db26666e5556681553176c0501652ab72985cce","3601":"https:\/\/www.semanticscholar.org\/paper\/2333016ded3dd7ff4f06ad0d7b0139e34559c4b0","3602":"https:\/\/www.semanticscholar.org\/paper\/b899d7813f24f617513d8a97a1d9ad90eead6b44","3603":"https:\/\/www.semanticscholar.org\/paper\/563daeb9d787f63af90e2f6d26721dcd1048aeee","3604":"https:\/\/www.semanticscholar.org\/paper\/680ac950d1c912ba16c260f1e437590279cd9247","3605":"https:\/\/www.semanticscholar.org\/paper\/0b999879e8c281e50cd883a41dba832c19d5fb31","3606":"https:\/\/www.semanticscholar.org\/paper\/bfe4586b650f39b4db7890a827fe35c108aa1f97","3607":"https:\/\/www.semanticscholar.org\/paper\/f4753951991868ce54050280ff4083a4a2fd6232","3608":"https:\/\/www.semanticscholar.org\/paper\/4654cb7d993914a7e055c36a761e566853a9832c","3609":"https:\/\/www.semanticscholar.org\/paper\/ad34a4b66a24f4607577da0994ecb7250957a599","3610":"https:\/\/www.semanticscholar.org\/paper\/0063f259380a5836d90f0b9ebc4a0639926ce32a","3611":"https:\/\/www.semanticscholar.org\/paper\/f94a5c8ad11b076c5d30748299041b68378e8bc9","3612":"https:\/\/www.semanticscholar.org\/paper\/7da6a9935f454769f0a091efb7a7c366a9736340","3613":"https:\/\/www.semanticscholar.org\/paper\/405087814efd38f2a8d852e9d523b8a9c10e841b","3614":"https:\/\/www.semanticscholar.org\/paper\/c4e255a80cb86de26003930a16a4c16b7270e8c5","3615":"https:\/\/www.semanticscholar.org\/paper\/614338b12f86f269e499e82541c4a97581a06127","3616":"https:\/\/www.semanticscholar.org\/paper\/a94ac484b3cc19cbf03bdbac0c727579a09fc3b9","3617":"https:\/\/www.semanticscholar.org\/paper\/49c58ea9d9a71913aef89d38c1994ff62c570634","3618":"https:\/\/www.semanticscholar.org\/paper\/dbca84ee9ffa80c7b0c0724eef0248ae0e019113","3619":"https:\/\/www.semanticscholar.org\/paper\/0f1c31aa40a7d0b3ba68a6d1ef2bfc92e7f8ae0d","3620":"https:\/\/www.semanticscholar.org\/paper\/a00ee7ae4642b986b622e0f48c845e79707b707b","3621":"https:\/\/www.semanticscholar.org\/paper\/35daaf19a893c1450dc2774fffef0c968dfc1616","3622":"https:\/\/www.semanticscholar.org\/paper\/f2607e52bf3f6badba25d69ce4b04688cfe22513","3623":"https:\/\/www.semanticscholar.org\/paper\/4af6c0c61bbaaca04d33deea73b69b8494e0c77e","3624":"https:\/\/www.semanticscholar.org\/paper\/1106db8568b404bfa6692f4fe41da56d1a68453e","3625":"https:\/\/www.semanticscholar.org\/paper\/48f8ad3745c03879586381b59452b501e55a98b6","3626":"https:\/\/www.semanticscholar.org\/paper\/383dc22d1d6ca1310626ec56a63cd578273c2f93","3627":"https:\/\/www.semanticscholar.org\/paper\/a7cb383d6ba33aba48855965b5e773d9eb0fc9a7","3628":"https:\/\/www.semanticscholar.org\/paper\/346b0a7d642b4d86f17388d5a4965520b818bb1b","3629":"https:\/\/www.semanticscholar.org\/paper\/672d7db068a6ee6eab17c462ef6ecd118bd005bb","3630":"https:\/\/www.semanticscholar.org\/paper\/78a9bab542e9fdb8f0b7d8e4cbaf4c6a9a0677b3","3631":"https:\/\/www.semanticscholar.org\/paper\/34ed480d4a84b03a3dbd6a9258d2343aa8694393","3632":"https:\/\/www.semanticscholar.org\/paper\/503a6e8896772cb1b4c4060380f9bec882476301","3633":"https:\/\/www.semanticscholar.org\/paper\/836eedb1f4f3f4bd6d9c7b77045ca74166417b29","3634":"https:\/\/www.semanticscholar.org\/paper\/426ed641ac50d6aadc2c748bb435af3da9b31d13","3635":"https:\/\/www.semanticscholar.org\/paper\/d2d79513f32c4d09b6255b18514d7ad07ebf43fe","3636":"https:\/\/www.semanticscholar.org\/paper\/5c15f88cf65f4bf542627a078f36bc03a49f75b6","3637":"https:\/\/www.semanticscholar.org\/paper\/b944b71f789393902e161bf5946b71036caa4863","3638":"https:\/\/www.semanticscholar.org\/paper\/2ce0df22eb4854e6f4210165b1ebc076d058430f","3639":"https:\/\/www.semanticscholar.org\/paper\/42e3d0da7b41407b3499a931c3dd95b2f37c2262","3640":"https:\/\/www.semanticscholar.org\/paper\/163fd9fac485940724e14fcd45d1fd6a4fce3932","3641":"https:\/\/www.semanticscholar.org\/paper\/d02f37dfd387ac4682d90bb0130a7d8fdad4c6c7","3642":"https:\/\/www.semanticscholar.org\/paper\/2d9597a69e58b27ffb8531abb885da62432666d3","3643":"https:\/\/www.semanticscholar.org\/paper\/cf4efa86481a961596e0d04050d6ff96e30e45e4","3644":"https:\/\/www.semanticscholar.org\/paper\/3edf4138c02354aedef0517f49b92ab6ae58539a","3645":"https:\/\/www.semanticscholar.org\/paper\/d92c390fcb0ea1cb9da8ee4a2e85bebdfc65c96f","3646":"https:\/\/www.semanticscholar.org\/paper\/34e22341d14ae680aae1da06f9ce6bf937058097","3647":"https:\/\/www.semanticscholar.org\/paper\/d48c29dcf7aa0fd7b139a003134c64d69110205a","3648":"https:\/\/www.semanticscholar.org\/paper\/e8be76c772fc754d2f3e586852f3b9b4e2ac56e0","3649":"https:\/\/www.semanticscholar.org\/paper\/3416b9257fb1446bc1d04bacd88ffe4498198d75","3650":"https:\/\/www.semanticscholar.org\/paper\/00d385a359eda4845dab37efc7c12a9c0987e66b","3651":"https:\/\/www.semanticscholar.org\/paper\/1683a3a8d4be1c72bf086251ffc02263674e9d53","3652":"https:\/\/www.semanticscholar.org\/paper\/b099ca77c4b9acbcdc9011e8f3a12c09f9b402fb","3653":"https:\/\/www.semanticscholar.org\/paper\/770058895898d098f149dafc86af9caff92f8427","3654":"https:\/\/www.semanticscholar.org\/paper\/494e89a013256934470c53145c2ad84b3d752280","3655":"https:\/\/www.semanticscholar.org\/paper\/58e0ca33ae3068fee7005f339bf6c444fc17d55f","3656":"https:\/\/www.semanticscholar.org\/paper\/e89af152ba2cfacc5092621eba746149b3b295af","3657":"https:\/\/www.semanticscholar.org\/paper\/4d21a2114cb5bbc152fec59f678a44c6633a6cff","3658":"https:\/\/www.semanticscholar.org\/paper\/a9ef68be93c896d91c1f87ce622e28da540f5fba","3659":"https:\/\/www.semanticscholar.org\/paper\/257d8f6a89b682f63a63a1f6b7e46e1803dfcc94","3660":"https:\/\/www.semanticscholar.org\/paper\/4a333658f27d620f6f37dc54868cea8dabb574e7","3661":"https:\/\/www.semanticscholar.org\/paper\/48b44b31b4d378d73bbdcf3c0a38346fea578177","3662":"https:\/\/www.semanticscholar.org\/paper\/1fccba11583dc9e1030713d61bd65e9e9990e39f","3663":"https:\/\/www.semanticscholar.org\/paper\/b735f2bbc005247cdb3fc4d4ce6a547163f63d81","3664":"https:\/\/www.semanticscholar.org\/paper\/9e2bb62eec93c81aa57a0b0e7f9c8f59a4af9464","3665":"https:\/\/www.semanticscholar.org\/paper\/497c55271b7d8558fb225c52512a97c9b6498974","3666":"https:\/\/www.semanticscholar.org\/paper\/bb0e6bc1b5d9a1f93b5f7e8975db72bde253710f","3667":"https:\/\/www.semanticscholar.org\/paper\/71ab4f9c2d36b4b05e58263a06e074a321505016","3668":"https:\/\/www.semanticscholar.org\/paper\/0285a1d292cefb248335d98d7950140fea7a5e37","3669":"https:\/\/www.semanticscholar.org\/paper\/7870593045c9f7ed926c4422c1b52d95a044bec2","3670":"https:\/\/www.semanticscholar.org\/paper\/da167ea53dcbe93866727ea0da1f3210dcda3cbf","3671":"https:\/\/www.semanticscholar.org\/paper\/7ff3f12f27ef8e9e7d754c23b1eeb09b664223bd","3672":"https:\/\/www.semanticscholar.org\/paper\/4f726dc4be701267a46284fc1ab5cd68548b83d5","3673":"https:\/\/www.semanticscholar.org\/paper\/164520a7125580d5b80ac7542f803b677abd96d8","3674":"https:\/\/www.semanticscholar.org\/paper\/2ada081d3c39ce4f6cb1e580dad0cae54e0c2304","3675":"https:\/\/www.semanticscholar.org\/paper\/48748a814d065c7cb450be330eb84e6188d8c4fd","3676":"https:\/\/www.semanticscholar.org\/paper\/6e087108aa8048da8cfc82cdecb7071a55bab488","3677":"https:\/\/www.semanticscholar.org\/paper\/4b9955402487fce0a69a8dcf4df177b2169d15ab","3678":"https:\/\/www.semanticscholar.org\/paper\/d2c22ca8e04941d0afd9688944d9bb9e6a80b0c2","3679":"https:\/\/www.semanticscholar.org\/paper\/97b446e2ec3b402ea104421dab5eb4b99a21e42a","3680":"https:\/\/www.semanticscholar.org\/paper\/1cb489f9d228557270c94917f102025832b83701","3681":"https:\/\/www.semanticscholar.org\/paper\/5fa5497dcfddd80cc5887714b743a152807caa86","3682":"https:\/\/www.semanticscholar.org\/paper\/22dbd797179eff25825e999a7928e1220946ea44","3683":"https:\/\/www.semanticscholar.org\/paper\/162bc18d2fd4cb498668e01ae4d46cde80871218","3684":"https:\/\/www.semanticscholar.org\/paper\/974cc2ac3babb39116314f88bb1903e36638d0d1","3685":"https:\/\/www.semanticscholar.org\/paper\/dce7838d3ae89946ff656ea8dc60c889d053fcb7","3686":"https:\/\/www.semanticscholar.org\/paper\/6624ff407678a1f5103b5a95f719bfd7490cdbb7","3687":"https:\/\/www.semanticscholar.org\/paper\/a20eea154ae75bfc34f1a9f9ac6465a8dff6c224","3688":"https:\/\/www.semanticscholar.org\/paper\/b80081576b761925150b69935b971ce6790197af","3689":"https:\/\/www.semanticscholar.org\/paper\/3d0d63b6517efa8f35e4207ca907b21103433941","3690":"https:\/\/www.semanticscholar.org\/paper\/89e504501aa80b7f0b9c0eaea4bdd80e1a4009e1","3691":"https:\/\/www.semanticscholar.org\/paper\/8d68dd845c6dadf7c8109a1e87358aab0e934b1b","3692":"https:\/\/www.semanticscholar.org\/paper\/24abed22b881ef5f007466625c14034d23c5b2f9","3693":"https:\/\/www.semanticscholar.org\/paper\/71844f56cc7478d8d558ca9f7188fa10e09bf7df","3694":"https:\/\/www.semanticscholar.org\/paper\/06d827686b973bc847a8b3db82f03df9a1b24010","3695":"https:\/\/www.semanticscholar.org\/paper\/b79127b48a3bf7ddcf7b5553889f35e4ed9a819e","3696":"https:\/\/www.semanticscholar.org\/paper\/c9546fc91e167fcd78476a6a1bb99232333ef075","3697":"https:\/\/www.semanticscholar.org\/paper\/ca9689ead45a7ad1cfcd764d547ea03fd21fd464","3698":"https:\/\/www.semanticscholar.org\/paper\/2af174490f57f4cfa6446e032a49825848c0d77d","3699":"https:\/\/www.semanticscholar.org\/paper\/e9d782ed5ab5ee4a60f56357099a9864369fefe8","3700":"https:\/\/www.semanticscholar.org\/paper\/41cb576653d218f8017d33ab9b4ceec971bebbe0","3701":"https:\/\/www.semanticscholar.org\/paper\/607bbab2db972aae32d378af45f586b496b0e82d","3702":"https:\/\/www.semanticscholar.org\/paper\/32754d496768685cfcc7fbdb323d87db2b33a09c","3703":"https:\/\/www.semanticscholar.org\/paper\/e9775a99ecd12f1c5accebd2da83e94708c6ab3f","3704":"https:\/\/www.semanticscholar.org\/paper\/b7f2f024a1498b2c47f3fce569efbc3154ee1e8f","3705":"https:\/\/www.semanticscholar.org\/paper\/ec054d982fc1c140b143460dd88bf942df941a8c","3706":"https:\/\/www.semanticscholar.org\/paper\/a2155552ca5afb784a3c1d67a5bcbd4e688b6e05","3707":"https:\/\/www.semanticscholar.org\/paper\/5f09d0768dca66d5e95d3c8e8706161d38e361e8","3708":"https:\/\/www.semanticscholar.org\/paper\/56559fcd8b588357051e0d3c46631bdb435b963b","3709":"https:\/\/www.semanticscholar.org\/paper\/0f76dba2e13a4ac2448cb4df978a95aec615aae1","3710":"https:\/\/www.semanticscholar.org\/paper\/2f3e450a967bd0455c51f573cb458dd58df2e8c9","3711":"https:\/\/www.semanticscholar.org\/paper\/f6940e511324c7e1580d6cafcd131545ebec7b41","3712":"https:\/\/www.semanticscholar.org\/paper\/84ebbe441e974eaf1e7bb0b53969f851fa6c210f","3713":"https:\/\/www.semanticscholar.org\/paper\/3136e1674a67c28aa32171b03f525c9ab0a56462","3714":"https:\/\/www.semanticscholar.org\/paper\/4435d6ba578fc061dfa66f14e163dc94cc0f7056","3715":"https:\/\/www.semanticscholar.org\/paper\/0813cbe9e6da09516ba013754485b5fa074a89f2","3716":"https:\/\/www.semanticscholar.org\/paper\/351993286a50aef045b27b2a8d166d414e6cd943","3717":"https:\/\/www.semanticscholar.org\/paper\/f3d7384f1f9240dabcb4eb906f9021237d8a4c42","3718":"https:\/\/www.semanticscholar.org\/paper\/fb75d77d6809249b8dd668506c40abb6ff851b7c","3719":"https:\/\/www.semanticscholar.org\/paper\/a485c4c937a4b229455d611129ff4cd6a628f259","3720":"https:\/\/www.semanticscholar.org\/paper\/9c51ef6b597ecce74173ac33e9bce0656278672d","3721":"https:\/\/www.semanticscholar.org\/paper\/117d8f1b33bfbedea1070a2df11b3b0bdda848d8","3722":"https:\/\/www.semanticscholar.org\/paper\/9aeb0b52eb8f2381f54976b0533ea73a7619b3b6","3723":"https:\/\/www.semanticscholar.org\/paper\/173d97b2a940775cbe65e639264845fc909f7405","3724":"https:\/\/www.semanticscholar.org\/paper\/4f50f2df1858f2427ec5e3586bdc4e4e5b8357ce","3725":"https:\/\/www.semanticscholar.org\/paper\/a4d513cfc9d4902ef1a80198582f29b8ba46ac28","3726":"https:\/\/www.semanticscholar.org\/paper\/bdb3516a0d661e67243c1d162d0a71e9ddbccbda","3727":"https:\/\/www.semanticscholar.org\/paper\/39593dc89d2c3d8c3e61b4a042f8e753b37e7106","3728":"https:\/\/www.semanticscholar.org\/paper\/7e9b75b60914c36cf94bc98078aee8bc01a193ff","3729":"https:\/\/www.semanticscholar.org\/paper\/2ea9467ef7e4470096682ae39f2e630332d1e033","3730":"https:\/\/www.semanticscholar.org\/paper\/0cc6dbfd929bc816d507527993f55f9b4e88615d","3731":"https:\/\/www.semanticscholar.org\/paper\/4079337b2e2f1e85dc3fea0ec3df73f711bcfc4c","3732":"https:\/\/www.semanticscholar.org\/paper\/2243f46e171cf184412b4b93325dc96e445252ca","3733":"https:\/\/www.semanticscholar.org\/paper\/8633309855589f4e56743437fb20fb4571616807","3734":"https:\/\/www.semanticscholar.org\/paper\/1d0819ed6716db9f1926301e38554acd11e0bd7a","3735":"https:\/\/www.semanticscholar.org\/paper\/3695fe14596fb102ccf8bcc8ee6694d382b17da8","3736":"https:\/\/www.semanticscholar.org\/paper\/c29ad537c20bbf57d1c1de75e6069ca3e157958a","3737":"https:\/\/www.semanticscholar.org\/paper\/0685c8eb9ce24ff32e726b18a0f394dbbcc88d75","3738":"https:\/\/www.semanticscholar.org\/paper\/0dff4441dca73fbefffd9444553fc640d62a9c77","3739":"https:\/\/www.semanticscholar.org\/paper\/a4b586888a784ad0790a029cca72f2f17740bb13","3740":"https:\/\/www.semanticscholar.org\/paper\/7f16ab21a77040c52f97b76495b692363fd63fc6","3741":"https:\/\/www.semanticscholar.org\/paper\/981ef48c9a1f0a33669f46f62fbc2188b67c5e5a","3742":"https:\/\/www.semanticscholar.org\/paper\/1478a455fe37178da9a5edacd9f28e4ef8f8bbbf","3743":"https:\/\/www.semanticscholar.org\/paper\/7c4a9643c701c0c91ea50fd587038f79187a0a5e","3744":"https:\/\/www.semanticscholar.org\/paper\/ad1a9e389d738635ffaf35e74bcf6bccb6c79b68","3745":"https:\/\/www.semanticscholar.org\/paper\/76cc14b26f01dc5e509c3f8b48181d80e1280d04","3746":"https:\/\/www.semanticscholar.org\/paper\/74a6826bcd97afbc7bcb175d622ab63f1596d260","3747":"https:\/\/www.semanticscholar.org\/paper\/756acf9c9f838424ea66204f9aee4216e0da071a","3748":"https:\/\/www.semanticscholar.org\/paper\/ac1b26081b65d65888c96fa4d2e67a1930662db2","3749":"https:\/\/www.semanticscholar.org\/paper\/20782ef639a17d39cce3a47b11814bbcc17d9421","3750":"https:\/\/www.semanticscholar.org\/paper\/ae62fcf249617941d7a9680ffcff1e7c456193b0","3751":"https:\/\/www.semanticscholar.org\/paper\/b5d688d8a03bda383fca82835bbfce13941670cf","3752":"https:\/\/www.semanticscholar.org\/paper\/a074e1fe9ada366367b2e7cb55c18d6881d1e48e","3753":"https:\/\/www.semanticscholar.org\/paper\/8c1a26267b0d3cac6ac987cbaebd7cade50029eb","3754":"https:\/\/www.semanticscholar.org\/paper\/c63f428911e9ea11a7d582e5f3f817197a6cbfe8","3755":"https:\/\/www.semanticscholar.org\/paper\/e4082122602b47d986de04cdbbc533a83291e77e","3756":"https:\/\/www.semanticscholar.org\/paper\/69f665ee4e38661268a2e9f8b348bfb94cb6a508","3757":"https:\/\/www.semanticscholar.org\/paper\/2c21a89a91e6920ab9ac9a40af05055b865d5666","3758":"https:\/\/www.semanticscholar.org\/paper\/20a6e47410ae158bd20958653edce9320ee7fde8","3759":"https:\/\/www.semanticscholar.org\/paper\/c134e874b0c59792979af74ceefe2e23921a504e","3760":"https:\/\/www.semanticscholar.org\/paper\/0adfae854ebf47d6f5a5c0f9c1033279793e62e0","3761":"https:\/\/www.semanticscholar.org\/paper\/f6a13db36687d61eb3751e6d769f19a52c49cf8d","3762":"https:\/\/www.semanticscholar.org\/paper\/e95efbe0fbe496e61da17cedacbe4357cf9db57c","3763":"https:\/\/www.semanticscholar.org\/paper\/561b15685fb900593f43ac8f7e615a73bb4ff965","3764":"https:\/\/www.semanticscholar.org\/paper\/9a44a1770114adfa50d64ba98ba8c954db3cb527","3765":"https:\/\/www.semanticscholar.org\/paper\/b700356155318f6c577905c0fa1adb53aa25a270","3766":"https:\/\/www.semanticscholar.org\/paper\/151c7cb45c75ac5573a6c03b2ea089b34512898c","3767":"https:\/\/www.semanticscholar.org\/paper\/4c0258d8996d53cb89b0c6a9571e1ef9fcbff4b5","3768":"https:\/\/www.semanticscholar.org\/paper\/2ac4db657ee8e9b556f68fb58d82c2f2f4351651","3769":"https:\/\/www.semanticscholar.org\/paper\/ab1d44fe7ee9165974a2487b6d10ddaba6c26549","3770":"https:\/\/www.semanticscholar.org\/paper\/78afd0948d037fba8de739bf47bb5534994af176","3771":"https:\/\/www.semanticscholar.org\/paper\/54edd7b3067751539c655868a964d69fcd8abf97","3772":"https:\/\/www.semanticscholar.org\/paper\/bb82053d4229357925b28ffe647377f56d93b06e","3773":"https:\/\/www.semanticscholar.org\/paper\/80179a17eab0f9fb6e21840f3fed96c4d75c3442","3774":"https:\/\/www.semanticscholar.org\/paper\/ea5bf3ead829ec8599ceab436884d2b62e436f11","3775":"https:\/\/www.semanticscholar.org\/paper\/ad680d13cd06ea8a8722d1da1121f105277adb73","3776":"https:\/\/www.semanticscholar.org\/paper\/416f7a41becd5a0491972bf49cd054a47f30e5b3","3777":"https:\/\/www.semanticscholar.org\/paper\/7a942329e2e9d96628e7714b6b47f98d6b347f97","3778":"https:\/\/www.semanticscholar.org\/paper\/61f810b3cbec7bf050f964ff069a05ac2f883303","3779":"https:\/\/www.semanticscholar.org\/paper\/a511a986af07804261d3b35badf764c622e3f502","3780":"https:\/\/www.semanticscholar.org\/paper\/2c38f86d6e5ea63a434534ffdfba5ea44de2b64e","3781":"https:\/\/www.semanticscholar.org\/paper\/26cad5ab89eeffc2341b7bec105efeb3c764a52e","3782":"https:\/\/www.semanticscholar.org\/paper\/2c7cabe4549d34641dd8a753b31fba4361b34ee5","3783":"https:\/\/www.semanticscholar.org\/paper\/b6a3b0b6956e496f726ffe945dc5cdae8718ba43","3784":"https:\/\/www.semanticscholar.org\/paper\/d094866f223e4ade23e4f3e455b511f98d8a5773","3785":"https:\/\/www.semanticscholar.org\/paper\/21412d1d0a8db97fa04e7ff44c65c44f89fa0680","3786":"https:\/\/www.semanticscholar.org\/paper\/99f2d8ba2ad1224ff08b31583b007580a2c48133","3787":"https:\/\/www.semanticscholar.org\/paper\/39179be8b3ccd48ca9593efe6b0f5b7e2e2ab43a","3788":"https:\/\/www.semanticscholar.org\/paper\/88cd352a02b017f8190686271594b62ca464b638","3789":"https:\/\/www.semanticscholar.org\/paper\/3596037df42e75efc31352f31103a512de7e9f61","3790":"https:\/\/www.semanticscholar.org\/paper\/cff7f7f5aa2393a343444f469b2419c8770fd65d","3791":"https:\/\/www.semanticscholar.org\/paper\/0945b1ea6338545bd9fb47626ca90abbb69a8820","3792":"https:\/\/www.semanticscholar.org\/paper\/8662297398bfa04fbff3b473c961928bc01b3970","3793":"https:\/\/www.semanticscholar.org\/paper\/4e3cf1f761b8749afbac46ab949ed30896d3f44a","3794":"https:\/\/www.semanticscholar.org\/paper\/9de90994a2bdde3d50926d966e5bac0bac690037","3795":"https:\/\/www.semanticscholar.org\/paper\/70206b09df8129ba35b7cf7468db310ccdd1ba0b","3796":"https:\/\/www.semanticscholar.org\/paper\/31e10c9205c53058f96ef473bd5fc01df60b676f","3797":"https:\/\/www.semanticscholar.org\/paper\/fe68fdd7af4e27d42fc336833fced0e217dca255","3798":"https:\/\/www.semanticscholar.org\/paper\/6c7634f8717df6199f994e8945046b7d227d0e20","3799":"https:\/\/www.semanticscholar.org\/paper\/12a392788f2eeabdf4d8bbe1c1f43f53d83e1973","3800":"https:\/\/www.semanticscholar.org\/paper\/e2a240cc67c2909128f2154db2facba5810a94b0","3801":"https:\/\/www.semanticscholar.org\/paper\/2629a7f7dfbdfae18490fe65e0f27ccc1566de05","3802":"https:\/\/www.semanticscholar.org\/paper\/d741963c725fdd771de95cb6dbf8d479fcdd32fa","3803":"https:\/\/www.semanticscholar.org\/paper\/737a59b9f7f45885bbae959ca6f254170bd2ade3","3804":"https:\/\/www.semanticscholar.org\/paper\/8d3eb4174ed35698654d98232dd378e68f383d12","3805":"https:\/\/www.semanticscholar.org\/paper\/3be53b322696e0fdfc7d5899046ed42cb4d45d2b","3806":"https:\/\/www.semanticscholar.org\/paper\/04b3d8f9d191cbf259c459b9f9f2e249db5270ba","3807":"https:\/\/www.semanticscholar.org\/paper\/b93d9995f9ce3b15f4c4855ae62f0bf6f9bc041f","3808":"https:\/\/www.semanticscholar.org\/paper\/0ad61da6040593c5fded671d6744c373c25d02a9","3809":"https:\/\/www.semanticscholar.org\/paper\/c482327b0c810317ef074c1c7a1f2421c14b1d53","3810":"https:\/\/www.semanticscholar.org\/paper\/5fd9091800f81f8acf0806f26ddb7dc9008db8ef","3811":"https:\/\/www.semanticscholar.org\/paper\/5dcdb410fc2bebac52552594287971f8a35a0d40","3812":"https:\/\/www.semanticscholar.org\/paper\/940374c38aed0ae2503d1678b8d25004d727e59b","3813":"https:\/\/www.semanticscholar.org\/paper\/fa5396d7bd7896bcb67c6570b73847250c92baae","3814":"https:\/\/www.semanticscholar.org\/paper\/a6129fb9debac83826e1ee921ea02a70668ef49a","3815":"https:\/\/www.semanticscholar.org\/paper\/901d3a07d09dfcc22cad212f301156420e942332","3816":"https:\/\/www.semanticscholar.org\/paper\/99ba545b268679fc3bc74a04df9f3596035d32b6","3817":"https:\/\/www.semanticscholar.org\/paper\/7b459e728c6fab51e57ef3461b14c71e5703a3cd","3818":"https:\/\/www.semanticscholar.org\/paper\/31bd19a1f6a4557840f84af253ac5395ae498433","3819":"https:\/\/www.semanticscholar.org\/paper\/6c5b4eb9cd89882b8a3ea3c243d15ae18aef85df","3820":"https:\/\/www.semanticscholar.org\/paper\/58b19320fbaeed3dbd57859ea60f761f5c0bc038","3821":"https:\/\/www.semanticscholar.org\/paper\/dc1eb76fdd0c39c8e53985a447a5f049f860a7e9","3822":"https:\/\/www.semanticscholar.org\/paper\/54edaae3b115c3e07902633ebc41013b1a647eb1","3823":"https:\/\/www.semanticscholar.org\/paper\/4312ae64a058d555ff1656ccad713dbd81200e79","3824":"https:\/\/www.semanticscholar.org\/paper\/48aee85dfb2ec9f38891c04ccd9116019f2dff9b","3825":"https:\/\/www.semanticscholar.org\/paper\/e6d18ecec7c9603127433b3c41a3ecb1d9117c08","3826":"https:\/\/www.semanticscholar.org\/paper\/f1fdc5810312ea877c18ce995c57394cb31c8713","3827":"https:\/\/www.semanticscholar.org\/paper\/21cc8cb216b37cd7849531d7bac470a0a76babb0","3828":"https:\/\/www.semanticscholar.org\/paper\/744311d96e757e4f5a05c56031e738501ffb92e8","3829":"https:\/\/www.semanticscholar.org\/paper\/b6662d116fce4b20e3baf157bda10f44b98a4125","3830":"https:\/\/www.semanticscholar.org\/paper\/d1680f7b6211fec36fee8ca976af89c48591b147","3831":"https:\/\/www.semanticscholar.org\/paper\/59c22bff89a102365814cc61eb06ce3325eda39d","3832":"https:\/\/www.semanticscholar.org\/paper\/64fda6dc4dfe08f515439c4f255c9766c7487880","3833":"https:\/\/www.semanticscholar.org\/paper\/a1994d7eb6207aaae93eadb850ad0eafab64077b","3834":"https:\/\/www.semanticscholar.org\/paper\/5a12074dda3b12d686dda1969d23fee9fed9a87f","3835":"https:\/\/www.semanticscholar.org\/paper\/147dcd1131752627a4c13acaee7f5f5f6485136d","3836":"https:\/\/www.semanticscholar.org\/paper\/e126e3dda4f5ea25614ab957f681246b542434d9","3837":"https:\/\/www.semanticscholar.org\/paper\/615db3435371b0bd545f7aeb9b21151416ed452e","3838":"https:\/\/www.semanticscholar.org\/paper\/3d5f388fff31bb9fd5c01e8831f0df411e3863f7","3839":"https:\/\/www.semanticscholar.org\/paper\/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","3840":"https:\/\/www.semanticscholar.org\/paper\/81600fd653a828d69f6160705be6814dd101beb7","3841":"https:\/\/www.semanticscholar.org\/paper\/7b6a8c6d44e0f77bf930484e438d77b7465a69fb","3842":"https:\/\/www.semanticscholar.org\/paper\/a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","3843":"https:\/\/www.semanticscholar.org\/paper\/d553d008f643622e87e3ac061226865cad3b2928","3844":"https:\/\/www.semanticscholar.org\/paper\/3936fd3c6187f606c6e4e2e20b196dbc41cc4654","3845":"https:\/\/www.semanticscholar.org\/paper\/acb0b04bf282a5a04bd2ca050d8be3dec78a4668","3846":"https:\/\/www.semanticscholar.org\/paper\/2f62ab1aa8b045aa8aec53e132d1e4d2a3ac9b8f","3847":"https:\/\/www.semanticscholar.org\/paper\/791522e891976ae6dae427b215f04df58be33298","3848":"https:\/\/www.semanticscholar.org\/paper\/1b32b77718be22ca31c2a0dba709c815e19d4672","3849":"https:\/\/www.semanticscholar.org\/paper\/514520ee917e1390db971deea0d3d5799e64d1f3","3850":"https:\/\/www.semanticscholar.org\/paper\/22548cb1c72e4966a3c465c8363c1e421ab66748","3851":"https:\/\/www.semanticscholar.org\/paper\/9b1323e42dfc02703def05ff8b97f96a6b168a4c","3852":"https:\/\/www.semanticscholar.org\/paper\/dda118e8154765f73cb8f5e2b1b8daa75faf726f","3853":"https:\/\/www.semanticscholar.org\/paper\/2a0f2a76df5af569a9d85d97d2a81ae10f02fccf","3854":"https:\/\/www.semanticscholar.org\/paper\/b4a35e548de27b6924e5f2ee41d37238a5c4a1d5","3855":"https:\/\/www.semanticscholar.org\/paper\/63d7e40da7f0d37308b8e97fca4a14a26a6b52ea","3856":"https:\/\/www.semanticscholar.org\/paper\/c18ff70a0a0ac21b22e9736ecebc627707924fa7","3857":"https:\/\/www.semanticscholar.org\/paper\/8e56f9663a7985d394a3880e9b54261e23a813da","3858":"https:\/\/www.semanticscholar.org\/paper\/89ab36ae8630f6e4058c926816fe8d9a676c54e3","3859":"https:\/\/www.semanticscholar.org\/paper\/e86f71ca2948d17b003a5f068db1ecb2b77827f7","3860":"https:\/\/www.semanticscholar.org\/paper\/5cc4100a67fd6f2ce3c760655ba7a12f358c7950","3861":"https:\/\/www.semanticscholar.org\/paper\/ebcbbb8fe297940d79b17aeb6d46bedff9db7fec","3862":"https:\/\/www.semanticscholar.org\/paper\/ff437a9a44dbce3faf9534454a9bbdbc12bac3c2","3863":"https:\/\/www.semanticscholar.org\/paper\/3a8c344f67d5081ead5f7dd5ebf0f760d69fc01d","3864":"https:\/\/www.semanticscholar.org\/paper\/70978b22b32a344545bfce3793b8dc7820e38974","3865":"https:\/\/www.semanticscholar.org\/paper\/f156ecbbb9243522275490d698c6825f4d2e01af","3866":"https:\/\/www.semanticscholar.org\/paper\/17859c19e41a29b657aa3323ed396f733086c47d","3867":"https:\/\/www.semanticscholar.org\/paper\/ed15d0f9a8eb3728931e772822314efde340e0b1","3868":"https:\/\/www.semanticscholar.org\/paper\/f83a207712fd4cf41aded79e9e6c4345ba879128","3869":"https:\/\/www.semanticscholar.org\/paper\/ad3cf68bae32d21f25ac142287d4a556155619d2","3870":"https:\/\/www.semanticscholar.org\/paper\/83b6a76ba5112d27bdbfca3efd2ed918d8e73db5","3871":"https:\/\/www.semanticscholar.org\/paper\/c165cd88aae9d8364b89a7cb83cdd182f722a3f0","3872":"https:\/\/www.semanticscholar.org\/paper\/dbde9692749150152354690e0706c3032b9c6be0","3873":"https:\/\/www.semanticscholar.org\/paper\/11159bdb213aaa243916f42f576396d483ba474b","3874":"https:\/\/www.semanticscholar.org\/paper\/00a407540a8bdd6d7425bd8a561eb21d69682511","3875":"https:\/\/www.semanticscholar.org\/paper\/3e9af4289e4bbd3fb912a68ac8430fd2d9a8ab4d","3876":"https:\/\/www.semanticscholar.org\/paper\/86b7cab6bf7222b8719af55a7dfb5900bee72c20","3877":"https:\/\/www.semanticscholar.org\/paper\/42f46676f991cce7352df9a57c46c134d7d4e562","3878":"https:\/\/www.semanticscholar.org\/paper\/89c8aad71433f7638d2e2c009e1ea20e039f832d","3879":"https:\/\/www.semanticscholar.org\/paper\/2014d6036276b470d010060c86cedd4c92e0958c","3880":"https:\/\/www.semanticscholar.org\/paper\/0c6c9c901d9c10639a12067804dc90c282fc0b92","3881":"https:\/\/www.semanticscholar.org\/paper\/c0cd4b4844c31a27a7900a754d0a91b160b00e55","3882":"https:\/\/www.semanticscholar.org\/paper\/f281ad6573ed8ea7fb3116a3747b0e80ef522ea9","3883":"https:\/\/www.semanticscholar.org\/paper\/65376df1c3ccf41c2cc4d2088d259c8788789f00","3884":"https:\/\/www.semanticscholar.org\/paper\/0ea5edaae43b26798190fd6ead50d447033e7d13","3885":"https:\/\/www.semanticscholar.org\/paper\/58bb24b72fea6d0ce172bdaf9c2f16c2bd7649e9","3886":"https:\/\/www.semanticscholar.org\/paper\/9543d4b9c6f71bdececdad7f397ad68cac359d2d","3887":"https:\/\/www.semanticscholar.org\/paper\/fc1da632130ad49c648c6e55a7726dddf8622e61","3888":"https:\/\/www.semanticscholar.org\/paper\/3c5f1ab37f70db503636075e15b3173f86eea00b","3889":"https:\/\/www.semanticscholar.org\/paper\/6e62a903dac8e643160bd1337d6cb8b09bf2f062","3890":"https:\/\/www.semanticscholar.org\/paper\/c7285702f255a5fdc4502d0b3a6606ae7ce094c0","3891":"https:\/\/www.semanticscholar.org\/paper\/2ee463bba9d4db6aec0eab17e54431a6dc80bf17","3892":"https:\/\/www.semanticscholar.org\/paper\/0e1f153576c7f9f2628cdd34a1067c4d26bdc096","3893":"https:\/\/www.semanticscholar.org\/paper\/4c33b3312875ce39d4475747e6e02f1672ff4886","3894":"https:\/\/www.semanticscholar.org\/paper\/58bb221c1e375f254826b7b7341f74057e87676c","3895":"https:\/\/www.semanticscholar.org\/paper\/420e771cfbd7d31aa33f29a5ba1c5540c0f5f7fd","3896":"https:\/\/www.semanticscholar.org\/paper\/e0ce6638f4a1936a64c6435aa197f112484fd052","3897":"https:\/\/www.semanticscholar.org\/paper\/f2611a09cf0942170785ee3025cb511de3bdec2e","3898":"https:\/\/www.semanticscholar.org\/paper\/65906e6027246ae9e4ecd18d6e019a24505c842e","3899":"https:\/\/www.semanticscholar.org\/paper\/abb33d75dc297993fcc3fb75e0f4498f413eb4f6","3900":"https:\/\/www.semanticscholar.org\/paper\/529025645c70a935221bd434484faee695ad0f25","3901":"https:\/\/www.semanticscholar.org\/paper\/2614a0d0213e8cc457ea62b435a8f43dea54245a","3902":"https:\/\/www.semanticscholar.org\/paper\/2cc3338709ea9c14ff422025ae4a8ad09f9598ba","3903":"https:\/\/www.semanticscholar.org\/paper\/8499e44d42b12518495069a54ae4400baccb7546","3904":"https:\/\/www.semanticscholar.org\/paper\/202f2e0b7ca0e0ac21857da2651384f5b07937de","3905":"https:\/\/www.semanticscholar.org\/paper\/c8965761083d80ff762ce76c08df92d66e01f37d","3906":"https:\/\/www.semanticscholar.org\/paper\/42a7a89fda7a4e6d88ba7d8b130705d5be4f237f","3907":"https:\/\/www.semanticscholar.org\/paper\/7700d89c3a2c898daadd111372566f7146310f5a","3908":"https:\/\/www.semanticscholar.org\/paper\/6baebfd292cb7b914b404afcc75a3956c360a741","3909":"https:\/\/www.semanticscholar.org\/paper\/5d8896396860d2b98af5dbaeb242cf66fe607c2a","3910":"https:\/\/www.semanticscholar.org\/paper\/ef8ab2a0be51a0cd04c2c0f01adfae956a2a84af","3911":"https:\/\/www.semanticscholar.org\/paper\/eb90f7a7f281ccbf64084e11adf822fae2d9bc3f","3912":"https:\/\/www.semanticscholar.org\/paper\/629ae83d63f558e16b530441d765dc822d2949e1","3913":"https:\/\/www.semanticscholar.org\/paper\/eaabb78d0bc44ed132e4d077e9486c86a9e4cda9","3914":"https:\/\/www.semanticscholar.org\/paper\/43dd4b095a267ed20251a40bceca1e447458016c","3915":"https:\/\/www.semanticscholar.org\/paper\/cf4bbe1e39c6f0f88aa69da2a95125b47de0ec7b","3916":"https:\/\/www.semanticscholar.org\/paper\/333671a5fbbf726f8819138f3670524ec0405726","3917":"https:\/\/www.semanticscholar.org\/paper\/c434fce129f205a31871d51c0706b6aa9cfcb3b5","3918":"https:\/\/www.semanticscholar.org\/paper\/5c5105eb4923932f6489b69a7651bff5cbcb77fe","3919":"https:\/\/www.semanticscholar.org\/paper\/2c6df83795cd5baf3b8c6e2639b85e2df0cee1d0","3920":"https:\/\/www.semanticscholar.org\/paper\/6f89a632ceb8fcb81eac3d7b52e937099659cc6a","3921":"https:\/\/www.semanticscholar.org\/paper\/cc1697038a9fcca37977b3b0e1bd9d434d3d9a3e","3922":"https:\/\/www.semanticscholar.org\/paper\/f779529aeee1b0b7ad9c0f1845523ff7352ba775","3923":"https:\/\/www.semanticscholar.org\/paper\/a473f545318325ba23b7a6b477485d29777ba873","3924":"https:\/\/www.semanticscholar.org\/paper\/264b9b136889da3b4d7e50ef58c77678b35dc3e0","3925":"https:\/\/www.semanticscholar.org\/paper\/dccd738bc67c1e4b807b07872ff065fadc4253da","3926":"https:\/\/www.semanticscholar.org\/paper\/74bfbcb8af4aee66613585c1c81b2cffe1a720b1","3927":"https:\/\/www.semanticscholar.org\/paper\/78681942efaa59e2aa92d0ebfdb8539e4f2df95b","3928":"https:\/\/www.semanticscholar.org\/paper\/8ee8d51b5c29f6aadd0b6dcff2a475373a3a021b","3929":"https:\/\/www.semanticscholar.org\/paper\/5d687576015178a9847d1a84f1592eb605193751","3930":"https:\/\/www.semanticscholar.org\/paper\/73eaa1c2888c804448c6d2db237ff1b247543bdf","3931":"https:\/\/www.semanticscholar.org\/paper\/54ca0d1f1adf6a7d525de07b79a5e7ea2bf4862e","3932":"https:\/\/www.semanticscholar.org\/paper\/6b93cedfe768eb8b5ece92612aac9cc8e986d12a","3933":"https:\/\/www.semanticscholar.org\/paper\/c194307b9dda7f53a0c639319d2ec1c23b86e6a6","3934":"https:\/\/www.semanticscholar.org\/paper\/b4c77c51c576b84035192f20a9af3063dcc6d946","3935":"https:\/\/www.semanticscholar.org\/paper\/488f4bed57ac98e03826e46faff8093f6bdd4b75","3936":"https:\/\/www.semanticscholar.org\/paper\/70043a0b612b6253b37df7d363b3bf2ec3d581c7","3937":"https:\/\/www.semanticscholar.org\/paper\/1a6fc05a37ae1b2fc7c6193cb6dc8282767c2cb6","3938":"https:\/\/www.semanticscholar.org\/paper\/449ff2404a7b11fa3c47cd228fa5469b00ffef20","3939":"https:\/\/www.semanticscholar.org\/paper\/e901bef9b0bbe3a706827b1ca5e406788e9e6681","3940":"https:\/\/www.semanticscholar.org\/paper\/9c404d02aefd850ac3d5a8bdc5860738e6cd2b04","3941":"https:\/\/www.semanticscholar.org\/paper\/3a83d8595e6727269c876fcebd23ee9ddd524b76","3942":"https:\/\/www.semanticscholar.org\/paper\/83311744b174550032cfe09cb2940703dc9c9245","3943":"https:\/\/www.semanticscholar.org\/paper\/d3b9a465ded4172e5056939debb4dae674c68af8","3944":"https:\/\/www.semanticscholar.org\/paper\/8b365890c0224f17fffb90bf33da46fccacd9331","3945":"https:\/\/www.semanticscholar.org\/paper\/14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9","3946":"https:\/\/www.semanticscholar.org\/paper\/0c432b171e52c86203843e200260f443a72ecaed","3947":"https:\/\/www.semanticscholar.org\/paper\/198838f8b7b504c04214fffb8646d11c6152ba5e","3948":"https:\/\/www.semanticscholar.org\/paper\/2dcd72e8c348052fc4bdb01851fb4f288e7492a2","3949":"https:\/\/www.semanticscholar.org\/paper\/62c3142956d54db158d190ce691e3c13e7897412","3950":"https:\/\/www.semanticscholar.org\/paper\/47a8355e76c3675c481f135cce3a5911c74aeac3","3951":"https:\/\/www.semanticscholar.org\/paper\/8e76c41f4df07fdc512cb13f8e9b14e1692577ed","3952":"https:\/\/www.semanticscholar.org\/paper\/e54faa25187ace6a6dbe01b0fa58e8ee9a250758","3953":"https:\/\/www.semanticscholar.org\/paper\/489abbb6e3afc7c8a378b6bc15e3cd4a13f8d5f5","3954":"https:\/\/www.semanticscholar.org\/paper\/61fc451fc27df73052ff24d807f282481779818a","3955":"https:\/\/www.semanticscholar.org\/paper\/4282528c34ad64a309a2d90404830a7788aafd5f","3956":"https:\/\/www.semanticscholar.org\/paper\/51eaaebdb856096ea5d8fceccd9bf1d40d0ac002","3957":"https:\/\/www.semanticscholar.org\/paper\/73a6e1234a3a38bef93f9097d59f01d5032cbc92","3958":"https:\/\/www.semanticscholar.org\/paper\/b7048c7b865949c306d4c6a583cd0235c440af1e","3959":"https:\/\/www.semanticscholar.org\/paper\/06deb11d97639514b9ef569256c8ebf0c7ffd471","3960":"https:\/\/www.semanticscholar.org\/paper\/86b2ebb930e75779d42ca0be798fb80921edcef5","3961":"https:\/\/www.semanticscholar.org\/paper\/98789b46f7be983300a0e93e1c53bab56b36efd1","3962":"https:\/\/www.semanticscholar.org\/paper\/5f6baf6a7342715060c389e9e1c71d23398e0f72","3963":"https:\/\/www.semanticscholar.org\/paper\/5936b8dcaa3f57c1202e2e75870d4eeb83eb2d21","3964":"https:\/\/www.semanticscholar.org\/paper\/0a309c2c47c34f51ef94e8075f11586ad2b1dd2b","3965":"https:\/\/www.semanticscholar.org\/paper\/19e074731a7e337e8cc6453a73fec44c0ae90b02","3966":"https:\/\/www.semanticscholar.org\/paper\/538e7b056aca5be7fa51b91241741d6107e6a3d7","3967":"https:\/\/www.semanticscholar.org\/paper\/00d51f2684dae05b8109f36dd9631b50f1f2f78e","3968":"https:\/\/www.semanticscholar.org\/paper\/aa6b9d6081646a5461d953dc2412b6e0344cc2d9","3969":"https:\/\/www.semanticscholar.org\/paper\/0f016f28e3b1cdb97ed50acc5b3c36026e083c91","3970":"https:\/\/www.semanticscholar.org\/paper\/15455ad33ae60c10a276de85fee304de6a978e18","3971":"https:\/\/www.semanticscholar.org\/paper\/406289a706af2e00006a5823f245ef7bcd811681","3972":"https:\/\/www.semanticscholar.org\/paper\/48de7d26deaebcef0043a50346835437f6b32cc4","3973":"https:\/\/www.semanticscholar.org\/paper\/75994ebb52094581dcb7d145795f6bafe6e276bb","3974":"https:\/\/www.semanticscholar.org\/paper\/d5901f15a0214b50e6a0085337e49a9b966775a7","3975":"https:\/\/www.semanticscholar.org\/paper\/4e80898c8c79c0bc770f29ac51ccd283b5e0d9de","3976":"https:\/\/www.semanticscholar.org\/paper\/01ceac73fc71d545063f0ebafbc112c80d29b016","3977":"https:\/\/www.semanticscholar.org\/paper\/7699f04998490d0df2da50348d03313a0f72641a","3978":"https:\/\/www.semanticscholar.org\/paper\/d51ebec3064f82ea4128fc1c3241003d4072c639","3979":"https:\/\/www.semanticscholar.org\/paper\/4cf99ce2fd012f494399f028f763422b8ad97a91","3980":"https:\/\/www.semanticscholar.org\/paper\/ceb31a1f6901a7aa5995af472abaac1035f3a154","3981":"https:\/\/www.semanticscholar.org\/paper\/21e68ec2a9eb28d1b4ade83622c65a6995b345d4","3982":"https:\/\/www.semanticscholar.org\/paper\/1a17b67534a1c4378dea6b1b72af1847ffdcaa66","3983":"https:\/\/www.semanticscholar.org\/paper\/0efdc031c42c031f3092f3a4f85ae2e0a6c9cac4","3984":"https:\/\/www.semanticscholar.org\/paper\/b4caa67681cbe4973b21e96e69ad7b213b19f28f","3985":"https:\/\/www.semanticscholar.org\/paper\/c57a28759b6e1ddd7be03fa41e82b01130c8f8b2","3986":"https:\/\/www.semanticscholar.org\/paper\/89153f52ca3db65db85b2da24d7970d9e1c15023","3987":"https:\/\/www.semanticscholar.org\/paper\/7734b98c370ced5d0a58bd6a808a1c51cb16557f","3988":"https:\/\/www.semanticscholar.org\/paper\/8d272a9948a8b97c3798f233d3631d466688da1b","3989":"https:\/\/www.semanticscholar.org\/paper\/725d0ee606a8c84fc055b5155f310dc2353398c2","3990":"https:\/\/www.semanticscholar.org\/paper\/5688b8077117b3aafd54c2e71d959284f4d5c8b9","3991":"https:\/\/www.semanticscholar.org\/paper\/6a505dbfb89cf05344457bf85b2e8307af5c4ad0","3992":"https:\/\/www.semanticscholar.org\/paper\/0f396da16c85de7abafd6d28a7eedc777ca87594","3993":"https:\/\/www.semanticscholar.org\/paper\/480ad315ab6f709791bdd2e7df95bb7fa42fadc9","3994":"https:\/\/www.semanticscholar.org\/paper\/62635461b44d9ff7901fbce982a5c1bbccfaf2b0","3995":"https:\/\/www.semanticscholar.org\/paper\/a53b798078e9f94e0059a34eb7c5078c054887c0","3996":"https:\/\/www.semanticscholar.org\/paper\/ab32452f63ccd601e3c17eccb55dc0a861c4b64b","3997":"https:\/\/www.semanticscholar.org\/paper\/5c3a72f47ed8d58c0554210828af1ce4bbf2dbcd","3998":"https:\/\/www.semanticscholar.org\/paper\/8ece479b5dfed4727d2d9b9763f777bb9a94096e","3999":"https:\/\/www.semanticscholar.org\/paper\/dd5a6c9ebde898e89e2d3db8153de774dce53c1a","4000":"https:\/\/www.semanticscholar.org\/paper\/e0e51b697701cca5ee1a46348c73a9eb80ef8094","4001":"https:\/\/www.semanticscholar.org\/paper\/758289be073e7d4e6be6e6e4a7f937ef48ae81bd","4002":"https:\/\/www.semanticscholar.org\/paper\/857c6d87352f40b31a08ca8f5e4ddae6c97aa937","4003":"https:\/\/www.semanticscholar.org\/paper\/a29ceec5ee2b32acc594f264e1a19dfb136af6b0","4004":"https:\/\/www.semanticscholar.org\/paper\/1df83e4dc0ab34d77179962e156b7bd1a65f0e0d","4005":"https:\/\/www.semanticscholar.org\/paper\/7eb03a3c0605d688959eef30518f6eb268996eae","4006":"https:\/\/www.semanticscholar.org\/paper\/3157ccb0c9812a6ee22a6c5c4cebfae3be42b148","4007":"https:\/\/www.semanticscholar.org\/paper\/cbdd39db7b246ec39b8c63c80ba89bfdcb35e40c","4008":"https:\/\/www.semanticscholar.org\/paper\/eb41a0cffa84d3d6035e6f5f420806ddc962b1e6","4009":"https:\/\/www.semanticscholar.org\/paper\/dc44e2be0f85b6225f05390c570885337a99ef83","4010":"https:\/\/www.semanticscholar.org\/paper\/5edfff6050dd16a5228ead3954ba670661f87802","4011":"https:\/\/www.semanticscholar.org\/paper\/1b1109ed7549c009738caccb4405a04373c82e51","4012":"https:\/\/www.semanticscholar.org\/paper\/2a1573cfa29a426c695e2caf6de0167a12b788ef","4013":"https:\/\/www.semanticscholar.org\/paper\/8fdb305a31cc52c7ef4bd11b01b5f8f63751f32c","4014":"https:\/\/www.semanticscholar.org\/paper\/77a5e08f361b6f91cac8a24b380a14c12bb93383","4015":"https:\/\/www.semanticscholar.org\/paper\/39d1f020a585d3f28cb4b4c14497649e6a469ef1","4016":"https:\/\/www.semanticscholar.org\/paper\/1288363a2ae927a6a041c4311f71012885f5f417","4017":"https:\/\/www.semanticscholar.org\/paper\/c777dd70d3ccd38e85b4532b5d6b91dce1fd29ce","4018":"https:\/\/www.semanticscholar.org\/paper\/71d9222b39d0539322b6083de6790fc630129b9f","4019":"https:\/\/www.semanticscholar.org\/paper\/d13d43e5fb57e44b44af58a4e97f9b5344dad97b","4020":"https:\/\/www.semanticscholar.org\/paper\/25a1070c7d2cca21731d1427038c49be370525a2","4021":"https:\/\/www.semanticscholar.org\/paper\/540e19cca64b2d9a13b6caac81124c5f097517fa","4022":"https:\/\/www.semanticscholar.org\/paper\/cf909cef5da4b7133b0b34c22ed341fcc56c1d86","4023":"https:\/\/www.semanticscholar.org\/paper\/9830c9d16293f8f87f998aa449143f0ed1554d1a","4024":"https:\/\/www.semanticscholar.org\/paper\/484188acf1600e6fda850118eb2b4d5738bca741","4025":"https:\/\/www.semanticscholar.org\/paper\/9ffcb3624f2637b5d0fe28c61ec8472293cfebc7","4026":"https:\/\/www.semanticscholar.org\/paper\/fb6e44d3f18523fc950473c3505783cd661cfc0e","4027":"https:\/\/www.semanticscholar.org\/paper\/610d20ce5caae463ddac70cc2ad128702e07371c","4028":"https:\/\/www.semanticscholar.org\/paper\/5100a9ea286ef799be96d9948067badf60613b95","4029":"https:\/\/www.semanticscholar.org\/paper\/fd4411f035e6ad587d265d01c28522f0e02206c8","4030":"https:\/\/www.semanticscholar.org\/paper\/a8b43f965dbecbd1c830a288a4ce0c6055cd243a","4031":"https:\/\/www.semanticscholar.org\/paper\/5619b00592912299690b8ae5d0d098af9815f606","4032":"https:\/\/www.semanticscholar.org\/paper\/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70","4033":"https:\/\/www.semanticscholar.org\/paper\/ae0ea06b05a19e11f5a85bdb1eb5553b8580ad96","4034":"https:\/\/www.semanticscholar.org\/paper\/8951eefd1ffc7ff515ee2bf1509358cbd07ae935","4035":"https:\/\/www.semanticscholar.org\/paper\/e6fbb8d3bededb92d1e311fbb6b2625453fc6642","4036":"https:\/\/www.semanticscholar.org\/paper\/33cf9b4d6c76f988380b1adff2c06c30010f93d3","4037":"https:\/\/www.semanticscholar.org\/paper\/14a51814b1f68d3f748d786c16dcd4047058867e","4038":"https:\/\/www.semanticscholar.org\/paper\/1378312e1b3f53c5cdb2809da31ad9e58b29955d","4039":"https:\/\/www.semanticscholar.org\/paper\/98cd74de068b43de110bca0225a1624bd15c2f24","4040":"https:\/\/www.semanticscholar.org\/paper\/41ada02f5d67a2ff3a1de7d44445896f1e8008a7","4041":"https:\/\/www.semanticscholar.org\/paper\/a0ba972791a530641cec11a7b8de18a3dcaa45fb","4042":"https:\/\/www.semanticscholar.org\/paper\/1599cca7f50f9bf84982c54a9e197ff51ec9a166","4043":"https:\/\/www.semanticscholar.org\/paper\/1a053814c50045d75673325c4edd2f2ad9c8162d","4044":"https:\/\/www.semanticscholar.org\/paper\/0e97f8694c65f5e28f17de6004dd4ef33db88014","4045":"https:\/\/www.semanticscholar.org\/paper\/c463e0a0b9a4c53ad6a90d5254bb9bc03de14033","4046":"https:\/\/www.semanticscholar.org\/paper\/ade3e857df5304ef198371df029f69d1625f7899","4047":"https:\/\/www.semanticscholar.org\/paper\/bd499c0b2b14fe1a9923c70cd412778df6641ca0","4048":"https:\/\/www.semanticscholar.org\/paper\/edc3481236ccf9839906fcbdf7de9d67f5999d8e","4049":"https:\/\/www.semanticscholar.org\/paper\/60f6c4ef63c6094ac05d261228f436e9304684b4","4050":"https:\/\/www.semanticscholar.org\/paper\/c8e1c539ecbfdfe26d9dee5f07d97ed757eac019","4051":"https:\/\/www.semanticscholar.org\/paper\/36f9d88c77ec6cf870d0f4c0713223d285757dcd","4052":"https:\/\/www.semanticscholar.org\/paper\/603325146378b449f75c71411b400a97b1fee5f6","4053":"https:\/\/www.semanticscholar.org\/paper\/25ed9b44cf41e38b3917b8dfba34e90b5a707c0b","4054":"https:\/\/www.semanticscholar.org\/paper\/b241da5a4dada67bc36b19c24b8cb73d03f97172","4055":"https:\/\/www.semanticscholar.org\/paper\/acb670a61a292109fa827d81749432eb316dd0b6","4056":"https:\/\/www.semanticscholar.org\/paper\/3d9e24e36725f3a138452636abb481d1d601ed89","4057":"https:\/\/www.semanticscholar.org\/paper\/0403275945c0f6d96fb22f69447b70c8967403f1","4058":"https:\/\/www.semanticscholar.org\/paper\/081d4a3f110609cf4ab430c3b5468e7f02d6e4a7","4059":"https:\/\/www.semanticscholar.org\/paper\/3307c819f60b6fbc0ac04d7ef2510f60a94eee5d","4060":"https:\/\/www.semanticscholar.org\/paper\/304f2d1cf97ac8fb071b5357e97691d64f4e8f6a","4061":"https:\/\/www.semanticscholar.org\/paper\/660a2244efa8af0b77fd314a1c75dcc01aa677fe","4062":"https:\/\/www.semanticscholar.org\/paper\/74e851dde981cacc20650aca29729cb8270309c5","4063":"https:\/\/www.semanticscholar.org\/paper\/8cc80339233348dc6ec2a4a72d66e5b6c2166849","4064":"https:\/\/www.semanticscholar.org\/paper\/3bd24f29705b5f8ca5077e571ed78ea6aa4b11f5","4065":"https:\/\/www.semanticscholar.org\/paper\/e62c3e7589ecd73d910da751143a49be5510616c","4066":"https:\/\/www.semanticscholar.org\/paper\/3fb4e0af816525a226d41ceae4b3299fbc3b9977","4067":"https:\/\/www.semanticscholar.org\/paper\/30c89ca3723a080f79cada709c198f538069306b","4068":"https:\/\/www.semanticscholar.org\/paper\/b2acbdf00bb1b25974f040de82cfec23dd9680fb","4069":"https:\/\/www.semanticscholar.org\/paper\/0a7109502e7fe91f4decc3dd3515e1fecbc02da7","4070":"https:\/\/www.semanticscholar.org\/paper\/eded05c804d2c91a1653efbe36143dca22fe9cfe","4071":"https:\/\/www.semanticscholar.org\/paper\/60758a8b15d843ed4f731b4eaa7832be8a7a7e13","4072":"https:\/\/www.semanticscholar.org\/paper\/a96f474155870f101a39b37521e83950ef7eb913","4073":"https:\/\/www.semanticscholar.org\/paper\/7a7b6b7499107eb4043d8a88b2880319c17c2c18","4074":"https:\/\/www.semanticscholar.org\/paper\/99e71f1d31ff6c555b71f1c908de206026e0cc86","4075":"https:\/\/www.semanticscholar.org\/paper\/c65a2b18be114d29b824ac74dbf30f79cc3a16e6","4076":"https:\/\/www.semanticscholar.org\/paper\/a109274aa61679a5d95058b4bd20fa7acba0df52","4077":"https:\/\/www.semanticscholar.org\/paper\/22f018dfbb389ff49d67de0eb8ab137de4848138","4078":"https:\/\/www.semanticscholar.org\/paper\/ecd3b160b4bd13c5a1dc085fcc17c540435a284a","4079":"https:\/\/www.semanticscholar.org\/paper\/8fd2b272d824d5322928bc40b55c047f35717f8a","4080":"https:\/\/www.semanticscholar.org\/paper\/70b2ab913d56cec4894187ff0a1a74f7b3a8da84","4081":"https:\/\/www.semanticscholar.org\/paper\/50c2f70e2d8393e2801c8bd0ff2ce423e6416408","4082":"https:\/\/www.semanticscholar.org\/paper\/d184b51b7d4351f40bd397e53409306c0ac9d87c","4083":"https:\/\/www.semanticscholar.org\/paper\/0276aecb97ffa1f7f279e2768abb3484933eacfb","4084":"https:\/\/www.semanticscholar.org\/paper\/dfa67fa1b5eafc1e350c4a7357ce6c57aa6b989a","4085":"https:\/\/www.semanticscholar.org\/paper\/7199d7de8c78f95a78142b63e5f4d38adf9a4cbc","4086":"https:\/\/www.semanticscholar.org\/paper\/3862698833e74d527b66a57901004092b26b11d1","4087":"https:\/\/www.semanticscholar.org\/paper\/a6c87b0515ce63b22967d12a96805db86f7c06a6","4088":"https:\/\/www.semanticscholar.org\/paper\/174f22193cc87d151d1e89b83c278dcc0fb86c89","4089":"https:\/\/www.semanticscholar.org\/paper\/d68e6770208634689788a31598212e46939202f7","4090":"https:\/\/www.semanticscholar.org\/paper\/0dacda0904797da8ab3ccafb26547813baff729f","4091":"https:\/\/www.semanticscholar.org\/paper\/54a4b505f267d5c27bcaa70d6a259a1035d22395","4092":"https:\/\/www.semanticscholar.org\/paper\/2d7d94dd0d2a3f80652babf40fe3414c631890be","4093":"https:\/\/www.semanticscholar.org\/paper\/42d1eacdc929c74f2c0ebfb342e4e8a94b0f099a","4094":"https:\/\/www.semanticscholar.org\/paper\/0c42c433e2b5b61028e9bb43c5e5f464feb3b140","4095":"https:\/\/www.semanticscholar.org\/paper\/c80a0c6fcdadc1afe848d7089a314192897f3ab1","4096":"https:\/\/www.semanticscholar.org\/paper\/abf4b28be13fec9934ac87aa43ae48f052fd57a1","4097":"https:\/\/www.semanticscholar.org\/paper\/42525a5143c6a87d3ab466684dfa471dc43a5bd0","4098":"https:\/\/www.semanticscholar.org\/paper\/8f33fd211dc35172f0f42ce7da7b3248009f2543","4099":"https:\/\/www.semanticscholar.org\/paper\/6766c295d1049085fd50b42798e11407360ac647","4100":"https:\/\/www.semanticscholar.org\/paper\/c86e78458721a0d02ece7ca376e60cb0ed076377","4101":"https:\/\/www.semanticscholar.org\/paper\/4e383bb858f43beacbef91fe65db5b81f5cc8d6c","4102":"https:\/\/www.semanticscholar.org\/paper\/eb22e505d84b31f52a5bd8bd973f89711891e836","4103":"https:\/\/www.semanticscholar.org\/paper\/d09bec5af4eef5038e48b26b6c14098f95997114","4104":"https:\/\/www.semanticscholar.org\/paper\/30272deaf0ff25e443c399f0902f775f03aaff69","4105":"https:\/\/www.semanticscholar.org\/paper\/89664effe9311a20a609d3a2cb514e5ef57cb9ce","4106":"https:\/\/www.semanticscholar.org\/paper\/a9f5e549984d0b581e99ef8d34db6438e98db039","4107":"https:\/\/www.semanticscholar.org\/paper\/5bc368778a5c82861282c809d8d80d486a233a9c","4108":"https:\/\/www.semanticscholar.org\/paper\/a9a65a4ef249d78d639b1879f7122b1c36fe886c","4109":"https:\/\/www.semanticscholar.org\/paper\/78388a6f7ced31b202f1891907e68115eacbb82a","4110":"https:\/\/www.semanticscholar.org\/paper\/f0975185f3e0d5230b10c95903bf8760c6d35cce","4111":"https:\/\/www.semanticscholar.org\/paper\/4f387e240d9428cb889ef4ccc1858ba2d39920b2","4112":"https:\/\/www.semanticscholar.org\/paper\/323fb2b9adbfaa7358a26d3f2438d17c2b4a53db","4113":"https:\/\/www.semanticscholar.org\/paper\/f7fea1eb9bf84d0689d88c3a3258e77b24210a4e","4114":"https:\/\/www.semanticscholar.org\/paper\/1e8e2a2baca26b80b5bb8d46a257d99a432e7c34","4115":"https:\/\/www.semanticscholar.org\/paper\/08e81791694650cd1e0a866d65214cac78cfc006","4116":"https:\/\/www.semanticscholar.org\/paper\/d43b47a09953c3bcc4855e1d64394a104b25a9f3","4117":"https:\/\/www.semanticscholar.org\/paper\/4c43be7370939034eaae33f274e9681414ba4d09","4118":"https:\/\/www.semanticscholar.org\/paper\/157592a892ee44dd5b3b6a06f9191ac5ec8407ac","4119":"https:\/\/www.semanticscholar.org\/paper\/2da8fb66124ae185404aea018e08705461978266","4120":"https:\/\/www.semanticscholar.org\/paper\/d83b033956e352559778c42b8997408fc8979e91","4121":"https:\/\/www.semanticscholar.org\/paper\/da12b03ad7cee8030b5e4c87b25edaae706d7106","4122":"https:\/\/www.semanticscholar.org\/paper\/7caddc503aab01e3225d1a8644a6af2ef0a8a9a7","4123":"https:\/\/www.semanticscholar.org\/paper\/57797c74ccf9330dbb608bb721d5c2b28500e678","4124":"https:\/\/www.semanticscholar.org\/paper\/a474767a821f0f91b502629b6b8deeb345ed58f4","4125":"https:\/\/www.semanticscholar.org\/paper\/15cf1e421bd6bea554f6cfdb7f6591faf92aad3a","4126":"https:\/\/www.semanticscholar.org\/paper\/7f329aa5e347a5275ecc8ccadf9e43ca91fd526b","4127":"https:\/\/www.semanticscholar.org\/paper\/18113dc7238722b722e333d903956b961ba48bd9","4128":"https:\/\/www.semanticscholar.org\/paper\/e74eb6147977d94ac5db4ff779e6d4e53feeed75","4129":"https:\/\/www.semanticscholar.org\/paper\/9e8742776899b5ab30b15122088cfa027ff632c8","4130":"https:\/\/www.semanticscholar.org\/paper\/c129d712fa315894545ca1d2ad32bdaf3bade97d","4131":"https:\/\/www.semanticscholar.org\/paper\/006c846c72e77cb913be4b2c76664967e9e01ee0","4132":"https:\/\/www.semanticscholar.org\/paper\/cb05457e4616a3b436c0cfb9306ae740fbd53c0c","4133":"https:\/\/www.semanticscholar.org\/paper\/dcd0cb1dfa352768a4189b25fcef8b59d51e291d","4134":"https:\/\/www.semanticscholar.org\/paper\/7fe873aef60126d59b3f352f5df8746bc6f52314","4135":"https:\/\/www.semanticscholar.org\/paper\/5a5a1d666e4b7b933bc5aafbbadf179bc447ee67","4136":"https:\/\/www.semanticscholar.org\/paper\/4f6b7f8daa322801887b2ef8c2c14788e607e3b8","4137":"https:\/\/www.semanticscholar.org\/paper\/50791e739e8b36e558c8ef8dfbc681045dc9e2bc","4138":"https:\/\/www.semanticscholar.org\/paper\/434be8e804a79843377d1b8d94d1fcde2feb4ad2","4139":"https:\/\/www.semanticscholar.org\/paper\/a03023d3aca966a96fa158850a42d737f097fe58","4140":"https:\/\/www.semanticscholar.org\/paper\/3f7d9025c9229c9e9e95c138ccef3c98e9b2547a","4141":"https:\/\/www.semanticscholar.org\/paper\/e1a1acc2f014e0bed294fb5594840ebbed3800d0","4142":"https:\/\/www.semanticscholar.org\/paper\/26680fec21a9885c81109e2a9b898131b0f0d880","4143":"https:\/\/www.semanticscholar.org\/paper\/dffbc930b456886195159ee4552ec33e6bd00032","4144":"https:\/\/www.semanticscholar.org\/paper\/215a8c716a98222bac5ffcf683d24779c82d5cd5","4145":"https:\/\/www.semanticscholar.org\/paper\/decb7e746acb87710c2a15585cd22133ffc2cc95","4146":"https:\/\/www.semanticscholar.org\/paper\/d3d1714b03ed1ff4d3826bfffed1759aed0f783a","4147":"https:\/\/www.semanticscholar.org\/paper\/3cb2bd7e165934b1e6c13e2e9efc6ac7fb6ac40a","4148":"https:\/\/www.semanticscholar.org\/paper\/ea5510d91e336445b4f27c28d535b6531fb29ea2","4149":"https:\/\/www.semanticscholar.org\/paper\/d89e69458239f8cef634671a6315310e75891362","4150":"https:\/\/www.semanticscholar.org\/paper\/41ca1118a5878af28be2ffacb322311e00c88bfa","4151":"https:\/\/www.semanticscholar.org\/paper\/7081b74ac451a4ce8944a4fa7a8ea729c38b4f16","4152":"https:\/\/www.semanticscholar.org\/paper\/f48b2ef3c2f2f9ef7402e6c25ba88a570084a688","4153":"https:\/\/www.semanticscholar.org\/paper\/39c40dbd69405e8a6b4fd893e0f85a1422285c06","4154":"https:\/\/www.semanticscholar.org\/paper\/30fa32143a3a93d0bfa5911f288a3e51b08c066a","4155":"https:\/\/www.semanticscholar.org\/paper\/1cfb9a49ebf79ecffd40b9277acef2e6bbb6017c","4156":"https:\/\/www.semanticscholar.org\/paper\/9b220dabcc7e2c7f339e7950839d8acf44d5b647","4157":"https:\/\/www.semanticscholar.org\/paper\/8efe8b523378650e23ac4c884a5dc1006fcaac0c","4158":"https:\/\/www.semanticscholar.org\/paper\/c9fd2a6b240caf3d18f641d5956e71cbd29930e3","4159":"https:\/\/www.semanticscholar.org\/paper\/3a03070c010783828702d51b2170e85b4b46fdec","4160":"https:\/\/www.semanticscholar.org\/paper\/a99caae8480c9d0a7186edba5d9009844a85c8d0","4161":"https:\/\/www.semanticscholar.org\/paper\/545f142a246db9e65c65ec81f619d3ef093cca64","4162":"https:\/\/www.semanticscholar.org\/paper\/01e17f10573d5efdf5abb4149137def7c062d469","4163":"https:\/\/www.semanticscholar.org\/paper\/b4a8461622e813390467fa754f06bd564cbfba7a","4164":"https:\/\/www.semanticscholar.org\/paper\/6fdb77260fc83dff91c44fea0f31a2cb8ed13d04","4165":"https:\/\/www.semanticscholar.org\/paper\/aa070c9be27ac21e8c3710caed11dee2a15876c4","4166":"https:\/\/www.semanticscholar.org\/paper\/4667516645316df96a84bab4717eeed56d415673","4167":"https:\/\/www.semanticscholar.org\/paper\/c37cc9be242f3596bd640298cd66869baabf6bf2","4168":"https:\/\/www.semanticscholar.org\/paper\/bd318e959236b0d33a7567b6d3afc8d5e92b8ea3","4169":"https:\/\/www.semanticscholar.org\/paper\/cdf411f8d1033b1ee15448f8fd0c2463e61d7626","4170":"https:\/\/www.semanticscholar.org\/paper\/0dcc5e613f2adff3f1eade232cd0488a4cde50ca","4171":"https:\/\/www.semanticscholar.org\/paper\/901a3e866cfae421407e2e3c8a872befa03cb4cd","4172":"https:\/\/www.semanticscholar.org\/paper\/60f432331abc0976cf236c738844f9427277b0de","4173":"https:\/\/www.semanticscholar.org\/paper\/92f092ef414d24b8facc6e2114c07abe4e6bc16d","4174":"https:\/\/www.semanticscholar.org\/paper\/0eca74b8f244a2329baccf5dd18fe91e260f152f","4175":"https:\/\/www.semanticscholar.org\/paper\/3ba94cfdeffbcab3f988b16b39571e6fe1ab4df9","4176":"https:\/\/www.semanticscholar.org\/paper\/0b6ae38488a4cf623a53cd25770b210ee6ba33fc","4177":"https:\/\/www.semanticscholar.org\/paper\/3be6455d00ff4a69e023b7b5c5d4367decb652c0","4178":"https:\/\/www.semanticscholar.org\/paper\/3020cdaab111f423e1d13f00e0844ed1191bfdd8","4179":"https:\/\/www.semanticscholar.org\/paper\/41326f8a19b7a20983d4aa092fb0318dc683404d","4180":"https:\/\/www.semanticscholar.org\/paper\/f9de2453c86fdf76d5f10c2e5dc20d93f286c66d","4181":"https:\/\/www.semanticscholar.org\/paper\/349ded19daa0a9395ce0e8f66bddda550f81caae","4182":"https:\/\/www.semanticscholar.org\/paper\/390808617b277987999b6e8e0aa5742aab54a6a0","4183":"https:\/\/www.semanticscholar.org\/paper\/fe5a8627e1f0b03a70adb5ff2b404084bca0d806","4184":"https:\/\/www.semanticscholar.org\/paper\/818b92bfad6e11d849bae552be60111579d91e91","4185":"https:\/\/www.semanticscholar.org\/paper\/ebd8ca0d893ae7fbb1c811a16f6e858e5caa8953","4186":"https:\/\/www.semanticscholar.org\/paper\/b60fc1de25d9c9f9c23e351f7c4664d56c6c0992","4187":"https:\/\/www.semanticscholar.org\/paper\/155108784e14747496af8ff5a1e2d40d72d2b7fb","4188":"https:\/\/www.semanticscholar.org\/paper\/c15238ccd8a17ff44c30527d24c15434ec8eeb7a","4189":"https:\/\/www.semanticscholar.org\/paper\/e7ef9b506b1e4614858351a6a148774e76c1efad","4190":"https:\/\/www.semanticscholar.org\/paper\/5b7529278aa88fbaabdb54c9f0026d38a9b23b33","4191":"https:\/\/www.semanticscholar.org\/paper\/ce901b01c3e47ebfeeb820b7f626301155478daf","4192":"https:\/\/www.semanticscholar.org\/paper\/3839aae244763195f1437e7c00d2c6c8f5e3b549","4193":"https:\/\/www.semanticscholar.org\/paper\/c0ec586c4e205b2d1cbd664c919a5a8af3912096","4194":"https:\/\/www.semanticscholar.org\/paper\/b2a466efec8c259ae789f91f413f342e0588792d","4195":"https:\/\/www.semanticscholar.org\/paper\/5284fa434c4f806b168c4eb3355ad463c2804ceb","4196":"https:\/\/www.semanticscholar.org\/paper\/d0bcefbdd145ae02890742010d187eda1619f685","4197":"https:\/\/www.semanticscholar.org\/paper\/2785e310a83fbc61c8cc018ef941e68175758573","4198":"https:\/\/www.semanticscholar.org\/paper\/bf4bf19f021c48910048741873627691d8277a61","4199":"https:\/\/www.semanticscholar.org\/paper\/165bb688bd07d8cf22f4f65a7996c7fbc5a020d4","4200":"https:\/\/www.semanticscholar.org\/paper\/851045a2ee84318a16a14e6286acf32744da74ca","4201":"https:\/\/www.semanticscholar.org\/paper\/5af10ef20f96afa4009170471e42ef5c4eb3bd94","4202":"https:\/\/www.semanticscholar.org\/paper\/2d0b34e31f02455c2d370a84645b295af6d59702","4203":"https:\/\/www.semanticscholar.org\/paper\/6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e","4204":"https:\/\/www.semanticscholar.org\/paper\/cc04e4a58098d675adf18aa772620fe3e620b5f9","4205":"https:\/\/www.semanticscholar.org\/paper\/6d78d67d4f7f5fe2e66933778ab1faf119d21547","4206":"https:\/\/www.semanticscholar.org\/paper\/1958aa35275227883bcbf95bb28f9eed0f7d8bd8","4207":"https:\/\/www.semanticscholar.org\/paper\/0d25a53184a9c56084416b292de9a8fef4b27347","4208":"https:\/\/www.semanticscholar.org\/paper\/eb21b2d1e82c7ec6f78f43e5eb4692456bf3e21e","4209":"https:\/\/www.semanticscholar.org\/paper\/42cabbf853768c1a57586e7963dc0f5ff02fbfbd","4210":"https:\/\/www.semanticscholar.org\/paper\/eddfb9be78cfe94193766e3722eb0e56c3d24cef","4211":"https:\/\/www.semanticscholar.org\/paper\/9ea07ec22ea06a59c21d326d9bf53fdc3b602f10","4212":"https:\/\/www.semanticscholar.org\/paper\/11fa695a4e6b0f20a396edc4010d4990c8d29fe9","4213":"https:\/\/www.semanticscholar.org\/paper\/fac8962a3e6b52291b629555a3d8de56db1a3c38","4214":"https:\/\/www.semanticscholar.org\/paper\/1eea999549154f0468ced9c83440a78abbd061b8","4215":"https:\/\/www.semanticscholar.org\/paper\/f94c58af515c4c9621762f2276adbe14ac1031d5","4216":"https:\/\/www.semanticscholar.org\/paper\/0570e8fc8b02e7eb66e798b00726fba0592ea90f","4217":"https:\/\/www.semanticscholar.org\/paper\/b950f3f6d7964507b324eb24dbfc2df7461404d6","4218":"https:\/\/www.semanticscholar.org\/paper\/6c7ba2af4b3e472bd8a5717367b88dcdd4abbd31","4219":"https:\/\/www.semanticscholar.org\/paper\/b82df2c16cf91cd2b0316ad0bb354a587870a377","4220":"https:\/\/www.semanticscholar.org\/paper\/50aea7bae4c478e850f218e58da0e24f501ab8fc","4221":"https:\/\/www.semanticscholar.org\/paper\/4a6e74d4bf4fd0106891e5518692a77c7aa8811d","4222":"https:\/\/www.semanticscholar.org\/paper\/372fe9428dff788274081b00634d484db0c2fda4","4223":"https:\/\/www.semanticscholar.org\/paper\/6f4486c3d8ccd638c2a6bcbfafa01b5a3225bcab","4224":"https:\/\/www.semanticscholar.org\/paper\/e2de7eb99484f5775fcbdf30513c4a59370dacdf","4225":"https:\/\/www.semanticscholar.org\/paper\/2460c7093c5dd78653b7c19a836334090e989256","4226":"https:\/\/www.semanticscholar.org\/paper\/0c72450890a54b68d63baa99376131fda8f06cf9","4227":"https:\/\/www.semanticscholar.org\/paper\/58ddaf8f62f8e71f82e1f8763b27c43ada947f7b","4228":"https:\/\/www.semanticscholar.org\/paper\/184ac0766262312ba76bbdece4e7ffad0aa8180b","4229":"https:\/\/www.semanticscholar.org\/paper\/04c5969db977a8adedf2ee73b278c55e4bdb5b24","4230":"https:\/\/www.semanticscholar.org\/paper\/f82e4ff4f003581330338aaae71f60316e58dd26","4231":"https:\/\/www.semanticscholar.org\/paper\/bc4f981d5a2d106dbcbd47aa3f012ff7b2b55752","4232":"https:\/\/www.semanticscholar.org\/paper\/4d2463ce5015bcb2a0a4b65e90f65e3ca4eeb605","4233":"https:\/\/www.semanticscholar.org\/paper\/18f96ef4398bb3893706c8e42d944a7108dbcb91","4234":"https:\/\/www.semanticscholar.org\/paper\/0090023afc66cd2741568599057f4e82b566137c","4235":"https:\/\/www.semanticscholar.org\/paper\/c1e48526eddd68b5bf98739a578ab69a009f570d","4236":"https:\/\/www.semanticscholar.org\/paper\/6c7f040a150abf21dbcefe1f22e0f98fa184f41a","4237":"https:\/\/www.semanticscholar.org\/paper\/3f116042f50a499ab794bcc1255915bee507413c","4238":"https:\/\/www.semanticscholar.org\/paper\/d1d015f53a924f64cc87cb801f6603df6415f854","4239":"https:\/\/www.semanticscholar.org\/paper\/3a58efcc4558727cc5c131c44923635da4524f33","4240":"https:\/\/www.semanticscholar.org\/paper\/aa245959d84734f92d1e8f179417eb7226868e62","4241":"https:\/\/www.semanticscholar.org\/paper\/75bc3edb3655a777b8a12e0237c18d571aa610fd","4242":"https:\/\/www.semanticscholar.org\/paper\/70e4a1ac020bff847061e0e9eefa042c2af39600","4243":"https:\/\/www.semanticscholar.org\/paper\/eb7d83b726a746b1a9b2a33ac34263b3047b1a48","4244":"https:\/\/www.semanticscholar.org\/paper\/8c2bb9c4ac0d20fd0a6bdb0c516208bf7f29c89f","4245":"https:\/\/www.semanticscholar.org\/paper\/bde2a76bcb7543ec2b18cd6037a05a84314d3524","4246":"https:\/\/www.semanticscholar.org\/paper\/11f4765ea4404742dc12dbdb06430873563a0a9d","4247":"https:\/\/www.semanticscholar.org\/paper\/20c7139595570f080fc85a054c84262d11a488bd","4248":"https:\/\/www.semanticscholar.org\/paper\/b19729b27a1b4c24b52f87308c907653300afa7f","4249":"https:\/\/www.semanticscholar.org\/paper\/608787bb357cd2dc5ff5dad44b3eaf7080fa9997","4250":"https:\/\/www.semanticscholar.org\/paper\/a71ba5637e76570d6267dca01a1781824f1fe99b","4251":"https:\/\/www.semanticscholar.org\/paper\/f70b2f20be241f445a61f33c4b8e76e554760340","4252":"https:\/\/www.semanticscholar.org\/paper\/a6582abc47397d96888108ea308c0168d94a230d","4253":"https:\/\/www.semanticscholar.org\/paper\/7cc44884667e9771d11491c142df90e091d3d4a7","4254":"https:\/\/www.semanticscholar.org\/paper\/935c7a26514dad9d11cdc4692cac10e08c766099","4255":"https:\/\/www.semanticscholar.org\/paper\/8b835a6dedd55e57c2d5328b94b839faa25faca8","4256":"https:\/\/www.semanticscholar.org\/paper\/8995b12a886a177e98b83e6e1ae2f1eda3df20ef","4257":"https:\/\/www.semanticscholar.org\/paper\/cc5ac1d3083b6663482ba6830dfa3bf65343286c","4258":"https:\/\/www.semanticscholar.org\/paper\/e5a45912fb23405fa6c9b0e7b7839e467823ae8d","4259":"https:\/\/www.semanticscholar.org\/paper\/03fc922e37f43bb719a9371c2b2c0e988ce4409d","4260":"https:\/\/www.semanticscholar.org\/paper\/b36acdfc67612d707c95d1ed282672d3ca262be7","4261":"https:\/\/www.semanticscholar.org\/paper\/a595767fa35bcc84362f629fbc4d2d9b05d7342a","4262":"https:\/\/www.semanticscholar.org\/paper\/21832c7ae5193450e5344f2d349e7c6c89741840","4263":"https:\/\/www.semanticscholar.org\/paper\/e89ed6bb1864558e3889f5f2fb8931643c633479","4264":"https:\/\/www.semanticscholar.org\/paper\/481dd25896ac531707870c9b8c179cce20013401","4265":"https:\/\/www.semanticscholar.org\/paper\/d7524f2ec8ad9866190c1b2d030fcac95cc3f565","4266":"https:\/\/www.semanticscholar.org\/paper\/78e3106101bbacdf6601a92eb39026c15f443861","4267":"https:\/\/www.semanticscholar.org\/paper\/c05552cfbf3188e0079f432b353403177abe7ad1","4268":"https:\/\/www.semanticscholar.org\/paper\/b408358a5d2300e1fc6cc1a58a18d45a2b75420d","4269":"https:\/\/www.semanticscholar.org\/paper\/2cafacb966ad7ea5e219175b52f4a8d708772c96","4270":"https:\/\/www.semanticscholar.org\/paper\/921512706ecacd88d762f1af86d28596ef7c339a","4271":"https:\/\/www.semanticscholar.org\/paper\/3633850c75fdcaeb3db8285ebbea31b9dc5cdba7","4272":"https:\/\/www.semanticscholar.org\/paper\/38115e80d805fb0fb8f090dc88ced4b24be07878","4273":"https:\/\/www.semanticscholar.org\/paper\/7d645a3fd276918374fd9483fd675c28e46506d1","4274":"https:\/\/www.semanticscholar.org\/paper\/83edcfbb206ddad38a971d605da09390604248ea","4275":"https:\/\/www.semanticscholar.org\/paper\/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","4276":"https:\/\/www.semanticscholar.org\/paper\/3487c12512fa41d3a4d64f00cb842525a8590ad3","4277":"https:\/\/www.semanticscholar.org\/paper\/ae736662f64d56f3ab1894fbd9c45f8f37251843","4278":"https:\/\/www.semanticscholar.org\/paper\/4a7f6c4e71e20311ade4e76e8d0945d499c31fcd","4279":"https:\/\/www.semanticscholar.org\/paper\/7d8905a1fd288068f12c8347caeabefd36d0dd6c","4280":"https:\/\/www.semanticscholar.org\/paper\/83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05","4281":"https:\/\/www.semanticscholar.org\/paper\/094883e42bb9a41f602c0715c1059bc431e33fb2","4282":"https:\/\/www.semanticscholar.org\/paper\/1ddbd08ad8cf22a5c66c4242194c4286328533bf","4283":"https:\/\/www.semanticscholar.org\/paper\/de519a1976d0f5005ffac09f2560b1b61b37603c","4284":"https:\/\/www.semanticscholar.org\/paper\/a1f8082505c7e90b0a033e1b9da0a97d67aad66c","4285":"https:\/\/www.semanticscholar.org\/paper\/28c6ac721f54544162865f41c5692e70d61bccab","4286":"https:\/\/www.semanticscholar.org\/paper\/42a30dc5470f54ec249f25d3c31e05d7c376c8e3","4287":"https:\/\/www.semanticscholar.org\/paper\/b458fc5261595f44b36325e5eaea1f874d65138f","4288":"https:\/\/www.semanticscholar.org\/paper\/3efb81de24eb88017d6dbcf22cb4215084223fd8","4289":"https:\/\/www.semanticscholar.org\/paper\/0383e049e98c9eedbc61be728d4ef037300bbedf","4290":"https:\/\/www.semanticscholar.org\/paper\/d1500f1dbd62e26ef0753f31e845078f58479968","4291":"https:\/\/www.semanticscholar.org\/paper\/4c4d176c6e28f48041f215d563f6ee8633534cff","4292":"https:\/\/www.semanticscholar.org\/paper\/2e92b3699668f920a8d692535622ebeaa53315e2","4293":"https:\/\/www.semanticscholar.org\/paper\/bda605928d6ebe4db906e69ab5d343df75918727","4294":"https:\/\/www.semanticscholar.org\/paper\/47c3b8dd2c8a9326249ac98900b2c3fc71f46ab1","4295":"https:\/\/www.semanticscholar.org\/paper\/e0f27336698c84709bd60b6b7f4ce588cbae66bf","4296":"https:\/\/www.semanticscholar.org\/paper\/c879413103f8950bdd414c7f60a39bd7748c9be8","4297":"https:\/\/www.semanticscholar.org\/paper\/ad13b213681b6f634bc83a264df246e83dd9a9d9","4298":"https:\/\/www.semanticscholar.org\/paper\/a4f16dda8d25bdc0f93d2deb3b0876d278de9284","4299":"https:\/\/www.semanticscholar.org\/paper\/04a96b66705858c988edfcb73191c1da7d54abfb","4300":"https:\/\/www.semanticscholar.org\/paper\/680c72c29b518398d9c45b5995a160583ea8e090","4301":"https:\/\/www.semanticscholar.org\/paper\/256979852e0e0a5fe5cc8ddbf54fa1af2a843722","4302":"https:\/\/www.semanticscholar.org\/paper\/e7bec2a9da36bda325cbdd287a46f8cddf45a316","4303":"https:\/\/www.semanticscholar.org\/paper\/c40aa04f1577e8f0f83208d1815e032d127b60a8","4304":"https:\/\/www.semanticscholar.org\/paper\/332dc8b2ca9d49fad607c7282f3360bb2a9aacf3","4305":"https:\/\/www.semanticscholar.org\/paper\/0f733817e82026f7c29909a51cb4df7d2685f0e7","4306":"https:\/\/www.semanticscholar.org\/paper\/bb0656031cb17adf6bac5fd0fe8d53dd9c291508","4307":"https:\/\/www.semanticscholar.org\/paper\/d3640eb3b542eaf36fee2261f037a6bf0d8eac9c","4308":"https:\/\/www.semanticscholar.org\/paper\/8798b3a01c29fe0ce45a271bedd934787343dfb5","4309":"https:\/\/www.semanticscholar.org\/paper\/f45261b7b53043c316f45f613cb735907b93fb5a","4310":"https:\/\/www.semanticscholar.org\/paper\/8fd11c6f3eb1d0aeb915369f3c4f0b1bb24cab0c","4311":"https:\/\/www.semanticscholar.org\/paper\/3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","4312":"https:\/\/www.semanticscholar.org\/paper\/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","4313":"https:\/\/www.semanticscholar.org\/paper\/ca6a2bc279be5a3349a22bfd6866ed633d18734b","4314":"https:\/\/www.semanticscholar.org\/paper\/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820","4315":"https:\/\/www.semanticscholar.org\/paper\/131f499e4d3503da93022d07fcf804a18483bea9","4316":"https:\/\/www.semanticscholar.org\/paper\/5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","4317":"https:\/\/www.semanticscholar.org\/paper\/7ca954844bc1dd405bc43445b1c990e42d865095","4318":"https:\/\/www.semanticscholar.org\/paper\/cf1f26e7cbed3958b3c2870656568c299fece6e3","4319":"https:\/\/www.semanticscholar.org\/paper\/7cbc2a7843411a1768ab762930707af0a3c33a19","4320":"https:\/\/www.semanticscholar.org\/paper\/cb5b71a622aff47014d4f28a958679629a8b6363","4321":"https:\/\/www.semanticscholar.org\/paper\/26218bdcc3945c7edae7aa2adbfba4cd820a2df3","4322":"https:\/\/www.semanticscholar.org\/paper\/3b6179c293df29e31d31cea46476f104ab6950f2","4323":"https:\/\/www.semanticscholar.org\/paper\/38fe8f324d2162e63a967a9ac6648974fc4c66f3","4324":"https:\/\/www.semanticscholar.org\/paper\/774591fdd988eaaff3917e7c5171d044b0843e63","4325":"https:\/\/www.semanticscholar.org\/paper\/964bd39b546f0f6625ff3b9ef1083f797807ef2e","4326":"https:\/\/www.semanticscholar.org\/paper\/0d1c76d45afa012ded7ab741194baf142117c495","4327":"https:\/\/www.semanticscholar.org\/paper\/cd4d112f3f9120d0715f22a9de2ce4720822368c","4328":"https:\/\/www.semanticscholar.org\/paper\/9e3c493fb09dcd61bb05e8c5659f23327b7b6340","4329":"https:\/\/www.semanticscholar.org\/paper\/697e0add95e880bd42e00bef838181e105f91981","4330":"https:\/\/www.semanticscholar.org\/paper\/3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e","4331":"https:\/\/www.semanticscholar.org\/paper\/835c305e52769a8433f8383e91d33ba6c66ad55b","4332":"https:\/\/www.semanticscholar.org\/paper\/e5c72b92c48d68594b290c84a8904da7c8335554","4333":"https:\/\/www.semanticscholar.org\/paper\/454c8fef2957aa2fb13eb2c7a454393a2ee83805","4334":"https:\/\/www.semanticscholar.org\/paper\/7ed0faa6720cd176d57badbc0455af31a03f080c","4335":"https:\/\/www.semanticscholar.org\/paper\/a4a41319d5805a29316f24ed9519f09db77d4c29","4336":"https:\/\/www.semanticscholar.org\/paper\/38d64919ba526868a850a0e5f6239d4c474b7e7e","4337":"https:\/\/www.semanticscholar.org\/paper\/0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","4338":"https:\/\/www.semanticscholar.org\/paper\/9ada8fa11b1cdece31f253acae50b62df8d5f823","4339":"https:\/\/www.semanticscholar.org\/paper\/da9683e826c37a6383c124b5c6cddefcb35ee8fd","4340":"https:\/\/www.semanticscholar.org\/paper\/42a14d824caa3348046eb34c37e2ab7985faa7a3","4341":"https:\/\/www.semanticscholar.org\/paper\/5cac6430bd379c9d2fe13137dfd6ae7721a2679f","4342":"https:\/\/www.semanticscholar.org\/paper\/dfbfa21a93c3164ae8a033398c8de42b03b1b84d","4343":"https:\/\/www.semanticscholar.org\/paper\/459c82205d2a27a8542bba7a4d478a8a23be2f5d","4344":"https:\/\/www.semanticscholar.org\/paper\/aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3","4345":"https:\/\/www.semanticscholar.org\/paper\/f22d71c7ce9720ba1f717a4f1181488200e78198","4346":"https:\/\/www.semanticscholar.org\/paper\/f4e723958a93762befb4d4a039b44a7d752f9917","4347":"https:\/\/www.semanticscholar.org\/paper\/43e6e8d6663d83f1b74cf5a2be7b040b0928f867","4348":"https:\/\/www.semanticscholar.org\/paper\/0d42221038c05cee8443c5b5af838505ee137dc3","4349":"https:\/\/www.semanticscholar.org\/paper\/17170575aa8b4fa4e3eef5d366ada706a94dd836","4350":"https:\/\/www.semanticscholar.org\/paper\/763eb8d43e2f8a5d9da26269a4985efd1c099a5b","4351":"https:\/\/www.semanticscholar.org\/paper\/5437e8adab596d7294124c0e798708e050e25321","4352":"https:\/\/www.semanticscholar.org\/paper\/f197bf0fc2f228483f6af3285000d54d8d97f9eb","4353":"https:\/\/www.semanticscholar.org\/paper\/407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","4354":"https:\/\/www.semanticscholar.org\/paper\/f5afaccfe90268485a9961c5771ec5e71e9b806c","4355":"https:\/\/www.semanticscholar.org\/paper\/89e184d2bc830af568e439db9476caa0c047e11a","4356":"https:\/\/www.semanticscholar.org\/paper\/017010b941d902a467f6d329ae5e74fd67e67912","4357":"https:\/\/www.semanticscholar.org\/paper\/80fd20e175f83a699258b8780cf365418d1538b0","4358":"https:\/\/www.semanticscholar.org\/paper\/0882a2b2787b35dbcc6e341c953d964b77abd4df","4359":"https:\/\/www.semanticscholar.org\/paper\/ebedc4d7a2356090904baba4104ef0832bc236df","4360":"https:\/\/www.semanticscholar.org\/paper\/59fc49dfd81b92661437eaf7e339c0792ccd8755","4361":"https:\/\/www.semanticscholar.org\/paper\/c18f2239a4bd8cc68db9a013416167357f5e1353","4362":"https:\/\/www.semanticscholar.org\/paper\/6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552","4363":"https:\/\/www.semanticscholar.org\/paper\/9004924824cc0b1c840bcc91ba79475882623790","4364":"https:\/\/www.semanticscholar.org\/paper\/6052486bc9144dc1730c12bf35323af3792a1fd0","4365":"https:\/\/www.semanticscholar.org\/paper\/ccc772d88c231275f24c4fac9b28bbe0942e1107","4366":"https:\/\/www.semanticscholar.org\/paper\/22e2f488ecd88bd2adf79092d0d390d8f7b06a0f","4367":"https:\/\/www.semanticscholar.org\/paper\/f5c73d9e6641b018b633690102121f5605d34fb0","4368":"https:\/\/www.semanticscholar.org\/paper\/dbac86036cb5ed4dd6bbdda4a8613b163e20ec90","4369":"https:\/\/www.semanticscholar.org\/paper\/aa23bcf357fc4f890e0a97c27e254d14fbacd460","4370":"https:\/\/www.semanticscholar.org\/paper\/23d01461a54505705649c1f5378f81dcd524d46c","4371":"https:\/\/www.semanticscholar.org\/paper\/4b0b56be0ae9479d2bd5c2f0943db1906343c10f","4372":"https:\/\/www.semanticscholar.org\/paper\/deb8f26509ae320fc975b32922416cb156c61bbd","4373":"https:\/\/www.semanticscholar.org\/paper\/5d321194696f1f75cf9da045e6022b2f20ba5b9c","4374":"https:\/\/www.semanticscholar.org\/paper\/acf90b4d165690fe27c62c4af1a28d540c784000","4375":"https:\/\/www.semanticscholar.org\/paper\/5b8f0460d408a8688d9ee0cba127c779d3291d99","4376":"https:\/\/www.semanticscholar.org\/paper\/7ec58d26c4dddb4bc3b6829fa0654a22cc26fdfe","4377":"https:\/\/www.semanticscholar.org\/paper\/26089bdfdbca1e6eaaceca71e3116b715bec6d47","4378":"https:\/\/www.semanticscholar.org\/paper\/16d83e930a4dab2d49f5d276838ddce79df3f787","4379":"https:\/\/www.semanticscholar.org\/paper\/a38e0f993e4805ba8a9beae4c275c91ffcec01df","4380":"https:\/\/www.semanticscholar.org\/paper\/fef6471c4a2a0e7abc4a2261a6cf916e34091d12","4381":"https:\/\/www.semanticscholar.org\/paper\/c49a0912595a1cc70aab63524f64ed08c92194a8","4382":"https:\/\/www.semanticscholar.org\/paper\/b32a6f6ef7dd775e0f876b4713ceccebc56e651e","4383":"https:\/\/www.semanticscholar.org\/paper\/df7d26339adf4eb0c07160947b9d2973c24911ba","4384":"https:\/\/www.semanticscholar.org\/paper\/5dbffedcabe3fa43060ebbe2b1789500edfd871f","4385":"https:\/\/www.semanticscholar.org\/paper\/4610ffb1b016acaa82a2065ffd1a3adbae1ce722","4386":"https:\/\/www.semanticscholar.org\/paper\/77e73174e606c0820a52a940088832b32d9a033e","4387":"https:\/\/www.semanticscholar.org\/paper\/d48b29889241551e1ee6622fa78c3fa4159255dd","4388":"https:\/\/www.semanticscholar.org\/paper\/0d08ffccc982781e310bb184397bbe64b9aef157","4389":"https:\/\/www.semanticscholar.org\/paper\/fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","4390":"https:\/\/www.semanticscholar.org\/paper\/b2542a738b75ee9b7ce1a13d8b78f9095d212412","4391":"https:\/\/www.semanticscholar.org\/paper\/3cbffab9d7981da6662d474aaa056dcbd3c1701e","4392":"https:\/\/www.semanticscholar.org\/paper\/75f7e9e2b59fb640ef9d1dff94097175daf46c4d","4393":"https:\/\/www.semanticscholar.org\/paper\/9dcee248452d84b6bf26911ba6726ae5ce1a46f3","4394":"https:\/\/www.semanticscholar.org\/paper\/59b3fbf146b29b581b677ec4384f14cee87997a4","4395":"https:\/\/www.semanticscholar.org\/paper\/8b293973061026d9d0eed90e71e30928e029171e","4396":"https:\/\/www.semanticscholar.org\/paper\/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","4397":"https:\/\/www.semanticscholar.org\/paper\/f0a0e8b6e84207f50db4d24cc4016e40601214ef","4398":"https:\/\/www.semanticscholar.org\/paper\/f02e8f1c9b5ab12ddfb1977570f9f5445a99a973","4399":"https:\/\/www.semanticscholar.org\/paper\/41531594d7e0f3b2e138ae43e0a0f6e24a9b014c","4400":"https:\/\/www.semanticscholar.org\/paper\/6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd","4401":"https:\/\/www.semanticscholar.org\/paper\/c2329c685f11efa25c562f97be71ff03103423fd","4402":"https:\/\/www.semanticscholar.org\/paper\/ed38c6b157c11476939c426ec6871c926f2f3524","4403":"https:\/\/www.semanticscholar.org\/paper\/933b37b21e9d61139660088adb032ff3fdf56d86","4404":"https:\/\/www.semanticscholar.org\/paper\/51000d9f79be0eefd7972fe94e3c71dddc90d2c6","4405":"https:\/\/www.semanticscholar.org\/paper\/0fe8793755d935773b90b218ed9a9b38c7215b9d","4406":"https:\/\/www.semanticscholar.org\/paper\/ee8de585183763ff64cb3c81ecda2fc75fa81507","4407":"https:\/\/www.semanticscholar.org\/paper\/a9e3e5dd7b30890553b7ae1c41f932e99192bb44","4408":"https:\/\/www.semanticscholar.org\/paper\/a42b091adaf29b06a092b67192ac07cb93312f2a","4409":"https:\/\/www.semanticscholar.org\/paper\/99ca5162211a895a5dfbff9d7e36e21e09ca646e","4410":"https:\/\/www.semanticscholar.org\/paper\/292da1c4640c105aa5d7919a1ded9a1225c07d4d","4411":"https:\/\/www.semanticscholar.org\/paper\/50296a5814c4ac7f58f3b0177233a8f63c701565","4412":"https:\/\/www.semanticscholar.org\/paper\/6da9a81b75e7ad02867860753d1aa276673a3a77","4413":"https:\/\/www.semanticscholar.org\/paper\/6e02a7eedad079451b9a8dd358268727cf599c6e","4414":"https:\/\/www.semanticscholar.org\/paper\/0dde065405210ebc399c58ab6b7e843a18caad51","4415":"https:\/\/www.semanticscholar.org\/paper\/21f377c5d89f85f2bd802f4f6abe1df4748ec07b","4416":"https:\/\/www.semanticscholar.org\/paper\/1fc1145ef6dbcac74b67cb5c8a742c67b43c4199","4417":"https:\/\/www.semanticscholar.org\/paper\/50796b0f3edf9cb5ff1e447c298b33755378aa4f","4418":"https:\/\/www.semanticscholar.org\/paper\/5e00596fa946670d894b1bdaeff5a98e3867ef13","4419":"https:\/\/www.semanticscholar.org\/paper\/c65583b6848454de5d56668a2788093280e3cbb6","4420":"https:\/\/www.semanticscholar.org\/paper\/2b5d234efd26e7377698cf16c901601a3d3c4e56","4421":"https:\/\/www.semanticscholar.org\/paper\/cc50f846ed7222698d130cddbc58ed4d547914ed","4422":"https:\/\/www.semanticscholar.org\/paper\/002c58077a1f1b296468b117230a1199e91f35c2","4423":"https:\/\/www.semanticscholar.org\/paper\/832fff14d2ed50eb7969c4c4b976c35776548f56","4424":"https:\/\/www.semanticscholar.org\/paper\/bb15f3727f827a3cb88b5d3ca48415c09b40a88f","4425":"https:\/\/www.semanticscholar.org\/paper\/4c2733d191e347753bb28afa46a1c55c65e085be","4426":"https:\/\/www.semanticscholar.org\/paper\/f577654d9dd29d88c6db9ee39a4fd831573b8770","4427":"https:\/\/www.semanticscholar.org\/paper\/ac3cdb50606f7770eef8e4cd951840a4f71287a0","4428":"https:\/\/www.semanticscholar.org\/paper\/d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","4429":"https:\/\/www.semanticscholar.org\/paper\/54523ff961a1ac57a86696ef9a53b3a630b482c0","4430":"https:\/\/www.semanticscholar.org\/paper\/7a79a3074e736bdd9e4ce1d46f1efe3abd6623fd","4431":"https:\/\/www.semanticscholar.org\/paper\/72cdd6ebe0221fb568ef20534f44ba5b35190a56","4432":"https:\/\/www.semanticscholar.org\/paper\/a5731122200fbb8b37f048010a1e1ca4474aa606","4433":"https:\/\/www.semanticscholar.org\/paper\/96c22a88ec3b9d3799daa41098555ab665c24ea8","4434":"https:\/\/www.semanticscholar.org\/paper\/29a3c6968888d8078d21ff5b7d9e6d78c470fdb0","4435":"https:\/\/www.semanticscholar.org\/paper\/ac5925e0be9695c70e01aa87aa7cf49d2a7f3db4","4436":"https:\/\/www.semanticscholar.org\/paper\/75acc731bdd2b626edc74672a30da3bc51010ae8","4437":"https:\/\/www.semanticscholar.org\/paper\/83b8108014e3db4f46354a28ae68193f143c4e7e","4438":"https:\/\/www.semanticscholar.org\/paper\/c96297261467b5daa2d01227496a70d444602434","4439":"https:\/\/www.semanticscholar.org\/paper\/19537be34dbadbcaa4fffcf028a8ada5095b1b5c","4440":"https:\/\/www.semanticscholar.org\/paper\/aa68ea557777908e76f02c433f14ef6b968d4a82","4441":"https:\/\/www.semanticscholar.org\/paper\/069e0d896da7c79faeee4cf057548d5da7ce885e","4442":"https:\/\/www.semanticscholar.org\/paper\/61f42619eca0afaf2cff9a899bbdea88182885f8","4443":"https:\/\/www.semanticscholar.org\/paper\/3578a7792904e6af3db8ffefdff86ab6a387c7c3","4444":"https:\/\/www.semanticscholar.org\/paper\/76b19363b10d7ea783e4a6494eae40d73c8e9628","4445":"https:\/\/www.semanticscholar.org\/paper\/a039ea239e37f53a2cb60c68e0a1967994353166","4446":"https:\/\/www.semanticscholar.org\/paper\/0abb08c4ec5feab4cdd82c471866dd4395c573ce","4447":"https:\/\/www.semanticscholar.org\/paper\/1187c70c4011f935642084e84186284ac0add3d0","4448":"https:\/\/www.semanticscholar.org\/paper\/f04df4e20a18358ea2f689b4c129781628ef7fc1","4449":"https:\/\/www.semanticscholar.org\/paper\/db95ba221305a3aa2ec8bbc78de0f42485c63c12","4450":"https:\/\/www.semanticscholar.org\/paper\/aad167be3c902388ea625da4117fcae4325b8b7d","4451":"https:\/\/www.semanticscholar.org\/paper\/ebb1f10d878e5720ac01796bfb6769290a71149b","4452":"https:\/\/www.semanticscholar.org\/paper\/28630034bb29760df01ab033b743e30b37f336ae","4453":"https:\/\/www.semanticscholar.org\/paper\/e9bc29cfcfbea4d137652d10715a9c9389349a90","4454":"https:\/\/www.semanticscholar.org\/paper\/da4b7f47d0d9c4d46740cdd6fb712c6a87c10db3","4455":"https:\/\/www.semanticscholar.org\/paper\/ead532bed5b61a8a6ced1eb9ba1ae1d6c0f047ab","4456":"https:\/\/www.semanticscholar.org\/paper\/319b84be7a843250bc81d7086f79a4126d550277","4457":"https:\/\/www.semanticscholar.org\/paper\/9695824d7a01fad57ba9c01d7d76a519d78d65e7","4458":"https:\/\/www.semanticscholar.org\/paper\/094ff971d6a8b8ff870946c9b3ce5aa173617bfb","4459":"https:\/\/www.semanticscholar.org\/paper\/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","4460":"https:\/\/www.semanticscholar.org\/paper\/47030369e97cc44d4b2e3cf1be85da0fd134904a","4461":"https:\/\/www.semanticscholar.org\/paper\/5f3e06ddedd4e6ac799679b65a20e9170a8b753e","4462":"https:\/\/www.semanticscholar.org\/paper\/13a0d8bb38f739990c8cd65a44061c6534f17221","4463":"https:\/\/www.semanticscholar.org\/paper\/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","4464":"https:\/\/www.semanticscholar.org\/paper\/e22f2bdada2445638f5fd8ee4bbfef42aada63f3","4465":"https:\/\/www.semanticscholar.org\/paper\/8b5eab31e1c5689312fff3181a75bfbf5c13e51c","4466":"https:\/\/www.semanticscholar.org\/paper\/2fd6f77540c1cc8e70b96208ccf9971b4251fc02","4467":"https:\/\/www.semanticscholar.org\/paper\/873a581320d928249609d3c07229d5af182a379c","4468":"https:\/\/www.semanticscholar.org\/paper\/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796","4469":"https:\/\/www.semanticscholar.org\/paper\/4780d0a027c5c5a8e01d7cf697f6296880ffc945","4470":"https:\/\/www.semanticscholar.org\/paper\/780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","4471":"https:\/\/www.semanticscholar.org\/paper\/cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","4472":"https:\/\/www.semanticscholar.org\/paper\/8323c591e119eb09b28b29fd6c7bc76bd889df7a","4473":"https:\/\/www.semanticscholar.org\/paper\/78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","4474":"https:\/\/www.semanticscholar.org\/paper\/e0c6abdbdecf04ffac65c440da77fb9d66bb474c","4475":"https:\/\/www.semanticscholar.org\/paper\/7a064df1aeada7e69e5173f7d4c8606f4470365b","4476":"https:\/\/www.semanticscholar.org\/paper\/38939304bb760473141c2aca0305e44fbe04e6e8","4477":"https:\/\/www.semanticscholar.org\/paper\/1359d2ef45f1550941e22bf046026c89f6edf315","4478":"https:\/\/www.semanticscholar.org\/paper\/154493f69d7db3d49da0e51df0192c6ad5f1724a","4479":"https:\/\/www.semanticscholar.org\/paper\/9a75e23639bfcc3a51da57a3b682a984d1d8ac0b","4480":"https:\/\/www.semanticscholar.org\/paper\/0b6f13177a90a02d44a41c62659988561f56c168","4481":"https:\/\/www.semanticscholar.org\/paper\/fb01415a0decfa3f3d6339930e95028ae1ff4170","4482":"https:\/\/www.semanticscholar.org\/paper\/1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","4483":"https:\/\/www.semanticscholar.org\/paper\/cb5e3f085caefd1f3d5e08637ab55d39e61234fc","4484":"https:\/\/www.semanticscholar.org\/paper\/e65b346d442e9962a4276dc1c1af2956d9d5f1eb","4485":"https:\/\/www.semanticscholar.org\/paper\/df59d0098c1b2c1ee8995da802dd6b12d158c2b8","4486":"https:\/\/www.semanticscholar.org\/paper\/c41a11c0e9b8b92b4faaf97749841170b760760a","4487":"https:\/\/www.semanticscholar.org\/paper\/ba786c46373892554b98df42df7af6f5da343c9d","4488":"https:\/\/www.semanticscholar.org\/paper\/ff0b2681d7b05e16c46dfb71d980cc2f605907cd","4489":"https:\/\/www.semanticscholar.org\/paper\/040ad14a2c97e51510889ae6a0c3c23b29da801d","4490":"https:\/\/www.semanticscholar.org\/paper\/7d884b40ef5892f61e0f6f358b8e29983f64a178","4491":"https:\/\/www.semanticscholar.org\/paper\/d318e0169f649656c71f02a1f84194a734fe1962","4492":"https:\/\/www.semanticscholar.org\/paper\/398e4061dde8f5c80606869cebfa2031de7b5b74","4493":"https:\/\/www.semanticscholar.org\/paper\/ab0e3d3e4d42369de5933a3b4c237780b41c0d77","4494":"https:\/\/www.semanticscholar.org\/paper\/222b9a7b8038120671a1610e857d3edbc7ac5550","4495":"https:\/\/www.semanticscholar.org\/paper\/126fb7df6bcab2b70000dfe5b940ada63ae1ba6a","4496":"https:\/\/www.semanticscholar.org\/paper\/56cafbac34f2bb3f6a9828cd228ff281b810d6bb","4497":"https:\/\/www.semanticscholar.org\/paper\/2ffcf8352223c95ae8cef4daaec995525ecc926b","4498":"https:\/\/www.semanticscholar.org\/paper\/60ee030773ba1b68eb222a265b052ca028353362","4499":"https:\/\/www.semanticscholar.org\/paper\/28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d","4500":"https:\/\/www.semanticscholar.org\/paper\/8c870bef01a4fbb20f60722ffc2f6bee3870b18b","4501":"https:\/\/www.semanticscholar.org\/paper\/774e560a2cadcb84f4b1def7b152e5398b062efb","4502":"https:\/\/www.semanticscholar.org\/paper\/e6c561d02500b2596a230b341a8eb8b921ca5bf2","4503":"https:\/\/www.semanticscholar.org\/paper\/d6d3604f369bb0415cbe814e43ca3131323b03e2","4504":"https:\/\/www.semanticscholar.org\/paper\/65a9c7b0800c86a196bc14e7621ff895cc6ab287","4505":"https:\/\/www.semanticscholar.org\/paper\/24d7b1487202e3aaf329df3d8135ae6eabefaa45","4506":"https:\/\/www.semanticscholar.org\/paper\/e816f788767eec6a8ef0ea9eddd0e902435d4271","4507":"https:\/\/www.semanticscholar.org\/paper\/16de2006e2960ba410772c6b6d460b83c0a5cc4b","4508":"https:\/\/www.semanticscholar.org\/paper\/68f141724814839d556a989646194be88641b143","4509":"https:\/\/www.semanticscholar.org\/paper\/42e741e0be43954ae684d14333e4074f4d0ae961","4510":"https:\/\/www.semanticscholar.org\/paper\/b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","4511":"https:\/\/www.semanticscholar.org\/paper\/23447f473cd240494b0a20ea008038aaef7e3391","4512":"https:\/\/www.semanticscholar.org\/paper\/540f074cb6f16563a357741837e41c44c0a38234","4513":"https:\/\/www.semanticscholar.org\/paper\/9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6","4514":"https:\/\/www.semanticscholar.org\/paper\/760561c57f68044e2f1d089088df1da6c627b09a","4515":"https:\/\/www.semanticscholar.org\/paper\/8666f9f379389a5dff31e72fb0f992a37763ba41","4516":"https:\/\/www.semanticscholar.org\/paper\/6f4cc536f9ed83d0dbf7e919dc609be12aa0848a","4517":"https:\/\/www.semanticscholar.org\/paper\/62f0db3a5ad5c795ec18fc7a6e7b01836809df57","4518":"https:\/\/www.semanticscholar.org\/paper\/818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","4519":"https:\/\/www.semanticscholar.org\/paper\/838fbfd9066dbbac6c10059c5b183046fb1cd9d1","4520":"https:\/\/www.semanticscholar.org\/paper\/3056add22b20e3361c38c0472d294a79d4031cb4","4521":"https:\/\/www.semanticscholar.org\/paper\/d3f79210b54e168c76b8c311488f42d7d1048b81","4522":"https:\/\/www.semanticscholar.org\/paper\/126a4776ff8315fd506766cb8f3c722cf746ad9e","4523":"https:\/\/www.semanticscholar.org\/paper\/8ae9a17c87a4518b513e860683a0ef7824be994d","4524":"https:\/\/www.semanticscholar.org\/paper\/80d0116d77beeded0c23cf48946d9d10d4faee14","4525":"https:\/\/www.semanticscholar.org\/paper\/c151f144c2c0e8d3b176edaf2ce5369c7707bd31","4526":"https:\/\/www.semanticscholar.org\/paper\/3950df97ea527009a32569cb7016bc3df1383dca","4527":"https:\/\/www.semanticscholar.org\/paper\/3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","4528":"https:\/\/www.semanticscholar.org\/paper\/510e26733aaff585d65701b9f1be7ca9d5afc586","4529":"https:\/\/www.semanticscholar.org\/paper\/f675c62abfa788ea0be85d3124eba15a14d5e9d6","4530":"https:\/\/www.semanticscholar.org\/paper\/b15ea460c77a4ee8aa159a30ab0331deedfcf392","4531":"https:\/\/www.semanticscholar.org\/paper\/da1d6445b6b64ce9eb4587ba8abbdc490f648ec1","4532":"https:\/\/www.semanticscholar.org\/paper\/11f0c389d1e74607964bdb5be64264e296e920c0","4533":"https:\/\/www.semanticscholar.org\/paper\/658721bc13b0fa97366d38c05a96bf0a9f4bb0ac","4534":"https:\/\/www.semanticscholar.org\/paper\/5cf3e46e6d427a87726c18f22def612519176938","4535":"https:\/\/www.semanticscholar.org\/paper\/9b3fa3a80afdb6d34984307e457656420e60e7e7","4536":"https:\/\/www.semanticscholar.org\/paper\/26133033149afb4b45e5d0a4bd1dc712a236810e","4537":"https:\/\/www.semanticscholar.org\/paper\/f5c165b6317896a65151050201c737536fa17c31","4538":"https:\/\/www.semanticscholar.org\/paper\/6f2ec690c0e2ecdb5eceda74154cb6e9e1ce91a5","4539":"https:\/\/www.semanticscholar.org\/paper\/6d1ef4436904de111c8b1975bbf25d3fe2f165f7","4540":"https:\/\/www.semanticscholar.org\/paper\/babeda48b10a4d638252118f2238d05a06f4ec55","4541":"https:\/\/www.semanticscholar.org\/paper\/2a8fa407e074bebeaf1e254be37fae7fc54610e3","4542":"https:\/\/www.semanticscholar.org\/paper\/cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","4543":"https:\/\/www.semanticscholar.org\/paper\/2cbf8688cbaddb28eac94fafb01251178f664dc7","4544":"https:\/\/www.semanticscholar.org\/paper\/5f994dc8cae24ca9d1ed629e517fcc652660ddde","4545":"https:\/\/www.semanticscholar.org\/paper\/b1cb867270f87f96397cb5f0d76cbb58cdf2c2f2","4546":"https:\/\/www.semanticscholar.org\/paper\/832fc9327695f7425d8759c6aaeec0fa2d7b0a90","4547":"https:\/\/www.semanticscholar.org\/paper\/e04a80263d252a3d8a382ba37a249b9345620570","4548":"https:\/\/www.semanticscholar.org\/paper\/83a6cacc126d85c45605797406262677c256a6af","4549":"https:\/\/www.semanticscholar.org\/paper\/878ba5458e9e51f0b341fd9117fa0b43ef4096d3","4550":"https:\/\/www.semanticscholar.org\/paper\/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0","4551":"https:\/\/www.semanticscholar.org\/paper\/5546e6073f3b82967b12c87d6b90ba722c4b85c6","4552":"https:\/\/www.semanticscholar.org\/paper\/f93a0a3e8a3e6001b4482430254595cf737697fa","4553":"https:\/\/www.semanticscholar.org\/paper\/949fef650da4c41afe6049a183b504b3cc91f4bd","4554":"https:\/\/www.semanticscholar.org\/paper\/c7462e0ee928f095a7fc40b91f1e7557d283ae8e","4555":"https:\/\/www.semanticscholar.org\/paper\/207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","4556":"https:\/\/www.semanticscholar.org\/paper\/7a31e2dcbaa1cf6e9f76084793a02a2a4e4c2d15","4557":"https:\/\/www.semanticscholar.org\/paper\/6648b4db5f12c30941ea78c695e77aded19672bb","4558":"https:\/\/www.semanticscholar.org\/paper\/06a73ad09664435f8b3cd90293f4e05a047cf375","4559":"https:\/\/www.semanticscholar.org\/paper\/9b56086e420ecb216f85d408a25264f640e46705","4560":"https:\/\/www.semanticscholar.org\/paper\/2997b26ffb8c291ce478bd8a6e47979d5a55c466","4561":"https:\/\/www.semanticscholar.org\/paper\/898b65bdec52856cd66b56dabe33e2a62df816f0","4562":"https:\/\/www.semanticscholar.org\/paper\/a9fd5511b42206a27748f373e0fdb7eb76a23055","4563":"https:\/\/www.semanticscholar.org\/paper\/d64e57b9780f30f5b49bf620fdfb8584651b7f85","4564":"https:\/\/www.semanticscholar.org\/paper\/4d9506257186023b78cf19ed4f9e77a4ae4fa0f0","4565":"https:\/\/www.semanticscholar.org\/paper\/83e7654d545fbbaaf2328df365a781fb67b841b4","4566":"https:\/\/www.semanticscholar.org\/paper\/e941f94c9b70c0dfb433871deecbf7d2df561352","4567":"https:\/\/www.semanticscholar.org\/paper\/458af0f3f03229290572a2630c75ac56e9dbec6e","4568":"https:\/\/www.semanticscholar.org\/paper\/a65b7ff72deb358bef29900a53c814771275c37c","4569":"https:\/\/www.semanticscholar.org\/paper\/9927a15ddf5313d97c98f0111fd191caf507ce72","4570":"https:\/\/www.semanticscholar.org\/paper\/a622332550eaf535cf0f0f6c3a3f3ba197c39cac","4571":"https:\/\/www.semanticscholar.org\/paper\/ab70853cd5912c470f6ff95e95481980f0a2a41b","4572":"https:\/\/www.semanticscholar.org\/paper\/6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9","4573":"https:\/\/www.semanticscholar.org\/paper\/738215a396f6eee1709c6b521a6199769f0ce674","4574":"https:\/\/www.semanticscholar.org\/paper\/d01fa0311e8e15b8b874b376123530c815f52852","4575":"https:\/\/www.semanticscholar.org\/paper\/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","4576":"https:\/\/www.semanticscholar.org\/paper\/4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca","4577":"https:\/\/www.semanticscholar.org\/paper\/2feced32a117069b48146bf4b3eb620fea6a9606","4578":"https:\/\/www.semanticscholar.org\/paper\/46c585ee9abf76779ea4b863d2da4358efd0d1d3","4579":"https:\/\/www.semanticscholar.org\/paper\/4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904","4580":"https:\/\/www.semanticscholar.org\/paper\/c553280c1fc1d0bc7b94683bb75910e309b0d579","4581":"https:\/\/www.semanticscholar.org\/paper\/157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3","4582":"https:\/\/www.semanticscholar.org\/paper\/32d59ab951be74be351f9777da2cbc71bb68c3c1","4583":"https:\/\/www.semanticscholar.org\/paper\/0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d","4584":"https:\/\/www.semanticscholar.org\/paper\/58fe64beb45b18f63cbc001849a0dee3e4e60482","4585":"https:\/\/www.semanticscholar.org\/paper\/47570e7f63e296f224a0e7f9a0d08b0de3cbaf40","4586":"https:\/\/www.semanticscholar.org\/paper\/c44120f765fc43994c5cfb4e12e4f62999efeae6","4587":"https:\/\/www.semanticscholar.org\/paper\/c07651110d3b98b63607557b57808d15d99013dd","4588":"https:\/\/www.semanticscholar.org\/paper\/4d00097433a538002b36cfd7a621daddde3e4c0d","4589":"https:\/\/www.semanticscholar.org\/paper\/71480da09af638260801af1db8eff6acb4e1122f","4590":"https:\/\/www.semanticscholar.org\/paper\/3dd61d97827e3f380bf9304101149a3f865051fc","4591":"https:\/\/www.semanticscholar.org\/paper\/83cf4b2f39bcc802b09fd59b69e23068447b26b7","4592":"https:\/\/www.semanticscholar.org\/paper\/dedcdc1fb3a6def9772dce674d89150923dd75b9","4593":"https:\/\/www.semanticscholar.org\/paper\/29599829d26b4acaaf0ad88698ed4362d33b2ae7","4594":"https:\/\/www.semanticscholar.org\/paper\/80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","4595":"https:\/\/www.semanticscholar.org\/paper\/c51b99f18f0ca9e1e2fea7132bf5654299817d6c","4596":"https:\/\/www.semanticscholar.org\/paper\/76ad0d37bd3845431b3ca9d07f8db74c82752298","4597":"https:\/\/www.semanticscholar.org\/paper\/d53733e06efd8fb53e584605ee601af0394d9c3d","4598":"https:\/\/www.semanticscholar.org\/paper\/81813379dde0fe90d67e5ee1fd6e1d4c72bcfe70","4599":"https:\/\/www.semanticscholar.org\/paper\/86311b182786bfde19446f6ded0854de973d4060","4600":"https:\/\/www.semanticscholar.org\/paper\/6a9e5377bbf28b86a1dd8cd5802998c949a34ff3","4601":"https:\/\/www.semanticscholar.org\/paper\/b9de9599d7241459db9213b5cdd7059696f5ef8d","4602":"https:\/\/www.semanticscholar.org\/paper\/c7ba20a46ddefed391655e2c60292d1b4382474a","4603":"https:\/\/www.semanticscholar.org\/paper\/ae29b936d437a93ad259ee008ba56fe82ab4db61","4604":"https:\/\/www.semanticscholar.org\/paper\/1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","4605":"https:\/\/www.semanticscholar.org\/paper\/927c25cdd384e8f39ed7db7ab1558eb7fd8f048c","4606":"https:\/\/www.semanticscholar.org\/paper\/1e688be9f4554aa981fe3db9e2a66388b05bd167","4607":"https:\/\/www.semanticscholar.org\/paper\/5d22b241836e30d5b0d852b463951ab7e3245ea4","4608":"https:\/\/www.semanticscholar.org\/paper\/4d23db55e6671a82c95dacec33b2967a4b8b677d","4609":"https:\/\/www.semanticscholar.org\/paper\/8b419080cd37bdc30872b76f405ef6a93eae3304","4610":"https:\/\/www.semanticscholar.org\/paper\/8cb72cf5490c2a532d52237f688f915a92afe04c","4611":"https:\/\/www.semanticscholar.org\/paper\/13bcfb944779165983aaef22cec8a3bbd3e98e62","4612":"https:\/\/www.semanticscholar.org\/paper\/c342798bafc1eaaa60c652fc90fd738941542133","4613":"https:\/\/www.semanticscholar.org\/paper\/e15725948c2ea8b190b825a0887e430dc4898428","4614":"https:\/\/www.semanticscholar.org\/paper\/578e050d6e007797d032a07e712142035f2666dc","4615":"https:\/\/www.semanticscholar.org\/paper\/185f078accb52be4faa13e4f470a9909cc6fe814","4616":"https:\/\/www.semanticscholar.org\/paper\/847a0a93ad97c8a31ef70d379517d8d72d4d9a46","4617":"https:\/\/www.semanticscholar.org\/paper\/e493205bc5151b02fe2712d813dd38f5f9ea9ba2","4618":"https:\/\/www.semanticscholar.org\/paper\/02e2b6bcf6a87fef41f029b9170ab84656774896","4619":"https:\/\/www.semanticscholar.org\/paper\/ffea7f0fd89dc940107cdf94f7decfcc42315c67","4620":"https:\/\/www.semanticscholar.org\/paper\/ea407573bfcd39f9a478fe33cf6ce0ee1780a5f0","4621":"https:\/\/www.semanticscholar.org\/paper\/cb45e9217fe323fbc199d820e7735488fca2a9b3","4622":"https:\/\/www.semanticscholar.org\/paper\/aea9f18d70a78d39ca927f2baa143e084c486086","4623":"https:\/\/www.semanticscholar.org\/paper\/8cd4054b41936ba0889edc26be8969c3dc8491d8","4624":"https:\/\/www.semanticscholar.org\/paper\/7fedd981f2769bd009f749a3dff7044d8378c9b4","4625":"https:\/\/www.semanticscholar.org\/paper\/c78468c2f87efabf8845ccd210ced364d45e5eab","4626":"https:\/\/www.semanticscholar.org\/paper\/a79214d4778e77b89f7899ddf9d316177ce8f56b","4627":"https:\/\/www.semanticscholar.org\/paper\/355f98e4827a1b6ad3f29d07ea2bcf9ad078295c","4628":"https:\/\/www.semanticscholar.org\/paper\/a4d5e425cac0bf84c86c0c9f720b6339d6288ffa","4629":"https:\/\/www.semanticscholar.org\/paper\/dc6826017bcb1f39f1bdf03c2c219e9fae804a17","4630":"https:\/\/www.semanticscholar.org\/paper\/81b0af7afc5b3ac9457191d3e73abf7a892ea3a7","4631":"https:\/\/www.semanticscholar.org\/paper\/efb1fc03a08db9fee8aa0e56ca079bda3f217fe7","4632":"https:\/\/www.semanticscholar.org\/paper\/a5036f31f0e629dc661f120b8c3b1f374d479ab8","4633":"https:\/\/www.semanticscholar.org\/paper\/86d4b2887d6834058865826231152adcae3380fc","4634":"https:\/\/www.semanticscholar.org\/paper\/34b2fb4d05b80cb73d0be3c855f7b236fbc3640c","4635":"https:\/\/www.semanticscholar.org\/paper\/28d0280e2b972155c203b96bd6eb9f826aa73850","4636":"https:\/\/www.semanticscholar.org\/paper\/9b96a0c9dff57484cc2b5b3a0a394ea20c9d669b","4637":"https:\/\/www.semanticscholar.org\/paper\/46f418bf6fab132f193661226c5c27d67f870ea5","4638":"https:\/\/www.semanticscholar.org\/paper\/f64a9b9019d064ac8b4482a5a166b2f8977d15ac","4639":"https:\/\/www.semanticscholar.org\/paper\/3febb2bed8865945e7fddc99efd791887bb7e14f","4640":"https:\/\/www.semanticscholar.org\/paper\/87e02a265606f31e65986f3c1c448a3e3a3a066e","4641":"https:\/\/www.semanticscholar.org\/paper\/adb272fbdea3631059cf88ab764bb6c2ce29f965","4642":"https:\/\/www.semanticscholar.org\/paper\/8e4fb17fff38a7834af5b4eaafcbbde02bf00975","4643":"https:\/\/www.semanticscholar.org\/paper\/5278a8eb2ba2429d4029745caf4e661080073c81","4644":"https:\/\/www.semanticscholar.org\/paper\/53d8b356551a2361020a948f64454a6d599af69f","4645":"https:\/\/www.semanticscholar.org\/paper\/1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","4646":"https:\/\/www.semanticscholar.org\/paper\/b836e2f2267bdf8ee6e55f5fa4c82da4cd27747e","4647":"https:\/\/www.semanticscholar.org\/paper\/0b0debb710366cdff461938c80763eace1651af6","4648":"https:\/\/www.semanticscholar.org\/paper\/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","4649":"https:\/\/www.semanticscholar.org\/paper\/381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55","4650":"https:\/\/www.semanticscholar.org\/paper\/17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","4651":"https:\/\/www.semanticscholar.org\/paper\/a2d2bbe4c542173662a444b33b76c66992697830","4652":"https:\/\/www.semanticscholar.org\/paper\/4972b88f8f324a4fa18e921f62a9857af2b5fc7b","4653":"https:\/\/www.semanticscholar.org\/paper\/3e4085e5869f1b7959707a1e1d7d273b6057eb4e","4654":"https:\/\/www.semanticscholar.org\/paper\/546d0624adfc6e18fb87d8cc77e7705bb9ea7445","4655":"https:\/\/www.semanticscholar.org\/paper\/eeb7492aa8176aeb349b33e2828778dcf9f4796c","4656":"https:\/\/www.semanticscholar.org\/paper\/3599a236f285af48782fc30b1341d13ec7320735","4657":"https:\/\/www.semanticscholar.org\/paper\/5308b9d6c001304a882a50891ccce9f7ccb1c3ec","4658":"https:\/\/www.semanticscholar.org\/paper\/8d6a67a5f4192280a35ccaaf4e660c1772c54a63","4659":"https:\/\/www.semanticscholar.org\/paper\/16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6","4660":"https:\/\/www.semanticscholar.org\/paper\/e3d3571533e43f42218bceaa8cfda9fdaedb89d0","4661":"https:\/\/www.semanticscholar.org\/paper\/c8b25fab5608c3e033d34b4483ec47e68ba109b7","4662":"https:\/\/www.semanticscholar.org\/paper\/5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","4663":"https:\/\/www.semanticscholar.org\/paper\/74157ae408173bf713f1e94f15aca1475c43bd74","4664":"https:\/\/www.semanticscholar.org\/paper\/62e176977d439aac2e2d7eca834a7a99016dfcaf","4665":"https:\/\/www.semanticscholar.org\/paper\/110b7719f16eae8d82af0f56fca6e802c842c181","4666":"https:\/\/www.semanticscholar.org\/paper\/6a10eaedec9d402fb8d8b8900994d0aa27c5e129","4667":"https:\/\/www.semanticscholar.org\/paper\/95a251513853c6032bdecebd4b74e15795662986","4668":"https:\/\/www.semanticscholar.org\/paper\/809cc93921e4698bde891475254ad6dfba33d03b","4669":"https:\/\/www.semanticscholar.org\/paper\/ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","4670":"https:\/\/www.semanticscholar.org\/paper\/aa8e8283f11913b08326a0f760f2a737e66388c7","4671":"https:\/\/www.semanticscholar.org\/paper\/44279244407a64431810f982be6d0c7da4429dd7","4672":"https:\/\/www.semanticscholar.org\/paper\/93f6dd2c761fdeac0af6d2253d57834439d7794f","4673":"https:\/\/www.semanticscholar.org\/paper\/41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76","4674":"https:\/\/www.semanticscholar.org\/paper\/1e88d5afe19aea324d33541f60a90b7036894c32","4675":"https:\/\/www.semanticscholar.org\/paper\/7c32e01d3b8511fe16d0bd1f385c43f95d2cc772","4676":"https:\/\/www.semanticscholar.org\/paper\/f6fbb6809374ca57205bd2cf1421d4f4fa04f975","4677":"https:\/\/www.semanticscholar.org\/paper\/972706306f85b1bfb40c7d35c796ad5174eb0c9c","4678":"https:\/\/www.semanticscholar.org\/paper\/5922f437512158970c417f4413bface021df5f78","4679":"https:\/\/www.semanticscholar.org\/paper\/771371fb288da26a9812f5808535847a0a9c9a80","4680":"https:\/\/www.semanticscholar.org\/paper\/23dd78e424d32f6a48660dcd67ce994b8a7db8be","4681":"https:\/\/www.semanticscholar.org\/paper\/d1dbf643447405984eeef098b1b320dee0b3b8a7","4682":"https:\/\/www.semanticscholar.org\/paper\/e2dba792360873aef125572812f3673b1a85d850","4683":"https:\/\/www.semanticscholar.org\/paper\/0adec918885dff698acf359988ed79a543157f80","4684":"https:\/\/www.semanticscholar.org\/paper\/2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413","4685":"https:\/\/www.semanticscholar.org\/paper\/fa3609e00f9f422a309c621a35394c4a38f88687","4686":"https:\/\/www.semanticscholar.org\/paper\/05f5f8b2065a520846d89771ebaea2bb1534e9c6","4687":"https:\/\/www.semanticscholar.org\/paper\/58ed1fbaabe027345f7bb3a6312d41c5aac63e22","4688":"https:\/\/www.semanticscholar.org\/paper\/a7aa150b55d64d339b1c154d6d88455fc3cbc44f","4689":"https:\/\/www.semanticscholar.org\/paper\/79c93274429d6355959f1e4374c2147bb81ea649","4690":"https:\/\/www.semanticscholar.org\/paper\/a4b828609b60b06e61bea7a4029cc9e1cad5df87","4691":"https:\/\/www.semanticscholar.org\/paper\/efbd381493bb9636f489b965a2034d529cd56bcd","4692":"https:\/\/www.semanticscholar.org\/paper\/17f5c7411eeeeedf25b0db99a9130aa353aee4ba","4693":"https:\/\/www.semanticscholar.org\/paper\/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","4694":"https:\/\/www.semanticscholar.org\/paper\/a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883","4695":"https:\/\/www.semanticscholar.org\/paper\/0e779fd59353a7f1f5b559b9d65fa4bfe367890c","4696":"https:\/\/www.semanticscholar.org\/paper\/e3d772986d176057aca2f5e3eb783da53b559134","4697":"https:\/\/www.semanticscholar.org\/paper\/7e5709d81558d3ef4265de29ea75931afeb1f2dd","4698":"https:\/\/www.semanticscholar.org\/paper\/053b1d7b97eb2c91fc3921d589c160b0923c70b1","4699":"https:\/\/www.semanticscholar.org\/paper\/eedf2748a9a1ba2779cde95fd8bad9c2260d5317","4700":"https:\/\/www.semanticscholar.org\/paper\/29acc890e521f7a6415666ab9eb3432c49b4587a","4701":"https:\/\/www.semanticscholar.org\/paper\/60a4a3a886338d0c8e3579d392cb32f493430255","4702":"https:\/\/www.semanticscholar.org\/paper\/06d7cb8c8816360feb33c3367073e0ef66d7d0b0","4703":"https:\/\/www.semanticscholar.org\/paper\/725264948d7b6946259af5b8d966e996b9570f99","4704":"https:\/\/www.semanticscholar.org\/paper\/6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e","4705":"https:\/\/www.semanticscholar.org\/paper\/687b13c44f849d23c2496996b5da83e706094db9","4706":"https:\/\/www.semanticscholar.org\/paper\/c0570d2981d1698c94070e9d6ef78bc588e307ff","4707":"https:\/\/www.semanticscholar.org\/paper\/dec69765fc6c188897b09c8282d32db788e2c261","4708":"https:\/\/www.semanticscholar.org\/paper\/47df3fd32d00220c85c2c51a571254fd99b2ecc7","4709":"https:\/\/www.semanticscholar.org\/paper\/bc8fa64625d9189f5801837e7b133e7fe3c581f7","4710":"https:\/\/www.semanticscholar.org\/paper\/48925fef94500cf19ee220ed74217816f1ab5e60","4711":"https:\/\/www.semanticscholar.org\/paper\/c7870b78c57e0bd2e9fb6907c0702e28eb87e239","4712":"https:\/\/www.semanticscholar.org\/paper\/1cb789ab8925bda02758bcb69eb0ed1547b5f4b9","4713":"https:\/\/www.semanticscholar.org\/paper\/9f4f3e96d39bbcce18f45c0f4212e6db7207b41c","4714":"https:\/\/www.semanticscholar.org\/paper\/1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6","4715":"https:\/\/www.semanticscholar.org\/paper\/50f76736c3090c6effac25400e5e40cc0b7b5ad9","4716":"https:\/\/www.semanticscholar.org\/paper\/834cb8e1e738b8d2c6d24e652ac966d6e7089a46","4717":"https:\/\/www.semanticscholar.org\/paper\/3f46b61a7216e5763ffab5d33e06b88c9d490c85","4718":"https:\/\/www.semanticscholar.org\/paper\/19dbb57ad106137553bff4282149ac2800b5c176","4719":"https:\/\/www.semanticscholar.org\/paper\/604764133befe7a0aaa692919545846197e6e065","4720":"https:\/\/www.semanticscholar.org\/paper\/c401e01c9ee32fab7d02670d1c754f44fc1ff99e","4721":"https:\/\/www.semanticscholar.org\/paper\/789b5441743c2e38cf4c38749ed820c0671d81b1","4722":"https:\/\/www.semanticscholar.org\/paper\/00ae51ba9340abc30d36804f9b51ab83b81cec23","4723":"https:\/\/www.semanticscholar.org\/paper\/ce9ca56036307217ea565644d3d3bd74b879e045","4724":"https:\/\/www.semanticscholar.org\/paper\/520bd2331cca8d5a9c032c186a2a0f7704ead6ff","4725":"https:\/\/www.semanticscholar.org\/paper\/656ed155c2d345c19d9bff4b50f2ae00db8407cc","4726":"https:\/\/www.semanticscholar.org\/paper\/3a33d36257a40d180bef5385c8586fb618fc1161","4727":"https:\/\/www.semanticscholar.org\/paper\/9a618cca0d2fc78db1be1aed70517401cb3f3859","4728":"https:\/\/www.semanticscholar.org\/paper\/8429d29385ae410cef9a5cf6118528bbfc39a751","4729":"https:\/\/www.semanticscholar.org\/paper\/5f6ab1d5f152486c340d51b70b5d16688fbb09b7","4730":"https:\/\/www.semanticscholar.org\/paper\/e66869e7e045eefa34dd8f8021304b1904f452b5","4731":"https:\/\/www.semanticscholar.org\/paper\/7ec028ace29244cb74c105327a7e4177a34aa6bd","4732":"https:\/\/www.semanticscholar.org\/paper\/e41498c05d4c68e4750fb84a380317a112d97b01","4733":"https:\/\/www.semanticscholar.org\/paper\/8b395470a57c48d174c4216ea21a7a58bc046917","4734":"https:\/\/www.semanticscholar.org\/paper\/7402b604f14b8b91c53ed6eed04af92c59636c97","4735":"https:\/\/www.semanticscholar.org\/paper\/12a763cb52f650710900790ca0bc43e5d5b88be6","4736":"https:\/\/www.semanticscholar.org\/paper\/063f8b1ecf2394ca776ac61869734de9c1953808","4737":"https:\/\/www.semanticscholar.org\/paper\/f1a8ff5542ffd51c3618953e5cf926c467154f79","4738":"https:\/\/www.semanticscholar.org\/paper\/591080c335daba2494f18cc04c9f7071501af0eb","4739":"https:\/\/www.semanticscholar.org\/paper\/476029ac9be26bf7f121a388f5c1e45d204efe52","4740":"https:\/\/www.semanticscholar.org\/paper\/b281a9d0c728979f7ffccba1a61d0fc1d29530c1","4741":"https:\/\/www.semanticscholar.org\/paper\/f98e135986414cccf29aec593d547c0656e4d82c"},"year":{"0":"2021","1":"2020","2":"2019","3":"2018","4":"2023","5":"2019","6":"1994","7":"2023","8":"2022","9":"2020","10":"2020","11":"2023","12":"2021","13":"2021","14":"2018","15":"2019","16":"2018","17":"2019","18":"2018","19":"2021","20":"2022","21":"2021","22":"2017","23":"2019","24":"2018","25":"2018","26":"2020","27":"2019","28":"2018","29":"2022","30":"2018","31":"2018","32":"2023","33":"2017","34":"2018","35":"2018","36":"2023","37":"2020","38":"2020","39":"2020","40":"2022","41":"2022","42":"2014","43":"2018","44":"2019","45":"2020","46":"1973","47":"2021","48":"2018","49":"2021","50":"2023","51":"2023","52":"2003","53":"2023","54":"1995","55":"2007","56":"2022","57":"2020","58":"2020","59":"2023","60":"2019","61":"2023","62":"2021","63":"2015","64":"1997","65":"2018","66":"2020","67":"2010","68":"2021","69":"1985","70":"1992","71":"2022","72":"1990","73":"2018","74":"2019","75":"2022","76":"2020","77":"2018","78":"2022","79":"1999","80":"2003","81":"2022","82":"2021","83":"1994","84":"2020","85":"1999","86":"2022","87":"2019","88":"2020","89":"1993","90":"2000","91":"1987","92":"2018","93":"1991","94":"2020","95":"2011","96":"2020","97":"2020","98":"1996","99":"2010","100":"1993","101":"1991","102":"2022","103":"2021","104":"2019","105":"2022","106":"2004","107":"2005","108":"2022","109":"2018","110":"2013","111":"2022","112":"2021","113":"1995","114":"1987","115":"1977","116":"1995","117":"1976","118":"1987","119":"2002","120":"2005","121":"2019","122":"1999","123":"2004","124":"2020","125":"1982","126":"2021","127":"1999","128":"2002","129":"2019","130":"2022","131":"2019","132":"2022","133":"2023","134":"2022","135":"2016","136":"2020","137":"2003","138":"2012","139":"2023","140":"1991","141":"1992","142":"1991","143":"2000","144":"2019","145":"2020","146":"2014","147":"2023","148":"2023","149":"2007","150":"2017","151":"2004","152":"2007","153":"2018","154":"1996","155":"1991","156":"2017","157":"2000","158":"2020","159":"2008","160":"1996","161":"2021","162":"2020","163":"2022","164":"1991","165":"2020","166":"1999","167":"2004","168":"2020","169":"1995","170":"2022","171":"2003","172":"2022","173":"2018","174":"1989","175":"1991","176":"1984","177":"2019","178":"2022","179":"1991","180":"2003","181":"1991","182":"2021","183":"2007","184":"2023","185":"2018","186":"2008","187":"2016","188":"1988","189":"2009","190":"2002","191":"2007","192":"2004","193":"1990","194":"1993","195":"2019","196":"2012","197":"1995","198":"2023","199":"2009","200":"1987","201":"2018","202":"1986","203":"1996","204":"2016","205":"2009","206":"1980","207":"1986","208":"2019","209":"2017","210":"2021","211":"2016","212":"2019","213":"2018","214":"1993","215":"2009","216":"2015","217":"1999","218":"2022","219":"2019","220":"2023","221":"2007","222":"1984","223":"2023","224":"2003","225":"1989","226":"2007","227":"2022","228":"2009","229":"2018","230":"1994","231":"1992","232":"2019","233":"1986","234":"2007","235":"2004","236":"2000","237":"2004","238":"2022","239":"2020","240":"1989","241":"2019","242":"1989","243":"2021","244":"1979","245":"2006","246":"2023","247":"2005","248":"2007","249":"2000","250":"2006","251":"2008","252":"1998","253":"2021","254":"2000","255":"2017","256":"2006","257":"2022","258":"2020","259":"2021","260":"2003","261":"1990","262":"2003","263":"2016","264":"2020","265":"2021","266":"2012","267":"1974","268":"1993","269":"1990","270":"1989","271":"2020","272":"1986","273":"1989","274":"1970","275":"2003","276":"1990","277":"2023","278":"2002","279":"2013","280":"2009","281":"2010","282":"2017","283":"2019","284":"2006","285":"1999","286":"2023","287":"2022","288":"2010","289":"2008","290":"1989","291":"1999","292":"1989","293":"1986","294":"2011","295":"2003","296":"2014","297":"2003","298":"2008","299":"2006","300":"2000","301":"2022","302":"2007","303":"2006","304":"1981","305":"1995","306":"2011","307":"2011","308":"2009","309":"1989","310":"2009","311":"1995","312":"1988","313":"1986","314":"2006","315":"2009","316":"2017","317":"2005","318":"2009","319":"1999","320":"1989","321":"2021","322":"2010","323":"2022","324":"2000","325":"2017","326":"2017","327":"2012","328":"1995","329":"2020","330":"2007","331":"2019","332":"2023","333":"2021","334":"2014","335":"2010","336":"2010","337":"1999","338":"2010","339":"2005","340":"1989","341":"1987","342":"2008","343":"2018","344":"2006","345":"2018","346":"2006","347":"2003","348":"2023","349":"1986","350":"2022","351":"2010","352":"2016","353":"2022","354":"2008","355":"2010","356":"1992","357":"1996","358":"1995","359":"2009","360":"2021","361":"2023","362":"1990","363":"2008","364":"2021","365":"1991","366":"2009","367":"2019","368":"2011","369":"2006","370":"2019","371":"2019","372":"2017","373":"1992","374":"2013","375":"2013","376":"2023","377":"2008","378":"2008","379":"2006","380":"2005","381":"2017","382":"2004","383":"2007","384":"2013","385":"1975","386":"2017","387":"2005","388":"1989","389":"2016","390":"2006","391":"2020","392":"2020","393":"1973","394":"2019","395":"1995","396":"2018","397":"2021","398":"2017","399":"2021","400":"2005","401":"1986","402":"1986","403":"2009","404":"2005","405":"2008","406":"2006","407":"2012","408":"2005","409":"2004","410":"2007","411":"1992","412":"2008","413":"2005","414":"2005","415":"2020","416":"2008","417":"2016","418":"{2006}","419":"2000","420":"2000","421":"2018","422":"2018","423":"2000","424":"2013","425":"1996","426":"2015","427":"2003","428":"1987","429":"1999","430":"2023","431":"2005","432":"1970","433":"2002","434":"2020","435":"2023","436":"1987","437":"2017","438":"2019","439":"2008","440":"2004","441":"2015","442":"2015","443":"2020","444":"2019","445":"2011","446":"2017","447":"2023","448":"2022","449":"2018","450":"2020","451":"1976","452":"2024","453":"2019","454":"2008","455":"2003","456":"1988","457":"2012","458":"2000","459":"2004","460":"2013","461":"2010","462":"2004","463":"1973","464":"1999","465":"2004","466":"2021","467":"1989","468":"2013","469":"2023","470":"2020","471":"2020","472":"2005","473":"2014","474":"2019","475":"2015","476":"2011","477":"2004","478":"1995","479":"2015","480":"2010","481":"2006","482":"2004","483":"1990","484":"2019","485":"2020","486":"2022","487":"2016","488":"2011","489":"2019","490":"2019","491":"2019","492":"2018","493":"2014","494":"2011","495":"2020","496":"2003","497":"1993","498":"2019","499":"2019","500":"2018","501":"1992","502":"1986","503":"1963","504":"1992","505":"2005","506":"1981","507":"2018","508":"2017","509":"2002","510":"2004","511":"2003","512":"2008","513":"2006","514":"2020","515":"1994","516":"1997","517":"2023","518":"2006","519":"2021","520":"2008","521":"1988","522":"1991","523":"1994","524":"2005","525":"1982","526":"2021","527":"2023","528":"2013","529":"2022","530":"2021","531":"2022","532":"2010","533":"2018","534":"2017","535":"2007","536":"2014","537":"2017","538":"2006","539":"1994","540":"2011","541":"2017","542":"2009","543":"2000","544":"2018","545":"2005","546":"2000","547":"2000","548":"2022","549":"1990","550":"2019","551":"2019","552":"2018","553":"1988","554":"1986","555":"2016","556":"2019","557":"2006","558":"1989","559":"2020","560":"2009","561":"2005","562":"2005","563":"2020","564":"2018","565":"2021","566":"2023","567":"2018","568":"1989","569":"2021","570":"2006","571":"2019","572":"1985","573":"1985","574":"2015","575":"2016","576":"1987","577":"2017","578":"1990","579":"2020","580":"2020","581":"2001","582":"1963","583":"1987","584":"1994","585":"1986","586":"2006","587":"1999","588":"1991","589":"2020","590":"1997","591":"1991","592":"1997","593":"1991","594":"2002","595":"1994","596":"1994","597":"2014","598":"2019","599":"2020","600":"2013","601":"2011","602":"2004","603":"2018","604":"2006","605":"2000","606":"1984","607":"2011","608":"2000","609":"1994","610":"1987","611":"2019","612":"2000","613":"2019","614":"1982","615":"1991","616":"1997","617":"2018","618":"1975","619":"1976","620":"1961","621":"1977","622":"1977","623":"1987","624":"2019","625":"2007","626":"2019","627":"2008","628":"2020","629":"2014","630":"1984","631":"1984","632":"2010","633":"1994","634":"1997","635":"1998","636":"2019","637":"2000","638":"2004","639":"2019","640":"1992","641":"2018","642":"1982","643":"1962","644":"2017","645":"1996","646":"2005","647":"1985","648":"2013","649":"2018","650":"2020","651":"1986","652":"1990","653":"2008","654":"1991","655":"1986","656":"2001","657":"1985","658":"1993","659":"1984","660":"1994","661":"2002","662":"2002","663":"2018","664":"2023","665":"2008","666":"1984","667":"1980","668":"1987","669":"1990","670":"1991","671":"2019","672":"2019","673":"2019","674":"2004","675":"2009","676":"1992","677":"2020","678":"2006","679":"1988","680":"1985","681":"1961","682":"1988","683":"1989","684":"1991","685":"2018","686":"1992","687":"2019","688":"2010","689":"1985","690":"1986","691":"1980","692":"2019","693":"2007","694":"2019","695":"2005","696":"1986","697":"2002","698":"2020","699":"2018","700":"2006","701":"2005","702":"2018","703":"2017","704":"2018","705":"2016","706":"1988","707":"1987","708":"1988","709":"1997","710":"2019","711":"1993","712":"2002","713":"1994","714":"2011","715":"1999","716":"1993","717":"1996","718":"1999","719":"2000","720":"1989","721":"2019","722":"2014","723":"1993","724":"1983","725":"2018","726":"1995","727":"2000","728":"2000","729":"2014","730":"2007","731":"2010","732":"2004","733":"2015","734":"2022","735":"1989","736":"2008","737":"2009","738":"2015","739":"2013","740":"2020","741":"2019","742":"2017","743":"2018","744":"2021","745":"2018","746":"2001","747":"2010","748":"2018","749":"2023","750":"2021","751":"2018","752":"2023","753":"2017","754":"2009","755":"2019","756":"2017","757":"2021","758":"2015","759":"1988","760":"2009","761":"1999","762":"2018","763":"2003","764":"2009","765":"2021","766":"2020","767":"1998","768":"2003","769":"2021","770":"2023","771":"2018","772":"2019","773":"2021","774":"2020","775":"2017","776":"2019","777":"2021","778":"2001","779":"2018","780":"2010","781":"2022","782":"2020","783":"2011","784":"2022","785":"2022","786":"2018","787":"2023","788":"2023","789":"1999","790":"2018","791":"2005","792":"2000","793":"2009","794":"2015","795":"2022","796":"2018","797":"2018","798":"2021","799":"2023","800":"2023","801":"2018","802":"2011","803":"2021","804":"2008","805":"2021","806":"2015","807":"2024","808":"2021","809":"2023","810":"2009","811":"2021","812":"2021","813":"2009","814":"2023","815":"2017","816":"2018","817":"2007","818":"2020","819":"2023","820":"2009","821":"1997","822":"1988","823":"2023","824":"2009","825":"2009","826":"2017","827":"2020","828":"2023","829":"2023","830":"2001","831":"2020","832":"2009","833":"2023","834":"2022","835":"2015","836":"2018","837":"2019","838":"2022","839":"2023","840":"2023","841":"2019","842":"2023","843":"2020","844":"2017","845":"2023","846":"2015","847":"2008","848":"2018","849":"2022","850":"2017","851":"2023","852":"2017","853":"2013","854":"2022","855":"2022","856":"2020","857":"2012","858":"2021","859":"2006","860":"2017","861":"2012","862":"2017","863":"2017","864":"2007","865":"2018","866":"2023","867":"2019","868":"2009","869":"2019","870":"2012","871":"2017","872":"2015","873":"2021","874":"2004","875":"2021","876":"2021","877":"1992","878":"2018","879":"2021","880":"2023","881":"2023","882":"2019","883":"2004","884":"1991","885":"1987","886":"2019","887":"2015","888":"2012","889":"2018","890":"2016","891":"1997","892":"2014","893":"2020","894":"2014","895":"2019","896":"2020","897":"2018","898":"1988","899":"2023","900":"2016","901":"2020","902":"2018","903":"2010","904":"2017","905":"2023","906":"2023","907":"2021","908":"2004","909":"2021","910":"2019","911":"2008","912":"2021","913":"2010","914":"2017","915":"2014","916":"2005","917":"2007","918":"2017","919":"2018","920":"2017","921":"2005","922":"1992","923":"1999","924":"2007","925":"1991","926":"2014","927":"2006","928":"2017","929":"2022","930":"1993","931":"2006","932":"2016","933":"2018","934":"1988","935":"1998","936":"2016","937":"2020","938":"1986","939":"1987","940":"2015","941":"2017","942":"2012","943":"1999","944":"1989","945":"2021","946":"2019","947":"2009","948":"2023","949":"1976","950":"2023","951":"2016","952":"2007","953":"2005","954":"2022","955":"2020","956":"1997","957":"2006","958":"1999","959":"1994","960":"2009","961":"1990","962":"2018","963":"2021","964":"2022","965":"2018","966":"1999","967":"1997","968":"2000","969":"2017","970":"2017","971":"1987","972":"2015","973":"2017","974":"1999","975":"2006","976":"2006","977":"2011","978":"2013","979":"2018","980":"2009","981":"1986","982":"1994","983":"2004","984":"2018","985":"2018","986":"2022","987":"2005","988":"2018","989":"2003","990":"2011","991":"2011","992":"2013","993":"2022","994":"2019","995":"2020","996":"2023","997":"2005","998":"2020","999":"2023","1000":"2018","1001":"2015","1002":"1988","1003":"1990","1004":"2019","1005":"2017","1006":"2018","1007":"1990","1008":"2015","1009":"2014","1010":"2018","1011":"2015","1012":"1997","1013":"1994","1014":"2018","1015":"2012","1016":"2013","1017":"1988","1018":"1989","1019":"2019","1020":"2018","1021":"2016","1022":"1987","1023":"2019","1024":"2016","1025":"2000","1026":"2018","1027":"2005","1028":"2003","1029":"1993","1030":"2019","1031":"2019","1032":"2005","1033":"2017","1034":"2008","1035":"2005","1036":"2018","1037":"2018","1038":"2015","1039":"2023","1040":"2017","1041":"2017","1042":"2016","1043":"2022","1044":"2007","1045":"2006","1046":"1991","1047":"2019","1048":"1993","1049":"2003","1050":"2006","1051":"2001","1052":"2010","1053":"1998","1054":"2003","1055":"1990","1056":"2001","1057":"1987","1058":"2018","1059":"1991","1060":"2005","1061":"1989","1062":"1998","1063":"2017","1064":"2006","1065":"2018","1066":"2017","1067":"2006","1068":"1993","1069":"2023","1070":"2009","1071":"2019","1072":"2019","1073":"2020","1074":"2018","1075":"2016","1076":"2019","1077":"2019","1078":"2019","1079":"2019","1080":"2012","1081":"1994","1082":"1986","1083":"2019","1084":"2018","1085":"2017","1086":"2017","1087":"2017","1088":"2007","1089":"2020","1090":"1993","1091":"1997","1092":"2002","1093":"1996","1094":"2020","1095":"2013","1096":"2017","1097":"2019","1098":"2009","1099":"2013","1100":"2018","1101":"2008","1102":"2005","1103":"2020","1104":"1992","1105":"1998","1106":"1982","1107":"2013","1108":"2012","1109":"2018","1110":"2017","1111":"1992","1112":"2022","1113":"1999","1114":"2023","1115":"2006","1116":"2004","1117":"1984","1118":"2017","1119":"2020","1120":"2017","1121":"1982","1122":"2013","1123":"2020","1124":"2018","1125":"2011","1126":"2011","1127":"2017","1128":"1998","1129":"2013","1130":"2005","1131":"2007","1132":"2011","1133":"2024","1134":"2022","1135":"1998","1136":"1983","1137":"1983","1138":"2005","1139":"2016","1140":"2015","1141":"2018","1142":"2014","1143":"2017","1144":"1998","1145":"2013","1146":"2020","1147":"1992","1148":"2017","1149":"2016","1150":"2002","1151":"2006","1152":"2015","1153":"2017","1154":"1987","1155":"1993","1156":"2011","1157":"1998","1158":"1989","1159":"2006","1160":"2010","1161":"2019","1162":"2016","1163":"2020","1164":"2015","1165":"1998","1166":"2012","1167":"2011","1168":"2001","1169":"2001","1170":"2001","1171":"2001","1172":"2001","1173":"2016","1174":"2020","1175":"2020","1176":"2020","1177":"2020","1178":"2020","1179":"2020","1180":"2020","1181":"2020","1182":"2020","1183":"2020","1184":"1998","1185":"1998","1186":"1998","1187":"2009","1188":"2009","1189":"2003","1190":"1998","1191":"2009","1192":"1998","1193":"1998","1194":"2003","1195":"2009","1196":"2009","1197":"2003","1198":"2003","1199":"2003","1200":"2012","1201":"2012","1202":"2012","1203":"2012","1204":"2012","1205":"2012","1206":"2012","1207":"2003","1208":"2003","1209":"2003","1210":"2012","1211":"2012","1212":"2012","1213":"2012","1214":"2003","1215":"2012","1216":"2003","1217":"2003","1218":"2003","1219":"2011","1220":"2003","1221":"2003","1222":"2008","1223":"2008","1224":"2011","1225":"2005","1226":"2011","1227":"2011","1228":"2011","1229":"2011","1230":"2005","1231":"2011","1232":"2011","1233":"2011","1234":"2008","1235":"2008","1236":"2011","1237":"2011","1238":"2005","1239":"2005","1240":"2018","1241":"2005","1242":"2018","1243":"2018","1244":"2018","1245":"2018","1246":"2002","1247":"2002","1248":"2005","1249":"2005","1250":"2002","1251":"2018","1252":"2002","1253":"2002","1254":"2002","1255":"2002","1256":"2002","1257":"2002","1258":"2016","1259":"2018","1260":"2018","1261":"2016","1262":"2016","1263":"2018","1264":"2018","1265":"2002","1266":"2018","1267":"2016","1268":"2016","1269":"2016","1270":"2007","1271":"2006","1272":"2016","1273":"2016","1274":"2006","1275":"2007","1276":"2007","1277":"2006","1278":"2006","1279":"2006","1280":"2006","1281":"2006","1282":"2007","1283":"2007","1284":"2007","1285":"2007","1286":"2006","1287":"2000","1288":"2000","1289":"2013","1290":"2000","1291":"2013","1292":"2013","1293":"2013","1294":"2013","1295":"2017","1296":"2013","1297":"2017","1298":"2017","1299":"2007","1300":"2013","1301":"2013","1302":"2013","1303":"2014","1304":"2014","1305":"2015","1306":"2014","1307":"2014","1308":"2014","1309":"2015","1310":"2014","1311":"2015","1312":"2015","1313":"2015","1314":"2017","1315":"2015","1316":"2010","1317":"2017","1318":"2017","1319":"2017","1320":"2010","1321":"2010","1322":"2010","1323":"2004","1324":"2004","1325":"2010","1326":"1996","1327":"2004","1328":"2004","1329":"2004","1330":"2010","1331":"1996","1332":"2010","1333":"2010","1334":"2004","1335":"2004","1336":"2004","1337":"2004","1338":"2004","1339":"2004","1340":"2004","1341":"2004","1342":"2004","1343":"2004","1344":"2004","1345":"2006","1346":"2008","1347":"2019","1348":"2019","1349":"2019","1350":"2019","1351":"2019","1352":"2019","1353":"2019","1354":"2019","1355":"2019","1356":"2019","1357":"2019","1358":"2019","1359":"2019","1360":"2023","1361":"2020","1362":"2019","1363":"2019","1364":"2015","1365":"2006","1366":"2014","1367":"2017","1368":"1992","1369":"2009","1370":"2007","1371":"2017","1372":"1995","1373":"2015","1374":"2023","1375":"2019","1376":"2019","1377":"2019","1378":"2019","1379":"2019","1380":"2019","1381":"2019","1382":"2019","1383":"2019","1384":"2019","1385":"2019","1386":"2014","1387":"2000","1388":"2019","1389":"2001","1390":"2001","1391":"2001","1392":"2001","1393":"2001","1394":"2001","1395":"2010","1396":"2009","1397":"2015","1398":"2011","1399":"2011","1400":"2011","1401":"2011","1402":"2008","1403":"2008","1404":"2008","1405":"2018","1406":"2018","1407":"2018","1408":"2002","1409":"2002","1410":"2018","1411":"2002","1412":"2002","1413":"2018","1414":"2002","1415":"2002","1416":"2002","1417":"2002","1418":"2002","1419":"2005","1420":"2016","1421":"2018","1422":"2018","1423":"2018","1424":"2016","1425":"2016","1426":"2007","1427":"2007","1428":"2016","1429":"2016","1430":"2006","1431":"2006","1432":"2006","1433":"2007","1434":"2007","1435":"2007","1436":"2006","1437":"2007","1438":"2006","1439":"2006","1440":"2000","1441":"2000","1442":"2013","1443":"2000","1444":"2000","1445":"2000","1446":"2000","1447":"2017","1448":"2017","1449":"2013","1450":"2013","1451":"2013","1452":"2017","1453":"2013","1454":"2013","1455":"2007","1456":"2000","1457":"2007","1458":"2007","1459":"2013","1460":"2013","1461":"2007","1462":"2007","1463":"2013","1464":"2014","1465":"2015","1466":"2017","1467":"2014","1468":"2015","1469":"2014","1470":"2015","1471":"2015","1472":"2010","1473":"2017","1474":"2015","1475":"2015","1476":"2014","1477":"2015","1478":"2017","1479":"2017","1480":"2017","1481":"2010","1482":"1996","1483":"1996","1484":"1996","1485":"2010","1486":"1996","1487":"1996","1488":"2004","1489":"1996","1490":"2010","1491":"2010","1492":"2010","1493":"1996","1494":"2010","1495":"2010","1496":"2004","1497":"2004","1498":"2010","1499":"1996","1500":"2010","1501":"2010","1502":"2010","1503":"2010","1504":"2004","1505":"2004","1506":"2004","1507":"2004","1508":"2004","1509":"2004","1510":"2004","1511":"2004","1512":"2009","1513":"2009","1514":"2009","1515":"1998","1516":"2009","1517":"2009","1518":"2009","1519":"2003","1520":"1998","1521":"1998","1522":"2009","1523":"2009","1524":"2009","1525":"1998","1526":"1998","1527":"2009","1528":"2009","1529":"2003","1530":"2003","1531":"2003","1532":"2003","1533":"2003","1534":"2003","1535":"2012","1536":"2012","1537":"2012","1538":"2012","1539":"2003","1540":"2012","1541":"2012","1542":"2012","1543":"2012","1544":"2003","1545":"2003","1546":"2005","1547":"2005","1548":"2008","1549":"2008","1550":"2008","1551":"2008","1552":"2005","1553":"2005","1554":"2005","1555":"2005","1556":"2011","1557":"2011","1558":"2011","1559":"2007","1560":"2006","1561":"2006","1562":"2011","1563":"2020","1564":"2014","1565":"2011","1566":"2013","1567":"2003","1568":"2017","1569":"2013","1570":"2020","1571":"2020","1572":"2020","1573":"2020","1574":"2020","1575":"2020","1576":"2020","1577":"2020","1578":"2020","1579":"2020","1580":"2020","1581":"2020","1582":"2020","1583":"2020","1584":"2017","1585":"2010","1586":"2019","1587":"1988","1588":"2011","1589":"2020","1590":"2020","1591":"2020","1592":"2020","1593":"2020","1594":"2020","1595":"2020","1596":"2020","1597":"2020","1598":"2020","1599":"2020","1600":"2019","1601":"2014","1602":"2010","1603":"2010","1604":"2018","1605":"2001","1606":"2001","1607":"2001","1608":"2001","1609":"2001","1610":"2001","1611":"2001","1612":"2001","1613":"2001","1614":"2010","1615":"2015","1616":"2004","1617":"2011","1618":"2010","1619":"2017","1620":"2019","1621":"2019","1622":"2020","1623":"2006","1624":"2019","1625":"2019","1626":"2019","1627":"2019","1628":"2019","1629":"2019","1630":"2019","1631":"2019","1632":"2019","1633":"2019","1634":"2013","1635":"2009","1636":"2009","1637":"2009","1638":"1998","1639":"1998","1640":"1998","1641":"1998","1642":"1998","1643":"1998","1644":"2009","1645":"2003","1646":"1998","1647":"1998","1648":"2003","1649":"2009","1650":"2009","1651":"2009","1652":"1998","1653":"1998","1654":"1998","1655":"1998","1656":"2017","1657":"2019","1658":"2020","1659":"2012","1660":"2017","1661":"2014","1662":"2021","1663":"2010","1664":"2018","1665":"2018","1666":"2017","1667":"2015","1668":"2022","1669":"2019","1670":"2021","1671":"2006","1672":"2016","1673":"2021","1674":"2015","1675":"1996","1676":"2012","1677":"2019","1678":"2020","1679":"2019","1680":"2000","1681":"2023","1682":"2012","1683":"2020","1684":"2020","1685":"2021","1686":"2019","1687":"2023","1688":"2021","1689":"2015","1690":"2019","1691":"2018","1692":"2021","1693":"2020","1694":"2021","1695":"2019","1696":"2004","1697":"2015","1698":"2021","1699":"1999","1700":"2023","1701":"2021","1702":"2019","1703":"2020","1704":"2023","1705":"2018","1706":"2009","1707":"2017","1708":"2012","1709":"2019","1710":"2019","1711":"2021","1712":"2020","1713":"2019","1714":"2021","1715":"2020","1716":"2022","1717":"2023","1718":"2020","1719":"2012","1720":"2020","1721":"2023","1722":"2017","1723":"2019","1724":"2020","1725":"2022","1726":"2020","1727":"2012","1728":"2018","1729":"2023","1730":"2017","1731":"2015","1732":"2019","1733":"2019","1734":"2019","1735":"2020","1736":"2017","1737":"2018","1738":"2018","1739":"2020","1740":"2023","1741":"2021","1742":"2012","1743":"2018","1744":"2021","1745":"2017","1746":"2018","1747":"2015","1748":"2020","1749":"2016","1750":"2017","1751":"2002","1752":"2018","1753":"2006","1754":"2020","1755":"2002","1756":"2017","1757":"2017","1758":"2002","1759":"1998","1760":"2006","1761":"2002","1762":"2022","1763":"2004","1764":"2020","1765":"2018","1766":"2020","1767":"2021","1768":"2018","1769":"1997","1770":"2017","1771":"1988","1772":"2018","1773":"2018","1774":"2019","1775":"2018","1776":"2018","1777":"2013","1778":"2019","1779":"2019","1780":"2023","1781":"2018","1782":"2019","1783":"2019","1784":"2012","1785":"2018","1786":"2008","1787":"2019","1788":"2022","1789":"2018","1790":"2018","1791":"2009","1792":"2021","1793":"2021","1794":"2020","1795":"2006","1796":"2022","1797":"2021","1798":"2019","1799":"2018","1800":"2018","1801":"2022","1802":"2022","1803":"2003","1804":"2020","1805":"2022","1806":"2023","1807":"2017","1808":"2017","1809":"2018","1810":"2016","1811":"2019","1812":"2019","1813":"2016","1814":"2017","1815":"2021","1816":"2019","1817":"2020","1818":"2018","1819":"2019","1820":"2013","1821":"2014","1822":"2020","1823":"1986","1824":"1998","1825":"2012","1826":"2022","1827":"2022","1828":"2019","1829":"2020","1830":"2022","1831":"2020","1832":"2019","1833":"1986","1834":"2015","1835":"2012","1836":"2022","1837":"2015","1838":"2010","1839":"2021","1840":"2022","1841":"2019","1842":"2018","1843":"2015","1844":"2018","1845":"2018","1846":"2009","1847":"2021","1848":"2021","1849":"2022","1850":"2016","1851":"1998","1852":"1998","1853":"1993","1854":"2009","1855":"2019","1856":"2020","1857":"2019","1858":"2017","1859":"2019","1860":"2001","1861":"2017","1862":"1997","1863":"2018","1864":"2015","1865":"2021","1866":"2020","1867":"2016","1868":"2023","1869":"2008","1870":"2020","1871":"2019","1872":"2007","1873":"2017","1874":"2006","1875":"2018","1876":"2016","1877":"2020","1878":"2018","1879":"2017","1880":"2017","1881":"2016","1882":"2007","1883":"2017","1884":"2002","1885":"2020","1886":"2023","1887":"2019","1888":"2019","1889":"2001","1890":"2017","1891":"2022","1892":"2017","1893":"2005","1894":"2019","1895":"2022","1896":"2018","1897":"2022","1898":"2019","1899":"2011","1900":"2020","1901":"2022","1902":"2003","1903":"2002","1904":"2021","1905":"2006","1906":"1997","1907":"2003","1908":"2019","1909":"2019","1910":"2019","1911":"2021","1912":"2023","1913":"2020","1914":"2019","1915":"1990","1916":"2010","1917":"1998","1918":"2002","1919":"2018","1920":"2020","1921":"2020","1922":"2022","1923":"2000","1924":"2017","1925":"2020","1926":"2019","1927":"2012","1928":"2015","1929":"2019","1930":"2007","1931":"2023","1932":"2000","1933":"2015","1934":"2018","1935":"2011","1936":"2007","1937":"2021","1938":"2019","1939":"2006","1940":"2010","1941":"2019","1942":"2017","1943":"2019","1944":"2023","1945":"2014","1946":"2012","1947":"2020","1948":"2006","1949":"2019","1950":"2019","1951":"2020","1952":"2016","1953":"2015","1954":"2021","1955":"2017","1956":"2005","1957":"2012","1958":"2022","1959":"2018","1960":"2018","1961":"2010","1962":"2021","1963":"2020","1964":"2001","1965":"2016","1966":"2015","1967":"2010","1968":"2005","1969":"2019","1970":"2001","1971":"2020","1972":"2017","1973":"2020","1974":"2019","1975":"2019","1976":"2004","1977":"2000","1978":"2013","1979":"2018","1980":"2018","1981":"2014","1982":"2017","1983":"2018","1984":"2023","1985":"2019","1986":"2022","1987":"2018","1988":"2018","1989":"2021","1990":"2023","1991":"2018","1992":"2016","1993":"2023","1994":"2017","1995":"2021","1996":"2015","1997":"2016","1998":"2004","1999":"2020","2000":"2020","2001":"2017","2002":"2023","2003":"1958","2004":"2018","2005":"2019","2006":"2011","2007":"2021","2008":"2015","2009":"2018","2010":"2002","2011":"2023","2012":"2017","2013":"2017","2014":"2016","2015":"2018","2016":"2004","2017":"2017","2018":"2023","2019":"2019","2020":"2013","2021":"2011","2022":"2017","2023":"2018","2024":"2023","2025":"2002","2026":"2014","2027":"2020","2028":"2001","2029":"2019","2030":"2011","2031":"2020","2032":"2021","2033":"2011","2034":"2010","2035":"2016","2036":"2014","2037":"2022","2038":"2018","2039":"2016","2040":"2019","2041":"2012","2042":"1989","2043":"2019","2044":"2021","2045":"2021","2046":"2018","2047":"2018","2048":"2021","2049":"2006","2050":"2008","2051":"2015","2052":"2016","2053":"2016","2054":"2016","2055":"1999","2056":"2021","2057":"2017","2058":"2002","2059":"2021","2060":"1988","2061":"2006","2062":"2017","2063":"2018","2064":"2011","2065":"2017","2066":"2022","2067":"2019","2068":"2006","2069":"2016","2070":"1987","2071":"2018","2072":"2015","2073":"2018","2074":"2006","2075":"2018","2076":"1999","2077":"2017","2078":"2017","2079":"2022","2080":"1986","2081":"2014","2082":"2008","2083":"2019","2084":"2018","2085":"2017","2086":"2012","2087":"2020","2088":"2016","2089":"2019","2090":"2019","2091":"2014","2092":"2019","2093":"2014","2094":"1999","2095":"2009","2096":"2018","2097":"1998","2098":"2019","2099":"2019","2100":"2019","2101":"2017","2102":"2020","2103":"2021","2104":"2013","2105":"2016","2106":"2010","2107":"1995","2108":"2020","2109":"2021","2110":"2020","2111":"2012","2112":"2012","2113":"2018","2114":"2018","2115":"2019","2116":"2008","2117":"2010","2118":"2020","2119":"2017","2120":"2018","2121":"2019","2122":"2011","2123":"2022","2124":"2006","2125":"2006","2126":"2016","2127":"2020","2128":"2009","2129":"2021","2130":"2016","2131":"2022","2132":"1991","2133":"2000","2134":"2018","2135":"2016","2136":"2017","2137":"2021","2138":"2015","2139":"2014","2140":"2012","2141":"2017","2142":"2022","2143":"2022","2144":"2010","2145":"2020","2146":"2009","2147":"2017","2148":"2010","2149":"2019","2150":"2010","2151":"2010","2152":"2019","2153":"2023","2154":"2010","2155":"2010","2156":"2019","2157":"2020","2158":"2018","2159":"2015","2160":"2011","2161":"2019","2162":"2019","2163":"2019","2164":"2018","2165":"2019","2166":"2019","2167":"2018","2168":"2001","2169":"2004","2170":"2004","2171":"2013","2172":"2014","2173":"2019","2174":"2020","2175":"2001","2176":"2019","2177":"2010","2178":"2017","2179":"2016","2180":"2018","2181":"2006","2182":"2022","2183":"1999","2184":"2023","2185":"2017","2186":"2009","2187":"1999","2188":"2007","2189":"2020","2190":"2012","2191":"1994","2192":"2001","2193":"1996","2194":"1999","2195":"2022","2196":"2018","2197":"2021","2198":"2011","2199":"2020","2200":"2016","2201":"1995","2202":"2007","2203":"2020","2204":"2018","2205":"2020","2206":"2020","2207":"2017","2208":"2018","2209":"2018","2210":"2016","2211":"2019","2212":"2018","2213":"2018","2214":"2019","2215":"2016","2216":"2019","2217":"2018","2218":"2018","2219":"2017","2220":"2000","2221":"2022","2222":"2007","2223":"2003","2224":"2017","2225":"1993","2226":"2001","2227":"2001","2228":"2016","2229":"2015","2230":"2021","2231":"2001","2232":"2001","2233":"2001","2234":"2016","2235":"2003","2236":"2014","2237":"2022","2238":"2001","2239":"2010","2240":"2023","2241":"2019","2242":"2006","2243":"2006","2244":"2014","2245":"2010","2246":"2019","2247":"2006","2248":"2019","2249":"2006","2250":"2015","2251":"2021","2252":"2019","2253":"2013","2254":"2011","2255":"2001","2256":"2012","2257":"2020","2258":"2018","2259":"2001","2260":"2004","2261":"2018","2262":"2020","2263":"2001","2264":"2010","2265":"2006","2266":"2017","2267":"2018","2268":"2019","2269":"1994","2270":"2013","2271":"2012","2272":"2015","2273":"2020","2274":"2021","2275":"2017","2276":"2007","2277":"2020","2278":"2018","2279":"2020","2280":"1999","2281":"2021","2282":"2009","2283":"2006","2284":"2020","2285":"2020","2286":"1997","2287":"2006","2288":"2015","2289":"2014","2290":"2022","2291":"2009","2292":"2019","2293":"2021","2294":"2019","2295":"2019","2296":"2002","2297":"2018","2298":"2022","2299":"2019","2300":"2001","2301":"2013","2302":"2020","2303":"2018","2304":"2014","2305":"1983","2306":"2008","2307":"2001","2308":"2001","2309":"2019","2310":"2018","2311":"2017","2312":"2018","2313":"1993","2314":"2017","2315":"2017","2316":"2019","2317":"2006","2318":"2021","2319":"2020","2320":"2023","2321":"2019","2322":"2000","2323":"2018","2324":"2019","2325":"2007","2326":"2005","2327":"2017","2328":"2013","2329":"2015","2330":"2023","2331":"1997","2332":"2020","2333":"2007","2334":"2023","2335":"2018","2336":"2012","2337":"2013","2338":"2020","2339":"2014","2340":"2005","2341":"2017","2342":"2018","2343":"2017","2344":"2019","2345":"2020","2346":"2011","2347":"2019","2348":"2019","2349":"2011","2350":"2019","2351":"2008","2352":"2016","2353":"2012","2354":"2004","2355":"2017","2356":"2014","2357":"2013","2358":"2003","2359":"2014","2360":"2012","2361":"2019","2362":"2020","2363":"2016","2364":"2011","2365":"2015","2366":"2020","2367":"2020","2368":"2010","2369":"2018","2370":"2006","2371":"2008","2372":"2015","2373":"2022","2374":"2018","2375":"2013","2376":"2022","2377":"2018","2378":"1997","2379":"2019","2380":"2019","2381":"2017","2382":"2006","2383":"2019","2384":"2016","2385":"2021","2386":"2018","2387":"2019","2388":"1997","2389":"2022","2390":"2014","2391":"2022","2392":"2019","2393":"2001","2394":"2023","2395":"2019","2396":"2018","2397":"2019","2398":"2017","2399":"2022","2400":"2018","2401":"2020","2402":"2023","2403":"2017","2404":"2022","2405":"2021","2406":"2017","2407":"1999","2408":"2018","2409":"2017","2410":"2020","2411":"2000","2412":"2018","2413":"2021","2414":"2010","2415":"2011","2416":"2018","2417":"2010","2418":"2010","2419":"2016","2420":"2019","2421":"2003","2422":"2004","2423":"2020","2424":"2019","2425":"2003","2426":"2019","2427":"2020","2428":"2016","2429":"2004","2430":"2010","2431":"2023","2432":"2018","2433":"2018","2434":"2013","2435":"2021","2436":"2006","2437":"2022","2438":"2016","2439":"2019","2440":"1996","2441":"1999","2442":"2021","2443":"2020","2444":"2017","2445":"2000","2446":"1994","2447":"2011","2448":"2018","2449":"1989","2450":"2018","2451":"2018","2452":"2001","2453":"2017","2454":"2001","2455":"2009","2456":"2013","2457":"2019","2458":"2018","2459":"2019","2460":"2006","2461":"2019","2462":"2018","2463":"2022","2464":"2016","2465":"2016","2466":"2018","2467":"2018","2468":"2010","2469":"2003","2470":"2017","2471":"2018","2472":"2008","2473":"2014","2474":"2014","2475":"2004","2476":"2000","2477":"2014","2478":"2014","2479":"2004","2480":"2018","2481":"2023","2482":"1992","2483":"2020","2484":"2022","2485":"2019","2486":"2019","2487":"2011","2488":"2003","2489":"2006","2490":"2021","2491":"2020","2492":"2023","2493":"2018","2494":"2014","2495":"2022","2496":"2009","2497":"2019","2498":"2000","2499":"2020","2500":"1999","2501":"2021","2502":"2019","2503":"2008","2504":"2019","2505":"2020","2506":"2008","2507":"2000","2508":"2009","2509":"2023","2510":"2020","2511":"2020","2512":"2004","2513":"2008","2514":"2022","2515":"1996","2516":"2023","2517":"2015","2518":"2007","2519":"2019","2520":"2008","2521":"2002","2522":"2007","2523":"2020","2524":"2019","2525":"2017","2526":"2016","2527":"2008","2528":"2013","2529":"2018","2530":"2009","2531":"2024","2532":"2003","2533":"2020","2534":"1988","2535":"2014","2536":"2010","2537":"2013","2538":"2003","2539":"2019","2540":"2002","2541":"2023","2542":"2002","2543":"2003","2544":"2006","2545":"1992","2546":"2019","2547":"2007","2548":"2010","2549":"2008","2550":"2008","2551":"2005","2552":"2003","2553":"2008","2554":"2015","2555":"2023","2556":"2008","2557":"2004","2558":"2004","2559":"2005","2560":"2019","2561":"1997","2562":"2012","2563":"2008","2564":"2012","2565":"2007","2566":"2022","2567":"2019","2568":"2004","2569":"2023","2570":"2006","2571":"2018","2572":"2021","2573":"2023","2574":"2009","2575":"2004","2576":"2022","2577":"2005","2578":"2019","2579":"2023","2580":"2022","2581":"2006","2582":"2003","2583":"2017","2584":"2023","2585":"2012","2586":"2005","2587":"2011","2588":"2003","2589":"2013","2590":"1972","2591":"2022","2592":"2019","2593":"2016","2594":"2016","2595":"2019","2596":"{2010}","2597":"2003","2598":"2013","2599":"2011","2600":"2013","2601":"2022","2602":"2020","2603":"2019","2604":"1998","2605":"2004","2606":"2022","2607":"2022","2608":"2022","2609":"1996","2610":"2012","2611":"1996","2612":"2019","2613":"2005","2614":"2020","2615":"2016","2616":"2020","2617":"2006","2618":"2014","2619":"1997","2620":"2003","2621":"1995","2622":"2017","2623":"2007","2624":"2003","2625":"2012","2626":"2016","2627":"2005","2628":"2004","2629":"2018","2630":"1998","2631":"2004","2632":"2003","2633":"2005","2634":"1997","2635":"2023","2636":"2010","2637":"2021","2638":"2019","2639":"2017","2640":"2005","2641":"2018","2642":"2019","2643":"2005","2644":"2020","2645":"2019","2646":"2023","2647":"2005","2648":"2021","2649":"2020","2650":"2009","2651":"2018","2652":"2008","2653":"1980","2654":"2015","2655":"2022","2656":"2019","2657":"1992","2658":"1992","2659":"2020","2660":"2014","2661":"1996","2662":"2019","2663":"2018","2664":"2001","2665":"2017","2666":"2005","2667":"2020","2668":"2014","2669":"2006","2670":"2000","2671":"2001","2672":"2017","2673":"2016","2674":"1990","2675":"2016","2676":"2012","2677":"1980","2678":"2011","2679":"2010","2680":"2008","2681":"2017","2682":"2016","2683":"2018","2684":"2003","2685":"2014","2686":"2018","2687":"1995","2688":"1980","2689":"2009","2690":"2009","2691":"1994","2692":"2005","2693":"2010","2694":"2014","2695":"2000","2696":"2013","2697":"2003","2698":"2012","2699":"2012","2700":"2020","2701":"2009","2702":"2006","2703":"2007","2704":"2010","2705":"2013","2706":"2005","2707":"2023","2708":"2022","2709":"2012","2710":"2018","2711":"2020","2712":"2012","2713":"2019","2714":"2007","2715":"2009","2716":"2012","2717":"2019","2718":"1995","2719":"2019","2720":"2020","2721":"2020","2722":"2000","2723":"2012","2724":"2017","2725":"2019","2726":"2009","2727":"2020","2728":"2021","2729":"2016","2730":"2003","2731":"2020","2732":"2009","2733":"2017","2734":"2013","2735":"2021","2736":"2014","2737":"2007","2738":"2010","2739":"2022","2740":"2015","2741":"2006","2742":"2006","2743":"2010","2744":"2019","2745":"2020","2746":"2010","2747":"1995","2748":"2008","2749":"2014","2750":"2012","2751":"2001","2752":"2006","2753":"2010","2754":"2008","2755":"2004","2756":"2008","2757":"2015","2758":"2008","2759":"2019","2760":"2004","2761":"1997","2762":"2005","2763":"2010","2764":"2016","2765":"2014","2766":"2009","2767":"2009","2768":"2019","2769":"2013","2770":"2005","2771":"2014","2772":"1995","2773":"2012","2774":"2016","2775":"2019","2776":"2000","2777":"2019","2778":"2018","2779":"2004","2780":"2018","2781":"2009","2782":"2003","2783":"2020","2784":"2020","2785":"2018","2786":"2016","2787":"2004","2788":"2003","2789":"1999","2790":"1998","2791":"2008","2792":"2014","2793":"2009","2794":"2013","2795":"2014","2796":"2014","2797":"2014","2798":"2004","2799":"2005","2800":"2003","2801":"1997","2802":"2018","2803":"1990","2804":"2011","2805":"2002","2806":"2004","2807":"2009","2808":"2020","2809":"2018","2810":"2009","2811":"2013","2812":"2009","2813":"2006","2814":"2010","2815":"2017","2816":"2018","2817":"2016","2818":"2014","2819":"2002","2820":"1999","2821":"2008","2822":"2013","2823":"2005","2824":"2005","2825":"2014","2826":"2011","2827":"2020","2828":"2019","2829":"1997","2830":"1998","2831":"2020","2832":"2015","2833":"2005","2834":"1989","2835":"2009","2836":"2012","2837":"2013","2838":"2005","2839":"1980","2840":"2016","2841":"2023","2842":"2019","2843":"2023","2844":"1998","2845":"1998","2846":"2007","2847":"2020","2848":"2019","2849":"2012","2850":"2013","2851":"1989","2852":"1981","2853":"2003","2854":"1996","2855":"2010","2856":"2019","2857":"2015","2858":"2008","2859":"2022","2860":"2020","2861":"2020","2862":"2012","2863":"2008","2864":"2010","2865":"2015","2866":"2018","2867":"2009","2868":"2002","2869":"2020","2870":"2012","2871":"2018","2872":"2020","2873":"2013","2874":"2005","2875":"2018","2876":"1999","2877":"1991","2878":"1997","2879":"2006","2880":"2000","2881":"2001","2882":"2017","2883":"2012","2884":"2014","2885":"2019","2886":"2019","2887":"2018","2888":"2001","2889":"2000","2890":"1995","2891":"2009","2892":"2018","2893":"2011","2894":"1991","2895":"2009","2896":"2013","2897":"2006","2898":"1999","2899":"2020","2900":"2014","2901":"1999","2902":"2002","2903":"2003","2904":"2007","2905":"2017","2906":"1997","2907":"2011","2908":"2017","2909":"2020","2910":"1991","2911":"2015","2912":"2017","2913":"2011","2914":"2008","2915":"2013","2916":"1995","2917":"1996","2918":"2003","2919":"2008","2920":"1980","2921":"2008","2922":"2008","2923":"2001","2924":"2010","2925":"1997","2926":"2015","2927":"2018","2928":"2019","2929":"2005","2930":"2006","2931":"2017","2932":"1998","2933":"2005","2934":"2005","2935":"2009","2936":"2016","2937":"1977","2938":"2002","2939":"1993","2940":"2004","2941":"2019","2942":"2007","2943":"2014","2944":"2009","2945":"1994","2946":"2020","2947":"2017","2948":"2002","2949":"2004","2950":"2011","2951":"2019","2952":"2016","2953":"2015","2954":"2010","2955":"2006","2956":"2008","2957":"2013","2958":"1991","2959":"2006","2960":"2001","2961":"2017","2962":"2008","2963":"2004","2964":"2009","2965":"2020","2966":"2014","2967":"2019","2968":"2015","2969":"2020","2970":"2020","2971":"2006","2972":"2014","2973":"2004","2974":"2003","2975":"2019","2976":"2017","2977":"2011","2978":"2002","2979":"2003","2980":"2012","2981":"2010","2982":"2018","2983":"2016","2984":"2013","2985":"2006","2986":"2014","2987":"1998","2988":"2018","2989":"2016","2990":"1989","2991":"2005","2992":"2003","2993":"2010","2994":"1999","2995":"1999","2996":"2006","2997":"1977","2998":"2008","2999":"2000","3000":"2005","3001":"2006","3002":"2016","3003":"2010","3004":"2010","3005":"2000","3006":"2011","3007":"2006","3008":"2014","3009":"1997","3010":"1993","3011":"1985","3012":"2020","3013":"1994","3014":"2006","3015":"1994","3016":"2007","3017":"2016","3018":"2009","3019":"2002","3020":"2017","3021":"2017","3022":"1999","3023":"2018","3024":"2010","3025":"2010","3026":"2017","3027":"2012","3028":"2010","3029":"2007","3030":"2013","3031":"1989","3032":"2017","3033":"2001","3034":"2008","3035":"2010","3036":"1997","3037":"2004","3038":"2013","3039":"2017","3040":"2012","3041":"2002","3042":"2016","3043":"2014","3044":"2001","3045":"2008","3046":"2005","3047":"1996","3048":"2001","3049":"2006","3050":"2006","3051":"2010","3052":"2008","3053":"2003","3054":"2001","3055":"1999","3056":"2004","3057":"2011","3058":"1983","3059":"2015","3060":"1990","3061":"1994","3062":"2008","3063":"2017","3064":"2013","3065":"2003","3066":"2000","3067":"1995","3068":"2019","3069":"2019","3070":"2000","3071":"2005","3072":"1981","3073":"2017","3074":"2013","3075":"2014","3076":"2015","3077":"2015","3078":"1978","3079":"2015","3080":"2008","3081":"2004","3082":"2000","3083":"2019","3084":"2003","3085":"2015","3086":"2013","3087":"2011","3088":"2008","3089":"1993","3090":"2019","3091":"2023","3092":"2019","3093":"2004","3094":"2007","3095":"2000","3096":"2020","3097":"2004","3098":"2018","3099":"2010","3100":"2015","3101":"2020","3102":"2019","3103":"2012","3104":"2009","3105":"2020","3106":"2020","3107":"2011","3108":"2019","3109":"2003","3110":"1991","3111":"2008","3112":"2000","3113":"2022","3114":"2023","3115":"2001","3116":"1976","3117":"2005","3118":"2020","3119":"2020","3120":"2018","3121":"1994","3122":"2005","3123":"2006","3124":"2000","3125":"2010","3126":"2010","3127":"2017","3128":"1992","3129":"2014","3130":"2017","3131":"2015","3132":"2014","3133":"2019","3134":"2019","3135":"2017","3136":"2012","3137":"2019","3138":"2012","3139":"2018","3140":"2019","3141":"2008","3142":"1988","3143":"1975","3144":"2009","3145":"2017","3146":"2019","3147":"2005","3148":"2019","3149":"2003","3150":"2003","3151":"2013","3152":"2017","3153":"2019","3154":"1993","3155":"2006","3156":"2018","3157":"2020","3158":"2012","3159":"2010","3160":"2020","3161":"2011","3162":"2009","3163":"2006","3164":"2007","3165":"2004","3166":"2020","3167":"2020","3168":"2017","3169":"2020","3170":"1993","3171":"1988","3172":"1988","3173":"2000","3174":"2014","3175":"2005","3176":"1999","3177":"2011","3178":"2016","3179":"2019","3180":"2007","3181":"2012","3182":"2011","3183":"2018","3184":"2018","3185":"2016","3186":"2000","3187":"2014","3188":"2019","3189":"1985","3190":"1980","3191":"2007","3192":"2018","3193":"2017","3194":"1996","3195":"2004","3196":"2018","3197":"2017","3198":"2019","3199":"1989","3200":"2018","3201":"2010","3202":"1986","3203":"2014","3204":"2019","3205":"2020","3206":"2013","3207":"2012","3208":"2004","3209":"2000","3210":"2018","3211":"2010","3212":"2005","3213":"2004","3214":"2000","3215":"1998","3216":"2009","3217":"2009","3218":"2011","3219":"2009","3220":"1985","3221":"2005","3222":"1992","3223":"2008","3224":"2009","3225":"2011","3226":"2019","3227":"2011","3228":"1998","3229":"2013","3230":"2000","3231":"2020","3232":"2020","3233":"2020","3234":"2009","3235":"2020","3236":"2011","3237":"1999","3238":"2004","3239":"2006","3240":"2007","3241":"1960","3242":"1997","3243":"2004","3244":"2004","3245":"2009","3246":"2001","3247":"2001","3248":"2012","3249":"2011","3250":"2008","3251":"2016","3252":"2012","3253":"2008","3254":"2000","3255":"2000","3256":"2020","3257":"2009","3258":"2015","3259":"2020","3260":"2020","3261":"2008","3262":"2009","3263":"2012","3264":"2014","3265":"2001","3266":"2020","3267":"2010","3268":"2009","3269":"2009","3270":"2020","3271":"2017","3272":"2020","3273":"2000","3274":"2007","3275":"2007","3276":"2000","3277":"2013","3278":"2017","3279":"2011","3280":"2016","3281":"2016","3282":"2018","3283":"2018","3284":"2006","3285":"2011","3286":"2007","3287":"2016","3288":"2014","3289":"1990","3290":"2009","3291":"2002","3292":"2016","3293":"1988","3294":"2015","3295":"2003","3296":"2000","3297":"2016","3298":"2020","3299":"2020","3300":"2018","3301":"2010","3302":"2013","3303":"1983","3304":"2000","3305":"1989","3306":"2003","3307":"1996","3308":"1992","3309":"1997","3310":"2007","3311":"2007","3312":"2019","3313":"2019","3314":"2019","3315":"2012","3316":"2004","3317":"2015","3318":"2016","3319":"2011","3320":"2009","3321":"2015","3322":"2018","3323":"2003","3324":"2016","3325":"2013","3326":"1988","3327":"2016","3328":"2020","3329":"2020","3330":"2017","3331":"2019","3332":"2019","3333":"2014","3334":"2012","3335":"2000","3336":"2012","3337":"2007","3338":"2008","3339":"2017","3340":"2015","3341":"2008","3342":"2010","3343":"1996","3344":"2019","3345":"1992","3346":"2012","3347":"2011","3348":"2013","3349":"2002","3350":"2002","3351":"2008","3352":"2010","3353":"2018","3354":"2015","3355":"2007","3356":"2020","3357":"2009","3358":"2016","3359":"2010","3360":"2018","3361":"2013","3362":"2006","3363":"2014","3364":2023,"3365":2019,"3366":2019,"3367":2023,"3368":2023,"3369":2023,"3370":2023,"3371":2023,"3372":2017,"3373":2023,"3374":2023,"3375":2023,"3376":2023,"3377":2023,"3378":2020,"3379":2020,"3380":2022,"3381":2022,"3382":2022,"3383":2022,"3384":2022,"3385":2022,"3386":2022,"3387":2022,"3388":2020,"3389":2022,"3390":2022,"3391":2022,"3392":2022,"3393":2019,"3394":2022,"3395":2022,"3396":2019,"3397":2021,"3398":2021,"3399":2019,"3400":2021,"3401":2021,"3402":2021,"3403":2021,"3404":2021,"3405":2020,"3406":2020,"3407":2020,"3408":2021,"3409":2021,"3410":2021,"3411":2018,"3412":2018,"3413":2021,"3414":2019,"3415":2021,"3416":2021,"3417":2020,"3418":2021,"3419":2021,"3420":2021,"3421":2021,"3422":2019,"3423":2021,"3424":2019,"3425":2022,"3426":2021,"3427":2019,"3428":2020,"3429":2020,"3430":2021,"3431":2020,"3432":2020,"3433":2019,"3434":2020,"3435":2020,"3436":2021,"3437":2020,"3438":2020,"3439":2020,"3440":2020,"3441":2021,"3442":2020,"3443":2020,"3444":2020,"3445":2019,"3446":2021,"3447":2020,"3448":2020,"3449":2021,"3450":2020,"3451":2017,"3452":2020,"3453":2020,"3454":2019,"3455":2020,"3456":2021,"3457":2019,"3458":2021,"3459":2021,"3460":2021,"3461":2020,"3462":2019,"3463":2021,"3464":2020,"3465":2020,"3466":2021,"3467":2020,"3468":2020,"3469":2020,"3470":2020,"3471":2020,"3472":2019,"3473":2021,"3474":2021,"3475":2021,"3476":2021,"3477":2021,"3478":2020,"3479":2021,"3480":2020,"3481":2019,"3482":2019,"3483":2020,"3484":2021,"3485":2019,"3486":2021,"3487":2019,"3488":2021,"3489":2020,"3490":2019,"3491":2019,"3492":2020,"3493":2019,"3494":2019,"3495":2019,"3496":2019,"3497":2020,"3498":2020,"3499":2020,"3500":2020,"3501":2019,"3502":2019,"3503":2020,"3504":2019,"3505":2020,"3506":2019,"3507":2019,"3508":2019,"3509":2019,"3510":2020,"3511":2019,"3512":2020,"3513":2020,"3514":2019,"3515":2020,"3516":2020,"3517":2020,"3518":2020,"3519":2020,"3520":2020,"3521":2020,"3522":2019,"3523":2014,"3524":2020,"3525":2020,"3526":2020,"3527":2020,"3528":2020,"3529":2020,"3530":2019,"3531":2020,"3532":2020,"3533":2020,"3534":2020,"3535":2020,"3536":2020,"3537":2020,"3538":2020,"3539":2019,"3540":2019,"3541":2019,"3542":2020,"3543":2020,"3544":2020,"3545":2020,"3546":2020,"3547":2020,"3548":2019,"3549":2020,"3550":2020,"3551":2020,"3552":2020,"3553":2020,"3554":2019,"3555":2019,"3556":2020,"3557":2020,"3558":2020,"3559":2020,"3560":2020,"3561":2020,"3562":2020,"3563":2020,"3564":2019,"3565":2020,"3566":2020,"3567":2020,"3568":2019,"3569":2019,"3570":2020,"3571":2020,"3572":2020,"3573":2020,"3574":2020,"3575":2020,"3576":2020,"3577":2020,"3578":2020,"3579":2019,"3580":2020,"3581":2020,"3582":2018,"3583":2020,"3584":2020,"3585":2019,"3586":2020,"3587":2019,"3588":2019,"3589":2019,"3590":2019,"3591":2020,"3592":2016,"3593":2019,"3594":2019,"3595":2019,"3596":2019,"3597":2019,"3598":2019,"3599":2020,"3600":2020,"3601":2020,"3602":2020,"3603":2020,"3604":2020,"3605":2020,"3606":2019,"3607":2020,"3608":2020,"3609":2020,"3610":2020,"3611":2019,"3612":2019,"3613":2019,"3614":2019,"3615":2019,"3616":2020,"3617":2019,"3618":2020,"3619":2019,"3620":2019,"3621":2019,"3622":2019,"3623":2019,"3624":2019,"3625":2019,"3626":2019,"3627":2020,"3628":2019,"3629":2019,"3630":2019,"3631":2019,"3632":2020,"3633":2019,"3634":2019,"3635":2018,"3636":2019,"3637":2019,"3638":2019,"3639":2020,"3640":2019,"3641":2019,"3642":2018,"3643":2019,"3644":2019,"3645":2018,"3646":2018,"3647":2020,"3648":2020,"3649":2020,"3650":2020,"3651":2019,"3652":2019,"3653":2018,"3654":2020,"3655":2017,"3656":2020,"3657":2020,"3658":2019,"3659":2018,"3660":2019,"3661":2019,"3662":2019,"3663":2019,"3664":2019,"3665":2020,"3666":2020,"3667":2020,"3668":2019,"3669":2019,"3670":2019,"3671":2018,"3672":2019,"3673":2019,"3674":2019,"3675":2019,"3676":2019,"3677":2019,"3678":2019,"3679":2019,"3680":2019,"3681":2019,"3682":2019,"3683":2019,"3684":2019,"3685":2019,"3686":2019,"3687":2019,"3688":2019,"3689":2019,"3690":2019,"3691":2019,"3692":2019,"3693":2019,"3694":2019,"3695":2019,"3696":2019,"3697":2019,"3698":2019,"3699":2019,"3700":2019,"3701":2019,"3702":2017,"3703":2019,"3704":2019,"3705":2019,"3706":2017,"3707":2019,"3708":2019,"3709":2019,"3710":2019,"3711":2019,"3712":2019,"3713":2019,"3714":2019,"3715":2019,"3716":2017,"3717":2019,"3718":2018,"3719":2019,"3720":2019,"3721":2017,"3722":2019,"3723":2019,"3724":2019,"3725":2018,"3726":2019,"3727":2018,"3728":2019,"3729":2019,"3730":2017,"3731":2019,"3732":2018,"3733":2019,"3734":2019,"3735":2018,"3736":2018,"3737":2018,"3738":2017,"3739":2018,"3740":2019,"3741":2019,"3742":2019,"3743":2001,"3744":2019,"3745":2019,"3746":2018,"3747":2017,"3748":2018,"3749":2019,"3750":2018,"3751":2019,"3752":2019,"3753":2018,"3754":2019,"3755":2018,"3756":2019,"3757":2017,"3758":2018,"3759":2018,"3760":2018,"3761":2019,"3762":2018,"3763":2018,"3764":2018,"3765":2018,"3766":2018,"3767":2017,"3768":2018,"3769":2018,"3770":2018,"3771":2017,"3772":2018,"3773":2018,"3774":2018,"3775":2018,"3776":2018,"3777":2018,"3778":2018,"3779":2018,"3780":2017,"3781":2018,"3782":2018,"3783":2018,"3784":2018,"3785":2018,"3786":2018,"3787":2017,"3788":2018,"3789":2017,"3790":2018,"3791":2018,"3792":2018,"3793":2018,"3794":2018,"3795":2018,"3796":2018,"3797":2016,"3798":2018,"3799":2017,"3800":2018,"3801":2018,"3802":2018,"3803":2018,"3804":2018,"3805":2018,"3806":2018,"3807":2018,"3808":2018,"3809":2019,"3810":2017,"3811":2018,"3812":2017,"3813":2018,"3814":2018,"3815":2018,"3816":2018,"3817":2018,"3818":2018,"3819":2018,"3820":2018,"3821":2017,"3822":2018,"3823":2017,"3824":2017,"3825":2018,"3826":2018,"3827":2018,"3828":2017,"3829":2015,"3830":2017,"3831":2017,"3832":2017,"3833":2018,"3834":2018,"3835":2018,"3836":2018,"3837":2018,"3838":2018,"3839":2023,"3840":2020,"3841":2023,"3842":2023,"3843":2023,"3844":2022,"3845":2020,"3846":2023,"3847":2023,"3848":2021,"3849":2022,"3850":2022,"3851":2022,"3852":2022,"3853":2020,"3854":2019,"3855":2021,"3856":2021,"3857":2022,"3858":2020,"3859":2016,"3860":2020,"3861":2020,"3862":2022,"3863":2022,"3864":2019,"3865":2020,"3866":2022,"3867":2022,"3868":2017,"3869":2019,"3870":2022,"3871":2022,"3872":2021,"3873":2019,"3874":2020,"3875":2019,"3876":2022,"3877":2022,"3878":2017,"3879":2019,"3880":2022,"3881":2021,"3882":2022,"3883":2018,"3884":2022,"3885":2020,"3886":2022,"3887":2021,"3888":2019,"3889":2020,"3890":2022,"3891":2019,"3892":2019,"3893":2020,"3894":2020,"3895":2020,"3896":2020,"3897":2022,"3898":2020,"3899":2015,"3900":2020,"3901":2019,"3902":2019,"3903":2019,"3904":2020,"3905":2021,"3906":2022,"3907":2019,"3908":2018,"3909":2022,"3910":2017,"3911":2021,"3912":2021,"3913":2018,"3914":2021,"3915":2018,"3916":2019,"3917":2015,"3918":2019,"3919":2021,"3920":2018,"3921":2021,"3922":2020,"3923":2016,"3924":2021,"3925":2022,"3926":2018,"3927":2019,"3928":2021,"3929":2021,"3930":2022,"3931":2021,"3932":2017,"3933":2017,"3934":2021,"3935":2020,"3936":2021,"3937":2021,"3938":2021,"3939":2021,"3940":2021,"3941":2018,"3942":2021,"3943":2021,"3944":2021,"3945":2020,"3946":2019,"3947":2021,"3948":2018,"3949":2020,"3950":2020,"3951":2021,"3952":2021,"3953":2021,"3954":2020,"3955":2021,"3956":2021,"3957":2020,"3958":2021,"3959":2016,"3960":2021,"3961":2020,"3962":2021,"3963":2020,"3964":2020,"3965":2019,"3966":2021,"3967":2021,"3968":2021,"3969":2021,"3970":2021,"3971":2020,"3972":2020,"3973":2020,"3974":2020,"3975":2020,"3976":2018,"3977":2021,"3978":2021,"3979":2020,"3980":2019,"3981":2021,"3982":2021,"3983":2020,"3984":2020,"3985":2019,"3986":2021,"3987":2020,"3988":2019,"3989":2020,"3990":2019,"3991":2019,"3992":2021,"3993":2021,"3994":2019,"3995":2020,"3996":2021,"3997":2019,"3998":2019,"3999":2021,"4000":2021,"4001":2021,"4002":2019,"4003":2019,"4004":2020,"4005":2020,"4006":2020,"4007":2020,"4008":2019,"4009":2019,"4010":2021,"4011":2021,"4012":2020,"4013":2020,"4014":2020,"4015":2020,"4016":2020,"4017":2022,"4018":2019,"4019":2020,"4020":2020,"4021":2018,"4022":2020,"4023":2017,"4024":2020,"4025":2020,"4026":2019,"4027":2019,"4028":2020,"4029":2018,"4030":2020,"4031":2019,"4032":2019,"4033":2018,"4034":2019,"4035":2018,"4036":2020,"4037":2020,"4038":2020,"4039":2020,"4040":2021,"4041":2017,"4042":2019,"4043":2021,"4044":2021,"4045":2020,"4046":2021,"4047":2019,"4048":2018,"4049":2020,"4050":2020,"4051":2020,"4052":2020,"4053":2019,"4054":2020,"4055":2017,"4056":2003,"4057":2017,"4058":2020,"4059":2020,"4060":2019,"4061":2020,"4062":2020,"4063":2021,"4064":2018,"4065":2020,"4066":2020,"4067":2019,"4068":2019,"4069":2020,"4070":2020,"4071":2019,"4072":2020,"4073":2020,"4074":2020,"4075":2020,"4076":2020,"4077":2020,"4078":2020,"4079":2020,"4080":2019,"4081":2020,"4082":2019,"4083":2018,"4084":2019,"4085":2019,"4086":2020,"4087":2020,"4088":2020,"4089":2019,"4090":2019,"4091":2018,"4092":2020,"4093":2020,"4094":2016,"4095":2019,"4096":2019,"4097":2019,"4098":2019,"4099":2019,"4100":2019,"4101":2019,"4102":2020,"4103":2017,"4104":2017,"4105":2020,"4106":2020,"4107":2020,"4108":2018,"4109":2019,"4110":2020,"4111":2018,"4112":2020,"4113":2018,"4114":2019,"4115":2018,"4116":2018,"4117":2019,"4118":2019,"4119":2020,"4120":2019,"4121":2017,"4122":2017,"4123":2019,"4124":2020,"4125":2019,"4126":2019,"4127":2017,"4128":2019,"4129":2019,"4130":2019,"4131":2016,"4132":2019,"4133":2019,"4134":2019,"4135":2018,"4136":2016,"4137":2018,"4138":2018,"4139":2018,"4140":2019,"4141":2020,"4142":2019,"4143":2018,"4144":2018,"4145":2016,"4146":2018,"4147":2018,"4148":2019,"4149":2018,"4150":2019,"4151":2019,"4152":2019,"4153":2019,"4154":2017,"4155":2017,"4156":2019,"4157":2019,"4158":2017,"4159":2018,"4160":2019,"4161":2016,"4162":2018,"4163":2018,"4164":2007,"4165":2019,"4166":2019,"4167":2018,"4168":2018,"4169":2018,"4170":2017,"4171":2017,"4172":2017,"4173":2017,"4174":2019,"4175":2017,"4176":2017,"4177":2018,"4178":2017,"4179":2018,"4180":2018,"4181":2016,"4182":2016,"4183":2017,"4184":2015,"4185":2017,"4186":2017,"4187":2015,"4188":2015,"4189":2000,"4190":2014,"4191":2015,"4192":2013,"4193":2014,"4194":2013,"4195":2012,"4196":2013,"4197":2013,"4198":2013,"4199":2012,"4200":2013,"4201":2013,"4202":2012,"4203":2012,"4204":2012,"4205":2012,"4206":2013,"4207":2023,"4208":2023,"4209":2010,"4210":2023,"4211":2011,"4212":2008,"4213":2012,"4214":2005,"4215":2023,"4216":2023,"4217":2011,"4218":2023,"4219":2011,"4220":2023,"4221":2021,"4222":2023,"4223":2023,"4224":2000,"4225":2023,"4226":2023,"4227":2007,"4228":2012,"4229":2001,"4230":2012,"4231":2010,"4232":2011,"4233":2007,"4234":2019,"4235":2009,"4236":2016,"4237":2017,"4238":2004,"4239":2018,"4240":2016,"4241":2010,"4242":2008,"4243":2004,"4244":2022,"4245":2001,"4246":2018,"4247":2010,"4248":2019,"4249":2018,"4250":2010,"4251":2019,"4252":2008,"4253":2020,"4254":2015,"4255":2019,"4256":2000,"4257":2017,"4258":2016,"4259":2020,"4260":2022,"4261":2019,"4262":2022,"4263":2022,"4264":2021,"4265":2022,"4266":2022,"4267":2006,"4268":2021,"4269":2008,"4270":2002,"4271":2009,"4272":2022,"4273":2022,"4274":2023,"4275":2023,"4276":2023,"4277":2023,"4278":2023,"4279":2023,"4280":2023,"4281":2023,"4282":2023,"4283":2023,"4284":2023,"4285":2023,"4286":2023,"4287":2023,"4288":2023,"4289":2023,"4290":2023,"4291":2023,"4292":2023,"4293":2023,"4294":2023,"4295":2023,"4296":2023,"4297":2023,"4298":2023,"4299":2023,"4300":2023,"4301":2023,"4302":2023,"4303":2023,"4304":2022,"4305":2022,"4306":2022,"4307":2021,"4308":2022,"4309":2021,"4310":2023,"4311":2023,"4312":2021,"4313":2023,"4314":2023,"4315":2023,"4316":2023,"4317":2023,"4318":2022,"4319":2022,"4320":2023,"4321":2022,"4322":2023,"4323":2023,"4324":2021,"4325":2022,"4326":2023,"4327":2023,"4328":2023,"4329":2023,"4330":2023,"4331":2023,"4332":2023,"4333":2023,"4334":2023,"4335":2023,"4336":2023,"4337":2023,"4338":2023,"4339":2023,"4340":2023,"4341":2023,"4342":2023,"4343":2023,"4344":2023,"4345":2023,"4346":2023,"4347":2023,"4348":2023,"4349":2023,"4350":2023,"4351":2022,"4352":2023,"4353":2023,"4354":2023,"4355":2023,"4356":2023,"4357":2023,"4358":2022,"4359":2023,"4360":2023,"4361":2023,"4362":2023,"4363":2023,"4364":2022,"4365":2023,"4366":2023,"4367":2023,"4368":2023,"4369":2023,"4370":2023,"4371":2023,"4372":2023,"4373":2023,"4374":2023,"4375":2023,"4376":2023,"4377":2023,"4378":2023,"4379":2021,"4380":2023,"4381":2022,"4382":2022,"4383":2020,"4384":2023,"4385":2022,"4386":2021,"4387":2022,"4388":2022,"4389":2022,"4390":2022,"4391":2022,"4392":2022,"4393":2022,"4394":2022,"4395":2022,"4396":2022,"4397":2022,"4398":2022,"4399":2022,"4400":2021,"4401":2022,"4402":2022,"4403":2022,"4404":2022,"4405":2023,"4406":2022,"4407":2022,"4408":2022,"4409":2022,"4410":2022,"4411":2022,"4412":2022,"4413":2022,"4414":2020,"4415":2022,"4416":2022,"4417":2021,"4418":2021,"4419":2023,"4420":2022,"4421":2020,"4422":2022,"4423":2020,"4424":2022,"4425":2021,"4426":2021,"4427":2021,"4428":2021,"4429":2020,"4430":2021,"4431":2020,"4432":2021,"4433":2020,"4434":2021,"4435":2018,"4436":2019,"4437":2019,"4438":2023,"4439":2021,"4440":2019,"4441":2019,"4442":2020,"4443":2020,"4444":2023,"4445":2019,"4446":2020,"4447":2020,"4448":2015,"4449":2019,"4450":2023,"4451":2019,"4452":2022,"4453":2022,"4454":2018,"4455":2018,"4456":2021,"4457":2022,"4458":2022,"4459":2018,"4460":2023,"4461":2016,"4462":2022,"4463":2023,"4464":2015,"4465":2022,"4466":2021,"4467":2023,"4468":2019,"4469":2023,"4470":2023,"4471":2022,"4472":2019,"4473":2021,"4474":2019,"4475":2019,"4476":2023,"4477":2020,"4478":2023,"4479":2023,"4480":2021,"4481":2021,"4482":2023,"4483":2022,"4484":2022,"4485":2021,"4486":2019,"4487":2007,"4488":2021,"4489":2021,"4490":2020,"4491":2023,"4492":2022,"4493":2022,"4494":2019,"4495":2020,"4496":2019,"4497":2020,"4498":2022,"4499":2022,"4500":2022,"4501":2013,"4502":2020,"4503":2023,"4504":2019,"4505":2019,"4506":2020,"4507":2022,"4508":2021,"4509":2023,"4510":2021,"4511":2022,"4512":2019,"4513":2022,"4514":2022,"4515":2022,"4516":2022,"4517":2022,"4518":2020,"4519":2018,"4520":2015,"4521":2023,"4522":2022,"4523":2020,"4524":2021,"4525":2016,"4526":2021,"4527":2021,"4528":2017,"4529":2021,"4530":2021,"4531":2022,"4532":2021,"4533":2019,"4534":2019,"4535":2022,"4536":2022,"4537":2022,"4538":2017,"4539":2021,"4540":2020,"4541":2021,"4542":2021,"4543":2021,"4544":2019,"4545":2016,"4546":2016,"4547":2019,"4548":2010,"4549":2015,"4550":2020,"4551":2020,"4552":2016,"4553":2019,"4554":2019,"4555":2019,"4556":2019,"4557":2019,"4558":2019,"4559":2021,"4560":2018,"4561":2021,"4562":2020,"4563":2021,"4564":2016,"4565":2016,"4566":2014,"4567":2021,"4568":2021,"4569":2020,"4570":2020,"4571":2019,"4572":2019,"4573":2020,"4574":2019,"4575":2016,"4576":2021,"4577":2011,"4578":2021,"4579":2021,"4580":2021,"4581":2019,"4582":2021,"4583":2021,"4584":2021,"4585":2014,"4586":2020,"4587":2021,"4588":2018,"4589":2013,"4590":2020,"4591":2015,"4592":2020,"4593":2020,"4594":2019,"4595":2020,"4596":2020,"4597":2019,"4598":2008,"4599":2015,"4600":2018,"4601":2018,"4602":2005,"4603":2009,"4604":2020,"4605":2009,"4606":2014,"4607":2019,"4608":2013,"4609":2016,"4610":2015,"4611":2020,"4612":2020,"4613":2001,"4614":2019,"4615":2014,"4616":2019,"4617":2003,"4618":2004,"4619":2005,"4620":2015,"4621":2011,"4622":2020,"4623":2019,"4624":2018,"4625":2016,"4626":2003,"4627":2014,"4628":2019,"4629":2018,"4630":2019,"4631":2017,"4632":2023,"4633":2012,"4634":2014,"4635":2011,"4636":2012,"4637":2014,"4638":2016,"4639":2018,"4640":2022,"4641":2022,"4642":2014,"4643":2023,"4644":2021,"4645":2022,"4646":2015,"4647":2023,"4648":2014,"4649":2023,"4650":2021,"4651":2022,"4652":2023,"4653":2023,"4654":2023,"4655":2011,"4656":2023,"4657":2018,"4658":2016,"4659":2023,"4660":2019,"4661":2021,"4662":2023,"4663":2016,"4664":2016,"4665":2013,"4666":2007,"4667":2019,"4668":2019,"4669":2021,"4670":2008,"4671":2022,"4672":2008,"4673":2019,"4674":2021,"4675":2010,"4676":2019,"4677":2021,"4678":2022,"4679":2022,"4680":2022,"4681":2016,"4682":2016,"4683":2021,"4684":2022,"4685":2022,"4686":2020,"4687":2020,"4688":2021,"4689":2019,"4690":2003,"4691":2016,"4692":2015,"4693":2015,"4694":2015,"4695":2016,"4696":2017,"4697":2020,"4698":2020,"4699":2020,"4700":2022,"4701":2020,"4702":2022,"4703":2020,"4704":2003,"4705":2020,"4706":2016,"4707":2019,"4708":2021,"4709":2017,"4710":2018,"4711":2010,"4712":2017,"4713":2009,"4714":2016,"4715":2019,"4716":2013,"4717":2003,"4718":2021,"4719":2016,"4720":2021,"4721":2021,"4722":2008,"4723":2021,"4724":2021,"4725":2021,"4726":2013,"4727":2019,"4728":2008,"4729":2002,"4730":2012,"4731":2010,"4732":2002,"4733":2005,"4734":2019,"4735":2021,"4736":2020,"4737":2016,"4738":2007,"4739":2019,"4740":2008,"4741":2019},"authors":{"0":["Matthew N. O. Sadiku","Sarhan M. Musa","Abayomi Ajayi-Majebi"],"1":["Seeta M. Chauhan"],"2":["P. Naveen","S. Nikitha","P. Sudeesh","V. Vaishnavi"],"3":["Krishanu Deb","Pankaj Agrawal","Harish Nawale","Shradha Jadhav","Prof. Manisha Darak"],"4":["Dr. Atul Kumar Mishra"],"5":["Tim Miller"],"6":["Georgios I. Doukidis","Marios C. Angelides"],"7":["Manish Verma"],"8":["Avani Goenka"],"9":["Deepak Kumar"],"10":["Chinmaya Naik","Sonali Jain","Jai Sehgal"],"11":["Shaikh Sameer Salim","Manoj Kumar"],"12":["Matthew N. O. Sadiku","Sarhan M. Musa","Abayomi Ajayi-Majebi"],"13":["Matthew N. O. Sadiku","Uwakwe C. Chukwu","Abayomi Ajayi-Majebi","Sarhan M. Musa"],"14":["Durgesh Raghuvanshi"],"15":["Shivani Gupta","Gargi Kalia","Preeti Sondhi"],"16":["Mrs. Pushpalata S. Patil"],"17":["Marta Garnelo","Murray Shanahan"],"18":["Rishikesh R"],"19":["Matthew N. O. Sadiku","Sarhan M. Musa","Abayomi Ajayi-Majebi"],"20":["Hassan Khosravi","Simon Buckingham Shum","Guanliang Chen","Cristina Conati","Dragan Gasevic","Judy Kay","Simon Knight","Roberto Martinez-Maldonado","Shazia Sadiq","Yi-Shan Tsai"],"21":["Priyankar Roy","Lalith Vivekanand","Gurman Preet Singh"],"22":["A\u00edda Ponce Del Castillo"],"23":["Ben Eubanks"],"24":["Mrs. Jyoti M Bohra","Ms. Bhagyashri G Joshi"],"25":["Kipp W. Johnson","Jessica Torres Soto","Benjamin S. Glicksberg","Khader Shameer","Riccardo Miotto","Mohsin Ali","Euan Ashley","Joel T. Dudley"],"26":["Prof. Mohammed Nawaz","Prof. Triveni. K","Prof. Bharathi. G. R"],"27":["Prof. Lakshmi Narayan. N","Naveena. N"],"28":["Mr. A. Kishorekumar Mr. E. Ezhilarasan Mr. R. Parthiban"],"29":["Matthew N. O. Sadiku","Uwakwe C. Chukwu","Abayomi Ajayi-Majebi","Sarhan M. Musa"],"30":["Prachi Shah Praveen Gautam"],"31":["M. Juno Isabel Susinthra S. Vinitha"],"32":["Nour Sadeq","Ghalia Nassreddine","Joumana Younis"],"33":["Demis Hassabis","Dharshan Kumaran","Christopher Summerfield","Matthew Botvinick"],"34":["Gary Marcus"],"35":["Krishna Ramanujam","Nethra Mohanram"],"36":["Suman Roy","Sujit Kumar Paul"],"37":["Anshika Gupta"],"38":["Changjun Wu","Fuyao Xiang"],"39":["Chidinma-Mary-Agbai"],"40":["Shrivallabh Walkade","Rajesh Sonnakula","Deepak Kumar","Shreyas Nambiar"],"41":["Adrienne Williams","Milagros Miceli","Timnit Gebru"],"42":["Marina Sokolova","Canadian Conference on Artificial Intelligence"],"43":["Dr. Amol Murgai"],"44":["Julia E. Reid","Eric Eaton"],"45":["Bhumika T J","Harshitha U","Meghana S","Dr. Ganashree T S"],"46":["Carl Hewitt","Peter Bishop","Richard Steiger"],"47":["Prof. Vijay Aithekar","Mr. Hitesh"],"48":["A\u00edda Ponce Del Castillo"],"49":["Obumneme Onyeka Okwonna","Amalate Ann Jonathan Obuebite"],"50":["Tirumala Durvasula","Pooja Rathod"],"51":["Mr. Ketan Ashok Bagade","Yogini Bagade"],"52":["Stuart J. Russell","Peter Norvig","Sturat J. Russell"],"53":["Kajal Gohane","Roshini S","Komal Pode"],"54":["Herbert Alexander Simon"],"55":["Michael Beetz","Martin Buss","Dirk Wollherr"],"56":["Mussaratjahan Korpali","Akshata Walikar","Kaveri Parshuram Vijapur"],"57":["Deme C. Abraham"],"58":["Aditya Sonar","Vinita Galande"],"59":["S\u00e9bastien Bubeck","Varun Chandrasekaran","Ronen Eldan","Johannes Gehrke","Eric Horvitz","Ece Kamar","Peter Lee","Yin Tat Lee","Yuanzhi Li","Scott Lundberg","Harsha Nori","Hamid Palangi","Marco Tulio Ribeiro","Yi Zhang"],"60":["Prof. Lakshminarayana. N","Ms. Deepthi B. R."],"61":["Ali Al Imari","Saeed Abdallah"],"62":["Kate Crawford"],"63":["Z Ghahramani"],"64":["Stan Openshaw","Christine Openshaw"],"65":["Erica Pascual-Garcia Guillermo De la Torre-Gea"],"66":["Vishal Dineshkumar Soni"],"67":["Jieqiong Zheng","Yunfang Chen","Wei Zhang"],"68":["Fan Ouyang","Pengcheng Jiao"],"69":["Rick Briggs"],"70":["Patrick Henry Winston"],"71":["Chih-Pu Dai","Fengfeng Ke"],"72":["Robert J. Schalkoff"],"73":["Kaveriappa. T"],"74":["Prof. Rekha D. M","Sandhya G N"],"75":["Niraj C. Chaudhari"],"76":["Dr. Pawan Whig"],"77":["Paul Friedrich Nemitz"],"78":["Olufunmilola Adunni Ogunyolu","Adewale O Adebayo"],"79":["Patrick Br\u00e9zillon"],"80":["Stuart J. Russell","Peter Norvig"],"81":["Jeroen Ooge","Katrien Verbert"],"82":["Garima Bhardwaj","Neelam Bhardwaj"],"83":["Judea Pearl"],"84":["Gary Marcus"],"85":["Patrick Br\u00e9zillon"],"86":["Adri\u00e1n Hern\u00e1ndez Gonz\u00e1lez","Celia Medina Lloret","Diana D\u00edaz Raboso"],"87":["Daniel Cebo"],"88":["European Commission"],"89":["Patrick Henry Winston"],"90":["Wolfgang Banzhaf"],"91":["Frank Puppe"],"92":["Ruonan Liu","Boyuan Yang","Enrico Zio","Xuefeng Chen"],"93":["N. J. Nilsson"],"94":["Arshid Mehraj","Harjit Singh","Onkar Singh"],"95":["Australasian Joint Conference on Artificial Intelligence","Dianhui Wang","Mark Reynolds"],"96":["Simon Tjoa","Christina Buttinger","Katharina Holzinger","Peter Kieseberg"],"97":["Neha Bhujbal","Prof. Shrikant Nagure"],"98":["Nicholas R. Jennings"],"99":["David Poole","Alan Mackworth"],"100":["D. G. Bobrow"],"101":["H Atabakhsh"],"102":["Sheriff Alim","Adewale O Adebayo"],"103":["Kourda Hayat","Pr. Trebucq St\u00c3\u00a9phane"],"104":["Lucas von Chamier","Romain F. Laine","Ricardo Henriques"],"105":["Benedict du Boulay"],"106":["Amruth N. Kumar"],"107":["Marcus Hutter"],"108":["Mrinalini Kochupillai","Matthias Kahl","Michael Schmitt","Hannes Taubenb\u00f6ck","Xiao Xiang Zhu"],"109":["Miles Brundage","Shahar Avin"],"110":["Joel Nothman","Nicky Ringland","Will Radford","Tara Murphy","James R. Curran"],"111":["Md Kamruzzaman Sarker","Lu Zhou","Aaron Eberhart","Pascal Hitzler"],"112":["Francisco Villaescusa-Navarro","Daniel Angl\u00e9s-Alc\u00e1zar","Shy Genel","David N. Spergel","Yin Li","Benjamin Wandelt","Andrina Nicola","Leander Thiele","Sultan Hassan","Jose Manuel Zorrilla Matilla","Desika Narayanan","Romeel Dave","Mark Vogelsberger"],"113":["H. A. Simon"],"114":["D. Barstow"],"115":["D. Marr"],"116":["H.A. Simon"],"117":["Drew McDermott"],"118":["Eugene Charniak","Christopher K. Riesbeck","Drew V. McDermott","James R. Meehan"],"119":["Stuart J. Russell","Peter Norvig"],"120":["Han Reichgelt","Llu\u00eds Vila"],"121":["Brian Thomas","Harley Thronson","Andrew Adrian","Alison Lowndes","James Mason","Nargess Memarsadeghi","Shahin Samadi","Giulio Varsi"],"122":["Alon Y. Levy"],"123":["Antonio Kr\u00fcger","Rainer Malaka"],"124":["Paola Tubaro","Antonio A Casilli","Marion Coville"],"125":["Nils J. Nilsson"],"126":["Celia Medina Lloret","The Bible of AI \u2122"],"127":["Selmer Bringsjord","David Ferrucci"],"128":["Catholijn M Jonker","Jacky L Snoep","Jan Treur","Hans V Westerhoff","Wouter C A Wijngaards"],"129":["Tim Wu"],"130":["Zachari Swiecki","Hassan Khosravi","Guanliang Chen","Roberto Martinez-Maldonado","Jason M. Lodge","Sandra Milligan","Neil Selwyn","Dragan Gasevi\u0107"],"131":["Alejandro Barredo Arrieta","Natalia D\u00edaz-Rodr\u00edguez","Javier Del Ser","Adrien Bennetot","Siham Tabik","Alberto Barbado","Salvador Garc\u00eda","Sergio Gil-L\u00f3pez","Daniel Molina","Richard Benjamins","Raja Chatila","Francisco Herrera"],"132":["Pascal Hitzler","Aaron Eberhart","Monireh Ebrahimi","Md Kamruzzaman Sarker","Lu Zhou"],"133":["Zhurakulova Gulkhayo Rizo kizi"],"134":["O\u2019rinov Nodirbek Toxirjonovich","Yunusov Odiljon Fozilovich"],"135":["Jerry Kaplan"],"136":["Jake Bryant","Christine Heitz","Saurabh Sanghvi","Dilip Wagle"],"137":["Michael Mateas"],"138":["John Pendlebury","Mark Humphrys","Ray Walshe"],"139":["Jason M Lodge","Sarah Howard","Margaret Bearman","Phillip Dawson","Shirley Agostinho","Simon Buckingham Shum","Chris Deneen","Cath Ellis","Tim Fawns","Helen Gniel","others"],"140":["Michael A. McRobbie","J\u00f6rg H. Siekmann"],"141":["Soundar R. T. Kumara","Setsuo Ohsuga","Inyong Ham"],"142":["Daniel Kayser"],"143":["Ivan Bratko"],"144":["Huiying Liang","Brian Y. Tsui","Hao Ni","Carolina C. S. Valentim","Sally L. Baxter","Guangjian Liu","Wenjia Cai","Daniel S. Kermany","Xin Sun","Jiancong Chen","Liya He","Jie Zhu","Pin Tian","Hua Shao","Lianghong Zheng","Rui Hou","Sierra Hewett","Gen Li","Ping Liang","Xuan Zang","Zhiqi Zhang","Liyan Pan","Huimin Cai","Rujuan Ling","Shuhua Li","Yongwang Cui","Shusheng Tang","Hong Ye","Xiaoyan Huang","Waner He","Wenqing Liang","Qing Zhang","Jianmin Jiang","Wei Yu","Jianqun Gao","Wanxing Ou","Yingmin Deng","Qiaozhen Hou","Bei Wang","Cuichan Yao","Yan Liang","Shu Zhang","Yaou Duan","Runze Zhang","Sarah Gibson","Charlotte L. Zhang","Oulan Li","Edward D. Zhang","Gabriel Karin","Nathan Nguyen","Xiaokang Wu","Cindy Wen","Jie Xu","Wenqin Xu","Bochu Wang","Winston Wang","Jing Li","Bianca Pizzato","Caroline Bao","Daoman Xiang","Wanting He","Suiqin He","Yugui Zhou","Weldon Haw","Michael Goldbaum","Adriana Tremoulet","Chun-Nan Hsu","Hannah Carter","Long Zhu","Kang Zhang","Huimin Xia"],"145":["Alejandro Barredo Arrieta","Natalia D\u00edaz-Rodr\u00edguez","Javier Del Ser","Adrien Bennetot","Siham Tabik","Alberto Barbado","Salvador Garcia","Sergio Gil-Lopez","Daniel Molina","Richard Benjamins","Raja Chatila","Francisco Herrera"],"146":["Mohammad Ziaaddini","Aref Tahmasb"],"147":["Thomas K.F. Chiu","Qi Xia","Xinyan Zhou","Ching Sing Chai","Miaoting Cheng"],"148":["Francis Hunger"],"149":["Virgil Griffith"],"150":["Abid Sarwar"],"151":["Ephraim Nissan"],"152":["Fran\u00e7ois Gagnon","Babak Esfandiari"],"153":["Toby Walsh"],"154":["P. van den Besselaar","L. Leydesdorff"],"155":["Tibor V\u00e1mos"],"156":["George Hurlburt"],"157":["Michael Luck","Ruth Aylett"],"158":["Kuo-Chen Chou"],"159":["P. Larra\ufffdaga","S. Moral"],"160":["Alastair D. Channon"],"161":["Anirban Chakraborty"],"162":["Lijia Chen","Pingping Chen","Zhijian Lin"],"163":["Jie You"],"164":["Peter Norvig"],"165":["Anirban Chakraborty"],"166":["Eric Bonabeau","Marco Dorigo","Guy Theraulaz"],"167":["Michael van Lent","William Fisher","Michael Mancuso"],"168":["Ben Shneiderman"],"169":["Thomas Dean","James Allen","Yiannis Aloimonos"],"170":["Mehmood Ali Mohammed"],"171":["Jessica Lindblom","Tom Ziemke"],"172":["Dan McQuillan"],"173":["Decebal Constantin Mocanu"],"174":["Bernhard Puppe","Frank Puppe"],"175":["L. Birnbaum"],"176":["Roger C. Schank","Peter G. Childers"],"177":["Marleen Balvert","Alexander Sch\u00f6nhuth"],"178":["Ahmed Muayad Younus","Muslim Najeeb Zaidan","Duaa shakir Mahmood"],"179":["Norman R. Nielsen"],"180":["Fabio Abbattista","Luciana Bordoni","Giovanni Semeraro"],"181":["Roger C. Schank","Stephen Slade"],"182":["Anirban Chakraborty"],"183":["Pavlos S. Georgilakis"],"184":["Eva Ritz","Donisi Fabio","Edona Elshan","Roman Rietsche"],"185":["A. Adadi","M. Berrada"],"186":["Eliezer Yudkowsky"],"187":["Roman V. Yampolskiy","M. S. Spellchecker"],"188":["Laveen N. Kanal"],"189":["Nils J. Nilsson"],"190":["N. V. Chawla","K. W. Bowyer","L. O. Hall","W. P. Kegelmeyer"],"191":["Ben Goertzel","Cassio Pennachin"],"192":["Ephraim Nissan","Antonio A. Martino"],"193":["Daniele Nardi","Marco Tucci"],"194":["Julian Hilton"],"195":["Yufei Liu","Yuan Zhou","Xin Liu","Fang Dong","Chang Wang","Zihong Wang"],"196":["Eduard Hovy","Roberto Navigli","Simone Paolo Ponzetto"],"197":["H. V. D. Parunak"],"198":["Chen Jiaqi","Zhen Yunuo","Guo Simeng"],"199":["Ephraim Nissan","Giuseppina C. Gini","Marco Colombetti"],"200":["J. McCarthy"],"201":["Manuel Le Gallo","Abu Sebastian","Evangelos Eleftheriou"],"202":["Ingeborg Steinacker","Robert Trappl","Werner Horn"],"203":["Bernard Moulin","Brahim Chaib-Draa"],"204":["Mark Purdy","Paul Daugherty"],"205":["Eduardo S\u00e1nchez","Manuel Lama"],"206":["Nils J. Nilsson"],"207":["Margaret A. Boden"],"208":["Julia Schneider","Lena Kadriye Ziyal"],"209":["Thomas G. Dietterich"],"210":["Ernesto Klengel","Johanna Wenckebach"],"211":["Luc De Raedt","Kristian Kersting","Sriraam Natarajan","David Poole"],"212":["Olivier Colliot"],"213":["Paul Nemitz"],"214":["Robert Trappl"],"215":["Pedro M. Domingos","Daniel Lowd"],"216":["Tony Veale"],"217":["P Br\u00e9zillon"],"218":["Wayne Holmes","Jen Persson","Irene-Angelica Chounta","Barbara Wasson","Vania Dimitrova"],"219":["Daron Acemoglu","Pascual Restrepo"],"220":["Arjun Reddy Kunduru"],"221":["Vladimir G. Red'ko"],"222":["Raymond Turner"],"223":["Saitakhmadov Maksud Boymamatovich"],"224":["Rolf Pfeifer","Fumiya Iida"],"225":["Les Gasser","Michael N. Huhns"],"226":["Cassio Pennachin","Ben Goertzel"],"227":["DIN","DKE","Ernestine Dickhaut","Mahei Manhai Li","Christoph Peters","many other authors see chapter 11"],"228":["Thomas Mandl"],"229":["Valerio De Stefano"],"230":["Kin H. Yu","Yu Hen Hu"],"231":["Jon Sticklen","Ahmed Kamel","Martin C. Hawley","John Delong"],"232":["Jyoti Shikha","Dr. Sanjeev Singh"],"233":["Nils J. Nilsson"],"234":["Katharina Kaiser","Cem Akkaya","Silvia Miksch"],"235":["Kristinn R. Th\u00f3risson","Christopher Pennock","Thor List","John DiPirro"],"236":["Miran Brezocnik","Joze Balic","Leo Gusel"],"237":["Ephraim Nissan","Antonio A. Martino"],"238":["Andrew M. Cox","Suvodeep Mazumdar"],"239":["Tovbaev Sirojiddin","Karshiboev Nizomiddin"],"240":["Jean-Pierre Courtial","John Law"],"241":["Fernando Elizalde-Ram\u00edrez","Romeo Sanchez Nigenda","Iris Abril Mart\u00ednez-Salazar","Yasm\u00edn \u00c1. R\u00edos-Sol\u00eds"],"242":["Kevin M. Passino","Panos J. Antsaklis"],"243":["Pablo Villanueva-Domingo","Francisco Villaescusa-Navarro","Shy Genel","Daniel Angl\u00e9s-Alc\u00e1zar","Lars Hernquist","Federico Marinacci","David N. Spergel","Mark Vogelsberger","Desika Narayanan"],"244":["Hubert DREYFUS"],"245":["Marco Gori","Oliviero Stock"],"246":["Sh. A. Zakirxodjaeva",""],"247":["American Association for Artificial Intelligence"],"248":["Peter Voss"],"249":["A. Sfetsos","A.H. Coonick"],"250":["Luigia Carlucci Aiello","Enrico Giunchiglia","Luciano Serafini"],"251":["Daniel Jurafsky","James H. Martin"],"252":["Nils J. Nilsson"],"253":["Sunami Dasgupta","Soham Das","Sayani Hazra Pal"],"254":["Aydede","Guzeldere"],"255":["Abid Sarwar"],"256":["Antonio Chella","Luca Iocchi","Irene Macaluso","Daniele Nardi"],"257":["Aude Cefaliello","Miriam Kullmann"],"258":["Ruslan Bernijazov","Leon \u00f6zcan","Roman Dumitrescu"],"259":["Helen Shao","Francisco Villaescusa-Navarro","Shy Genel","David N. Spergel","Daniel Angles-Alcazar","Lars Hernquist","Romeel Dave","Desika Narayanan","Gabriella Contardo","Mark Vogelsberger"],"260":["Olaf Sporns","Teresa K. Pegors"],"261":["John R. Koza"],"262":["O. Sporns","T. Pegors"],"263":["Andreas Theodoros Lianos"],"264":["Sirojiddin Tavboev","Tavboev Islom"],"265":["Kate Crawford"],"266":["Casey Bennett","Tom Doub","Rebecca Selove"],"267":["John Seely Brown","Richard R. Burton"],"268":["John Kelly"],"269":["Matthew Zeidenberg"],"270":["M. Malitza"],"271":["Sebastian Raschka","Joshua Patterson","Corey Nolet"],"272":["Robert Trappl"],"273":["Inger Lytje","Ann Bygholm"],"274":["Marvin Minsky","Seymour Papert"],"275":["B. Russell","k. Norvig"],"276":["David C. Marr"],"277":["Tojimatov Dostonbek","Mirzaev Jamshid"],"278":["J. McCarthy"],"279":["Dunja Mladeni\u0107","Marko Grobelnik"],"280":["John Atkinson","Mauricio Solar"],"281":["Anton Bogdanovych","Juan A. Rodr\u00edguez-Aguilar","Simeon J. Simoff","Alex Cohen"],"282":["Max Tegmark"],"283":["Martin Cunneen","Martin Mullins","Finbarr Murphy"],"284":["Sindre Berg Stene"],"285":["Peter Werner Eklund"],"286":["Umirzoqova Sevara"],"287":["Vishakha Singh Sukit Kitichalermkiat"],"288":["Wikipedia"],"289":["David Chapman"],"290":["Mike Sharples","David Hogg","Chris Hutchinson","Steve Torrance","David Young"],"291":["Alon Y. Levy"],"292":["Michael J. Shaw","Andrew B. Whinston"],"293":["Alan Bundy"],"294":["Peng Dai","Mausam","Daniel S. Weld"],"295":["Owen Holland"],"296":["Ben Goertzel"],"297":["K. Wolff"],"298":["Ali Aytek","M Asce","Murat Alp"],"299":["Pei Wang","Ben Goertzel"],"300":["Matt Smith"],"301":["Lina Markauskaite","Rebecca Marrone","Oleksandra Poquet","Simon Knight","Roberto Martinez-Maldonado","Sarah Howard","Jo Tondeur","Maarten De Laat","Simon Buckingham Shum","Dragan Ga\u0161evi\u0107","George Siemens"],"302":["Enn Tyugu"],"303":["Patrick Doherty"],"304":["Drew McDermott"],"305":["Stuart Russell","Peter Norvig"],"306":["Keith L. Downing"],"307":["Seth A. Herd","Geoffrey Urland","Brian Mingus","Randall C. O'Reilly"],"308":["Nikos Vlassis"],"309":["Jozef Kelemen","Peter Mikuleck\u00fd"],"310":["Paulo Eduardo Ambr\u00f3sio"],"311":["Stuart J. Russell","Peter Norvig"],"312":["Stephen R. GRAUBARD"],"313":["Roger C. Schank","Stephen Slade"],"314":["German Research Center for Artificial Intelligence"],"315":["Gregorio Iglesias Rodriguez","Alberte Castro Ponte","Rodrigo Carballo Sanchez","Miguel Losada Rodriguez"],"316":["Stephen Russell","Ira S. Moskowitz","Adrienne Raglin"],"317":["Karthi Selvarajah","Debbie Richards"],"318":["Bart Verheij"],"319":["Gerhard Weiss"],"320":["Brian Lees"],"321":["Anirban Chakraborty"],"322":["Wikipedia"],"323":["Sahila Fareed","Kirti Bhatia","Shalini Bhadola"],"324":["A. Sfetsos","A. H. Coonick"],"325":["Toby Walsh"],"326":["Daniel Erenrich"],"327":["Kristinn R. Th\u00f3risson","Eric Nivel","Ricardo Sanz","Pei Wang"],"328":["S. Russell","P. Norvig"],"329":["Wolfgang Hoffmann-Riem"],"330":["Lukasz Kaiser"],"331":["Annika Kettenburg"],"332":["Patrick Butlin","Robert Long","Eric Elmoznino","Yoshua Bengio","Jonathan Birch","Axel Constant","George Deane","Stephen M. Fleming","Chris Frith","Xu Ji","Ryota Kanai","Colin Klein","Grace Lindsay","Matthias Michel","Liad Mudrik","Megan A. K. Peters","Eric Schwitzgebel","Jonathan Simon","Rufin VanRullen"],"333":["Souvik Banerjee","Dr. A Rengarajan"],"334":["Berardina De Carolis","Cristina Gena"],"335":["Juan de Dalmau"],"336":["Wikipedia"],"337":["Steven Minton","Michael P. Wellman"],"338":["Wikipedia"],"339":["H. Chad Lane","Mark G. Core","Michael van Lent","Steve Solomon","Dave Gomboc"],"340":["Murray Shanahan","Richard Southwick"],"341":["Michael Genesereth","Nils Nilsson"],"342":["A. Mellit","S. A. Kalogirou","L. Hontoria","S. Shaari"],"343":["Carlos M\u00e9rida"],"344":["Richard Loosemore"],"345":["Yuliyan Maksimov","Samuel Fricker","Kurt Tutschku"],"346":["Stan Franklin"],"347":["Stuart J. Russell","Peter Norvig"],"348":["Prathit A. Kulkarni","Hardeep Singh"],"349":["Masoud Yazdani"],"350":["Bernd Waas"],"351":["Brandon Rohrer"],"352":["Howard Manning Williams","Peter W. McOwan"],"353":["Jurij Matija Kalisnik","Andr\u00e9 Bauer","Ferdinand Aurel Vogt","Franziska Josephine Stickl","Janez Zibert","Matthias Fittkau","Thomas Bertsch","Samuel Kounev","Theodor Fischlein"],"354":["Thomas Lukasiewicz"],"355":["Alessio Bottrighi","Laura Giordano","Gianpaolo Molino","Stefania Montani","Paolo Terenziani","Mauro Torchio"],"356":["Ivan M. Havel"],"357":["J. Ferber"],"358":["Kamal Karna"],"359":["Pedro Domingos","Daniel Lowd"],"360":["Ashish Jobson","Dr. Kamlraj R"],"361":["Tapan Golakiya"],"362":["Ranan B. Banerji"],"363":["Maria Santofimia","Francisco Moya","Felix Villanueva","David Villa","Juan Lopez"],"364":["Ranjith Dr. E. J. Thomson Fredrik","P. C. Rithika"],"365":["Lawrence Hunter"],"366":["Attilio Pedrazzoli","Luisa Dall'Acqua"],"367":["Tom M. van Engers","Dennis M. de Vries"],"368":["Peter Nordin"],"369":["Marie Duz\u00ed","Daniela Dur\u00e1kov\u00e1","Pavel Dergel","Petr Gajdos","Jaroslav M\u00fcller"],"370":["Sunilkumar Malge","Pallavi Singh"],"371":["Alessandro Oltramari"],"372":["Paula Boddington"],"373":["John H. Holland"],"374":["David Milne","Ian H. Witten"],"375":["Ben Hachey","Will Radford","Joel Nothman","Matthew Honnibal","James R. Curran"],"376":["Manish Verma"],"377":["Yong K. Hwang","Samuel B. Hwang","David B. Hwang"],"378":["Zoubin Ghahramani"],"379":["Nicholas L. Cassimatis"],"380":["Sylvie Thiebaux","Jorg Hoffmann","Bernhard Nebel"],"381":["William F. Lawless","Donald A. Sofge"],"382":["?"],"383":["Lin Liao","Donald J. Patterson","Dieter Fox","Henry Kautz"],"384":["Jim Prentzas"],"385":["John H. Holland"],"386":["Madamidola O. A Daramola O.A Akintola K .G"],"387":["Jordi Sabater","Carles Sierra"],"388":["Patrick Suppes"],"389":["Shoumen Palit Austin Datta"],"390":["Mark G. Core","H. Chad Lane","Michael van Lent","Dave Gomboc","Steve Solomon","Milton Rosenberg"],"391":["Dr. Sachin K. Korde","Manoj J. Munda","Yogesh B. Chintamani","Yasir L. Pirjade","Akshay V. Gurme"],"392":["Deme C. Abraham"],"393":["Bertram Raphael"],"394":["Francesco Scarcello"],"395":["K. Dautenhahn"],"396":["Katharina Holzinger","Klaus Mak","Peter Kieseberg","Andreas Holzinger"],"397":["Shreya Khare","Yogeshchandra Puranik"],"398":["Abid Sarwar"],"399":["Ashish Jobson","Dr. Kamalraj R"],"400":["A. J. Hoffman","R. E. Tessendorf"],"401":["Makoto Nagao"],"402":["R. J. Solomonoff"],"403":["Emmanuel G. Blanchard","Boris Volfson","Yuan-Jin Hong","Susanne P. Lajoie"],"404":["Kazumi Nakamatsu","Atsuyuki Suzuki"],"405":["Ben Goertzel","Stephan Vladimir Bugaj"],"406":["Marvin L. Minsky","Nathaniel Rochester","Claude E. Shannon","John McCarthy"],"407":["Victor Raskin","Julia M. Taylor"],"408":["Kazumi Nakamatsu","Atsuyuki Suzuki"],"409":["H. Halpin"],"410":["Mark Schwabacher","Kai Goebel"],"411":["David J. Chalmers","Robert M. French","Douglas R. Hofstadter"],"412":["David Hart","Ben Goertzel"],"413":["Kazumi Nakamatsu","Atsuyuki Suzuki"],"414":["Kazumi Nakamatsu","Atsuyuki Suzuki"],"415":["Frances Flanagan","Michael Walker"],"416":["Brian Milch"],"417":["Luc De Raedt","Kristian Kersting","Sriraam Natarajan","David Poole"],"418":["Bernd Ludwig","Stefan Mandl"],"419":["Antonio Camurri"],"420":["Simon Holland"],"421":["Yuhong Zhang","Umer Nauman"],"422":["S. Christophe Antoinette O. Adams Michael G. Robinson Carroll"],"423":["Simon Holland"],"424":["Elena N. Benderskaya","Sofya V. Zhukova"],"425":["D. Cockburn","N. R. Jennings"],"426":["Artur Arsenio"],"427":["B\u00f6rje Felipe Fernandes Karlsson"],"428":["Philippe Jorrand"],"429":["Wojciech Jamroga"],"430":["Kun Chen","Lei Bai","Fenghua Ling","Peng Ye","Tao Chen","Kang Chen","Tao Han","Wanli Ouyang"],"431":["Miguel A. Carreira-Perpinan","Geoffrey E. Hinton"],"432":["J. R. Carbonell"],"433":["Stuart J. Russell","Peter Norvig"],"434":["Xian-Da Zhang"],"435":["Nikhil R. Sahni","Brandon Carrus"],"436":["Gail E. Kaiser","Peter H. Feiler"],"437":["Darko Brodic","Sanja Petrovska","Radmila Jankovic","Alessia Amelio","Ivo R. Draganov"],"438":["Matthew N. O. Sadiku","Nana K. Ampah","Sarhan M. Musa"],"439":["Stasinos Konstantopoulos","Rui Camacho","Nuno A. Fonseca","V\\'\u0131tor Santos Costa"],"440":["B. Jack. Copeland"],"441":["Sharmin Sultana Sheuly","Sudhangathan Bankarusamy","Shahina Begum","Moris Behnam"],"442":["Giorgio Metta"],"443":["S\u00f3nia Teixeira","Jo\u00e3o Gama","Pedro Amorim","Gon\u00e7alo Figueira"],"444":["Matjaz Gams","Irene Yu-Hua Gu","Aki H\u00e4rm\u00e4","Andr\u00e9s Mu\u00f1oz Ortega","Vincent W. L. Tam"],"445":["Daniel S. Weld","Mausam","Peng Dai"],"446":["Yasufumi Takama"],"447":["Veniamin Veselovsky","Manoel Horta Ribeiro","Robert West"],"448":["Kimon Kieslich","Birte Keller","Christopher Starke"],"449":["Sagar Deshmukh Sanjay Rawat Shubhangi Patil"],"450":["Aswathy Madhu","Veena S Nair"],"451":["L. K. Schubert"],"452":["Matt Bower","Jodie Torrington","Jennifer W. M. Lai","Peter Petocz","Mark Alfano"],"453":["Joan Serr\u00e0"],"454":["Leslie S. Smith"],"455":["Miquel Montaner","Beatriz L\u00f3pez","Josep Llu\u00eds de la Rosa"],"456":["Svein I. Sagatun","Bernt A. Bremdal"],"457":["Alyona Ivanova"],"458":["C D Spyropoulos"],"459":["Sven Meyer zu Ei\u00dfen","Benno Stein"],"460":["Maurizio Lenzerini","Marco Schaerf"],"461":["Pijush Kanti Bhattacharjee"],"462":["Robin Hunicke","Marc LeBlanc","Robert Zubek"],"463":["Herbert Alexander Simon"],"464":["Alon Levy"],"465":["Emilio Remolina","Benjamin Kuipers"],"466":["Hiteshkumar Babubhai Vora","Hardik Anilbhai Mirani","Vraj Bhatt"],"467":["Robert M. Aiken"],"468":["Clare Bates Congdon","Philip Hingston","Graham Kendall"],"469":["Chen Jiaqi","Zhen Yunuo","Guo Simeng"],"470":["Daniel Karl I. Weidele","Justin D. Weisz","Erick Oduor","Michael Muller","Josh Andres","Alexander Gray","Dakuo Wang"],"471":["Christos Kyrkou","Andreas Papachristodoulou","Andreas Kloukiniotis","Andreas Papandreou","Aris S. Lalos","Konstantinos Moustakas","Theocharis Theocharides"],"472":["Daniel Zelterman"],"473":["Kieran Greer"],"474":["Thomas Fehlmann"],"475":["Newton Howard"],"476":["Tijn van der Zant","Matthijs Kouw","Lambert Schomaker"],"477":["Kevin B Korb","Ann E Nicholson"],"478":["Nils J. Nilsson"],"479":["Ben Goertzel"],"480":["R Geraci"],"481":["Kerstin Dautenhahn"],"482":["Robin Hunicke","Vernell Chapman"],"483":["Hyacinth S. Nwana"],"484":["Ashish Jobson","Dr. A. Rengarajan"],"485":["Feyza Altunbey Ozbay","Bilal Alatas"],"486":["Kristina Dingel","Thorsten Otto","Lutz Marder","Lars Funke","Arne Held","Sara Savio","Andreas Hans","Gregor Hartmann","David Meier","Jens Viefhaus","Bernhard Sick","Arno Ehresmann","Markus Ilchen","Wolfram Helml"],"487":["Eric Neufeld","Sonje Finnestad"],"488":["Selmer Bringsjord"],"489":["Yingjie Hu","Wenwen Li","Dawn Wright","Orhun Aydin","Daniel Wilson","Omar Maher","Mansour Raad"],"490":["Andreas Holzinger","Georg Langs","Helmut Denk","Kurt Zatloukal","Heimo M\u00fcller"],"491":["Molham Aref"],"492":["Daniel S. Weld","Gagan Bansal"],"493":["Bill Hibbard"],"494":["Zhongzhi Shi"],"495":["Tomer Barak","Yehonatan Avidan","Yonatan Loewenstein"],"496":["H. Brighton","H. Selina"],"497":["Jerzy W. Grzymala-Busse"],"498":["Louis Rosenberg","Gregg Willcox"],"499":["Jay Lee","Jaskaran Singh","Moslem Azamfar"],"500":["Marina Krakovsky"],"501":["D.B. Fogel"],"502":["John A. Campbell"],"503":["Edward A. Feigenbaum"],"504":["David B. Fogel"],"505":["Armand Layne"],"506":["J. Norwood Crout"],"507":["Rakeshkumar H Yadav Brajgopal Agarwal Sheeba James"],"508":["Steven James","George Konidaris","Benjamin Rosman"],"509":["G\u00fcnter Neumann"],"510":["Richard H. Lathrop"],"511":["Ron Chrisley"],"512":["Aaron Spaulding","Anthony Jameson","Jonathan Grudin","Neil Yorke-Smith","Jack Zaientz"],"513":["Melanie Mitchell"],"514":["SRIKANTH REDDY MANDATI"],"515":["Lucia Kleiber"],"516":["P. Domingos"],"517":["Saeed Mulawwah H Almutairi",""],"518":["S. Kotsiantis","I. Zaharakis","P. Pintelas"],"519":["Atharva Chitnavis","Yogeshchandra Puranik"],"520":["Simon D. Levy","Ross W. Gayler"],"521":["M. Bouteldja","P. Lepist\u00f6"],"522":["Cyrus F. Nourani"],"523":["Lucia Kleiber"],"524":["Leo Sauermann","Ansgar Bernardi","Andreas Dengel"],"525":["Jon A. Webb","J.K. Aggarwal"],"526":["Carolin Wienrich","Marc Erich Latoschik"],"527":["Manish Verma"],"528":["Andrea Vico"],"529":["Matthew N. O. Sadiku","Kirtikumar K. Patel","Sarhan M. Musa"],"530":["Carolin Wienrich","Marc Erich Latoschik"],"531":["Matthew N. O. Sadiku","Kirtikumar K. Patel","Sarhan M. Musa"],"532":["Ola Amayri","Nizar Bouguila"],"533":["Huimin Lu","Yujie Li","Min Chen","Hyoungseop Kim","Seiichi Serikawa"],"534":["Yong Rui"],"535":["Paolo Remagnino","Daniel Shapiro"],"536":["Daniel Zeng","Zhaohui Wu"],"537":["Huimin Lu","Yujie Li","Min Chen","Hyoungseop Kim","Seiichi Serikawa"],"538":["Patrizia Fattori","Rossella Breveglieri","Nicoletta Marzocchi","Michail Maniadakis","Claudio Galletti"],"539":["Sven Hartrumpf"],"540":["Robert Anthony Edgell","Roland Vogl"],"541":["Feng Liu","Yong Shi","Ying Liu"],"542":["Charalampos N. Moschopoulos","Panagiotis Tsiatsis","Grigorios N. Beligiannis","Dimitris Fotakis","Spiridon D. Likothanassis"],"543":["Geraint Wiggins","Alan Smaill"],"544":["Ben du Boulay","Alexandra Poulovasillis","Wayne Holmes","Manolis Mavrikis"],"545":["Zhao Xing","Weixiong Zhang"],"546":["Geraint A. Wiggins","Alan Smaill"],"547":["Christian Setzkorn"],"548":["Ahmet \u00c3\u2013zcan","Mahmut \u00c3\u0153nver","Atilla Erg\u00c3\u00bczen"],"549":["G. F. Cooper"],"550":["Will Serrano"],"551":["Helin Yang","Arokiaswami Alphones","Zehui Xiong","Dusit Niyato","Jun Zhao","Kaishun Wu"],"552":["Taehyun Ha","Sangwon Lee","Sangyeon Kim"],"553":["Joao P. Martins","Stuart C. Shapiro"],"554":["Robert TRAPPL"],"555":["Chong-U Lim","Antonios Liapis","D. Fox Harrell"],"556":["Rahul Bhatia","Vishakha Gautam","Yash Kumar","Ankush Garg"],"557":["Ton Sales"],"558":["Klaus K. Obermeier"],"559":["Sreejith S P","Kuldeep Baban Vayadande"],"560":["Boris Motik","Bernardo Cuenca Grau","Ian Horrocks","Ulrike Sattler"],"561":["S. Legg","M. Hutter"],"562":["G.F. Luger"],"563":["Mari Carmen Su\u00e1rez-Figueroa","Edna Ruckhaus","Jorge L\u00f3pez-Guerrero","Isabel Cano","\u00c1lvaro Cervera"],"564":["Michael Filzmoser","Sabine T. Koeszegi"],"565":["Arpit Seth","Vijayakumar A"],"566":["Julian Tritscher","Anna Krause","Andreas Hotho"],"567":["K Aishwarya J Priyadharshini G Rajeswari"],"568":["Zdenek Renc"],"569":["Tanmay Pachpande","Dewang Solanki","Venkat. P. Patil"],"570":["Alejandro Hern\u00e1ndez Arieta","Ryu Kato","Wenwei Yu","Hiroshi Yokoi"],"571":["Prathama V","Thippeswamy G"],"572":["David R. Barstow"],"573":["William M. Chace"],"574":["Jonathan M. Blackledge","Sergei V. Bezobrazov","Paul Tobin"],"575":["Gabriella Pigozzi","Alexis Tsouki\u00e0s","Paolo Viappiani"],"576":["Tuncer I. \u00d6ren"],"577":["Steven Stenberg Hansen"],"578":["Saul Amarel"],"579":["Liming Chen","Huansheng Ning","Chris D. Nugent","Zhiwen Yu"],"580":["George Anthony Gal","Cristiana Santos","Lucien Rapp","R\u00e9ka Markovich","Leendert W. N. van der Torre"],"581":["Dov Monderer","Moshe Tennenholtz","Hal Varian"],"582":["M. Minsky"],"583":["J. McCarthy"],"584":["Rosli Omar"],"585":["Derek Partridge"],"586":["Ian Millington"],"587":["Richard E. Korf"],"588":["John F. Horty","Richmond H. Thomason"],"589":["Carlos E. Jimenez-Gomez","Jes\u00fas Cano Carrillo","Francisco Falcone"],"590":["Stan Openshaw","Christine Openshaw"],"591":["Elaine Rich","Kevin Knight"],"592":["Nils J. Nilsson"],"593":["Tom Bylander"],"594":["David Corne","Andrew C. R. Martin"],"595":["L. Steels"],"596":["Luc Steels"],"597":["Daniel Zeng","Wenji Mao"],"598":["Zibin Zheng","Hong-Ning Dai"],"599":["Wenwu Zhu","Xin Wang","Wen Gao"],"600":["Alma Lilia Garc\u00eda-Almanza","Biliana Alexandrova-Kabadjova","Seraf\u00edn Mart\u00ednez-Jaramillo"],"601":["Francesca Rossi","Kristen Brent Venable","Toby Walsh"],"602":["Vladan Devedzic"],"603":["Maria L. Gini","Noa Agmon","Fausto Giunchiglia","Sven Koenig","Kevin Leyton-Brown"],"604":["Lee Spector"],"605":["Kurshid Ahmad"],"606":["Peter E. Hart"],"607":["Wolfgang Ertel"],"608":["Nachum Dershowitz"],"609":["Simon Parsons"],"610":["John McCarthy"],"611":["Xin Zhang","Shaohong Zhang","Jianyu Liu","Liqing Cai","Jing Wang"],"612":["Graham J. Williams","Dickson Lukose"],"613":["Donato Impedovo","Giuseppe Pirlo"],"614":["Carl Hewitt"],"615":["Vladim\u00edr Mar\u00edk","Jir\u00ed Lazansk\u00fd"],"616":["Margaret A. Boden"],"617":["S\u00e9bastien Konieczny","Emmanuel Lonca"],"618":["James Gips","George Stiny"],"619":["Alistair D. C. Holden"],"620":["Marvin Minsky"],"621":["Saul Amarel","John Seely Brown","Bruce G. Buchanan","Peter E. Hart","Casimir A. Kulikowski","William A. Martin","Harry E. Pople"],"622":["Pamela McCorduck","Marvin Minsky","Oliver G. Selfridge","Herbert A. Simon"],"623":["Laveen N. Kanal","John F. Lemmer","Andrew P. Sage"],"624":["David Gunning","Mark Stefik","Jaesik Choi","Timothy Miller","Simone Stumpf","Guang-Zhong Yang"],"625":["Trevor J. M. Bench-Capon","Paul E. Dunne"],"626":["David Gunning","Mark Stefik","Jaesik Choi","Timothy Miller","Simone Stumpf","Guang-Zhong Yang"],"627":["Kevin Curran","Christian Becker","Alan Marshall"],"628":["Simon Lucas"],"629":["Franziska Kl\u00fcgl"],"630":["Patrick Henry Winston"],"631":["Johannes Retti"],"632":["Daniel P. Berrar","Naoyuki Sato","Alfons Schuster"],"633":["Antonio A. Martino"],"634":["?"],"635":["?"],"636":["Wai Wai Tun"],"637":["Alon Y. Levy","Daniel S. Weld"],"638":["Gregory R. Wheeler","Lu\u00eds Moniz Pereira"],"639":["Hanno Hildmann","Benjamin Hirsch"],"640":["F. Allard"],"641":["Don Monroe"],"642":["Herv\u00e9 Gallaire"],"643":["Marvin Minsky"],"644":["Jerry Kaplan"],"645":["Jon Doyle"],"646":["Nathan Dawson","Hunter Hale"],"647":["James Hightower"],"648":["Michael Swan Laufer"],"649":["Tshilidzi Marwala","Bo Xing"],"650":["Peter J. Denning","Dorothy E. Denning"],"651":["Stephen J. Andriole","Andrew P. Sage"],"652":["Jeff Rothenberg"],"653":["Fei-Yue Wang","Ruqian Lu","Daniel Zeng"],"654":["Nils J. Nilsson"],"655":["Eugene Charniak","Drew McDermott"],"656":["Ilkka Niemel\u00e4","Patrik Simons"],"657":["E. Charniak","D. McDermott"],"658":["M. L. Ginsberg"],"659":["Wolfgang Bibel"],"660":["Tuncer I. \u00d6ren"],"661":["Lakhmi C. Jain","Zhengxin Chen"],"662":["Peter C. Y. Chen","Aun Neow Poo"],"663":["Valerio Targon"],"664":["Antonios Liapis","Maren Awiszus","Alex J. Champandard","Michael Cook","Alena Denisova","Alexander Dockhorn","Tommy Thompson","Jichen Zhu"],"665":["Ajith Abraham"],"666":["Gordon D. Robinson"],"667":["Gregg Collins"],"668":["Robert E. Shannon"],"669":["Janice I. Glasgow"],"670":["Hiroaki Kitano","James A. Hendler","Tetsuya Higuchi","Dan I. Moldovan","David L. Waltz"],"671":["Rog\u00e9rio de Lemos","Marek Grzes"],"672":["Patrick O'Driscoll","Jaehoon Lee","Bo Fu"],"673":["Wojciech Samek","Klaus-Robert M\u00fcller"],"674":["Deyi Li"],"675":["Dana S. Nau"],"676":["N. Di Ruocco","A. Vitale","S. Vitulano"],"677":["Xiao-Yun Zhou","Yao Guo","Mali Shen","Guang-Zhong Yang"],"678":["Subhash C. Kak"],"679":["Luis Eduardo Castillo Hern"],"680":["Karl G. Kempf"],"681":["M. Minsky"],"682":["Neil C. Rowe"],"683":["M. A. Boden"],"684":["Franz Josef Radermacher"],"685":["Bin Yu","Karl Kumbier"],"686":["Patrick Henry Winston"],"687":["Lorijn Zaadnoordijk","Tarek R. Besold"],"688":["Kyle E. Jennings"],"689":["Joseph L. Katz"],"690":["Roy Rada"],"691":["Jarrett Rosenberg"],"692":["Yi Zeng","Enmeng Lu","Cunqing Huangfu"],"693":["J. McCarthy"],"694":["Kjell J\u00f8rgen Hole","Subutai Ahmad"],"695":["Gary Bradshaw"],"696":["Laurence L. Leff"],"697":["Horia-Nicolai Teodorescu","Abraham Kandel"],"698":["Bart Verheij"],"699":["Komelia Lazanyi"],"700":["Daniele Nardi","Luca Iocchi"],"701":["Subhash C. Kak","C. Donald","Elaine T. Delaune"],"702":["Nick R. Jennings"],"703":["Bin Yu","Karl Kumbier"],"704":["William J. Dally","C. Thomas Gray","John Poulton","Brucek Khailany","John M. Wilson","Larry R. Dennison"],"705":["Sanjit A. Seshia","Dorsa Sadigh"],"706":["Neil C. Rowe"],"707":["Richard Ennals"],"708":["Alberto Oliverio"],"709":["Nir Friedman","Joseph Y. Halpern"],"710":["Brenda K. Wiederhold"],"711":["Luc Steels"],"712":["?"],"713":["?"],"714":["KR4HC 2010","David Ria\u00f1o Ramos","European Conference on Artificial Intelligence"],"715":["J. Ferber"],"716":["?"],"717":["?"],"718":["?"],"719":["?"],"720":["Alexander Krushanov"],"721":["Virginia Dignum"],"722":["Pierre Marquis","Odile Papini","Henri Prade"],"723":["Valmir Carneiro Barbosa"],"724":["Patrick H. Winston"],"725":["Mr. Ajmal Rasi Dr. Rajasimha A Makram Ms. Shilpa Das"],"726":["Andrew G. Barto","Steven J. Bradtke","Satinder P. Singh"],"727":["Nicholas R. Jennings"],"728":["Alexandre Linhares"],"729":["Burak Turhan","Ayse Basar Bener","\u00c7etin Meri\u00e7li","Andriy V. Miranskyy","Leandro L. Minku"],"730":["Rinke Hoekstra","Joost Breuker"],"731":["Denis Ferraretti","Luca Tagliavini","Raffaele Di Cuia","Mariachiara Puviani","Evelina Lamma","Sergio Storari"],"732":["A. Ruiz","A. Ramos","J. L. San Emeterio"],"733":["Kristian Kersting","Sriraam Natarajan"],"734":["Alberto Romele"],"735":["J\u00falius Csont\u00f3"],"736":["Chia-Chuan Hung","Yi-Ching Huang","Jane Yung-jen Hsu","David Kuan-Chun Wu","Chun Wu"],"737":["Jose Antonio Martin H.","Javier de Lope"],"738":["Salvatore Ruggieri"],"739":["Oliviero Stock"],"740":["Kavitha"],"741":["Pierpaolo Dondio","Luca Longo","Stefano Bistarelli"],"742":["Camilo Isaza Fonseca","Octavio J. Salcedo Parra","Brayan S. Reyes Daza"],"743":["Feng Tao","Xiaohui Zou","Danni Ren"],"744":["Sumit Mishra"],"745":["K. Ravikanth Mishra D. Brahmeswara Rao A. Dinesh Chowdary"],"746":["J. D. Fouks","L. Signac"],"747":["Joan Condell","John Wade","Leo Galway","Michael McBride","Padhraig Gormley","Joseph Brennan","Thiyagesan Somasundram"],"748":["St\u00e9phane P. A. Bordas","Sundararajan Natarajan","Andreas Zilian"],"749":["Design Unique Brochures with AI: Simplified's Free AI Brochure Maker"],"750":["Celia Medina Lloret"],"751":["Noah Gift"],"752":["AI logo maker"],"753":["Yoav Shoham"],"754":["Henry Lieberman"],"755":["High-Level Expert Group on AI"],"756":["Reid G. Smith","Joshua Eckroth"],"757":["Davy Tsz Kit Ng","Jac Ka Lok Leung","Samuel Kai Wah Chu","Maggie Shen Qiao"],"758":["Jian Wu","Kyle Williams","Hung-Hsuan Chen","Madian Khabsa","Cornelia Caragea","Suppawong Tuarob","Alexander G. Ororbia","Douglas Jordan","Prasenjit Mitra","C. Lee Giles"],"759":["The AI Element Periodic Table is a fascinating concept that brings together two diverse yet interconnected realms: artificial intelligence","elements of AI are categorized the periodic table. In this innovative framework","much like chemical elements in the traditional periodic table. organized"],"760":["Jonathan Grudin"],"761":["Daniel S. Weld"],"762":["Andreas Holzinger"],"763":["Tim M. Jones"],"764":["Henry Lieberman"],"765":["A\u00edda Ponce Del Castillo"],"766":["Renata S. S. Guizzardi","Glenda Carla Moura Amaral","Giancarlo Guizzardi","John Mylopoulos"],"767":["Darse Billings","Denis Papp","Jonathan Schaeffer","Duane Szafron"],"768":["Matthew McNaughton","James Redford","Jonathan Schaeffer","Duane Szafron"],"769":["Vivian Lai","Chacha Chen","Q. Vera Liao","Alison Smith-Renner","Chenhao Tan"],"770":["Manish Verma"],"771":["Jess Hohenstein","Malte Jung"],"772":["Danding Wang","Qian Yang","Ashraf Abdul","Brian Y. Lim"],"773":["Juan Antonio Lloret Egea","Celia Medina Lloret","Adri\u00e1n Hern\u00e1ndez Gonz\u00e1lez","Diana D\u00edaz Raboso","Carlos Campos","Kimberly Riveros Guzm\u00e1n","Luis Miguel Cort\u00e9s Carballo"],"774":["Robin Cohen","Rishav Raj Agarwal","Dhruv Kumar","Alexandre Parmentier","Tsz Him Leung"],"775":["Mark Purdy","Paul Daugherty"],"776":["Raheena Dahya","Alexis Morris"],"777":["Henry A. Kissinger","Eric Schmidt","Daniel Huttenlocher"],"778":["Tim Menzies","Harshinder Singh"],"779":["Randy Goebel","Ajay Chander","Katharina Holzinger","Freddy Lecue","Zeynep Akata","Simone Stumpf","Peter Kieseberg","Andreas Holzinger"],"780":["Petro Verkhogliad","B. John Oommen"],"781":["Benedetta Brevini"],"782":["EASA"],"783":["Carlos Ramos","Chen-Ching Liu"],"784":["Celia Medina Lloret","Diana D\\'\u0131az Raboso","Adri\u00e1n Hern\u00e1ndez Gonz\u00e1lez"],"785":["Wayne Holmes","Ilkka Tuomi"],"786":["Maayan Shvo","Shirin Sohrabi","Sheila A. McIlraith"],"787":["Daniel H. Chang","Michael Pin-Chuan Lin","Shiva Hajian","Quincy Q. Wang"],"788":["Ali Darvishi","Hassan Khosravi","Shazia Sadiq","Dragan Ga\u0161evi\u0107","George Siemens"],"789":["Rodney A. Brooks"],"790":["Marcelo Prates","Pedro Avelar","Luis C. Lamb"],"791":["Anna Perini","Angelo Susi"],"792":["Luc De Raedt","Johannes A. La Poutr\u00e9","Floor Verdenius"],"793":["Evgeniy Gabrilovich"],"794":["Thomas G. Dietterich","Eric J. Horvitz"],"795":["Krzysztof Z. Gajos","Lena Mamykina"],"796":["Don Monroe"],"797":["Robert R. Hoffman","Shane T. Mueller","Gary Klein","Jordan Litman"],"798":["Andrey Ignatov","Radu Timofte","Maurizio Denna","Abdel Younes","Andrew Lek","Mustafa Ayazoglu","Jie Liu","Zongcai Du","Jiaming Guo","Xueyi Zhou","Hao Jia","Youliang Yan","Zexin Zhang","Yixin Chen","Yunbo Peng","Yue Lin","Xindong Zhang","Hui Zeng","Kun Zeng","Peirong Li","Zhihuang Liu","Shiqi Xue","Shengpeng Wang"],"799":["Patrick Hemmer","Monika Westphal","Max Schemmer","Sebastian Vetter","Michael V\u00f6ssing","Gerhard Satzger"],"800":["Aida Ponce Del Castillo"],"801":["Virginia Dignum"],"802":["Carlos Ramos","Chen-Ching Liu"],"803":["Ke Zhang","Ayse Begum Aslan"],"804":["Henry Lieberman"],"805":["Tumaini Kabudi","Ilias Pappas","Dag H\u00e5kon Olsen"],"806":["Spence Green","Jeffrey Heer","Christopher D. Manning"],"807":["Emily M. Bender"],"808":["Michael Chromik","Malin Eiband","Felicitas Buchner","Adrian Kr\u00fcger","Andreas Butz"],"809":["Samantha Straka","Martin Jakobus Koch","Astrid Carolus","Marc Erich Latoschik","Carolin Wienrich"],"810":["Lester Ingber"],"811":["Ramya Srinivasan","Ajay Chander"],"812":["Yoshua Bengio","Yann Lecun","Geoffrey Hinton"],"813":["Amira Djebbari","Aed\u00edn C. Culhane","Alice Armstrong","John Quackenbush"],"814":["Tanja Carstensen","Kathrin Ganz"],"815":["McKinsey"],"816":["Jianfeng Gao","Michel Galley","Lihong Li"],"817":["Vais Report Thomas","Fakultat Informatik Und Automatisierung","Fachgebiet Kunstliche Intelligenz","Thomas Abel","Thomas Abel","Avelino Gonzalez","Avelino Gonzalez"],"818":["Alasdair Hill","Ekaterina Komendantskaya","Ronald P. A. Petrick"],"819":["Julian Togelius","Georgios N. Yannakakis"],"820":["Anthony Jameson","Aaron Spaulding","Neil Yorke-Smith"],"821":["Peter Werner Eklund","Andrew Lincoln Burrow"],"822":["Paul Cohen","Adele Howe"],"823":["David Gray Widder","Sarah West","Meredith Whittaker"],"824":["Yoshua Bengio"],"825":["Jos\u00e9 David Mart\u00edn-Guerrero","Emilio Soria-Olivas","Paulo J. G. Lisboa","Antonio J. Serrano-L\u00f3pez"],"826":["Carissa Schoenick","Peter Clark","Oyvind Tafjord","Peter Turney","Oren Etzioni"],"827":["Abhishek Gupta"],"828":["Michael Cook","Maren Awiszus","Duygu Cakmak","Alena Denisova","Alexander Dockhorn","Casper Harteveld","Antonios Liapis","Mirjam Palosaari Eladhari","Diego Perez Liebana","Lisa Rombout","Tommy Thompson"],"829":["Jichen Zhu","Maren Awiszus","Michael Cook","Alexander Dockhorn","Manuel Eberhardinger","Daniele Loiacono","Simon M. Lucas","Ana Matran-Fernandez","Diego Perez Liebana","Tommy Thompson","Remco Veltkamp"],"830":["Gerhard Widmer"],"831":["Leman Kirme","Vivek Jha","Punit Chauhan","Soumya Ranjan Mohanty","Rahul Ghode"],"832":["Andrea Bunt","Cristina Conati","Joanna McGrenere"],"833":["Mike Perkins","Leon Furze","Jasper Roe","Jason MacVaugh"],"834":["Izabel Cvetkovic","Sarah Oeste-Rei\u00df","Nale Lehmann-Willenbrock","Eva Bittner"],"835":["Priyanka Y. Patil","Manju R. Patil","Nilima R. Barhate","Ashvini P. Patil","Prashant C. Harne"],"836":["Matthew Arnold","Rachel K. E. Bellamy","Michael Hind","Stephanie Houde","Sameep Mehta","Aleksandra Mojsilovic","Ravi Nair","Karthikeyan Natesan Ramamurthy","Darrell Reimer","Alexandra Olteanu","David Piorkowski","Jason Tsay","Kush R. Varshney"],"837":["Jason Furman","Robert Seamans"],"838":["Pradyumna Saini","Mohd Tajammul"],"839":["Adele Smolansky","Andrew Cram","Corina Raduescu","Sandris Zeivots","Elaine Huber","Rene F. Kizilcec"],"840":["Federico Maria Cau","Hanna Hauptmann","Lucio Davide Spano","Nava Tintarev"],"841":["Silviu-Marian Udrescu","Max Tegmark"],"842":["Punya Mishra","Melissa Warr","Rezwana Islam"],"843":["Ziming Liu","Max Tegmark"],"844":["Benjamin Kuipers","Edward A. Feigenbaum","Peter E. Hart","Nils J. Nilsson"],"845":["Niloy Ganguly","Dren Fazlija","Maryam Badar","Marco Fisichella","Sandipan Sikdar","Johanna Schrader","Jonas Wallat","Koustav Rudra","Manolis Koubarakis","Gourab K. Patro","Wadhah Zai El Amri","Wolfgang Nejdl"],"846":["Burak Turhan","Ayse Bener","Rachel Harrison","Andriy Miransky","Cetin Mericli","Leandro Minku"],"847":["David B. Leake","James Gary"],"848":["Peng Sun","Xinghai Sun","Lei Han","Jiechao Xiong","Qing Wang","Bo Li","Yang Zheng","Ji Liu","Yongsheng Liu","Han Liu","Tong Zhang"],"849":["Sergio E Baranzini","Katy B\u00f6rner","John Morris","Charlotte A Nelson","Karthik Soman","Erica Schleimer","Michael Keiser","Mark Musen","Roger Pearce","Tahsin Reza","others"],"850":["Paul Bello","Will Bridewell"],"851":["Yuval Noah Harari"],"852":["Ashok K. Goel","David A. Joyner"],"853":["Nguyen-Thinh Le","Sven Strickroth","Sebastian Gross","Niels Pinkwart"],"854":["Osma Suominen","Juho Inkinen","Mona Lehtinen"],"855":["Avik Roy"],"856":["A\u00efda Ponce Del Castillo"],"857":["J. Kay"],"858":["Prem Krishna","Saheel Ahamed","Roshan Kartik"],"859":["Edwina L. Rissland"],"860":["Toby Walsh"],"861":["Emanuele Carpanzano","Amedeo Cesta","Fernando Marin\u00f2","Andrea Orlandini","Riccardo Rasconi","Anna Valente"],"862":["Yorick Wilks"],"863":["Michael Rovatsos"],"864":["Jonathan Glick"],"865":["Larry R. Medsker"],"866":["Ethan R. Mollick","Lilach Mollick"],"867":["Drew Hemment","Ruth Aylett","Vaishak Belle","Dave Murray-Rust","Ewa Luger","Jane Hillston","Michael Rovatsos","Frank Broz"],"868":["Paolo Coppola","Vincenzo Della Mea","Luca Di Gaspero","Raffaella Lomuscio","Danny Mischis","Stefano Mizzaro","Elena Nazzi","Ivan Scagnetto","Luca Vassena"],"869":["Drew Hemment","Ruth Aylett","Vaishak Belle","Dave Murray-Rust","Ewa Luger","Jane Hillston","Michael Rovatsos","Frank Broz"],"870":["Nick Barnes","Peter Baumgartner","Tib\u00e9rio S. Caetano","Hugh F. Durrant-Whyte","Gerwin Klein","Penelope Sanderson","Abdul Sattar","Peter J. Stuckey","Sylvie Thi\u00e9baux","Pascal Van Hentenryck","Toby Walsh"],"871":["Todd W. Neller"],"872":["Frank van Harmelen","James A. Hendler","Pascal Hitzler","Krzysztof Janowicz"],"873":["Michael Desmond","Michael Muller","Zahra Ashktorab","Casey Dugan","Evelyn Duesterwald","Kristina Brimijoin","Catherine Finegan-Dollak","Michelle Brachman","Aabhas Sharma","Narendra Nath Joshi","Qian Pan"],"874":["Marcus Hutter"],"875":["Andrey Ignatov","Kim Byeoung su","Radu Timofte","Angeline Pouget","Fenglong Song","Cheng Li","Shuai Xiao","Zhongqian Fu","Matteo Maggioni","Yibin Huang","Shen Cheng","Xin Lu","Yifeng Zhou","Liangyu Chen","Donghao Liu","Xiangyu Zhang","Haoqiang Fan","Jian Sun","Shuaicheng Liu","Minsu Kwon","Myungje Lee","Jaeyoon Yoo","Changbeom Kang","Shinjo Wang","Bin Huang","Tianbao Zhou","Shuai Liu","Lei Lei","Chaoyu Feng","Liguang Huang","Zhikun Lei","Feifei Chen"],"876":["Anirban Chakraborty"],"877":["Elaine Rich"],"878":["Kate Crawford","Vladan Joler"],"879":["Celia Medina Lloret","Adri\u00e1n Hern\u00e1ndez Gonz\u00e1lez","Diana D\u00edaz Raboso","Jos\u00e9 Alejandro Orozco Leal","\u00c1ngel Manuel P\u00e9rez L\u00f3pez","Marta Riquelme"],"880":["Lorena Casal-Otero","Alejandro Catala","Carmen Fern\u00e1ndez-Morante","Maria Taboada","Beatriz Cebreiro","Sen\u00e9n Barro"],"881":["Chrissi Nerantzi","Sandra Abegglen","Marianna Karatsiori","Antonio Mart\u00ednez-Arboleda (Eds.)"],"882":["Adi Botea"],"883":["Janice I. Glasgow","Igor Jurisica","Burkhard Rost"],"884":["Roger C. Schank"],"885":["Ajit Narayanan"],"886":["Larry R. Medsker"],"887":["Jean-Daniel Dessimoz","Jana Koehler","Thilo Stadelmann"],"888":["Maria Fox","Manuela M. Veloso","Eric Horvitz"],"889":["Alexandra Coman","David W. Aha"],"890":["Ashok K. Goel"],"891":["David Blanchard"],"892":["Francesca Rossi","Manuela M. Veloso"],"893":["Karamjit S. Gill"],"894":["The Best AI Content Writer Plugin For WordPress"],"895":["Virginia Dignum"],"896":["Ronald J. Brachman","David Gunning","Murray Burke"],"897":["Michael Rovatsos"],"898":["Wendy Hall","Gillian Lovegrove"],"899":["Astrid Carolus","Martin J. Koch","Samantha Straka","Marc Erich Latoschik","Carolin Wienrich"],"900":["Sascha Caron","Jong S. Kim","Krzysztof Rolbiecki","Roberto R. de Austri","Bob Stienen"],"901":["Thilo Hagendorff","Katharina Wezel"],"902":["Heidi Furey","Fred Martin"],"903":["David Gunning","Vinay K. Chaudhri","Chris Welty"],"904":["Nestor Rychtyckyj","Venkatesh Raman","Baskaran Sankaranarayanan","P. Sreenivasa Kuma","Deepak Khemani"],"905":["Greta Warren","Ruth M. J. Byrne","Mark T. Keane"],"906":["UNESCO"],"907":["Imon Banerjee","Ananth Reddy Bhimireddy","John L. Burns","Leo Anthony Celi","Li-Ching Chen","Ramon Correa","Natalie Dullerud","Marzyeh Ghassemi","Shih-Cheng Huang","Po-Chih Kuo","Matthew P Lungren","Lyle Palmer","Brandon J Price","Saptarshi Purkayastha","Ayis Pyrros","Luke Oakden-Rayner","Chima Okechukwu","Laleh Seyyed-Kalantari","Hari Trivedi","Ryan Wang","Zachary Zaiman","Haoran Zhang","Judy W Gichoya"],"908":["Ernst D. Dickmanns"],"909":["Tobias Koopmann","Maximilian Stubbemann","Matthias Kapa","Michael Paris","Guido Buenstorf","Tom Hanika","Andreas Hotho","Robert J\u00e4schke","Gerd Stumme"],"910":["Anna Jobin","Marcello Ienca","Effy Vayena"],"911":["James M. Crawford"],"912":["Xinru Wang","Ming Yin"],"913":["David Gunning","Vinay K. Chaudhri","Peter E. Clark","Ken Barker","Shaw-Yi Chaw","Mark Greaves","Benjamin Grosof","Alice Leung","David D. McDonald","Sunil Mishra","John Pacheco","Bruce Porter","Aaron Spaulding","Dan Tecuci","Jing Tien"],"914":["Seth Earley"],"915":["Douglas H. Fisher"],"916":["Soren Grubov","Rasmus Hartvig"],"917":["Vincent M\u00fcller"],"918":["Keith Kirkpatrick"],"919":["Jeffrey Heer"],"920":["Karamjit S. Gill"],"921":["Jonathan Glick"],"922":["Godela Unseld"],"923":["Petros Gelepithis"],"924":["Alan Bundy"],"925":["Keizo Sato"],"926":["Kiri Wagstaff"],"927":["Edward A. Feigenbaum"],"928":["Yoav Shoham"],"929":["Philipp Reinhard","Mahei Manhai Li","Jan Marco Leimeister"],"930":["Sara Reese Hedberg"],"931":["Jonathan Glick"],"932":["Sehar Shahzad Farooq","In-Suk Oh","Man-Jae Kim","Kyung-Joong Kim"],"933":["Joshua Eckroth"],"934":["Satinder P. Gill"],"935":["Toby Walsh"],"936":["Ariel Felner"],"937":["Stuart Maitland","Philip McElnay","Richard Brady"],"938":["Richard C. Waters"],"939":["Roger C. Schank"],"940":["Eric Eaton","Tom Dietterich","Maria L. Gini","Barbara J. Grosz","Charles L. Isbell Jr.","Subbarao Kambhampati","Michael L. Littman","Francesca Rossi","Stuart J. Russell","Peter Stone","Toby Walsh","Michael J. Wooldridge"],"941":["Ashok K. Goel"],"942":["David B. Leake"],"943":["Dana S. Nau"],"944":["Lionel Snell"],"945":["Maria Perez-Ortiz","Claire Dormann","Yvonne Rogers","Sahan Bulathwela","Stefan Kreitmayer","Emine Yilmaz","Richard Noss","John Shawe-Taylor"],"946":["Oleksii Kuchaiev","Jason Li","Huyen Nguyen","Oleksii Hrinchuk","Ryan Leary","Boris Ginsburg","Samuel Kriman","Stanislav Beliaev","Vitaly Lavrukhin","Jack Cook","Patrice Castonguay","Mariya Popova","Jocelyn Huang","Jonathan M. Cohen"],"947":["Aaron Spaulding","Julie S. Weber"],"948":["Filipe Dwan Pereira","Elaine Oliveira","Luiz Rodrigues","Luciano Cabral","David Oliveira","Leandro Carvalho","Dragan Gasevic","Alexandra Cristea","Diego Dermeval","Rafael Ferreira Mello"],"949":["I. Goldstein","M. Miller"],"950":["Iris van Rooij","Olivia Guest","Federico G Adolfi","Ronald de Haan","Antonina Kolokolova","Patricia Rich"],"951":["Ethem Alpaydin"],"952":["Yoshua Bengio","Yann Lecun"],"953":["Natalya F. Noy","AnHai Doan","Alon Y. Halevy"],"954":["Dominik Siemon","Edona Elshan","Triparna de Vreede","Sarah Oeste-Rei\u00df","Gert Jan de Vreede","Philipp Ebel"],"955":["Zana Bucinca","Phoebe Lin","Krzysztof Z. Gajos","Elena L. Glassman"],"956":["Kreator Prezentacji AI"],"957":["Rudi Studer","Anupriya Ankolekar","Pascal Hitzler","York Sure"],"958":["Daniel S. Weld"],"959":["Walter Hamscher"],"960":["Henry Lieberman"],"961":["James A. Hendler","Austin Tate","Mark Drummond"],"962":["Cameron Hughes","Tracey Hughes"],"963":["Martin Lindvall","Claes Lundstr\u00f6m","Jonas L\u00f6wgren"],"964":["Pranav Rajpurkar","Emma Chen","Oishi Banerjee","Eric J Topol"],"965":["Grace Su"],"966":["Daniel S. Weld"],"967":["Selmer Bringsjord"],"968":["Denis L. Baggi"],"969":["Tom Williams"],"970":["Todd W. Neller"],"971":["David Smith"],"972":["Jochen Renz","Xiaoyu Ge","Stephen Gould","Peng Zhang"],"973":["Daniel Garijo"],"974":["D. Weld"],"975":["Victoria Vesna"],"976":["Alan K. Mackworth"],"977":["Andy Hon Wai Chun"],"978":["Beverly Park Woolf","H. Chad Lane","Vinay K. Chaudhri","Janet L. Kolodner"],"979":["Sean McGregor","Amir Banifatemi"],"980":["Anthony Jameson"],"981":["Charles Marshall","Dennis E. O'Connor","Frank Lynch","Mike Kiskiel II"],"982":["Robert B. Fisher"],"983":["Sven Koenig","Maxim Likhachev","Yaxin Liu","David Furcy"],"984":["Cameron Hughes","Tracey Hughes"],"985":["Odd Erik Gundersen","Yolanda Gil","David W. Aha"],"986":["Vincent Robbemond","Oana Inel","Ujwal Gadiraju"],"987":["Richard F. Flor"],"988":["Pierre-Luc Bacon","Doina Precup"],"989":["Ulrich Furbach"],"990":["Carmel Domshlak","Eyke H\u00fcllermeier","Souhila Kaci","Henri Prade"],"991":["Dan Bohus","Eric Horvitz","Takayuki Kanda","Bilge Mutlu","Antoine Raux"],"992":["Santiago Onta\\ n\u00f3n","Gabriel Synnaeve","Alberto Uriarte","Florian Richoux","David Churchill","Mike Preuss"],"993":["Christian Herzog"],"994":["Don Sofge","William F. Lawless","Ranjeev Mittu"],"995":["Margaret Burnett"],"996":["Ethan R. Mollick","Lilach Mollick"],"997":["Michael L. Anderson"],"998":["Marina Danilevsky","Kun Qian","Ranit Aharonov","Yannis Katsis","Ban Kawas","Prithviraj Sen"],"999":["Manish Verma"],"1000":["Amy McGovern","Eric Eaton"],"1001":["Kiri L. Wagstaff"],"1002":["Alfred Kobsa"],"1003":["Toshiyuki Furukawa"],"1004":["Annie Zhou"],"1005":["Kenneth D. Forbus","Thomas Hinrich"],"1006":["Urjit A. Yajnik"],"1007":["Fumihiko Satofuka","Katsuhiko Nakamura"],"1008":["Carles Sierra","Francesca Toni"],"1009":["Kiri Wagstaff"],"1010":["Amy McGovern"],"1011":["Kenneth M. Ford","Patrick J. Hayes","Clark Glymour","James F. Allen"],"1012":["Hiroaki Kitano","Minoru Asada","Yasuo Kuniyoshi","Itsuki Noda","Eiichi Osawa","Hitoshi Matsubara"],"1013":["Robert R. Hoffman"],"1014":["Ashok K. Goel"],"1015":["Joshua Eckroth","Liang Dong","Reid G. Smith","Bruce G. Buchanan"],"1016":["Julian Togelius","Noor Shaker","Sergey Karakovskiy","Georgios N. Yannakakis"],"1017":["Philip Leith"],"1018":["David Hirschberg"],"1019":["Amy McGovern","Iolanda Leite"],"1020":["Nathan R. Sturtevant"],"1021":["Todd W. Neller"],"1022":["N. S. Sridharan"],"1023":["Amy McGovern","Iolanda Leite"],"1024":["Rodrigo Ventura"],"1025":["Drew V. McDermott"],"1026":["Amy McGovern"],"1027":["David B. Leake"],"1028":["Philippe Laborie"],"1029":["Jacobijn Sandberg","Yvonne Barnard"],"1030":["Amy McGovern","Iolanda Leite","Anuj Karpatne"],"1031":["Amy McGovern","Iolanda Leite"],"1032":["John McCarthy"],"1033":["Ashok K. Goel"],"1034":["Neil Jacobstein"],"1035":["Alan K. Mackworth"],"1036":["James A. Hendler"],"1037":["Amy McGovern","Iolanda Leite"],"1038":["Kiri Wagstaff"],"1039":["Form Writer Long"],"1040":["Matthias Scheutz"],"1041":["John E. Laird","Christian Lebiere","Paul S. Rosenbloom"],"1042":["Elizabeth Gibney"],"1043":["Francesco Lelli"],"1044":["John McCarthy"],"1045":["Roger C. Schank"],"1046":["David Kirsh"],"1047":["Brian Lubars","Chenhao Tan"],"1048":["Randall Davis","Howard Shrobe","Peter Szolovits"],"1049":["Ulisses Ferreira"],"1050":["Predro Domingos"],"1051":["Bruno Bouzy","Tristan Cazenave"],"1052":["Mingsheng Ying"],"1053":["Cristiano Castelfranchi"],"1054":["Edwina L. Rissland","Kevin D. Ashley","Ronald Prescott Loui"],"1055":["Wei-Min Shen"],"1056":["Anne-Marie McEwan","Richard Ennals"],"1057":["Jeffrey Stone"],"1058":["Junwen Bai","Yexiang Xue","Johan Bjorck","Ronan Le Bras","Brendan Rappazzo","Richard Bernstein","Santosh K. Suram","Robert Bruce van Dover","John M. Gregoire","Carla P. Gomes"],"1059":["Jacobijn Sandberg"],"1060":["Barbara J. Grosz"],"1061":["Oris Friesen","Forouzan Golshani"],"1062":["James F. Allen"],"1063":["Reid G. Smith","Joshua Eckroth"],"1064":["Douglas S. Blank","Deepak Kumar","Lisa Meeden","Holly A. Yanco"],"1065":["Marion Neumann"],"1066":["Larry R. Medsker"],"1067":["GuoQiang Peng"],"1068":["Randall Davis","Howard Shrobe","Peter Szolovits"],"1069":["Andreas Janson","Anuschka Schmitt","Tatjana Bevilacqua"],"1070":["Ayse Tosun","Burak Turhan","Ayse Bener"],"1071":["Matteo Manica","Ali Oskooei","Jannis Born"],"1072":["Riccardo Guidotti","Anna Monreale","Dino Pedreschi"],"1073":["Poornima S","Muthukumarasamy S"],"1074":["Todd W. Neller"],"1075":["Amy McGovern","Eric Eaton"],"1076":["Marion Neumann"],"1077":["Cathrine Hasse"],"1078":["David S. Touretzky","Christina Gardner-McCune","Cynthia Breazeal","Fred Martin","Deborah W. Seehorn"],"1079":["Stephen K. Reed"],"1080":["Toyoaki Nishida"],"1081":["Lee Spector"],"1082":["Edward L. Fisher"],"1083":["Marion Neumann"],"1084":["Marion Neumann"],"1085":["Cameron Hughes"],"1086":["Amy McGovern","Eric Eaton"],"1087":["Todd W. Neller"],"1088":["Mike Cooley"],"1089":["Jocelyn Maclure"],"1090":["Sara Reese Hedberg"],"1091":["Wai K. Yeap"],"1092":["Eduardo Alonso"],"1093":["Brian C. Williams","P. Pandurang Nayak"],"1094":["Louis Armand"],"1095":["Ajit Narayanan"],"1096":["Amy McGovern","Eric Eaton"],"1097":["Joshua Eckroth"],"1098":["Daniela Petrelli","Aba-Sah Dadzie","Vitaveska Lanfranchi"],"1099":["David Leake"],"1100":["Martin Ford"],"1101":["F. Menczer","L.-S. Wu","R. Akavipat"],"1102":["Ben Goertzel"],"1103":["Sebastian Raschka","Benjamin Kaufman"],"1104":["Ken Dickey"],"1105":["Katia P. Sycara"],"1106":["Madeleine Bates"],"1107":["David Leake"],"1108":["Deepak Khemani"],"1109":["Ferdinando Fioretto","William Yeoh"],"1110":["Amy McGovern","Eric Eaton"],"1111":["Jacobijn Sandberg","Yvonne Barnard"],"1112":["Edona Elshan","Dominik Siemon","Triparna de Vreede","Gert-Jan de Vreede","Sarah Oeste-Rei\u00df","Philipp Ebel"],"1113":["Peter Jarvis","Jonathan Moore","Jussi Stader","Ann Macintosh","Paul Chung"],"1114":["Sara Salimzadeh","Gaole He","Ujwal Gadiraju"],"1115":["Drew V. McDermott"],"1116":["Luciano Serafini","Paolo Bouquet"],"1117":["Graeme D. Ritchie","F. K. Hanna"],"1118":["Yoshihiro Maruyama"],"1119":["Sanket Godbole","Jaivardhan Shelke"],"1120":["Max et al. Tegmark"],"1121":["Daniel Dennett"],"1122":["Klaus Mainzer"],"1123":["Q. Vera Liao","Daniel Gruen","Sarah Miller"],"1124":["Luciano Floridi","Josh Cowls","Monica Beltrametti","Raja Chatila","Patrice Chazerand","Virginia Dignum","Christoph Luetge","Robert Madelin","Ugo Pagallo","Francesca Rossi","Burkhard Schafer","Peggy Valcke","Effy Vayena"],"1125":["Sam Freed"],"1126":["Jay M. Tenenbaum","Jeff Shrager"],"1127":["Eric Eaton","Amy McGovern"],"1128":["Bradley L. Whitehall"],"1129":["Carlos S\u00e1nchez Quintana","Francisco Moreno Arcas","David Albarrac\u00edn Molina","Jose David Fern\u00e1ndez Rodriguez","Francisco J. Vico"],"1130":["Peter Jarvis","Teresa F. Lunt","Karen L. Myers"],"1131":["Harry Bovik","Sergey Shishkin","Yuri Smirnov"],"1132":["David Davenport"],"1133":["Philipp Reinhard","Mahei Manhai Li","Christoph Peters","Jan Marco Leimeister"],"1134":["Nithya Sambasivan","Rajesh Veeraraghavan"],"1135":["S. J. J. Smith","D. Nau","T. Throop"],"1136":["Alan Bundy"],"1137":["Roger C. Schank"],"1138":["Tom Mitchell"],"1139":["Eric Eaton","Amy McGovern"],"1140":["Eric Eaton","Amy McGovern"],"1141":["Eric Eaton","Amy McGovern"],"1142":["Mich\u00e8le Sebag"],"1143":["Eric Eaton"],"1144":["Stephen J. J. Smith","Dana S. Nau","Thomas A. Throop"],"1145":["Bruce G. Buchanan","Joshua Eckroth","Reid G. Smith"],"1146":["Stephen J. DeCanio"],"1147":["Ian H. Witten","Bruce A. MacDonald","David Maulsby","Rosanna Heise"],"1148":["Federico Chesani","Paola Mello","Michela Milano"],"1149":["Kenneth D. Forbus"],"1150":["Ram\u00f3n L\u00f3pez de M\u00e1ntaras","Josep Llu\u00eds Arcos"],"1151":["Craig Schlenoff","James S. Albus","Elena Messina","Anthony Barbera","Raj Madhavan","Stephen Balakirsky"],"1152":["Eric Eaton","Kiri L. Wagstaff"],"1153":["Hang Ma","Sven Koenig"],"1154":["Derek Partridge"],"1155":["Oliver G. Selfridge"],"1156":["Jay M. Tenenbaum","Jeff Shrager"],"1157":["Tom Fawcett","Ira J. Haimowitz","Foster J. Provost","Salvatore J. Stolfo"],"1158":["William C. Hill"],"1159":["Jacques Chelin","Leila Kosseim","Thiruvengadam Radhakrishnan"],"1160":["Hyunggu Jung"],"1161":["Richard Hugh Moulton"],"1162":["Nitin Kumar","Maheep Singh","Mahesh Chandra Govil","Emmanuel S. Pilli","Ajay Jaiswal"],"1163":["Mohammad Etemad","Nader Zare","Mahtab Sarvmaili","Am\u00edlcar Soares","Bruno Brandoli Machado","Stan Matwin"],"1164":["Yiming Qian","Minglun Gong","Li Cheng"],"1165":["Teresa Gon\u00e7alves","Fernando Moura-Pires"],"1166":["Federico Heras","Ant\u00f3nio Morgado","Jo\u00e3o Marques-Silva"],"1167":["Wegdan Abdelsalam","David Chiu","Siu-Cheung Chau","Yasser Ebrahim","Maher Ahmed"],"1168":["Zequn Zhou","C. I. Ezeife"],"1169":["Saeed Hashemi"],"1170":["Michael E. Bergen","Peter van Beek","Tom Carchrae"],"1171":["Joe MacInnes","Omid Banyasad","Muhammad Afzal Upal"],"1172":["Nathalie Japkowicz"],"1173":["Fangfang Liu","Yi Bi","Md. Solimul Chowdhury","Jia-Huai You","Zhiyong Feng"],"1174":["Rong Xiang","Emmanuele Chersoni","Yunfei Long","Qin Lu","Chu-Ren Huang"],"1175":["Alexandre Parmentier","Robin Cohen"],"1176":["Khalid Moustapha Askia","Marie-Jean Meurs"],"1177":["Nabil Belacel","Cheng Duan","Diana Inkpen"],"1178":["Sushrut Bhalla","Sriram Ganapathi Subramanian","Mark Crowley"],"1179":["Kaixuan Zhang","Qinglong Wang","C. Lee Giles"],"1180":["Chudamani Aryal","Yllias Chali"],"1181":["Margarita P. Castro","Meinolf Sellmann","Zhaoyuan Yang","Nurali Virani"],"1182":["Fatma Najar","Nizar Bouguila"],"1183":["Xinyu Yun","Tanner A. Bohn","Charles X. Ling"],"1184":["Aijun An","Nick Cercone"],"1185":["Bengt R. Knudsen"],"1186":["Steffen Staab","Udo Hahn"],"1187":["Sajjad Hussain"],"1188":["Calin Anton","Lane Olson"],"1189":["Sehl Mellouli","Guy W. Mineau","Bernard Moulin"],"1190":["Jacky Baltes"],"1191":["Dong Song","Anoop Sarkar"],"1192":["Scott M. Brown","Eugene Santos Jr.","Sheila B. Banks"],"1193":["Guy W. Mineau","Mounsif Lahboub","Jean-Marie Beaulieu"],"1194":["Charles X. Ling","Jin Huang","Harry Zhang"],"1195":["Yingying She","Peter Grogono"],"1196":["Wei Gu","Xin Wang","Danielle Zi\u00e9belin"],"1197":["Jeff Taylor"],"1198":["Keping Jia","Bruce Spencer"],"1199":["George V. Lashkia","Laurence Anthony"],"1200":["Denton Cockburn","Ziad Kobti"],"1201":["Haibin Zhu"],"1202":["Xing Tan"],"1203":["Maher A. Alhossaini","J. Christopher Beck"],"1204":["Daniel L. Silver","Liangliang Tu"],"1205":["Christian J. Muise","Sheila A. McIlraith","J. Christopher Beck","Eric I. Hsu"],"1206":["Aminul Islam","Evangelos E. Milios","Vlado Keselj"],"1207":["Xiangrui Wang","Narendra S. Chaudhari"],"1208":["Fang-Xiang Wu","Wen-Jun Zhang","Anthony J. Kusalik"],"1209":["Pawan Lingras","Rui Yan","Chad West"],"1210":["Alistair Kennedy","Anna Kazantseva","Diana Inkpen","Stan Szpakowicz"],"1211":["Yang Xiang","Franklin Hanshar"],"1212":["Dunwei Wen","John Cuzzola","Lorna M. Brown","Kinshuk"],"1213":["Khalid Mansour"],"1214":["Xiangji Huang","Fuchun Peng","Aijun An","Dale Schuurmans","Nick Cercone"],"1215":["Dequan Zhou","Bin Wang","Seyyed Mohammadreza Rahimi","Xin Wang"],"1216":["Fletcher Lu","Dale Schuurmans"],"1217":["Luis E. Da Costa","Jacques-Andr\u00e9 Landry"],"1218":["Zhongmin Shi","Evangelos E. Milios","A. Nur Zincir-Heywood"],"1219":["Lisa Gaudette","Nathalie Japkowicz"],"1220":["William Elazmeh"],"1221":["Michael Janzen","Yang Xiang"],"1222":["Bin Wang","Bruce Spencer","Charles X. Ling","Harry Zhang"],"1223":["Daniel Cabrera","Claudio Cubillos"],"1224":["Naveed Afzal","Ruslan Mitkov","Atefeh Farzindar"],"1225":["Ahmed Amrani","Mathieu Roche","Yves Kodratoff","Oriane Matte-Tailliez"],"1226":["Othalia Larue"],"1227":["Thang M. Do","Seng Wai Loke","Fei Liu"],"1228":["Patrick C. Connor","Thomas P. Trappenberg"],"1229":["Machel Higgins","Christopher Ward","Silvio De Angelis"],"1230":["Ricardo Sousa Silvestre","Tarcisio H. C. Pequeno"],"1231":["Yang Xiang","Franklin Hanshar"],"1232":["Alistair Kennedy","Stan Szpakowicz"],"1233":["Colette Joubarne","Diana Inkpen"],"1234":["M. Mahdi Shafiei","Evangelos E. Milios"],"1235":["Guichong Li","Nathalie Japkowicz","Trevor J. Stocki","R. Kurt Ungar"],"1236":["Ehsan Mokhtari","Zeinab Noorian","Behrouz Tork Ladani","Mohammad Ali Nematbakhsh"],"1237":["Ming Su","Elizabeth Thompson"],"1238":["Wonil Kim","Han-Ku Lee","Jinman Park","Kyoungro Yoon"],"1239":["Hui Li","Nathalie Japkowicz","Caroline Barri\u00e8re"],"1240":["Saeede Sadat Asadi Kakhki","Can Kavaklioglu","Ayse Bener"],"1241":["Nabil Abdullah","Richard A. Frost"],"1242":["Yang Xiang","Benjamin Baird"],"1243":["Tao Chen","Jeffrey Parsons"],"1244":["Aliakbar Gorji Daronkolaei","Amir Hajian","Tonya Custis"],"1245":["Peng Xu","Denilson Barbosa"],"1246":["Luc Plamondon","Leila Kosseim"],"1247":["Golha Sharifi","Julita Vassileva","Ralph Deters"],"1248":["Sung Baik","Ju Cho","Jerzy W. Bala"],"1249":["Xin Wang","Howard J. Hamilton"],"1250":["Silvia Breban","Julita Vassileva"],"1251":["Abbas Taher"],"1252":["Daniel L. Silver","Robert E. Mercer"],"1253":["Yang Xiang","Chenwen Ye"],"1254":["S. K. Michael Wong","Tao Lin","Dan Wu"],"1255":["Gary William Grewal","Thomas Charles Wilson","Christopher W. Nell"],"1256":["Michael C. Horsch","William S. Havens","Aditya Ghose"],"1257":["Alan Fedoruk","Ralph Deters"],"1258":["Michael Procter","Fuhua Lin","Robert Heller"],"1259":["Laurent Jakubina","Philippe Langlais"],"1260":["Ala'a Alslaity","Thomas Tran"],"1261":["Fabrizio Gotti","Philippe Langlais"],"1262":["Joshua D. A. Jung","Jesse Hoey","Jonathan H. Morgan","Tobias Schr\u00f6der","Ingo Wolf"],"1263":["Rodrigo Toro Icarte","Toryn Q. Klassen","Richard Anthony Valenzano","Sheila A. McIlraith"],"1264":["Yingcong Tan","Daria Terekhov"],"1265":["Mathias Sall\u00e9"],"1266":["Matthew Dirks"],"1267":["M. Ali Akber Dewan","Dan Qiao","Fuhua Lin","Dunwei Wen","Kinshuk"],"1268":["Shervin Shahryari","Cameron R. Hamilton"],"1269":["Hayda Almeida","Ludovic Jean-Louis","Marie-Jean Meurs"],"1270":["Bernhard Heinemann"],"1271":["Bruno Bouchard","Sylvain Giroux","Abdenour Bouzouane"],"1272":["Wojdan Alsaeedan","Mohamed El Bachir Menai"],"1273":["Hossein Fani","Fattane Zarrinkalam","Ebrahim Bagheri","Weichang Du"],"1274":["Peter Harvey","Chee Fon Chang","Aditya Ghose"],"1275":["Yang Yu","Yun Peng"],"1276":["Michael Wachter","Rolf Haenni"],"1277":["Alireza Shaneh","Gregory Butler"],"1278":["Allan Caine","Robin Cohen"],"1279":["Masoumeh T. Izadi","Doina Precup","Danielle Azar"],"1280":["Diego C. Mart\u00ednez","Alejandro Javier Garc\u00eda","Guillermo Ricardo Simari"],"1281":["Michael Y. K. Cheng","Robin Cohen"],"1282":["Dan Wu","Liu He"],"1283":["Robert D. Vincent","Joelle Pineau","Philip de Guzman","Massimo Avoli"],"1284":["Salem Benferhat","Salma Smaoui"],"1285":["Murat Ekinci","Murat Aykut"],"1286":["Han Liang","Yuhong Yan"],"1287":["Ken Barker","Nadia Cornacchia"],"1288":["Jeff Bowes","Eric Neufeld","Jim E. Greer","John Cooke"],"1289":["Yifeng Li"],"1290":["Bernard Moulin","Driss Kettani","Benjamin Gauthier","Walid Chaker"],"1291":["Christopher Armbrust","Thorsten Ropertz","Lisa Kiekbusch","Karsten Berns"],"1292":["Sakshi Babbar","Didi Surian","Sanjay Chawla"],"1293":["Christopher A. Brooks","G. Scott Johnston","Craig Thompson","Jim E. Greer"],"1294":["Cory J. Butz","Wen Yan","Anders L. Madsen"],"1295":["Shan Zong","Richard Khoury","Rachid Benlamri"],"1296":["Rushdi Shams"],"1297":["Colin Lee","Peter van Beek"],"1298":["A. N. K. Zaman","Charlie Obimbo","Rozita A. Dara"],"1299":["Terran Lane","Martin Ridens","Scott Stevens"],"1300":["Peter Lach"],"1301":["Marek Lipczak","Tomasz Niewiarowski","Vlado Keselj","Evangelos E. Milios"],"1302":["Sourodeep Bhattacharjee","Scott D. Goodwin"],"1303":["Mina Yousefi","Adam Krzyzak","Ching Y. Suen"],"1304":["Bruno Moreno","Am\u00edlcar Soares J\u00fanior","Val\u00e9ria Ces\u00e1rio Times","Patr\u00edcia C. A. R. Tedesco","Stan Matwin"],"1305":["Elnaz Bigdeli","Mahdi Mohammadi","Bijan Raahemi","Stan Matwin"],"1306":["Sylvain Labranche","Eric Beaudry"],"1307":["Ahmad Soleimani","Ziad Kobti"],"1308":["Josh Weissbock","Diana Inkpen"],"1309":["Morteza Mashayekhi","Robin Gras"],"1310":["Jia Qiu","Ruisheng Wang","Xin Wang"],"1311":["Jason P. Rhinelander","Mathew Kallada","Pawan Lingras"],"1312":["Daniel L. Silver","Geoffrey Mason","Lubna Eljabu"],"1313":["Ti Wang","Daniel L. Silver"],"1314":["Shubhashis Kumar Shil","Samira Sadaoui"],"1315":["Gabriel Murray"],"1316":["Houman Abbasian","Chris Drummond","Nathalie Japkowicz","Stan Matwin"],"1317":["Rodolfo Garcia","Emerson Cabrera Paraiso","J\u00falio C\u00e9sar Nievola"],"1318":["Henrique F. Lacerda","Allan R. S. Feitosa","Abel G. Silva-Filho","Wellington P. dos Santos","Filipe R. Cordeiro"],"1319":["Akash Patel","Brigitte Jaumard"],"1320":["Mehdi Yousfi Monod","Atefeh Farzindar","Guy Lapalme"],"1321":["Yunlong Liu","Guoli Ji","Zijiang Yang"],"1322":["Khoa Luu"],"1323":["Ahmed Hussein","Eugene Santos Jr."],"1324":["Yang Jun","Elhadi M. Shakshuki"],"1325":["Mikhail Jiline","Stan Matwin","Marcel Turcotte"],"1326":["Martha E. Pollack"],"1327":["Marina Sokolova","Stan Szpakowicz","Vivi Nastase"],"1328":["Syed Sibte Raza Abidi","Yong Han Chong"],"1329":["Qi Zou","Siwei Luo"],"1330":["Richong Zhang","Thomas T. Tran"],"1331":["Andrew Fall"],"1332":["John Champaign"],"1333":["Evangelos E. Milios"],"1334":["Shenshen Gu","Songnian Yu"],"1335":["Yongzheng Zhang","A. Nur Zincir-Heywood","Evangelos E. Milios"],"1336":["John A. Fitzgerald","Franz Geiselbrechtinger","M. Tahar Kechadi"],"1337":["John Zhong Lei","Ali A. Ghorbani"],"1338":["Ron Coleman","Matthew A. Johnson"],"1339":["Fei Chen","Daming Shi","Geok See Ng"],"1340":["Wei Liu","William S. Havens"],"1341":["Heejin Lim","John Yen"],"1342":["Liqiang Geng","Howard J. Hamilton"],"1343":["Katia Dilkina","Fred Popowich"],"1344":["Hong Yao"],"1345":["Esma A\u00efmeur","Gilles Brassard","S\u00e9bastien Gambs"],"1346":["Sang-Woon Kim","B. John Oommen"],"1347":["J\u00e9r\u00e9my Charlier","Radu State","Jean Hilger"],"1348":["Sultan Ahmed","Malek Mouhoub"],"1349":["Haoran Zhang","Jagadish Rangrej","Saad Rais","Michael Hillmer","Frank Rudzicz","Kamil Malikov"],"1350":["Hyeju Jang","Young Ji Lee","Giuseppe Carenini","Raymond T. Ng","Grace Campbell","Kendall Ho"],"1351":["David B. Skillicorn","Nasser Alsadhan","Richard Billingsley","Mary-Anne Williams"],"1352":["Ting-Yuan Lin","Jeng-Sheng Yeh","Fu-Che Wu","Yung-Yu Chuang","Andrew Dellinger"],"1353":["Luis Da Costa","Jean-Fran\u00e7ois Rajotte"],"1354":["Cory J. Butz","Andr\u00e9 E. dos Santos","Jhonatan de S. Oliveira","Anders L. Madsen"],"1355":["Mohamed Amine Menacer","David Langlois","Denis Jouvet","Dominique Fohr","Odile Mella","Kamel Sma\u00efli"],"1356":["Yoshimasa Kubo","Thomas P. Trappenberg"],"1357":["Matthew T. Mann","Howard J. Hamilton"],"1358":["Farzana Anowar","Samira Sadaoui"],"1359":["Benyamin Ghojogh"],"1360":["Roberto Gozalo-Brizuela","Eduardo C. Garrido-Merchan"],"1361":["Kaira Sekiguchi","Koichi Hori"],"1362":["Shari Trewin","Sara Basson","Michael J. Muller","Stacy Branham","Jutta Treviranus","Daniel M. Gruen","Daniel Hebert","Natalia Lyckowski","Erich Manser"],"1363":["Michael Guerzhoy","Lisa Zhang","Georgy Noarov"],"1364":["Jian Wu","Kyle Mark Williams","Hung-Hsuan Chen","Madian Khabsa","Cornelia Caragea","Suppawong Tuarob","Alexander Ororbia","Douglas Jordan","Prasenjit Mitra","C. Lee Giles"],"1365":["Lloyd G. Greenwald","Donovan Artz","Yogi Mehta","Babak Shirmohammadi"],"1366":["Glen Robertson","Ian D. Watson"],"1367":["Steven N. Minton"],"1368":["Anil Rewari","Mark Adler","Peter G. Anick","Meyer A. Billmers","Mike Carifio","Alan Gunderson","Neil Pundit","Mark W. Swartwout"],"1369":["Anthony Jameson","Aaron Spaulding","Neil Yorke-Smith"],"1370":["Jatin Pancholi"],"1371":["Eric Eaton","Amy McGovern"],"1372":["Huang Guoxing","Nicolaas J. I. Mars"],"1373":["Jagjit Singh Srai"],"1374":["Manish Verma"],"1375":["Yosef Ashibani","Qusay H. Mahmoud"],"1376":["Fabrizio Gotti","Philippe Langlais"],"1377":["Mashrura Tasnim","Eleni Stroulia"],"1378":["Andrea Pagotto","Patrick Littell","Yunli Wang","Cyril Goutte"],"1379":["Kalyani Selvarajah"],"1380":["J\u00e9r\u00e9my Foxcroft","Adrian d'Alessandro","Luiza Antonie"],"1381":["Jean-Thomas Baillargeon","Luc Lamontagne","\u00c9tienne Marceau"],"1382":["Mark Thomas"],"1383":["Muhammad Usman Arif"],"1384":["Shady A. Mohammed"],"1385":["Xiaoyu Yang","Xiaodan Zhu","Huasha Zhao","Qiong Zhang","Yufei Feng"],"1386":["Michael Guerzhoy","Aaron Hertzmann"],"1387":["Qihua Situ","Eleni Stroulia"],"1388":["Rayhan Shikder","Pourang Irani","Pingzhao Hu"],"1389":["Philippe Langlais","Guy Lapalme","S\u00e9bastien Sauv\u00e9"],"1390":["Shawn Grant","Gordon I. McCalla"],"1391":["Z. M. Ma","Wen-Jun Zhang","Weiyin Ma"],"1392":["Patrick Pantel","Dekang Lin"],"1393":["Adam Beacham","Xinguang Chen","Jonathan Sillito","Peter van Beek"],"1394":["Rabih Neouchi","Ahmed Y. Tawfik","Richard A. Frost"],"1395":["Hassan Khosravi","Bahareh Bina"],"1396":["Maciej Piasecki","Bartosz Broda","Michal Marcinczuk","Stan Szpakowicz"],"1397":["Victoria Bobicev","Marina Sokolova","Michael P. Oakes"],"1398":["Daniel S. Ferguson","Pantelis Elinas"],"1399":["Bushra Khawaja","Lisa Fan"],"1400":["Jigang Luo","Yiyu Yao"],"1401":["Amir Aavani","Xiongnan (Newman) Wu","Eugenia Ternovska","David G. Mitchell"],"1402":["Fran\u00e7ois Laviolette","Mario Marchand","Sara Shanian"],"1403":["Alan Davoust","Michael W. Floyd","Babak Esfandiari"],"1404":["Ahmad El Sayed","Julien Velcin","Djamel A. Zighed"],"1405":["Nuha Zamzami","Nizar Bouguila"],"1406":["Ashwin Panchapakesan","Rami S. Abielmona","Rafael Falcon","Emil M. Petriu"],"1407":["Ala'a Alslaity"],"1408":["Yllias Chali"],"1409":["Kevin Kennedy","Robert E. Mercer"],"1410":["Chris Drummond"],"1411":["Guan-Shieng Huang","Xiumei Jia","Churn-Jung Liau","Jia-Huai You"],"1412":["S. K. Michael Wong","Dan Wu","Tao Lin"],"1413":["Tamarafinide V. Dittimi","Ching Y. Suen"],"1414":["Elhadi M. Shakshuki","P. Kajonpotisuwan"],"1415":["Kamran Karimi","Howard J. Hamilton"],"1416":["Richard A. Frost","Pierre Boulos"],"1417":["Peter Yap"],"1418":["Scott A. DeLoach"],"1419":["Kun Wu","William S. Havens"],"1420":["Cr\u00edcia Z. Fel\u00edcio","Claudianne M. M. de Almeida","Guilherme Alves","Fabiola S. F. Pereira","Kl\u00e9risson Vin\u00edcius Ribeiro Paix\u00e3o","Sandra de Amo"],"1421":["Chao Bian","Shehroz S. Khan","Alex Mihailidis"],"1422":["Yingcong Tan"],"1423":["Dan Russell","Aaron Hunter"],"1424":["Arya Rahgozar","Diana Inkpen"],"1425":["Ahmad Soleimani","Ziad Kobti"],"1426":["Suling Yang","Alan K. Mackworth"],"1427":["Murlikrishna Viswanathan"],"1428":["Cory J. Butz","Andr\u00e9 E. dos Santos","Jhonatan de S. Oliveira","Christophe Gonzales"],"1429":["Mohomed Shazan Mohomed Jabbar","Osmar R. Za\u00efane"],"1430":["Chattrakul Sombattheera","Aditya Ghose"],"1431":["Michel Gagnon","Lyne Da Sylva"],"1432":["Yan Zhao","Yiyu Yao"],"1433":["Sehl Mellouli"],"1434":["Michel Nathan"],"1435":["Yang Xiang","Wanling Zhang"],"1436":["Dave A. D. Tompkins","Holger H. Hoos"],"1437":["Yan Zhao","Yiyu Yao","Mingwu Yan"],"1438":["Theresa Jickels","Grzegorz Kondrak"],"1439":["Jean-Fran\u00e7ois Paiement","Douglas Eck","Samy Bengio"],"1440":["Gernot Stenz","Andreas Wolf"],"1441":["Jianjun Yan","Naoyuki Tokuda","Juichi Miyamichi"],"1442":["Robert C. Holte"],"1443":["Petr Kubon","Fred Popowich","Gordon Tisher"],"1444":["Jianchao Han","Nick Cercone"],"1445":["Yannick Lallement","Mark S. Fox"],"1446":["Scott McDonald","Davide Turcato","Paul McFetridge","Fred Popowich","Janine Toole"],"1447":["Matthew Smith","Laurent Charlin","Joelle Pineau"],"1448":["Victoria Bobicev","Marina Sokolova"],"1449":["Erico N. de Souza","Stan Matwin"],"1450":["Aaron Hunter"],"1451":["Amani T. Jamal","Ching Y. Suen"],"1452":["Sultan Ahmed"],"1453":["Xiang Ji","Shrinu Kushagra","Jeff Orchard"],"1454":["Ki Hyang Lee","Scott Buffett","Michael W. Fleming"],"1455":["Yu Zhang"],"1456":["Ali A. Ghorbani","Lila Bayat"],"1457":["Marina Sokolova","Guy Lapalme"],"1458":["Zhongmin Shi","Gabor Melli","Yang Wang","Yudong Liu","Baohua Gu","Mehdi M. Kashani","Anoop Sarkar","Fred Popowich"],"1459":["Xin Liang","Zuoquan Lin","Jan Van den Bussche"],"1460":["Madeena Sultana"],"1461":["Hakim Hacid","Tetsuya Yoshida"],"1462":["Ralf Krestel","Ren\u00e9 Witte","Sabine Bergler"],"1463":["Shehroz S. Khan"],"1464":["Shane Bergsma","Regan L. Mandryk","Gordon McCalla"],"1465":["Khantil Patel","Orland Hoeber","Howard J. Hamilton"],"1466":["Sultan Ahmed","Malek Mouhoub"],"1467":["Mohammed Alliheedi","Chrysanne Di Marco"],"1468":["John A. Doucette"],"1469":["Serguei A. Mokhov","Joey Paquet","Mourad Debbabi"],"1470":["Mohammed Shameer Iqbal"],"1471":["Robert C. Kremer","Roberto A. Flores"],"1472":["Martin Scaiano","Diana Inkpen","Robert Lagani\u00e8re","Adele Reinhartz"],"1473":["Tom Lebrun"],"1474":["Elnaz Davoodi"],"1475":["Eric J. Friedman","Mikl\u00f3s Z. R\u00e1cz","Scott Shenker"],"1476":["Qingliang Chen","Kaile Su","Yong Hu","Guiwu Hu"],"1477":["Spencer Polk"],"1478":["Vahid Vaezian","James P. Delgrande"],"1479":["Bertrand Sodjahin","Vivekanandan Suresh Kumar","Shawn Lewenza","Shauna Reckseidler-Zenteno"],"1480":["Gabriel Murray"],"1481":["Marina Sokolova","Khaled El Emam","Sadrul Chowdhury","Emilio Neri","Sean Rose","Elizabeth Jonker"],"1482":["Chris Thornton"],"1483":["Mark D. Pendrith","Malcolm R. K. Ryan"],"1484":["Riverson Rios","Stan Matwin"],"1485":["Craig D. S. Thompson"],"1486":["Charles Elkan"],"1487":["Karan Bhatia","Charles Elkan"],"1488":["Tiffany Ya Tang","Gordon I. McCalla"],"1489":["Takeshi Koshiba"],"1490":["Amir Hossein Razavi","Diana Inkpen","Sasha Uritsky","Stan Matwin"],"1491":["Jennifer Lee"],"1492":["Dmitry O. Gorodnichy","Richard Hoshino"],"1493":["Fred Popowich"],"1494":["Misha Denil","Thomas P. Trappenberg"],"1495":["Andr\u00e9 Trudel"],"1496":["Mohamed Aounallah","S\u00e9bastien Quirion","Guy W. Mineau"],"1497":["Ryan Wegner","John Anderson"],"1498":["Oliver Schulte","Gustavo Frigo","Russell Greiner","Hassan Khosravi"],"1499":["Daniel Charlebois","David G. Goodenough","Stan Matwin","A. S. (Pal) Bhogal","Hugh Barclay"],"1500":["Calin Anton","Christopher Neal"],"1501":["Xiaowang Zhang","Zhihu Zhang","Dai Xu","Zuoquan Lin"],"1502":["Maxim Roy","Fred Popowich"],"1503":["Alexandre Kouznetsov","Nathalie Japkowicz"],"1504":["Jianning Wang","Charles X. Ling"],"1505":["Kyoung Min Kim","Joong Jo Park","Myung Hyun Song","In-Cheol Kim","Ching Y. Suen"],"1506":["Eric J. Mulvaney","Scott D. Goodwin"],"1507":["Robert E. Mercer","Chrysanne DiMarco","Frederick W. Kroon"],"1508":["Michael W. Fleming"],"1509":["Yue Shi","Richard A. Frost"],"1510":["Roberto A. Flores","Robert C. Kremer"],"1511":["Wayne J. Pullan","Liang Zhao"],"1512":["Lisa Gaudette","Nathalie Japkowicz"],"1513":["Maxim Roy"],"1514":["Hamidreza Baghi","Yevgen Biletskiy"],"1515":["Richard Nock","Babak Esfandiari"],"1516":["Olga Acosta"],"1517":["Alan K. Mackworth"],"1518":["Cristina E. Manfredotti"],"1519":["Mingyan Huang","Zhiyong Liu","Scott D. Goodwin"],"1520":["Jian Zhang","Howard J. Hamilton"],"1521":["Marc Sebban","Anne M. Landraud"],"1522":["Yang Xiang","Zoe Jingyu Zhu","Yu Li"],"1523":["Maria Fernanda Caropreso","Diana Inkpen","Shahzad Khan","Fazel Keshtkar"],"1524":["Marina Sokolova","Guy Lapalme"],"1525":["Ken Barker"],"1526":["Brett Hodges","Larry Hodgson"],"1527":["Salsabil Trabelsi","Zied Elouedi","Pawan Lingras"],"1528":["Md. Shafiul Alam","Scott D. Goodwin"],"1529":["Kamran Karimi"],"1530":["Tom M. Mitchell"],"1531":["Bozhena Bidyuk","Rina Dechter"],"1532":["Richard A. Baldwin","Eric Neufeld"],"1533":["Leila Kosseim","Luc Plamondon","Louis-Julien Guillemette"],"1534":["Man Hon Lo","Kwok Yip Szeto"],"1535":["Othalia Larue","Pierre Poirier","Roger Nkambou"],"1536":["Daniel G. Schwartz","Stanislav Ustymenko"],"1537":["Martin Scaiano"],"1538":["Dave Carter","Diana Inkpen"],"1539":["Ron Coleman"],"1540":["Victoria Bobicev","Marina Sokolova","Yasser Jafer","David Schramm"],"1541":["Baskaran Sankaran","Majid Razmara","Atefeh Farzindar","Wael Khreich","Fred Popowich","Anoop Sarkar"],"1542":["Zeinab Noorian","Mahdi Noorian","Michael Fleming","Stephen Marsh"],"1543":["Craig D. S. Thompson","Michael C. Horsch"],"1544":["Pascal Soucy","Guy W. Mineau"],"1545":["Victor R. Lesser"],"1546":["Shane Bergsma"],"1547":["Quintin Armour","William Elazmeh","Nour El-Kadri","Nathalie Japkowicz","Stan Matwin"],"1548":["Daniel L. Silver","Liangliang Tu"],"1549":["Marina Sokolova","Guy Lapalme"],"1550":["Jonathan Schuman","Sabine Bergler"],"1551":["Ziad Kobti","Shamual F. Rahaman","Anne W. Snowdon","Robert D. Kent"],"1552":["Xin Wang","Howard J. Hamilton"],"1553":["Alexandre Patry","Philippe Langlais"],"1554":["Tianqiang Huang","Xiaolin Qin","Qinmin Wang","Chongcheng Chen"],"1555":["Leon French","Alioune Ngom","Luis Rueda"],"1556":["Zhengya Sun","Wei Jin","Jue Wang"],"1557":["Kinfe Tadesse Mengistu","Frank Rudzicz"],"1558":["Maryam Khordad","Robert E. Mercer","Peter K. Rogan"],"1559":["Roger Nkambou","Engelbert Mephu Nguifo","Olivier Couturier","Philippe Fournier-Viger"],"1560":["Pierrick Plamondon","Brahim Chaib draa","Abder Rezak Benaskeur"],"1561":["Julien Laumonier","Brahim Chaib draa"],"1562":["Hamid R. Chinaei","Brahim Chaib draa"],"1563":["Mohammad Etemad","Zahra Etemad","Am\u00edlcar Soares","Vania Bogorny","Stan Matwin","Lu\u00eds Torgo"],"1564":["Armin Sajadi"],"1565":["Eric Charton","Michel Gagnon","Beno\u00eet Ozell"],"1566":["Xuan Liu","Xiaoguang Wang","Nathalie Japkowicz","Stan Matwin"],"1567":["Paulo Gomes","Francisco C. Pereira","Paulo Paiva","Nuno Seco","Paulo Carreiro","Jos\u00e9 Lu\u00eds Ferreira","Carlos Bento"],"1568":["Yue Gu","Xinyu Li","Shuhong Chen","Jianyu Zhang","Ivan Marsic"],"1569":["Ramakanth Kavuluru","Sifei Han","Daniel R. Harris"],"1570":["Shikhar Sakhuja","Robin Cohen"],"1571":["Maryam Tajeddin"],"1572":["Sukhjit Singh Sehra","Tamer Abdou","Ayse Basar","Sumeet Kaur Sehra"],"1573":["Hadi Jahanshahi","Mucahit Cevik","Ayse Basar"],"1574":["Rishav Raj Agarwal","Robin Cohen","Lukasz Golab","Alan Tsang"],"1575":["Michael Roher","Yang Xiang"],"1576":["Mohammad S. Jassas","Qusay H. Mahmoud"],"1577":["J\u00e9r\u00e9my Charlier","Vladimir Makarenkov"],"1578":["Shainen M. Davidson","Kenton White"],"1579":["Meetkumar Patel"],"1580":["Sedigheh Mahdavi","Aijun An","Heidar Davoudi","Marjan Delpisheh","Emad Gohari"],"1581":["Ayman Yafoz"],"1582":["Hawre Hosseini","Ebrahim Bagheri"],"1583":["Valery Anisimovskiy","Andrey Shcherbinin","Sergey Turko","Ilya Kurilin"],"1584":["Qingliang Chen","Xiaowei Huang","Kaile Su","Abdul Sattar"],"1585":["Bing Zhou","Yiyu Yao","Jigang Luo"],"1586":["Piotr Boltuc"],"1587":["R. Peter Bonasso"],"1588":["Nabil Belacel","Feras N. Al-Obeidat"],"1589":["Benyamin Ghojogh","Fakhri Karray","Mark Crowley"],"1590":["Elham Mohammadi","Nada Naji","Louis Marceau","Marc Queudot","Eric Charton","Leila Kosseim","Marie-Jean Meurs"],"1591":["Sara Hosseinzadeh Kassani","Peyman Hosseinzadeh Kassani","Michal J. Wesolowski","Kevin A. Schneider","Ralph Deters"],"1592":["Hamed H. Aghdam","Martin Bouchard","Robert Lagani\u00e8re","Emil M. Petriu","Philip Wort"],"1593":["Igor Ilic","Berk G\u00f6rg\u00fcl\u00fc","Mucahit Cevik"],"1594":["Nilay Jha","Dhruv Parekh","Malek Mouhoub","Varun Makkar"],"1595":["Dennis J. Drown","Roger Villemaire","Serge Robert"],"1596":["Colin Bellinger","Rory Coles","Mark Crowley","Isaac Tamblyn"],"1597":["Xuejun Han"],"1598":["Xavier Sumba","Nizar Bouguila"],"1599":["Guillaume Le Berre","Philippe Langlais"],"1600":["Rongtian Ye","Fangyu Liu","Liqiang Zhang"],"1601":["Xiaoguang Wang","Xuan Liu","Nathalie Japkowicz","Stan Matwin"],"1602":["Mahbod Tavallaee","Wei Lu","Ebrahim Bagheri","Ali A. Ghorbani"],"1603":["Adel Jebali","Isma\u00efl Biskri","Louisette Emirkanian"],"1604":["Behrouz Haji Soleimani","Stan Matwin"],"1605":["Gary W. King","Tim Oates"],"1606":["Hanh H. Pham","Nguyen Van Hop"],"1607":["Marius-Calin Silaghi","Djamila Sam-Haroud","Boi Faltings"],"1608":["David Nadeau","Nicole Tourigny"],"1609":["Kevin Kennedy","Robert E. Mercer"],"1610":["Mauricio Osorio","Juan Carlos Nieves"],"1611":["Robert C. Holte"],"1612":["Bradley Bart","James P. Delgrande","Oliver Schulte"],"1613":["Bob Price","Craig Boutilier"],"1614":["Ansel Y. Rodr\u00edguez Gonz\u00e1lez","Jos\u00e9 Francisco Mart\u00ednez Trinidad","Jes\u00fas Ariel Carrasco-Ochoa","Jos\u00e9 Ruiz-Shulcloper"],"1615":["Jason M. Bindewald","Gilbert L. Peterson","Michael E. Miller"],"1616":["William S. Havens","Bistra N. Dilkina"],"1617":["Axel J. Soto","Marc Strickert","Gustavo E. Vazquez","Evangelos E. Milios"],"1618":["Hyunggu Jung","Robin Cohen"],"1619":["Md. Mohaiminul Islam","Rasif Ajwad","Chen Chi","Michael Domaratzki","Yang Wang","Pingzhao Hu"],"1620":["Ayangleima Laishram","Vineet Padmanabhan"],"1621":["Sazia Mahfuz"],"1622":["Gashin Ghazizadeh","Mirerfan Gheibi","Stan Matwin"],"1623":["Jos\u00e9e Desharnais","Fran\u00e7ois Laviolette","Krishna Priya Darsini Moturu","Sami Zhioua"],"1624":["Arushi Jain"],"1625":["Kaleb E. Smith","Phillip Williams"],"1626":["Mahtab Ahmed","Robert E. Mercer"],"1627":["Zakaria Soliman","Philippe Langlais","Ludovic Bourg"],"1628":["Diego Maupom\u00e9","Marc Queudot","Marie-Jean Meurs"],"1629":["Okwudili M. Ezeme","Michael Lescisin","Qusay H. Mahmoud","Akramul Azim"],"1630":["Karim El Mokhtari","John Maidens","Ayse Bener"],"1631":["Benyamin Ghojogh","Mark Crowley"],"1632":["Nicholas Denis","Maia Fraser"],"1633":["Bonaventure C. Molokwu"],"1634":["Mohammad Ali Bagheri","Qigang Gao","Sergio Escalera"],"1635":["Michael Demko","Gerald Penn"],"1636":["Hassan Khosravi","Recep Colak"],"1637":["Alexandre Kouznetsov","Stan Matwin","Diana Inkpen","Amir Hossein Razavi","Oana Frunza","Morvarid Sehatkar","Leanne Seaward","Peter O'Blenis"],"1638":["Eduardo P\u00e9rez","Larry A. Rendell"],"1639":["Susan L. Epstein","Jenngang Shih"],"1640":["Jun Liu","Kuo-Chu Chang","Jing Zhou"],"1641":["Charles X. Ling","Bei Zhang"],"1642":["Don Banks","Peter van Beek","Amnon Meisels"],"1643":["Kay C. Wiese","Scott D. Goodwin"],"1644":["Martin Scaiano","Diana Inkpen"],"1645":["Hong Tang","Ali A. Ghorbani"],"1646":["Michael McGarity"],"1647":["Dong-Guk Shin","Lung-Yung Chu"],"1648":["Eugene Fink","Josh Johnson","John Hershberger"],"1649":["Mohammad-Amin Jashki","Majid Makki","Ebrahim Bagheri","Ali A. Ghorbani"],"1650":["Bartosz Broda","Maciej Piasecki","Stan Szpakowicz"],"1651":["Safa Yahi","Salem Benferhat"],"1652":["Jason Morrison","Franz Oppacher"],"1653":["Jean Berger","Martin Salois","Regent Begin"],"1654":["Sorin Draghici"],"1655":["Riverson Rios","Stan Matwin"],"1656":["Roland Becker"],"1657":["Vladimir Alexiev"],"1658":["Marc Peter Deisenroth","Aldo Faisal","Cheng Soon Ong"],"1659":["Kevin P. Murphy"],"1660":["I. H. Witten","Eibe Frank","Mark A. Hall","Christopher J. Pal"],"1661":["Ethem Alpaydin"],"1662":["Matthew N. O. Sadiku","Guddi K. Suman","Sarhan M. Musa"],"1663":["Tom M. Mitchell"],"1664":["Mehryar Mohri","Afshin Rostamizadeh","Ameet Talwalkar"],"1665":["Atilim Gunes Baydin","Barak A. Pearlmutter","Alexey Andreyevich Radul","Jeffrey Mark Siskind"],"1666":["Roland Becker"],"1667":["Jason Bell"],"1668":["Cora Dvorkin","Siddharth Mishra-Sharma","Brian Nord","V. Ashley Villar","Camille Avestruz","Keith Bechtol","Aleksandra \u0106iprijanovi\u0107","Andrew J. Connolly","Lehman H. Garrison","Gautham Narayan","Francisco Villaescusa-Navarro"],"1669":["Joost Verbraeken","Matthijs Wolting","Jonathan Katzy","Jeroen Kloppenburg","Tim Verbelen","Jan S. Rellermeyer"],"1670":["Ms. A. Benazir Begum","Ajith Manoj","Nithya E","Anamika S S","Sneshna"],"1671":["P. Kolari","A. Java","T. Finin","T. Oates","A. Joshi"],"1672":["Luca Oneto","Sandro Ridella","Davide Anguita"],"1673":["Arnd Scharpegge"],"1674":["Manuel Blum","Matthias Feurer","Aaron Klein","Jost Springenberg","Frank Hutter","Katharina Eggensperger"],"1675":["Frank Puppe","Bettina Reinhardt"],"1676":["Drew Conway","John Myles White"],"1677":["Matthew N. O Sadiku","Sarhan M. Musa","Adedamola Omotoso"],"1678":["Martin Lindvall","Jesper Molin"],"1679":["Nakkala Srinivas Mudiraj"],"1680":["Dayne Freitag"],"1681":["Kana Moriwaki","Takahiro Nishimichi","Naoki Yoshida"],"1682":["Kevin P. Murphy"],"1683":["Priya N","Ishita Popli"],"1684":["Nikhil Sharma"],"1685":["Seethal V","Dr. A. Vijayakumar"],"1686":["Giuseppe Carleo","Ignacio Cirac","Kyle Cranmer","Laurent Daudet","Maria Schuld","Naftali Tishby","Leslie Vogt-Maranto","Lenka Zdeborov\u00e1"],"1687":["Porimal Mollik"],"1688":["Matthew D. Schwartz"],"1689":["Joseph Misiti"],"1690":["Oscar Chang","Hod Lipson"],"1691":["Alexander A. Alemi","Ian Fischer"],"1692":["Barida Baah","Onate Egerton Taylor","Chioma Lizzy Nwagbo"],"1693":["Kayalvizhi. K. R","N Kanimozhi"],"1694":["Mohit Chaudhari"],"1695":["Timothy L. Wiemken","Robert R. Kelley"],"1696":["B. Marlin"],"1697":["Atilim Gunes Baydin","Barak A. Pearlmutter","Alexey Andreyevich Radul","Jeffrey Mark Siskind"],"1698":["Sarfraj Alam","Vipul Kumar","Sweta Singh","Sweta Joshi","Madhu Kirola"],"1699":["G. I. Webb","J. Wells","Z. Zheng"],"1700":["Subrata Saha","Md. Motinur Rahman","Md. Mahbub Alam"],"1701":["Sreejith S P","Vijayakumar A"],"1702":["Elad Hazan"],"1703":["Nikolas Wehner"],"1704":["R. S. Parmar","G. B. Chaudhari","S. H. Bhojani"],"1705":["Catherine Wong","Neil Houlsby","Yifeng Lu","Andrea Gesmundo"],"1706":["N. Ramakrishnan"],"1707":["Dr. Prabha Shreeraj Nair"],"1708":["Mehryar Mohri","Afshin Rostamizadeh","Ameet Talwalkar"],"1709":["Dr. R. S. Kamath","Dr. S. S. Jamsandekar","Dr. P. G. Naik"],"1710":["Bernhard Sch\u00f6lkopf"],"1711":["Ravi Kumar Singh","Dr. Kamalraj Ramalingam"],"1712":["Dr. Abhay Kumar Agarwal","Swati Kumari"],"1713":["Prof. Mrs. Dhanamma Jagli","Ms. Pooja Shetty"],"1714":["Ravi Kumar Singh","Dr. A Rengarajan"],"1715":["Pujitha E","Dr. B S Shylaja"],"1716":["Subham Kumar Gupta","Dr. Bhuvana J","Dr. M N Nachappa"],"1717":["Amit Kapoor","Prof. Vinod Mahor"],"1718":["Umang Bhatt","McKane Andrus","Adrian Weller","Alice Xiang"],"1719":["Simon Rogers","Mark Girolami"],"1720":["Harpreet Singh","Er. Ravneet Kaur",""],"1721":["Marcelino Concepcion Collado Jr","Gilbert Malawit Tumibay"],"1722":["Peter Bailis","Kunle Olukotun","Christopher Re","Matei Zaharia"],"1723":["David Rolnick","Priya L. Donti","Lynn H. Kaack","Kelly Kochanski","Alexandre Lacoste","Kris Sankaran","Andrew Slavin Ross","Nikola Milojevic-Dupont","Natasha Jaques","Anna Waldman-Brown","Alexandra Luccioni","Tegan Maharaj","Evan D. Sherwin","S. Karthik Mukkavilli","Konrad P. Kording","Carla Gomes","Andrew Y. Ng","Demis Hassabis","John C. Platt","Felix Creutzig","Jennifer Chayes","Yoshua Bengio"],"1724":["Changyu Deng","Xunbi Ji","Colton Rainey","Jianyu Zhang","Wei Lu"],"1725":["Laurens Martin Tetzlaff"],"1726":["Mangesh Limbitote","Pimpri Chinchwad College of Engineering","Pune"],"1727":["Pedro Domingos"],"1728":["Dawid Kopczyk"],"1729":["Tripti Gautam","Ghanshyam Sahu","Lalit Kumar P. Bhiaya"],"1730":["Prateek Jain","Purushottam Kar"],"1731":["Li-C. Wang","Malgorzata Marek-Sadowska"],"1732":["Vijayalakshmi M M"],"1733":["Dalya Baron"],"1734":["Mylapalle Yeshwanth","Palla Ratna Sai Kumar","Dr. G. Mathivanan M.E. Ph.D"],"1735":["Jason Brownlee"],"1736":["Daniel Selsam","Percy Liang","David L. Dill"],"1737":["Yuan Yu","Mart\u00edn Abadi","Paul Barham","Eugene Brevdo","Mike Burrows","Andy Davis","Jeff Dean","Sanjay Ghemawat","Tim Harley","Peter Hawkins","Michael Isard","Manjunath Kudlur","Rajat Monga","Derek Murray","Xiaoqiang Zheng"],"1738":["Zachary C. Lipton","Jacob Steinhardt"],"1739":["Mr. Rishabh Dubey"],"1740":["Wan-Bing He","Yu-Gang Ma","Long-Gang Pang","Huichao Song","Kai Zhou"],"1741":["Benazir Begum A","Sreeyuktha R","Haritha M P","Vishnuprasad"],"1742":["Shu Rong","Xing Niu","Evan Wei Xiang","Haofen Wang","Qiang Yang","Yong Yu"],"1743":["G. Abinaya K. Sridevi Nattar Dr. Rajini Girinath"],"1744":["Preeti Sondhi","Aakib Jabbar"],"1745":["Ashia C. Wilson","Rebecca Roelofs","Mitchell Stern","Nati Srebro","Benjamin Recht"],"1746":["Pankaj Mehta","Marin Bukov","Ching-Hao Wang","Alexandre G. R. Day","Clint Richardson","Charles K. Fisher","David J. Schwab"],"1747":["Tianqi Chen","Mu Li","Yutian Li","Min Lin","Naiyan Wang","Minjie Wang","Tianjun Xiao","Bing Xu","Chiyuan Zhang","Zheng Zhang"],"1748":["Poonam L Patil","Dr. S. R. Jadhao"],"1749":["L\u00e9on Bottou","Frank E. Curtis","Jorge Nocedal"],"1750":["Stephan Rabanser","Oleksandr Shchur","Stephan G\u00fcnnemann"],"1751":["Alexander Maedche"],"1752":["Yevgeniy Vorobeychik","Murat Kantarcioglu"],"1753":["Inderjeet Mani","Marc Verhagen","Ben Wellner","Chong Min Lee","James Pustejovsky"],"1754":["Ajayi Kemi Patience","Dr. Lakshmi J. V. N"],"1755":["Fabrizio Sebastiani"],"1756":["Finale Doshi-Velez","Been Kim"],"1757":["Alexey A. Melnikov","Hendrik Poulsen Nautrup","Mario Krenn","Vedran Dunjko","Markus Tiersch","Anton Zeilinger","Hans J. Briegel"],"1758":["Bo Pang","Lillian Lee","Shivakumar Vaithyanathan"],"1759":["Thomas G. Dietterich"],"1760":["D. Sculley","Carla E. Brodley"],"1761":["Fabrizio Sebastiani"],"1762":["Yuan He","Jiaoyan Chen","Hang Dong","Ernesto Jim\u00e9nez-Ruiz","Ali Hadian","Ian Horrocks"],"1763":["Ethem Alpaydin"],"1764":["Anu","Ms. Preeti"],"1765":["Daniel C. Elton","Zois Boukouvalas","Mark S. Butrico","Mark D. Fuge","Peter W. Chung"],"1766":["Suraj Kapse","Akshay Kurumkar","Vighnesh Manthapurwar","Prof. Rajesh Tak"],"1767":["Kanchi Tank"],"1768":["Mikhail Belkin","Daniel Hsu","Siyuan Ma","Soumik Mandal"],"1769":["Thomas M. Mitchell"],"1770":["Randal S. Olson","William G. La Cava","Zairah Mustahsan","Akshay Varik","Jason H. Moore"],"1771":["Pat Langley"],"1772":["Daisuke Komura","Shumpei Ishikawa"],"1773":["Georgios Damaskinos","El Mahdi El Mhamdi","Rachid Guerraoui","Rhicheek Patra","Mahsa Taziki"],"1774":["Kelsey Tsipis"],"1775":["Georgios Damaskinos","El Mahdi El Mhamdi","Rachid Guerraoui","Rhicheek Patra","Mahsa Taziki"],"1776":["Cynthia Rudin"],"1777":["Anna Melekhova"],"1778":["Laura von Rueden","Sebastian Mayer","Katharina Beckh","Bogdan Georgiev","Sven Giesselbach","Raoul Heese","Birgit Kirsch","Julius Pfrommer","Annika Pick","Rajkumar Ramamurthy","Michal Walczak","Jochen Garcke","Christian Bauckhage","Jannis Schuecker"],"1779":["Laura von Rueden","Sebastian Mayer","Katharina Beckh","Bogdan Georgiev","Sven Giesselbach","Raoul Heese","Birgit Kirsch","Julius Pfrommer","Annika Pick","Rajkumar Ramamurthy","Michal Walczak","Jochen Garcke","Christian Bauckhage","Jannis Schuecker"],"1780":["Akimoto Nakayama","Kosuke Mitarai","Leonardo Placidi","Takanori Sugimoto","Keisuke Fujii"],"1781":["C\u00e8dric Renggli","Dan Alistarh","Torsten Hoefler"],"1782":["Chase Roberts","Ashley Milsted","Martin Ganahl","Adam Zalcman","Bruce Fontaine","Yijian Zou","Jack Hidary","Guifre Vidal","Stefan Leichenauer"],"1783":["Hui Jiang"],"1784":["Masashi Sugiyama","Motoaki Kawanabe"],"1785":["Sebastian Raschka"],"1786":["Thomas G. Dietterich","Pedro Domingos","Lise Getoor","Stephen Muggleton","Prasad Tadepalli"],"1787":["Sarah Bird","K. Kenthapadi","Emre K\u0131c\u0131man","Margaret Mitchell"],"1788":["Michael U. Gutmann"],"1789":["Hephzibah Thomas","Thyla B"],"1790":["E Pellegrini","L Ballerini","M D C V Hernandez","F M Chappell","V Gonz\u00e1lez-Castro","D Anblagan","S Danso","S Mu\u00f1oz-Maniega","D Job","C Pernet","G Mair","T J MacGillivray","E Trucco","J M Wardlaw"],"1791":["Manda Banerji","Ofer Lahav","Chris J. Lintott","Filipe B. Abdalla","Kevin Schawinski","Steven P. Bamford","Dan Andreescu","Phil Murray","M. Jordan Raddick","Anze Slosar","Alex Szalay","Daniel Thomas","Jan Vandenberg"],"1792":["Cynthia Rudin","Chaofan Chen","Zhi Chen","Haiyang Huang","Lesia Semenova","Chudi Zhong"],"1793":["Michael A. Lones"],"1794":["Dr. C. Umarani","Vinay Singh Dhapola"],"1795":["C.M. Bishop"],"1796":["Manuri Raju","Dr. D. Jakir Hussain"],"1797":["Isaac Okyere Paintsil","Zhao Xicang","Oliver Joseph Abban"],"1798":["Giacomo Torlai","Roger G. Melko"],"1799":["Mikhail Belkin","Daniel Hsu","Siyuan Ma","Soumik Mandal"],"1800":["Zhiyuan Chen","Bing Liu"],"1801":["Qi Wang","Yue Ma","Kun Zhao","Yingjie Tian"],"1802":["Jean Kaddour","Aengus Lynch","Qi Liu","Matt J. Kusner","Ricardo Silva"],"1803":["Tessa Lau","Steven A. Wolfman","Pedro Domingos","Daniel S. Weld"],"1804":["Rohini K R","Sreehari T Anil","Sreejith P M","Yedumohan P M"],"1805":["Zhenyu Zhang","Rui Ma","Jifeng Hu","Qian Wang"],"1806":["Ifeyinwa Nkemdilim Obodoeze Fidelis C.","Oliver Ifeoma Catherine","Onyemachi George Olisamaka","Udeh Ifeanyi Frank Gideon","Obiokafor"],"1807":["Sirui Lu","Shilin Huang","Keren Li","Jun Li","Jianxin Chen","Dawei Lu","Zhengfeng Ji","Yi Shen","Duanlu Zhou","Bei Zeng"],"1808":["Dominik Sacha","Michael Sedlmair","Leishi Zhang","John A. Lee","Jaakko Peltonen","Daniel Weiskopf","Stephen C. North","Daniel A. Keim"],"1809":["Ms. Anjum Shaikh Ms. Firdos Shaikh Mr. Suhaib Ramzan Prof. M. M. Patil"],"1810":["Fr\u00e9d\u00e9ric Alexandre"],"1811":["Dimitri Bourilkov"],"1812":["Shakir Mohamed","Mihaela Rosca","Michael Figurnov","Andriy Mnih"],"1813":["Colin J Brown","Ghassan Hamarneh"],"1814":["Vijaykumar Janga Prof. E Sreenivasa Reddy"],"1815":["Patel Smitkumar Hareshbhai"],"1816":["Felipe Bravo-Marquez","Steve Reeves","Mart\u00edn Ugarte"],"1817":["Amir Fazelinia","Issa Annamoradnejad","Jafar Habibi"],"1818":["Yi Li","Yitao Duan","Yu Yu","Shuoyao Zhao","Wei Xu"],"1819":["Peter Buchlovsky","David Budden","Dominik Grewe","Chris Jones","John Aslanides","Frederic Besse","Andy Brock","Aidan Clark","Sergio G\u00f3mez Colmenarejo","Aedan Pope","Fabio Viola","Dan Belov"],"1820":["Christopher M. Bishop"],"1821":["Kun Han","Dong Yu","Ivan Tashev"],"1822":["S M Abdullah Al Shuaeb","Md. Kamruzaman","Mohammad Al-Amin"],"1823":["Jeffrey C. Schlimmer","Richard H. Granger"],"1824":["T. Joachims"],"1825":["Burak Kanber"],"1826":["Makoto Takamoto","Timothy Praditia","Raphael Leiteritz","Dan MacKinlay","Francesco Alesiani","Dirk Pfl\u00fcger","Mathias Niepert"],"1827":["Kiran Bala","Sakshi sharma","Meenakshi Garg","Deeksha Verma"],"1828":["W. James Murdoch","Chandan Singh","Karl Kumbier","Reza Abbasi-Asl","Bin Yu"],"1829":["P. Devi Mahalakshmi","Dr. M. Babu"],"1830":["A. Hammad","Myeonghun Park","Raymundo Ramos","Pankaj Saha"],"1831":["Shruti Chandrakant Zarekar","Priyanka Dattatray Desai","Manjusha Prabhakar Randhave","Surajsingh Rajendrasingh Chauhan","Dr. Shyam Gupta"],"1832":["Chandni Jain","S. Vignesh"],"1833":["Richard Forsyth","Roy Rada"],"1834":["D. Sculley","Gary Holt","Daniel Golovin","Eugene Davydov","Todd Phillips","Dietmar Ebner","Vinay Chaudhary","Michael Young","Jean-Fran\u00e7ois Crespo","Dan Dennison"],"1835":["Jasper Snoek","Hugo Larochelle","Ryan P. Adams"],"1836":["Gurjeet Singh"],"1837":["Harshil M. Kamdar","Matthew J. Turk","Robert J. Brunner"],"1838":["Martin Sturm","Michael Hackenberg","David Langenberger","Dmitrij Frishman"],"1839":["Kexin Huang","Tianfan Fu","Wenhao Gao","Yue Zhao","Yusuf Roohani","Jure Leskovec","Connor W. Coley","Cao Xiao","Jimeng Sun","Marinka Zitnik"],"1840":["Dominik Kreuzberger","Niklas K\u00fchl","Sebastian Hirschl"],"1841":["Matthias Feurer","Aaron Klein","Katharina Eggensperger","Jost Tobias Springenberg","Manuel Blum","Frank Hutter"],"1842":["Sheetal Sharma S K Bharti Raj Kumar Goel"],"1843":["Fernando Maciano de Paula Neto","Lucas Fernando da Silva Cambuim","Rafael M. Macieira","Teresa Bernarda Ludermir","Cleber Zanchettin","Edna Natividade da Silva Barros"],"1844":["R. Aswin E. Ganesh M. Babu"],"1845":["Judea Pearl"],"1846":["Saso Dzeroski","Pierre Geurts","Juho Rousu"],"1847":["Rasha Thamer Shawe","Kawther Thabt Saleh","Farah Neamah Abbas"],"1848":["Jialin Wang","Jing Yang","Hong-Li Ren","Jinxiao Li","Qing Bao","Miaoni Gao"],"1849":["Fa Li","Qing Zhu","William Riley","Lei Zhao","Li Xu","Kunxiaojia Yuan","Min Chen","Huayi Wu","Zhipeng Gui","Jianya Gong","James Randerson"],"1850":["Marco T. Ribeiro","Sameer Singh","Carlos Guestrin"],"1851":["C.L. Blake D.J. Newman","C.J. Merz"],"1852":["Miroslav Kubat","Robert C. Holte","Stan Matwin"],"1853":["J. Ross Quinlan"],"1854":["Yihua Chen","Eric K. Garcia","Maya R. Gupta","Ali Rahimi","Luca Cazzanti"],"1855":["Mehreen Ali","Tero Aittokallio"],"1856":["Linwei Hu","Jie Chen","Joel Vaughan","Hanyu Yang","Kelly Wang","Agus Sudjianto","Vijayan N. Nair"],"1857":["Jinzhao Chen","Japan K. Patel","Richard Vasques"],"1858":["Abid Sarwar"],"1859":["Giuseppe Carleo","Kenny Choo","Damian Hofmann","James E. T. Smith","Tom Westerhout","Fabien Alet","Emily J. Davis","Stavros Efthymiou","Ivan Glasser","Sheng-Hsuan Lin","Marta Mauri","Guglielmo Mazzola","Christian B. Mendl","Evert van Nieuwenburg","Ossian O'Reilly","Hugo Th\u00e9veniaut","Giacomo Torlai","Alexander Wietek"],"1860":["Marc G. Genton"],"1861":["A L Dallora","S Eivazzadeh","E Mendes","J Berglund","P Anderberg"],"1862":["Michael Pazzani","Daniel Billsus"],"1863":["Kevin Hartnett"],"1864":["Avrim Blum","Moritz Hardt"],"1865":["Alessandro Castelnovo","Riccardo Crupi","Greta Greco","Daniele Regoli"],"1866":["Florian Lautenschlager","Martin Becker","Konstantin Kobs","Michael Steininger","Padraig Davidson","Anna Krause","Andreas Hotho"],"1867":["Elena Erdmann","Karin Boczek","Lars Koppers","Gerret von Nordheim","Christian P\u00f6litz","Alejandro Molina","Katharina Morik","Henrik M\u00fcller","J\u00f6rg Rahnenf\u00fchrer","Kristian Kersting"],"1868":["Parthasarathi Mukhopadhyay"],"1869":["Volker Tresp","Markus Bundschus","Achim Rettinger","Yi Huang"],"1870":["Anna Kasprzik"],"1871":["Paul G\u00f6lz","Anson Kahng","Ariel D. Procaccia"],"1872":["Alexander J. Smola","S. V. N. Vishwanathan","Quoc V. Le"],"1873":["Michael Kamp","Mario Boley","Olana Missura","Thomas G\u00e4rtner"],"1874":["Andrea Lockerd Thomaz"],"1875":["Siddhartha Giri","Hubert Truchan","Markus Rokicki","Jan-Hendrik Zab","Hendrik Noske","Claudia Nieder\u00e9e","Behrend Denkena","Wolfgang Nejdl","Sergej Zerr"],"1876":["Di He","Yingce Xia","Tao Qin","Liwei Wang","Nenghai Yu","Tie-Yan Liu","Wei-Ying Ma"],"1877":["Mr. Umesh R Maurya","Prof. Sudeshna Roy"],"1878":["Mario A Mu\u00f1oz","Laura Villanova","Davaatseren Baatar","Kate Smith-Miles"],"1879":["Robert Nishihara","Philipp Moritz","Stephanie Wang","Alexey Tumanov","William Paul","Johann Schleier-Smith","Richard Liaw","Mehrdad Niknami","Michael I. Jordan","Ion Stoica"],"1880":["Battista Biggio","Fabio Roli"],"1881":["Naman Agarwal","Brian Bullins","Elad Hazan"],"1882":["S B Kotsiantis"],"1883":["Peva Blanchard","El Mahdi El Mhamdi","Rachid Guerraoui","Julien Stainer"],"1884":["Constantin F. Aliferis","Ioannis Tsamardinos","Pierre Mansion","Alexander Statnikov","Douglas Hardin"],"1885":["Cheonbok Park","Yunwon Tae","Taehee Kim","Soyoung Yang","Mohammad Azam Khan","Lucy Park","J. Choo"],"1886":["Christian Rack","Lukas Schach","Marc Latoschik"],"1887":["Vladimir Vapnik","Rauf Izmailov"],"1888":["Randal S. Olson","Jason H. Moore"],"1889":["Wee Meng Soon","Hwee Tou Ng","Daniel Chung Yong Lim"],"1890":["G P Way","R J Allaway","S J Bouley","C E Fadul","Y Sanchez","C S Greene"],"1891":["J. Bayron Orjuela-Quintana","Savvas Nesseris","Wilmar Cardona"],"1892":["Giuseppe Casalicchio","Jakob Bossek","Michel Lang","Dominik Kirchhoff","Pascal Kerschke","Benjamin Hofner","Heidi Seibold","Joaquin Vanschoren","Bernd Bischl"],"1893":["C.E. Rasmussen","C.K.I. Williams"],"1894":["Wang-Zhou Dai","Qiu-Ling Xu","Yang Yu","Zhi-Hua Zhou"],"1895":["Jeff Choi","Nima Aghaeepour","Martin Becker"],"1896":["Henggang Cui","Gregory R. Ganger","Phillip B. Gibbons"],"1897":["Manju D C","Murugan R"],"1898":["Ravichandra Addanki","Shaileshh Bojja Venkatakrishnan","Shreyan Gupta","Hongzi Mao","Mohammad Alizadeh"],"1899":["Fabian Pedregosa","Ga\u00ebl Varoquaux","Alexandre Gramfort","Vincent Michel","Bertrand Thirion","Olivier Grisel","Mathieu Blondel","Peter Prettenhofer","Ron Weiss","Vincent Dubourg"],"1900":["Caroline Wang","Bin Han","Bhrij Patel","Feroze Mohideen","Cynthia Rudin"],"1901":["Oliver Kennion","Stuart Maitland","Richard Brady"],"1902":["Christophe Andrieu","Nando de Freitas","Arnaud Doucet","Michael I. Jordan"],"1903":["D. Zhang","J. Tsai"],"1904":["Pim de Haan","Corrado Rainone","Miranda C. N. Cheng","Roberto Bondesan"],"1905":["Christopher M. Bishop"],"1906":["Peter Werner Eklund","Stephen Kirkby"],"1907":["Christophe Andrieu","Nando de Freitas","Arnaud Doucet","Michael I. Jordan"],"1908":["Tao Zhang","Kun Zhu","Ekram Hossain"],"1909":["Karol Bartkiewicz","Clemens Gneiting","Anton\u00edn \u010cernoch","Kate\u0159ina Jir\u00e1kov\u00e1","Karel Lemr","Franco Nori"],"1910":["Jenna Wiens","Suchi Saria","Mark Sendak","Marzyeh Ghassemi","Vincent X. Liu","Finale Doshi-Velez","Kenneth Jung","Katherine Heller","David Kale","Mohammed Saeed","Pilar N. Ossorio","Sonoo Thadaney-Israni","Anna Goldenberg"],"1911":["Tai Le Quy","Arjun Roy","Vasileios Iosifidis","Eirini Ntoutsi"],"1912":["Hannes Bajohr"],"1913":["Francisco Villaescusa-Navarro","Daniel Angl\u00e9s-Alc\u00e1zar","Shy Genel","David N. Spergel","Rachel S. Somerville","Romeel Dave","Annalisa Pillepich","Lars Hernquist","Dylan Nelson","Paul Torrey","Desika Narayanan","Yin Li","Oliver Philcox","Valentina La Torre","Ana Maria Delgado","Shirley Ho","Sultan Hassan","Blakesley Burkhart","Digvijay Wadekar","Nicholas Battaglia","Gabriella Contardo"],"1914":["Muhammad Usman","Wenxi Wang","Kaiyuan Wang","Marko Vasic","Haris Vikalo","Sarfraz Khurshid"],"1915":["J. R. Quinlan"],"1916":["Robert Holley","Daniel Rosenfeld"],"1917":["Peter Werner Eklund","Stephen Kirkby","Ahmed Salim"],"1918":["Thomas G. Dietterich"],"1919":["Giulio Petrucci","Marco Rospocher","Chiara Ghidini"],"1920":["Machine Learning","AJ Tall\u00f3n-Ballesteros","C Chen"],"1921":["Fumeng Yang","Zhuanyi Huang","Jean Scholtz","Dustin L. Arendt"],"1922":["Akbar Telikani","Amirhessam Tahmassebi","Wolfgang Banzhaf","Amir H. Gandomi"],"1923":["Peter Stone","Manuela Veloso"],"1924":["Abhishek Kar","Christian H\u00e4ne","Jitendra Malik"],"1925":["Nikolas Wehner"],"1926":["Christian Bauckhage","Daniel Schulz","Dirk Hecker"],"1927":["Pedro Domingos"],"1928":["Raphael Bost","Raluca Ada Popa","Stephen Tu","Shafi Goldwasser"],"1929":["Matteo Turchetta","Felix Berkenkamp","Andreas Krause"],"1930":["Bernard Haasdonk","Hans Burkhardt"],"1931":["Miles Cranmer"],"1932":["Arno Sch\u00f6dl","Irfan A. Essa"],"1933":["Matthias Feurer","Aaron Klein","Katharina Eggensperger","Jost Tobias Springenberg","Manuel Blum","Frank Hutter"],"1934":["Leena Chennuru Vankadara","Ulrike von Luxburg"],"1935":["Yanan Qian","Yunhua Hu","Jianling Cui","Qinghua Zheng","Zaiqing Nie"],"1936":["B. Baskeles","B. Turhan","A. Bener"],"1937":["Samantha D'Alonzo","Max Tegmark"],"1938":["Anh Truong","Austin Walters","Jeremy Goodsitt","Keegan Hines","Bayan Bruss","Reza Farivar"],"1939":["Chris Drummond"],"1940":["Takayuki Yoshinaka","Soichi Ishii","Tomohiro Fukuhara","Hidetaka Masuda","Hiroshi Nakagawa"],"1941":["T A Burke","B A Ammerman","R Jacobucci"],"1942":["Jonas Rauber","Wieland Brendel","Matthias Bethge"],"1943":["Philipp Probst","Anne-Laure Boulesteix","Bernd Bischl"],"1944":["Mathias Louboutin","Ziyi Yin","Rafael Orozco","Thomas J. Grady II","Ali Siahkoohi","Gabrio Rizzuti","Philipp A. Witte","Olav M\u00f8yner","Gerard J. Gorman","Felix J. Herrmann"],"1945":["Shai Shalev-Shwartz","Shai Ben-David"],"1946":["Masashi Sugiyama","Taiji Suzuki","Takafumi Kanamori"],"1947":["Hamid Asgari","Juha Kortelainen","Mikko Tahkola"],"1948":["Alauddin Yousif Al-Omary","Mohammad Shahid Jamil"],"1949":["Anton Bakhtin","Sam Gross","Myle Ott","Yuntian Deng","Marc'Aurelio Ranzato","Arthur Szlam"],"1950":["Debjani Saha","Candice Schumann","Duncan C. McElfresh","John P. Dickerson","Michelle L. Mazurek","Michael Carl Tschantz"],"1951":["Dishi Liu","Daigo Maruyama","Stefan G\u00f6rtz"],"1952":["Masashi Sugiyama"],"1953":["John D. Kelleher","Brian MacNamee","Aoife D'Arcy"],"1954":["Ludwig Bothmann","Sven Strickroth","Giuseppe Casalicchio","David R\u00fcgamer","Marius Lindauer","Fabian Scheipl","Bernd Bischl"],"1955":["Denis Baylor","Eric Breck","Heng-Tze Cheng","Noah Fiedel","Chuan Yu Foo","Zakaria Haque","Salem Haykal","Mustafa Ispir","Vihan Jain","Levent Koc","Chiu Yuen Koo","Lukasz Lew","Clemens Mewald","Akshay Naresh Modi","Neoklis Polyzotis","Sukriti Ramesh","Sudip Roy","Steven Euijong Whang","Martin Wicke","Jarek Wilkiewicz","Xin Zhang","Martin Zinkevich"],"1956":["Ian H. Witten","Eibe Frank"],"1957":["K P Murphy"],"1958":["Mohammed Khaja Faizan"],"1959":["Yi Zhang","A. Mesaros","K. Fujita","S. D. Edkins","M. H. Hamidian","K. Ch'ng","H. Eisaki","S. Uchida","J. C. S\u00e9amus Davis","E. Khatami","Eun-Ah Kim"],"1960":["Chiheb Chebbi"],"1961":["E. Rosten","R. Porter","T. Drummond"],"1962":["Goda Klumbyte","Claude Draude","Alex Taylor"],"1963":["Andree Thieltges"],"1964":["Peter Werner Eklund"],"1965":["Eric P. Xing","Qirong Ho","Pengtao Xie","Dai Wei"],"1966":["M. I. Jordan","T. M. Mitchell"],"1967":["David Barber"],"1968":["Carl Edward Rasmussen","Christopher K. I. Williams"],"1969":["Andr\u00e9s Goens","Alexander Brauckmann","Sebastian Ertel","Chris Cummins","Hugh Leather","Jeronimo Castrillon"],"1970":["G. I. Webb","M. J. Pazzani","D. Billsus"],"1971":["Tommaso Dorigo","Pablo de Castro"],"1972":["Han Xiao","Kashif Rasul","Roland Vollgraf"],"1973":["Esther Rolf","Max Simchowitz","Sarah Dean","Lydia T. Liu","Daniel Bj\u00f6rkegren","Moritz Hardt","Joshua Blumenstock"],"1974":["Drew A. Hudson","Christopher D. Manning"],"1975":["S\u00f6ren Laue","Matthias Mitterreiter","Joachim Giesen"],"1976":["Olivier Chapelle","Za\u00efd Harchaoui"],"1977":["Gert Cauwenberghs","Tomaso A. Poggio"],"1978":["Lars Buitinck","Gilles Louppe","Mathieu Blondel","Fabian Pedregosa","Andreas Mueller","Olivier Grisel","Vlad Niculae","Peter Prettenhofer","Alexandre Gramfort","Jaques Grobler","Robert Layton","Jake Vanderplas","Arnaud Joly","Brian Holt","Ga\u00ebl Varoquaux"],"1979":["Nicol\u00f3 Fusi","Rishit Sheth","Melih Elibol"],"1980":["Gustavo Correa Publio","Diego Esteves","Agnieszka Lawrynowicz","Pance Panov","Larisa Soldatova","Tommaso Soru","Joaquin Vanschoren","Hamid Zafar"],"1981":["Peter Norvig"],"1982":["Kun Zhang"],"1983":["Cinzia Giannetti","Biagio Lucini","Davide Vadacchino"],"1984":["Swati Mishra","Jeffrey M Rzeszotarski"],"1985":["Lev Reyzin"],"1986":["K. Karthick","S. Aruna","R. Samikannu","Ramya Kuppusamy","Yuvaraja Teekaraman","A. Thelkar"],"1987":["Chi Nhan Nguyen","Oliver Zeigermann"],"1988":["L. Pilozzi","F. A. Farrelly","G. Marcucci","C. Conti"],"1989":["Shivani Jain","Anju Saha"],"1990":["Omeye Emmanuel C.","Ngene John N.","Dr. Anyaragbu Hope U.","Dr. Ozioko Ekene","Dr. Iloka Bethram C.","Prof. Inyiama Hycent C."],"1991":["Caleb Gannon","Rongguang Liang"],"1992":["Been Kim","Dmitry M. Malioutov","Kush R. Varshney"],"1993":["Arjun Roy","Jan Horstmann","Eirini Ntoutsi"],"1994":["S Zheng","J J Lu","N Ghasemzadeh","S S Hayek","A A Quyyumi","F Wang"],"1995":["El-Ghazali Talbi"],"1996":["Xingjian SHI","Zhourong Chen","Hao Wang","Dit-Yan Yeung","Wai-kin Wong","Wang-chun WOO"],"1997":["Andreas Holzinger"],"1998":["Neil D. Lawrence","John C. Platt"],"1999":["Bastian Sch\u00e4fermeier","Gerd Stumme","Tom Hanika"],"2000":["Sebastian Raubitzek","Thomas Neubauer"],"2001":["Randal S. Olson","William La Cava","Zairah Mustahsan","Akshay Varik","Jason H. Moore"],"2002":["Anirban Chakraborty",""],"2003":["R. M. Friedberg"],"2004":["H. Lassoued","R. Ketata"],"2005":["Li Zhao","Jie Zhu"],"2006":["Josh Attenberg","Prem Melville","Foster Provost","Maytal Saar-Tsechansky"],"2007":["Omar Alfarisi","Aikifa Raza","Hongtao Zhang","Djamel Ozzane","Mohamed Sassi","Tiejun Zhang"],"2008":["Xingjian Shi","Zhourong Chen","Hao Wang","Dit-Yan Yeung","Wai-kin Wong","Wang-chun Woo"],"2009":["Andrew J Steele","Spiros C Denaxas","Anoop D Shah","Harry Hemingway","Nicholas M Luscombe"],"2010":["M. Wurst","J. Novak","M. Schneider"],"2011":["Majid Khan","Ghassan Husnain","Waqas Ahmad","Zain Shaukat","Latif Jan","Ihtisham Ul Haq","Shahab Ul Islam","Atif Ishtiaq"],"2012":["Alessio Rossi","Luca Pappalardo","Paolo Cintia","Marcello Iaia","Javier Fernandez","Daniel Medina"],"2013":["Peng Xu","Farbod Roosta-Khorasani","Michael W. Mahoney"],"2014":["P J Navarro","C Fern\u00e1ndez","R Borraz","D Alonso"],"2015":["Scott H Lee","Matthew J Maenner","Charles M Heilig"],"2016":["Ethem Alpaydin"],"2017":["Adri\u00e1n Soto","Deyu Lu","Shinjae Yoo","Mariv\u00ed Fern\u00e1ndez-Serra"],"2018":["Mohamed Asmaan M","Mohamed Ismail P. M. H","Sudharshan R","Bhuvana Priya"],"2019":["Rebecca Roelofs","Vaishaal Shankar","Benjamin Recht","Sara Fridovich-Keil","Moritz Hardt","John Miller","Ludwig Schmidt"],"2020":["Maumita Bhattacharya"],"2021":["Matthew A. Kayala","Pierre Baldi"],"2022":["Peva Blanchard","El Mahdi El Mhamdi","Rachid Guerraoui","Julien Stainer"],"2023":["Celestine D\u00fcnner","Thomas P. Parnell","Dimitrios Sarigiannis","Nikolas Ioannou","Andreea Anghel","Gummadi Ravi","Madhusudanan Kandasamy","Haralampos Pozidis"],"2024":["Goda Klumbyt\u0117","Hannah Piehl","Claude Draude"],"2025":["Bo Pang","Lillian Lee","Shivakumar Vaithyanathan"],"2026":["Shai Shalev-Shwartz","Shai Ben-David"],"2027":["Maria Virvou","Efthimios Alepis","George A. Tsihrintzis","Lakhmi C. Jain"],"2028":["J. Kivinen","MK Warmuth"],"2029":["FangXia An","J. M. Simpson","Ian Smail","A. M. Swinbank","Cong Ma","Daizhong Liu","P. Lang","E. Schinnerer","A. Karim","B. Magnelli","S. Leslie","F. Bertoldi","Chian-Chou Chen","J. E. Geach","Y. Matsuda","S. M. Stach","J. L. Wardlow","B. Gullberg","R. J. Ivison","Y. Ao","R. T. Coogan","A. P. Thomson","S. C. Chapman","R. Wang","Wei-Hao Wang","Y. Yang","R. Asquith","N. Bourne","K. Coppin","N. K. Hine","L. C. Ho","H. S. Hwang","Y. Kato","K. Lacaille","A. J. R. Lewis","I. Oteo","J. Scholtz","M. Sawicki","D. Smith"],"2030":["M. S. Gashler"],"2031":["Andree Thieltges"],"2032":["Aqsa Rahim","Yawar Rasheed","Farooque Azam","Muhammad Waseem Anwar","Muhammad Abdul Rahim","Abdul Wahab Muzaffar"],"2033":["Georgios Petasis"],"2034":["Claudia Perlich"],"2035":["Stephen Gang G. Wu","Yuxuan Wang","Wu Jiang","Tolutola Oyetunde","Ruilian Yao","Xuehong Zhang","Kazuyuki Shimizu","Yinjie J. Tang","Forrest Sheng S. Bao"],"2036":["Volker Tresp","Yi Huang","Maximilian Nickel"],"2037":["Yuan He","Jiaoyan Chen","Hang Dong","Ernesto Jim\u00e9nez-Ruiz","Ali Hadian","Ian Horrocks"],"2038":["Emilio Serrano","Pedro del Pozo-Jim\u00e9nez","Mari Carmen Su\u00e1rez-Figueroa","Jacinto Gonz\u00e1lez-Pach\u00f3n","Javier Bajo","Asunci\u00f3n G\u00f3mez-P\u00e9rez"],"2039":["Dionysios N. Sotiropoulos","George A. Tsihrintzis"],"2040":["Ripon Patgiri","Sabuzima Nayak","Tanya Akutota","Bishal Paul"],"2041":["Pedro Domingos"],"2042":["G. I. Webb"],"2043":["Adrian V. Dalca","Matthew B. A. McDermott","Emily Alsentzer","Samuel G. Finlayson","Michael Oberst","Fabian Falck","Corey Chivers","Andrew Beam","Tristan Naumann","Brett K. Beaulieu-Jones"],"2044":["S. J. Curran"],"2045":["George Em Karniadakis","Ioannis G. Kevrekidis","Lu Lu","Paris Perdikaris","Sifan Wang","Liu Yang"],"2046":["Naima Chouikhi","Adel M. Alimi"],"2047":["Kenneth Holstein","Jennifer Wortman Vaughan","Hal Daum\u00e9 III","Miro Dud\u00edk","Hanna Wallach"],"2048":["Iris A. M. Huijben","Wouter Kool","Max B. Paulus","Ruud J. G. van Sloun"],"2049":["Y. Shan","D. Paull","R. I. McKay"],"2050":["Thomas Hofmann","Bernhard Sch\u00f6lkopf","Alexander J. Smola"],"2051":["Valentine Fontama","Roger Barga","Wee Hyong Tok"],"2052":["Peter Wittek"],"2053":["Pierre-Yves Oudeyer","Manuel Lopes","Celeste Kidd","Jacqueline Gottlieb"],"2054":["Diego Esteves","Pablo N. Mendes","Diego Moussallem","Julio Cesar Duarte","Amrapali Zaveri","Jens Lehmann","Ciro Baron Neto","Igor Costa","Maria Claudia Cavalcanti"],"2055":["Ian H. Witten","Eibe Frank"],"2056":["Roberto Vega","Leonardo Flores","Russell Greiner"],"2057":["Andrew J. Larkoski","Ian Moult","Benjamin Nachman"],"2058":["G. Paliouras","C. Papatheodorou","V. Karkaletsis","C. D. Spyropoulos"],"2059":["Joelle Pineau","Philippe Vincent-Lamarre","Koustuv Sinha","Vincent Larivi\u00e8re","Alina Beygelzimer","Florence d\u2019Alch\u00e9 Buc","Emily Fox","Hugo Larochelle"],"2060":["Ron Kohavi","Foster Provost"],"2061":["Carl Edward Rasmussen","Christopher K. I. Williams"],"2062":["Chenxin Ma","Martin Jaggi","Frank E. Curtis","Nathan Srebro","Martin Tak\u00e1\u010d"],"2063":["Roy Mor","Ben Kantor","Emma Rapoport"],"2064":["F. Pedregosa","G. Varoquaux","A. Gramfort","V. Michel","B. Thirion","O. Grisel","M. Blondel","P. Prettenhofer","R. Weiss","V. Dubourg","J. Vanderplas","A. Passos","D. Cournapeau","M. Brucher","M. Perrot","E. Duchesnay"],"2065":["Aur\u00e9lien G\u00e9ron"],"2066":["Sudipta Sikder","Rennan Barkana","Itamar Reis","Anastasia Fialkov"],"2067":["Thomas Daniel Ullmann"],"2068":["Cheng-Tao Chu","Sang Kyun Kim","Yi-An Lin","YuanYuan Yu","Gary R. Bradski","Andrew Y. Ng","Kunle Olukotun"],"2069":["Zhiyuan Chen","Bing Liu"],"2070":["Stewart W. Wilson"],"2071":["Xiong Luo","Xiaona Yang","Changwei Jiang","Xiaojuan Ban"],"2072":["Jan Wildenhain","Michaela Spitzer","Sonam Dolma","Nick Jarvik","Rachel White","Marcia Roy","Emma Griffiths","David S. Bellows","Gerard D. Wright","Mike Tyers"],"2073":["Jiatao Gu","Yong Wang","Yun Chen","Kyunghyun Cho","V. Li"],"2074":["Lin Wu","Pierre Baldi"],"2075":["Nicolas Vasilache","Oleksandr Zinenko","Theodoros Theodoridis","Priya Goyal","Zachary DeVito","William S. Moses","Sven Verdoolaege","Andrew Adams","Albert Cohen"],"2076":["A. R. Golding","D. Roth"],"2077":["Sam Foreman","Joel Giedt","Yannick Meurice","Judah Unmuth-Yockey"],"2078":["Frank Alexander Kraemer","Nattachart Tamkittikhun","Anders Eivind Braten"],"2079":["Goda Klumbyt\u0117","Claude Draude","Alex S. Taylor"],"2080":["Thomas G. Dietterich"],"2081":["Alexandre Abraham","Fabian Pedregosa","Michael Eickenberg","Philippe Gervais","Andreas Mueller","Jean Kossaifi","Alexandre Gramfort","Bertrand Thirion","Gael Varoquaux"],"2082":["Sa\u00efd Tazi","Khaldoun Zreik"],"2083":["Vasilis Syrgkanis","Victor Lei","Miruna Oprescu","Maggie Hei","Keith Battocchi","Greg Lewis"],"2084":["Jamie Hayes","Olga Ohrimenko"],"2085":["James McInerney"],"2086":["Kiri Wagstaff"],"2087":["Nicolas M\u00fcller"],"2088":["Albert S. Berahas","Jorge Nocedal","Martin Tak\u00e1c"],"2089":["Edward Raff"],"2090":["Antonio Ginart","Melody Y. Guan","Gregory Valiant","James Zou"],"2091":["Kyunghyun Cho","Bart van Merrienboer","Caglar Gulcehre","Dzmitry Bahdanau","Fethi Bougares","Holger Schwenk","Yoshua Bengio"],"2092":["Iordanis Kerenidis","Jonas Landman","Alessandro Luongo","Anupam Prakash"],"2093":["Mu Li","David G. Andersen","Alexander J. Smola","Kai Yu"],"2094":["Mark A. Hall"],"2095":["Bertrand Salem Clarke","Ernest Fokou\u00e9","Hao Helen Zhang"],"2096":["Alireza Seif","Kevin A. Landsman","Norbert M. Linke","Caroline Figgatt","Christopher Monroe","Mohammad Hafezi"],"2097":["Peter A. Flach"],"2098":["Itamar Reis","Michael Rotman","Dovi Poznanski","J. Xavier Prochaska","Lior Wolf"],"2099":["Grazina Korvel","Adam Kurowski","Bozena Kostek","Andrzej Czyzewski"],"2100":["Cuiying Zhou","Jinwu Ouyang","Weihua Ming","Guohao Zhang","Zichun Du","Zhen Liu"],"2101":["Maziar Raissi","George Em Karniadakis"],"2102":["Arpita Patra","Ajith Suresh"],"2103":["Werner Zellinger","Volkmar Wieser","Mohit Kumar","David Brunner","Natalia Shepeleva","Rafa G\u00e1lvez","Josef Langer","Lukas Fischer","Bernhard Moser"],"2104":["Dr. R Satya Prasad Bhanu Prakash Battula"],"2105":["Li Erran Li","Eric Chen","Jeremy Hermann","Pusheng Zhang","Luming Wang"],"2106":["Tianbao Yang","Yun Chi","Shenghuo Zhu","Yihong Gong","Rong Jin"],"2107":["Ming Li"],"2108":["Rickard Br\u00fcel Gabrielsson","Bradley J. Nelson","Anjan Dwaraknath","Primoz Skraba"],"2109":["Benjamin J. Heil","Michael M. Hoffman","Florian Markowetz","Su-In Lee","Casey S. Greene","Stephanie C. Hicks"],"2110":["Aissa Boudjella","Sarah Arab","Manal Y. Boudjella","Sarah Khiter","Bachir Bellebna"],"2111":["Kevin P. Murphy"],"2112":["Bruce Ratner","Bruce Ratner"],"2113":["Lydia T. Liu","Sarah Dean","Esther Rolf","Max Simchowitz","Moritz Hardt"],"2114":["Ilias Zadik","Lester W. Mackey","Vasilis Syrgkanis"],"2115":["Liwei Song","Reza Shokri","Prateek Mittal"],"2116":["Julian D Olden","Joshua J Lawler","N LeRoy Poff"],"2117":["Xi-Zhao Wang"],"2118":["Weiru Wang","Yanfen Gan","Chi-Man Vong","Chuangquan Chen"],"2119":["Shifei Ding","Nan Zhang","Jian Zhang","Xinzheng Xu","Zhongzhi Shi"],"2120":["Kristin Verena Kaltdorf","Maria Theiss","Sebastian Matthias Markert","Mei Zhen","Thomas Dandekar","Christian Stigloher","Philip Kollmannsberger"],"2121":["Javier Martinez","Carla Iglesias Comesa\u00f1a","Paulino Jos\u00e9 Garc\u00eda Nieto"],"2122":["Amol Ghoting","Rajasekar Krishnamurthy","Edwin Pednault","Berthold Reinwald","Vikas Sindhwani","Shirish Tatikonda","Yuanyuan Tian","Shivakumar Vaithyanathan"],"2123":["Ga\u00ebl Varoquaux","Veronika Cheplygina"],"2124":["E. Banos","I. Katakis","N. Bassiliades","G. Tsoumakas","I. Vlahavas"],"2125":["Eva Alfaro-Cid","Ken Sharman","Anna I. Esparcia-Alcazar"],"2126":["Sander M. Bohte","Hung Son Nguyen"],"2127":["Christopher Ganz"],"2128":["Peter Bod\u00edk","Rean Griffith","Charles Sutton","Armando Fox","Michael I. Jordan","David A. Patterson"],"2129":["Matthew Scowen","Ioannis N. Athanasiadis","James M. Bullock","Felix Eigenbrod","Simon Willcock"],"2130":["Jan Stampfli","Kurt Stockinger"],"2131":["Sikha Bagui"],"2132":["G.I. Webb"],"2133":["T. Menzies"],"2134":["Raffaele Tito D'Agnolo","Andrea Wulzer"],"2135":["Oriol Vinyals","Charles Blundell","Timothy Lillicrap","koray kavukcuoglu","Daan Wierstra"],"2136":["Mingming Liu","Bing Liu","Chen Zhang","Weidong Wang","Wei Sun"],"2137":["Sebastian Schindler"],"2138":["Janis Keuper","Franz-Josef Pfreundt"],"2139":["Nehemiah T. Liu","John B. Holcomb","Charles E. Wade","Andriy I. Batchinsky","Leopoldo C. Cancio","Mark I. Darrah","Jos\u00e9 Salinas"],"2140":["Alex Kulesza","Ben Taskar"],"2141":["Prateek Jain","Purushottam Kar"],"2142":["C. Shang","S. Liu","R. Shao","P. Han","X. Zang","X.l. Zhang","K. N. Salama","W. Gao","C. H. Lee","R. Thomale","A. Manchon","S. Zhang","T. J. Cui","U. Schwingenschl\u00f6gl"],"2143":["Nima Chartab","Bahram Mobasher","Asantha Cooray","Shoubaneh Hemmati","Zahra Sattari","Henry C. Ferguson","David B. Sanders","John R. Weaver","Daniel Stern","Henry J. McCracken","Daniel C. Masters","Sune Toft","Peter L. Capak","Iary Davidzon","Mark Dickinson","Jason Rhodes","Andrea Moneti","Olivier Ilbert","Lukas Zalesky","Conor McPartland","Istvan Szapudi","Anton M. Koekemoer","Harry I. Teplitz","Mauro Giavalisco"],"2144":["Lorenza Saitta","Mich\u00e8le Sebag"],"2145":["Jaimie Drozdal","Justin Weisz","Dakuo Wang","Gaurav Dass","Bingsheng Yao","Changruo Zhao","Michael Muller","Lin Ju","Hui Su"],"2146":["Thomas Abeel","Yves Van de Peer","Yvan Saeys"],"2147":["Jie Ren","Lei Wu","Jin M. Yang","Jun Zhao"],"2148":["Johannes F\u00fcrnkranz"],"2149":["George A. Tsihrintzis","Dionisios N. Sotiropoulos","Lakhmi C. Jain"],"2150":["Katharina Morik"],"2151":["Philip K. Chan"],"2152":["Tsuyoshi Kato","Yoshihiro Hirohashi"],"2153":["A. Krishna","V. Akhilesh","Animikh Aich","C. Hegde"],"2154":["Nicholas M. Ball","Robert J. Brunner"],"2155":["Kyumin Lee","James Caverlee","Steve Webb"],"2156":["Aurick Qiao","Bryon Aragam","Bingjing Zhang","Eric P. Xing"],"2157":["Markus Hittmeir","Andreas Ekelhart","Rudolf Mayer"],"2158":["Harits Ar Rosyid"],"2159":["Xingjian Shi","Zhourong Chen","Hao Wang","Dit-Yan Yeung","Wai-Kin Wong","Wang chun Woo"],"2160":["Francis R. Bach","Eric Moulines"],"2161":["Wonjae Kim","Yoonho Lee"],"2162":["Keyang Xu","Mike Lam","Jingzhi Pang","Xin Gao","Charlotte Band","Piyush Mathur","Frank Papay","Ashish K. Khanna","Jacek B. Cywinski","Kamal Maheshwari","Pengtao Xie","Eric P. Xing"],"2163":["Hong-Bo Wang","Xi Liu","Peng Song","Xu-Yan Tu"],"2164":["Jing Wang","Lin Zhang","Juanjuan Cao","Di Han"],"2165":["Yunfei Chu","Chunyan Feng","Caili Guo","Yaqing Wang"],"2166":["Jinga Liu","Muhammed J. A. Patwary","XiaoYun Sun","Kai Tao"],"2167":["Pedro Casas","Michael Seufert","Nikolas Wehner","Anika Schwind","Florian Wamser"],"2168":["Michael E. Tipping"],"2169":["Kumar Chellapilla","Patrice Y. Simard"],"2170":["Felix A. Wichmann","Arnulf B. A. Graf","Eero P. Simoncelli","Heinrich H. B\u00fclthoff","Bernhard Sch\u00f6lkopf"],"2171":["Kamalika Chaudhuri","Staal A. Vinterbo"],"2172":["Seunghak Lee","Jin Kyu Kim","Xun Zheng","Qirong Ho","Garth A. Gibson","Eric P. Xing"],"2173":["Zohar S. Karnin","Edo Liberty"],"2174":["Guanghao Zhang","Dongshun Cui","Shangbo Mao","Guang-Bin Huang"],"2175":["M. Tipping"],"2176":["Jamal Zaherpour","Nick Mount","Simon N. Gosling","Rutger Dankers","Stephanie Eisner","Dieter Gerten","Xingcai Liu","Yoshimitsu Masaki","Hannes M\u00fcller Schmied","Qiuhong Tang","Yoshihide Wada"],"2177":["Jason Weston","Samy Bengio","Nicolas Usunier"],"2178":["Ashia C. Wilson","Rebecca Roelofs","Mitchell Stern","Nati Srebro","Benjamin Recht"],"2179":["Boram Yoon"],"2180":["Albert Bifet","Ricard Gavald\u00e0","Geoffrey Holmes","Bernhard Pfahringer"],"2181":["Tatjana Eitrich","Wolfgang Frings","Bruno Lang"],"2182":["Anuschka Schmitt","Maximilian Walser","Tobias Benjamin Fahse"],"2183":["Ian H. Witten","Eibe Frank"],"2184":["David Werner"],"2185":["Ariel Linden","Paul R Yarnold"],"2186":["Francisco Pereira","Tom Mitchell","Matthew Botvinick"],"2187":["E. Eskin","Eric V. Siegel"],"2188":["Christopher M. Bishop"],"2189":["Christopher Rackauckas","Yingbo Ma","Julius Martensen","Collin Warner","Kirill Zubov","Rohit Supekar","Dominic Skinner","Ali Ramadhan","Alan Edelman"],"2190":["Peter A. Flach"],"2191":["P. Langley"],"2192":["Simon Tong","Daphne Koller"],"2193":["G. I. Webb"],"2194":["C. Cardie","R. Mooney"],"2195":["Tracy X. Chen","Rick Ebert","Joseph M. Mazzarella","Cren Frayer","Scott Terek","Ben H. P. Chan","David Cook","Tak Lo","Marion Schmitz","Xiuqin Wu"],"2196":["Poorya Zaremoodi","Wray L. Buntine","Gholamreza Haffari"],"2197":["Ga\u00ebl Varoquaux","Veronika Cheplygina"],"2198":["A. DasGupta"],"2199":["Guy Y. Cornejo Maceda","Fran\u00e7ois Lusseyran","Bernd R. Noack"],"2200":["Bernd Malle","Peter Kieseberg","Sebastian Schrittwieser","Andreas Holzinger"],"2201":["Peter Werner Eklund","Stephen Kirkby"],"2202":["Saleema Amershi","Cristina Conati"],"2203":["Jo\u00e3o Gante","Gabriel Falc\u00e3o","Leonel Sousa"],"2204":["Georgios Damaskinos","El Mahdi El Mhamdi","Rachid Guerraoui","Rhicheek Patra","Mahsa Taziki"],"2205":["Heinrich Jiang","Ofir Nachum"],"2206":["Ildar Rakhmatulin"],"2207":["Dimitris Bertsimas","Jack Dunn"],"2208":["Scott Allen Cambo","Darren Gergle"],"2209":["Nadia Boukhelifa","Anastasia Bezerianos","Evelyne Lutton"],"2210":["Jian Zhang","Shifei Ding","Nan Zhang","Zhongzhi Shi"],"2211":["Erik Cuevas","Jorge G\u00e1lvez"],"2212":["Abobakr Khalil Alshamiri","Alok Singh","Bapi Raju Surampudi"],"2213":["Reuben Binns"],"2214":["Yunfei Ye"],"2215":["Peng Liu","Yihua Huang","Lei Meng","Siyuan Gong","Guopeng Zhang"],"2216":["Imran Ahmed","Misbah Ahmad","Awais Adnan","Awais Ahmad","Murad Khan"],"2217":["Aur\u00e9lien Bellet","Rachid Guerraoui","Mahsa Taziki","Marc Tommasi"],"2218":["Christoph Raab","Frank-Michael Schleif"],"2219":["Maria Schuld","Francesco Petruccione"],"2220":["Joseph E. Beck","Beverly Park Woolf","Carole R. Beal"],"2221":["Juan Kenyhy Hancco-Quispe","Jordan Piero Borda-Colque","Fred Torres-Cruz"],"2222":["Lisha Zhang","Zhengxing Sun"],"2223":["Arnulf B. A. Graf","Felix A. Wichmann"],"2224":["Jonas Peters","Dominik Janzing","Bernhard Sch\u00f6lkopf"],"2225":["John Ross Quinlan"],"2226":["Jonathan L. Shapiro"],"2227":["Blaz Zupan","Ivan Bratko","Marko Bohanec","Janez Demsar"],"2228":["Paulos Charonyktakis","Maria Plakia","Ioannis Tsamardinos","Maria Papadopouli"],"2229":["Diego Esteves","Diego Moussallem","Ciro Baron Neto","Tommaso Soru","Ricardo Usbeck","Markus Ackermann","Jens Lehmann"],"2230":["Marwin Z\u00fcfle","Felix Moog","Veronika Lesch","Christian Krupitzer","Samuel Kounev"],"2231":["Christos Papatheodorou"],"2232":["Themis Panayiotopoulos","Nick Z. Zacharis"],"2233":["Lorenza Saitta"],"2234":["Andreas Holzinger"],"2235":["Carl Edward Rasmussen"],"2236":["Alesia Zuccala","Maarten van Someren","Maurits van Bellen"],"2237":["Ismail I Aminu"],"2238":["George D. Magoulas","Andriana Prentza"],"2239":["Joseph M. Dale","Liviu Popescu","Peter D. Karp"],"2240":["Zongyu Yin","Federico Reuben","Susan Stepney","Tom Collins"],"2241":["Vasileios Christou"],"2242":["Hua Xu","Marianthi Markatou","Rositsa Dimova","Hongfang Liu","Carol Friedman"],"2243":["Andrea L. Thomaz","Guy Hoffman","Cynthia Breazeal"],"2244":["Siegfried Rasthofer","Steven Arzt","Eric Bodden"],"2245":["Hugo Larochelle","Geoffrey E. Hinton"],"2246":["Jean Feydy","Thibault S\u00e9journ\u00e9","Francois-Xavier Vialard","Shun-ichi Amari","Alain Trouve","Gabriel Peyr\u00e9"],"2247":["Kristin P. Bennett","Emilio Parrado-Hern\u00e1ndez"],"2248":["Katarzyna Stapor","Irena Roterman-Konieczna","Piotr Fabian"],"2249":["Cheng T. Chu","Sang K. Kim","Yi A. Lin","Yuanyuan Yu","Gary R. Bradski","Andrew Y. Ng","Kunle Olukotun"],"2250":["Masashi Sugiyama"],"2251":["Sherif Kamel","Rehab Al harbi"],"2252":["Amirata Ghorbani","James Y. Zou"],"2253":["K. Bache","M. Lichman"],"2254":["Guang-Bin Huang","Dian Hui Wang","Yuan Lan"],"2255":["S. Tong","D. Koller"],"2256":["Binu P. Chacko","V. R. Vimal Krishnan","G. Raju","P. Babu Anto"],"2257":["Harsh Chaudhari","Rahul Rachuri","Ajith Suresh"],"2258":["Cynthia Dwork","Nicole Immorlica","Adam Tauman Kalai","Mark D. M. Leiserson"],"2259":["Simon Tong","Daphne Koller"],"2260":["Bryan Klimt","Yiming Yang"],"2261":["Raghu Bollapragada","Dheevatsa Mudigere","Jorge Nocedal","Hao-Jun Michael Shi","Ping Tak Peter Tang"],"2262":["Monica Agrawal","Chloe O'Connell","Yasmin Fatemi","Ariel Levy","David Sontag"],"2263":["Vincent Ng","Claire Cardie"],"2264":["John Case","Sanjay Jain"],"2265":["Marta Benito Garz\u00f3n","Radim Blazek","Markus Neteler","Rut S\u00e1nchez de Dios","Helios Sainz Ollero","Cesare Furlanello"],"2266":["Daniel Selsam","Percy Liang","David L. Dill"],"2267":["Nathan Kallus","Angela Zhou"],"2268":["Urvashi Modi","Anurag Jain"],"2269":["J. Shavlik"],"2270":["Daniel L. Silver","Qiang Yang","Lianghao Li"],"2271":["Rosanna Upstill-Goddard","Diana Eccles","Joerg Fliege","Andrew Collins"],"2272":["Todd Kulesza","Margaret Burnett","Weng K. Wong","Simone Stumpf"],"2273":["JongHyok Ri","Guanzhong Tian","Yong Liu","Wei-Hua Xu","Jun gang Lou"],"2274":["Daniel Kottke","Marek Herde","Christoph Sandrock","Denis Huseljic","Georg Krempl","Bernhard Sick"],"2275":["Marina Sapir"],"2276":["O.F. Zaidan","J. Eisner","C.D. Piatko"],"2277":["Emir Karavelic","Hermann G. Matthies","Adnan Ibrahimbegovic"],"2278":["Xing Lin","Yair Rivenson","Nezih T. Yardimci","Muhammed Veli","Yi Luo","Mona Jarrahi","Aydogan Ozcan"],"2279":["Manisha Singla","Debdas Ghosh","K. K. Shukla"],"2280":["Ian H. Witten","Eibe Frank"],"2281":["Christopher C. Lovell","Stephen M. Wilkins","Peter A. Thomas","Matthieu Schaller","Carlton M. Baugh","Giulio Fabbian","Yannick Bah\u00e9"],"2282":["Stephen Marsland"],"2283":["Edward Rosten","Tom Drummond"],"2284":["No\u00e9mi Friedman","Abdel Labbi"],"2285":["Anni K\u00e4\u00e4ri\u00e4inen","Vilma Pesola","Annalena Dittmann","Juho Kontio","Jarkko Koivunen","Taina Pihlajaniemi","Valerio Izzi"],"2286":["Jonathan Baxter"],"2287":["Valerie Sessions","Marco Valtorta"],"2288":["M Yasodha","M. Selva Boopathi"],"2289":["Osman Hegazy","Omar S. Soliman","Mustafa Abdul Salam"],"2290":["Thirukumaran S","Regin Rajan"],"2291":["\u00d6zlem Uzuner","Xiaoran Zhang","Tawanda Sibandab"],"2292":["Shreya Agrawal","Luke Barrington","Carla Bromberg","John Burge","Cenk Gazen","Jason Hickey"],"2293":["Ildar Rakhmatulin"],"2294":["Roheet Bhatnagar"],"2295":["Noureldin Laban","Bassam Abdellatif","Hala M. Ebeid","Howida A. Shedeed","Mohamed F. Tolba"],"2296":["Hillol Kargupta","Samiran Ghosh"],"2297":["Lukas Ruff","Robert Vandermeulen","Nico Goernitz","Lucas Deecke","Shoaib Ahmed Siddiqui","Alexander Binder","Emmanuel M\u00fcller","Marius Kloft"],"2298":["Christian Rack","Andreas Hotho","Marc Erich Latoschik"],"2299":["Fei Lv","Min Han"],"2300":["Grigoris I. Karakoulas","Giovanni Semeraro"],"2301":["Satish K. Jain","Amalendu Patnaik","Sachendra N. Sinha"],"2302":["Vinod Kumar Chauhan","Anuj Sharma","Kalpana Dahiya"],"2303":["Heath Yates","Brent Chamberlain","William H. Hsu"],"2304":["Harsh Jain","Keshav Mathur"],"2305":["J.G. Carbonell","R.S. Michalski","T.M. Mitchell"],"2306":["Roberto Valenti","Nicu Sebe","Theo Gevers","Ira Cohen"],"2307":["Nikos D. Hatziargyriou"],"2308":["Nikos Fakotakis","Kyriakos N. Sgarbas"],"2309":["Fengxue Li","Huaping Liu","Xinying Xu","Fuchun Sun"],"2310":["Laizhong Cui","Shu Yang","Fei Chen","Zhong Ming","Nan Lu","Jing Qin"],"2311":["Junhua Ding","Xin-Hua Hu","Venkat Gudivada"],"2312":["Kerstin Weber"],"2313":["Stephen Jose Hanson","Werner Remmele","Ronald L. Rivest"],"2314":["Aaron Klein","Stefan Falkner","Simon Bartels","Philipp Hennig","Frank Hutter"],"2315":["Mariusz Bojarski","Anna Choromanska","Krzysztof Choromanski","Francois Fagan","C\u00e9dric Gouy-Pailler","Anne Morvan","Nourhan Sakr","Tam\u00e1s Sarl\u00f3s","Jamal Atif"],"2316":["Ross Kleiman","David Page"],"2317":["Mark W. Stephenson"],"2318":["Carsten Felix Draschner","Claus Stadler","Farshad Bakhshandegan Moghaddam","Jens Lehmann","Hajira Jabeen"],"2319":["Florian Skopik","Markus Wurzenberger","Max Landauer"],"2320":["Muhammad Waqas Arshad"],"2321":["Stefano Teso","Kristian Kersting"],"2322":["Peter Nordin","Wolfgang Banzhaf"],"2323":["Alberto Paolo Tonda","Nadia Boukhelifa","Thomas Chabin","Marc Barnab\u00e9","Beno\u00eet G\u00e9not","Evelyne Lutton","Nathalie Perrot"],"2324":["Susan Athey","Guido W. Imbens"],"2325":["Y. Yang","G.I. Webb","K. Korb","K-M. Ting"],"2326":["Joaquin Qui\u00f1onero-Candela","Ido Dagan","Bernardo Magnini","Florence d\u2019Alch\u00e9 Buc"],"2327":["Giorgos Borboudakis","Txiarchis Stergiannakos","Maria Frysali","Emmanuel Klontzas","Ioannis Tsamardinos","George E. Froudakis"],"2328":["Yangqing Jia","Joshua T. Abbott","Joseph L. Austerweil","Thomas L. Griffiths","Trevor Darrell"],"2329":["Han Liu"],"2330":["Alireza Shafizadeh","Hossein Shahbeik","Shahin Rafiee","Aysooda Moradi","Mohammadreza Shahbaz","Meysam Madadi","Cheng Li","Wanxi Peng","Meisam Tabatabaei","Mortaza Aghbashlo"],"2331":["A. Joshi","N. Ramakrishman","E.N. Houstis","J.R. Rice"],"2332":["Steven L. Brunton","Bernd R. Noack","Petros Koumoutsakos"],"2333":["Nitin Muttil","Kwok-Wing Chau"],"2334":["S. Suman Rajest","S. Silvia Priscila","R. Regin","Shynu T","Steffi. R"],"2335":["Vinitha S Sweetlin S Vinusha H Sajini S"],"2336":["Saleema Amershi","James Fogarty","Daniel Weld"],"2337":["Li Deng","Xiao Li"],"2338":["W. D. Jennings","C. A. Watkinson","F. B. Abdalla"],"2339":["A. Mayr","H. Binder","O. Gefeller","M. Schmid"],"2340":["Fabio Ciravegna","Sam Chapman"],"2341":["Samuel Yeom","Irene Giacomelli","Matt Fredrikson","Somesh Jha"],"2342":["Shuaihua Lu","Qionghua Zhou","Yixin Ouyang","Yilv Guo","Qiang Li","Jinlan Wang"],"2343":["Kasper Christensen","Sladjana N\u00f8rskov","Lars Frederiksen","Joachim Scholderer"],"2344":["Archana Kale","Shefali Sonavane"],"2345":["Xiong Luo","Ying Li","Weiping Wang","Xiaojuan Ban","Jenq-Haur Wang","Wenbing Zhao"],"2346":["Qiang He","Congxin Wu"],"2347":["Bhagat Singh Raghuwanshi","Sanyam Shukla"],"2348":["Siqi Wang","Qiang Liu","Xifeng Guo","En Zhu","Jianping Yin"],"2349":["Jun Wu","Shitong Wang","Fu-Lai Chung"],"2350":["Ardalan Ghasemzadeh","Saeed Sarbazi Azad","Elham Esmaeili"],"2351":["Tim Paek","Roberto Pieraccini"],"2352":["Daniel R. Schrider","Andrew D. Kern"],"2353":["John C. Snyder","Matthias Rupp","Katja Hansen","Klaus-Robert M\u00fcller","Kieron Burke"],"2354":["Thomas Gaertner","John W. Lloyd","Peter A. Flach"],"2355":["Heath Yates","Brent Chamberlain","Greg Norman","William H. Hsu"],"2356":["D. Sculley","Gary Holt","Daniel Golovin","Eugene Davydov","Todd Phillips","Dietmar Ebner","Vinay Chaudhary","Michael Young"],"2357":["Christian Federmann"],"2358":["Avrim Blum","M.-F. Balcan","J.D. Hartline","Y. Mansour"],"2359":["Pradeep Chowriappa","Sumeet Dua","Yavor Todorov"],"2360":["Qatawneh"],"2361":["Sana Tonekaboni","Shalmali Joshi","Melissa D. McCradden","Anna Goldenberg"],"2362":["Castro Mayleen Dorcas Bondoc","Tumibay Gilbert Malawit"],"2363":["Davide Bacciu","Antonio Carta","Stefania Gnesi","Laura Semini"],"2364":["Simon Rogers","Mark A. Girolami"],"2365":["Sebastian Raschka"],"2366":["Nehal M. Ali","Mohamed Shaheen","Mai S. Mabrouk","Mohamed A. AboRezka"],"2367":["Joelle Pineau","Philippe Vincent-Lamarre","Koustuv Sinha","Vincent Larivi\u00e8re","Alina Beygelzimer","Florence d'Alch\u00e9 Buc","Emily Fox","Hugo Larochelle"],"2368":["Halina Kwasnicka","Mariusz Paradowski"],"2369":["Jianlong Zhou","Kun Yu","Fang Chen"],"2370":["Martin J. Wainwright"],"2371":["Sargur N. Srihari","Harish Srinivasan","Siyuan Chen","Matthew J. Beal"],"2372":["Justin Solomon"],"2373":["Pablo Villalobos","Jaime Sevilla","Lennart Heim","Tamay Besiroglu","Marius Hobbhahn","Anson Ho"],"2374":["David V. Pynadath","Michael J. Barnes","Ning Wang","Jessie Y. C. Chen"],"2375":["J R Coelho","J A Carri\u00e7o","D Knight","J L Mart\u00ednez","I Morrissey","M R Oggioni","A T Freitas"],"2376":["Hongxin Wei","Renchunzi Xie","Hao Cheng","Lei Feng","Bo An","Yixuan Li"],"2377":["Siddhartha Giri","Hendrik Noske","Sergej Zerr"],"2378":["Gholamreza Nakhaeizadeh","Charles C. Taylor"],"2379":["Aude Genevay","L\u00e9na\u00efc Chizat","Francis Bach","Marco Cuturi","Gabriel Peyr\u00e9"],"2380":["Barry Fitzgerald","Conor Ryan","Joe Sullivan"],"2381":["Pedro Casas","Alessandro D'Alconzo","Florian Wamser","Michael Seufert","Bruno Gardlo","Anika Schwind","Phuoc Tran-Gia","Raimund Schatz"],"2382":["S. J. Barrett","W. B. Langdon"],"2383":["Matthias Boehm","Arun Kumar","Jun Yang"],"2384":["Stephen G. Wu","Yuxuan Wang","Wu Jiang","Tolutola Oyetunde","Ruilian Yao","Xuehong Zhang","Kazuyuki Shimizu","Yinjie J. Tang","Forrest S. Bao"],"2385":["Nicolas Tempelmeier","Elena Demidova"],"2386":["Xingmin Zhao","Weipeng Cao","Hongyu Zhu","Zhong Ming","Rana Aamir Raza Ashfaq"],"2387":["Juho Lee","Yoonho Lee","Jungtaek Kim","Adam Kosiorek","Seungjin Choi","Yee Whye Teh"],"2388":["Hilbert J. Kappen","Francisco de Borja Rodr\u00edguez Ortiz"],"2389":["Dmitry Chernyavsky","Jeroen van den Brink","Gyu-Hyeon Park","Kornelius Nielsch","Andy Thomas"],"2390":["Antoine Bordes","Xavier Glorot","Jason Weston","Yoshua Bengio"],"2391":["Milagros Miceli","Julian Posada","Tianling Yang"],"2392":["Yanika Kongsorot","Punyaphol Horata","Pakarat Musikawan","Khamron Sunat"],"2393":["AnHai Doan","Pedro Domingos","Alon Y. Halevy"],"2394":["Arjun Reddy Kunduru"],"2395":["P C Chen","Y Liu","L Peng"],"2396":["Murali Ravuri","Anitha Kannan","Geoffrey J. Tso","Xavier Amatriain"],"2397":["Kshitij Bansal","Sarah M. Loos","Markus N. Rabe","Christian Szegedy","Stewart Wilcox"],"2398":["Lam M. Nguyen","Jie Liu","Katya Scheinberg","Martin Tak\u00e1c"],"2399":["Jens Decke","Anna Engelhardt","Lukas Rauch","Sebastian Degener","Seyedvahid Sajjadifar","Emad Scharifi","Kurt Steinhoff","Thomas Niendorf","Bernhard Sick"],"2400":["Michele Dolfi","Christoph Auer","Peter W. J. Staar","Costas Bekas"],"2401":["Rishabh Agarwal","Nicholas Frosst","Xuezhou Zhang","Rich Caruana","Geoffrey E. Hinton"],"2402":["Ren\u00e9 Heinrich","Christoph Scholz","Stephan Vogt","Malte Lehna"],"2403":["Puram Surya Prudvi"],"2404":["St\u00e9phane D'Ascoli","Pierre-Alexandre Kamienny","Guillaume Lample","Francois Charton"],"2405":["Ninareh Mehrabi","Fred Morstatter","Nripsuta Saxena","Kristina Lerman","Aram Galstyan"],"2406":["Puram Surya Prudvi","Ershad Sharifahmadian"],"2407":["A. Mccallum","K. Nigam","J. Rennie","K. Seymore"],"2408":["Sarah Wassermann","Nikolas Wehner","Pedro Casas"],"2409":["Puram Surya Prudvi","Ershad Sharifahmadian"],"2410":["Grace W. Lindsay"],"2411":["Andrew Kachites McCallum","Kamal Nigam","Jason Rennie","Kristie Seymore"],"2412":["L. Yao","Z. Ge"],"2413":["Cen Wang","Zhaoying Jia","Zhaohui Yin","Fei Liu","Gaopeng Lu","Jianqiu Zheng"],"2414":["Kitiporn Plaimas","Roland Eils","Rainer Konig"],"2415":["Zeeshan Ahmed","Saman Majeed"],"2416":["Dominik Dellermann","Nikolaus Lipusch","Mahei Li"],"2417":["Aboul Ella Hassanien","Hameed Al-Qaheri","V\u00e1clav Sn\u00e1sel","James F. Peters"],"2418":["Boris Kryzhanovsky","Vladimir Kryzhanovsky","Leonid B. Litinskii"],"2419":["Smaranda Belciug"],"2420":["Dan Lu","Daniel Ricciuto"],"2421":["Tessa Lau","Steven A. Wolfman","Pedro Domingos","Daniel S. Weld"],"2422":["D S Myers","M P Cummings"],"2423":["Wei Zhou","Shaojie Qiao","Yugen Yi","Nan Han","Yuqi Chen","Gang Lei"],"2424":["Senlin Cheng","Yang Xu","Ruixue Zong","Chuanhai Wang"],"2425":["Christopher J. C. Burges"],"2426":["Jing Fang","Xinying Xu","Huaping Liu","Fuchun Sun"],"2427":["Du Ni","Zhi Xiao","Ming Kim Lim"],"2428":["Sebastian Robert","Sebastian B\u00fcttner","Carsten R\u00f6cker","Andreas Holzinger"],"2429":["AnHai Doan","Jayant Madhavan","Pedro Domingos","Alon Halevy"],"2430":["Ivan Bratko"],"2431":["R. Regin","S. Suman Rajest","Shynu T","Steffi. R",""],"2432":["Cosima Gretton"],"2433":["Robert Zheng","Kevin Greenberg"],"2434":["A. Perini","A. Susi","P. Avesani"],"2435":["Jakir Khan","Dr. Ganesh D"],"2436":["Y. Lu","F. Peng","X. Li","N. Ahmed"],"2437":["Gaoxiang Li","Khalid T. Mursi","Ahmad O. Aseeri","Mohammed S. Alkatheiri","Yu Zhuang"],"2438":["Joseph Jay Williams","Juho Kim","Anna Rafferty","Samuel Maldonado","Krzysztof Z. Gajos","Walter S. Lasecki","Neil Heffernan"],"2439":["Anne-Laure Rousseau"],"2440":["Sven Koenig","Yury V. Smirnov"],"2441":["Natalya Fridman Noy","Mark A. Musen"],"2442":["Chris Rackauckas","Ranjan Anantharaman","Alan Edelman","Shashi Gowda","Maja Gwozdz","Anand Jain","Chris Laughman","Yingbo Ma","Francesco Martinuzzi","Avik Pal","Utkarsh Rajput","Elliot Saba","Viral B. Shah"],"2443":["Mark Boyer"],"2444":["Savannah L. Bergquist","Gabriel A. Brooks","Nancy L. Keating","Mary Beth Landrum","Sherri Rose"],"2445":["Peter Stone","Manuela M. Veloso"],"2446":["David Heckerman","Dan Geiger"],"2447":["K. Chaudhuri","C. Monteleoni","A.D. Sarwate"],"2448":["Victor Chernozhukov","Kaspar W\u00fcthrich","Yinchu Zhu"],"2449":["Brian Falkenhainer","Kenneth D. Forbus","Dedre Gentner"],"2450":["Selin Merdan","Khurshid Ghani","Brian Denton"],"2451":["Mariano Rico","Rizkallah Touma","Anna Queralt","Mar\u00eda S. P\u00e9rez"],"2452":["Georgios Petasis","Frantz Vichot","Francis Wolinski","Georgios Paliouras","Vangelis Karkaletsis","Constantine D. Spyropoulos"],"2453":["Ashish Kumar","Saurabh Goyal","Manik Varma"],"2454":["Leo Breiman"],"2455":["Charles Parker","Yasemin Altun","Prasad Tadepalli"],"2456":["Claire D\u2019Este Ritaban Dutta"],"2457":["Solon Barocas","Moritz Hardt","Arvind Narayanan"],"2458":["Haigang Zhang","Sen Zhang","Yixin Yin","Xianzhong Chen"],"2459":["Pak-Kin Wong","Xiang Hui Gao","Ka In Wong","Chi-Man Vong","Zhi-Xin Yang"],"2460":["Laurens J. P. van der Maaten","Paul J. Boon","Guus Lange","Hans Paijmans","Eric O. Postma"],"2461":["Zeshui Xu","Dejian Yu","Xizhao Wang"],"2462":["Yang Luo","Benqiang Yang","Lisheng Xu","Liling Hao","Jun Liu","Yang Yao","Frans N. van de Vosse"],"2463":["Tai Le Quy","Arjun Roy","Vasileios Iosifidis","Wenbin Zhang","Eirini Ntoutsi"],"2464":["Fernanda Ostrovski","Richard G. McMahon","Andrew J. Connolly","Cameron A. Lemon","Matthew W. Auger","Manda Banerji","Johnathan M. Hung","Sergey E. Koposov","Christopher E. Lidman","Sophie L. Reed","Sahar Allam","Aur\u00e9lien Benoit-L\u00e9vy","Emmanuel Bertin","David Brooks","Elizabeth Buckley-Geer","Aurelio Carnero Rosell","Matias Carrasco Kind","Jorge Carretero","Carlos E. Cunha","Luiz N. da Costa","Shantanu Desai","H. Thomas Diehl","J\u00f6rg P. Dietrich","August E. Evrard","David A. Finley","Brenna Flaugher","Pablo Fosalba","Josh Frieman","David W. Gerdes","Daniel A. Goldstein","Daniel Gruen","Robert A. Gruendl","Gaston Gutierrez","Klaus Honscheid","David J. James","Kyler Kuehn","Nikolay Kuropatkin","Marcos Lima","Huan Lin","Marcio A. G. Maia","Jennifer L. Marshall","Paul Martini","Peter Melchior","Ramon Miquel","Ricardo Ogando","Andr\u00e9s Plazas Malag\u00f3n","Kevin Reil","Kathy Romer","Eusebio Sanchez","Basilio Santiago","Vic Scarpine","Ignacio Sevilla-Noarbe","Marcelle Soares-Santos","Flavia Sobreira","Eric Suchyta","Gregory Tarle","Daniel Thomas","Douglas L. Tucker","Alistair R. Walker"],"2465":["Fleur Jeanquartier","Claire Jean-Quartier","Max Kotlyar","Tom\u00e1s Tok\u00e1r","Anne-Christin Hauschild","Igor Jurisica","Andreas Holzinger"],"2466":["Behnoush Abdollahi","Olfa Nasraoui"],"2467":["Jianlong Zhou","Fang Chen"],"2468":["Ashis Kumer K. Biswas","Nasimul Noman","Abdur Rahman R. Sikder"],"2469":["Sung-Bae Cho","Hong-Hee Won"],"2470":["Martin Toepfer"],"2471":["Benjamin Aubin","Antoine Maillard","Jean Barbier","Florent Krzakala","Nicolas Macris","Lenka Zdeborov\u00e1"],"2472":["Csaba Moln\u00c3\u00a1r","Fr\u00c3\u00a9d\u00c3\u00a9ric Kaplan","Pierre Roy","Fran\u00c3\u00a7ois Pachet","P\u00c3\u00a9ter Pongr\u00c3\u00a1cz","Antal D\u00c3\u00b3ka","\u00c3\u0081d\u00c3\u00a1m Mikl\u00c3\u00b3si"],"2473":["Roshan Joy Martis","Chandan Chakraborty","Ajoy Kumar Ray"],"2474":["Walter F. Stewart","Jason Roy","Jimeng Sun","Shahram Ebadollahi"],"2475":["Frank D. Francone","Larry M. Deschaine"],"2476":["S. Sette","L. Boullart"],"2477":["Quoc Le","Tomas Mikolov"],"2478":["Dinggang Shen","Chong-Yaw Wee","Daoqiang Zhang","Luping Zhou","Pew-Thian Yap"],"2479":["Sudip Regmi","Larry M. Deschaine","Sharad R. Regmi"],"2480":["Jonatas Wehrmann","Ricardo Cerri","Rodrigo Barros"],"2481":["D. M. Okhunov"],"2482":["J. Agar","G. I. Webb"],"2483":["Branden Chan","Stefan Schweter","Timo M\u00f6ller"],"2484":["David Dohan","Winnie Xu","Aitor Lewkowycz","Jacob Austin","David Bieber","Raphael Gontijo Lopes","Yuhuai Wu","Henryk Michalewski","Rif A. Saurous","Jascha Sohl dickstein","Kevin Murphy","Charles Sutton"],"2485":["Guillaume Lample","Alexis Conneau"],"2486":["Nitish Shirish Keskar","Bryan McCann","Lav R. Varshney","Caiming Xiong","Richard Socher"],"2487":["Luis Quesada","Fernando Berzal","Juan-Carlos Cubero"],"2488":["Yoshua Bengio","R\u00e9jean Ducharme","Pascal Vincent","Christian Janvin"],"2489":["G\u00fcnes Erkan"],"2490":["D. A. Khidoyatova"],"2491":["Adam Roberts","Colin Raffel","Noam Shazeer"],"2492":["Yuan He","Jiaoyan Chen","Ernesto Jim\u00e9nez-Ruiz","Hang Dong","Ian Horrocks"],"2493":["Jeremy Howard","Sebastian Ruder"],"2494":["Lance De Vine","Guido Zuccon","Bevan Koopman","Laurianne Sitbon","Peter Bruza"],"2495":["Jiafeng Liu","Yuanliang Dong","Zehua Cheng","Xinran Zhang","Xiaobing Li","Feng Yu","Maosong Sun"],"2496":["M. W\u00fcthrich","M. Liwicki","A. Fischer","E. Inderm\u00fchle","H. Bunke","G. Viehhauser","M. Stolz"],"2497":["Alexis Conneau","Guillaume Lample"],"2498":["Yoshua Bengio","R\u00e9jean Ducharme","Pascal Vincent"],"2499":["Ehsan Hosseini-Asl","Bryan McCann","Chien-Sheng Wu","Semih Yavuz","Richard Socher"],"2500":["Fei Song","Bruce W. Croft"],"2501":["D. A. Khidoyatova"],"2502":["Li Dong","Nan Yang","Wenhui Wang","Furu Wei","Xiaodong Liu","Yu Wang","Jianfeng Gao","Ming Zhou","Hsiao-Wuen Hon"],"2503":["Andriy Mnih","Geoffrey E. Hinton"],"2504":["Mbeng Sampson Tambe"],"2505":["Beliz Gunel","Jingfei Du","Alexis Conneau","Ves Stoyanov"],"2506":["Takahiro Minamikawa","Tatsuhiro Tsuchiya","Tohru Kikuno"],"2507":["T. Bultan"],"2508":["J. Ignacio Serrano","M. Dolores del Castillo","A. Iglesias"],"2509":["Max Sch\u00e4fer","Sarah Nadi","Aryaz Eghbali","Frank Tip"],"2510":["Louis Martin","Benjamin Muller","Pedro Javier Ortiz Su\u00e1rez","Yoann Dupont","Laurent Romary","\u00c9ric Villemonte de la Clergerie","Djam\u00e9 Seddah","Beno\u00eet Sagot"],"2511":["Bin Bi","Chenliang Li","Chen Wu","Ming Yan","Wei Wang","Songfang Huang","Fei Huang","Luo Si"],"2512":["Peng Xu","Frederick Jelinek"],"2513":["Yik-Cheung Tam","Tanja Schultz"],"2514":["Hangfeng He","Hongming Zhang","Dan Roth"],"2515":["Thorsten Brants"],"2516":["Hong Liu","Zhiyuan Li","David Hall","Percy Liang","Tengyu Ma"],"2517":["Yoon Kim","Yacine Jernite","David Sontag","Alexander M. Rush"],"2518":["Luiz Pizzato","Diego Moll\u00e1"],"2519":["Stewart M McCauley","Morten H Christiansen"],"2520":["M. Markogova"],"2521":["Dae-Kyoo Kim","Robert France","Sudipto Ghosh","Eunjee Song"],"2522":["D'aniel Varr'o","Andras Balogh"],"2523":["Martin M\u00fcller","Marcel Salath\u00e9","Per E Kummervold"],"2524":["Mohammad Shoeybi","Mostofa Patwary","Raul Puri","Patrick LeGresley","Jared Casper","Bryan Catanzaro"],"2525":["Edouard Grave","Moustapha Ciss\u00e9","Armand Joulin"],"2526":["Yasumasa Miyamoto","Kyunghyun Cho"],"2527":["Azucena Montes","Hasdai Pacheco","Hugo Estrada","Oscar Pastor"],"2528":["Yuangen Lai","Jianxun Zeng"],"2529":["Patrick H. Chen","Si Si","Yang Li","Ciprian Chelba","Cho-Jui Hsieh"],"2530":["Qiang Pu","Daqing He"],"2531":["Yichen Zhu","Minjie Zhu","Ning Liu","Zhicai Ou","Xiaofeng Mou","Jian Tang"],"2532":["Timo Niemi","Lasse Hirvonen","Kalervo J\u00ed?rvelin"],"2533":["Tom B. Brown","Benjamin Mann","Nick Ryder","Melanie Subbiah","Jared Kaplan","Prafulla Dhariwal","Arvind Neelakantan","Pranav Shyam","Girish Sastry","Amanda Askell","Sandhini Agarwal","Ariel Herbert-Voss","Gretchen Krueger","Tom Henighan","Rewon Child","Aditya Ramesh","Daniel M. Ziegler","Jeffrey Wu","Clemens Winter","Christopher Hesse","Mark Chen","Eric Sigler","Mateusz Litwin","Scott Gray","Benjamin Chess","Jack Clark","Christopher Berner","Sam McCandlish","Alec Radford","Ilya Sutskever","Dario Amodei"],"2534":["S. Pinker","A. Prince"],"2535":["Andreas W\u00f6\u00df","Christian Wirth","Daniele Bonetta","Chris Seaton","Christian Humer","Hanspeter M\u00f6ssenb\u00f6ck"],"2536":["David W. Archer","Lois M. L. Delcambre"],"2537":["Fahad Kamal Alsheref Prof. Dr. Torkey I.Sultan Dr. Ayman E. Khedr"],"2538":["Dan Klein","Christopher D. Manning"],"2539":["Ping Wang","Tian Shi","Chandan K. Reddy"],"2540":["Dan Klein","Christopher D. Manning"],"2541":["Timo Schick","Jane Dwivedi-Yu","Roberto Dess\u00ec","Roberta Raileanu","Maria Lomeli","Luke Zettlemoyer","Nicola Cancedda","Thomas Scialom"],"2542":["T. Lodderstedt","D. Basin","J. Doser"],"2543":["Yoshua Bengio","R\u00e9jean Ducharme","Pascal Vincent","Christian Jauvin"],"2544":["Kris Gybels","Roel Wuyts","St\u00e9phane Ducasse","Maja D'Hondt"],"2545":["Hercules Dalianis"],"2546":["Alec Radford","Jeff Wu","Rewon Child","D. Luan","Dario Amodei","Ilya Sutskever"],"2547":["F. Guo"],"2548":["Tomas Mikolov","Martin Karafi\u00e1t","Lukas Burget","Jan Cernock\\`y","Sanjeev Khudanpur"],"2549":["Yishay Mor","Niall Winters"],"2550":["Bernardo Cuenca Grau","Boris Motik"],"2551":["Guihong Gao","Jian-Yun Nie","Jing Bai"],"2552":["Z. Valsan","M. Emele"],"2553":["Ivan Kurtev"],"2554":["J\u00fcrgen Walter","Simon Eismann","Adrian Hildebrandt"],"2555":["Prabhat Mali","Aman Shakya","Sanjeeb Prasad Panday"],"2556":["Vincenzo Grassi","Raffaela Mirandola","Enrico Randazzo","Antonino Sabetta"],"2557":["Roswitha Bardohl","Hartmut Ehrig","Juan de Lara","Gabriele Taentzer"],"2558":["Jean B\u00e9zivin","Reiko Heckel"],"2559":["Gilad Mishne","David Carmel","Ronny Lempel"],"2560":["Victor Sanh","Lysandre Debut","Julien Chaumond","Thomas Wolf"],"2561":["G. S. Dell","L. K. Burger","W. R. Svec"],"2562":["V\u00edctor L\u00f3pez Salazar","Eduardo M. Eisman Cabeza","Juan Luis Castro Pe\u00f1a","Jose Manuel Zurita L\u00f3pez"],"2563":["Nicolas Le Novcre","Michael Hucka","Stefan Hoops","Sarah Keating","Sven Sahle","Darren Wilkinson"],"2564":["V\\'\u0131ctor L\u00f3pez Salazar","Eduardo M. Eisman Cabeza","Juan Luis Castro Pe\\ na","Jose Manuel Zurita L\u00f3pez"],"2565":["Daisuke Okanohara","Jun'ichi Tsujii"],"2566":["Stephen MacNeil","Andrew Tran","Dan Mogil","Seth Bernstein","Erin Ross","Ziheng Huang"],"2567":["Jinhyuk Lee","Wonjin Yoon","Sungdong Kim","Donghyeon Kim","Sunkyu Kim","Chan Ho So","Jaewoo Kang"],"2568":["Jean B\u00e9zivin","Reiko Heckel"],"2569":["Steven I. Ross","Fernando Martinez","Stephanie Houde","Michael Muller","Justin D. Weisz"],"2570":["Roland Hausser"],"2571":["Alec Radford","Jeffrey Wu","Rewon Child","David Luan","Dario Amodei","Ilya Sutskever"],"2572":["Aytu\u011f Onan","Mansur Alp To\u00e7o\u011flu"],"2573":["Yuxian Gu","Li Dong","Furu Wei","Minlie Huang"],"2574":["Eduard Hoenkamp","Peter Bruza","Dawei Song","Qiang Huang"],"2575":["Jonathan Sprinkle","G. Gabor Karsai"],"2576":["Frank F. Xu","Uri Alon","Graham Neubig","Vincent J. Hellendoorn"],"2577":["Karsten Ehrig","Claudia Ermel","Stefan Haensgen"],"2578":["Kuchillapati Chinnari","Dr. Adusumalli Balaji"],"2579":["Weijia Shi","Sewon Min","Michihiro Yasunaga","Minjoon Seo","Rich James","Mike Lewis","Luke Zettlemoyer","Wen-tau Yih"],"2580":["Jordan Hoffmann","Sebastian Borgeaud","Arthur Mensch","Elena Buchatskaya","Trevor Cai","Eliza Rutherford","Diego de Las Casas","Lisa Anne Hendricks","Johannes Welbl","Aidan Clark","Tom Hennigan","Eric Noland","Katie Millican","George van den Driessche","Bogdan Damoc","Aurelia Guy","Simon Osindero","Karen Simonyan","Erich Elsen","Jack W. Rae","Oriol Vinyals","Laurent Sifre"],"2581":["Dimitrios Kolovos","Richard F. Paige","Fiona Polack"],"2582":["Takashi Tomokiyo","Matthew Hurst"],"2583":["Nikolaus Huber","Fabian Brosig","Simon Spinner","Samuel Kounev","Manuel B\u00e4hr"],"2584":["Wayne Xin Zhao","Kun Zhou","Junyi Li","Tianyi Tang","Xiaolei Wang","Yupeng Hou","Yingqian Min","Beichen Zhang","Junjie Zhang","Zican Dong","Yifan Du","Chen Yang","Yushuo Chen","Zhipeng Chen","Jinhao Jiang","Ruiyang Ren","Yifan Li","Xinyu Tang","Zikang Liu","Peiyu Liu","Jian-Yun Nie","Ji-Rong Wen"],"2585":["Bruno Mermet","Ga\u00eble Simon"],"2586":["Quan Z. Sheng","Boualem Benatallah"],"2587":["Fabian Chersi","Marcello Ferro","Giovanni Pezzulo","Vito Pirrelli"],"2588":["Chunyan Yu","Minghui Wu","Nairuo Liu","Yueting Zhuang","Yunhe Pan"],"2589":["Eugene Syriani","Jeff Gray","Hans Vangheluwe"],"2590":["T Winograd","R. Schank"],"2591":["Ryan Brate","Minh-Hoang Dang","Fabian Hoppe","Yuan He","Albert Mero\u00f1o-Pe\u00f1uela","Vijay Sadashivaiah"],"2592":["Mark Braverman","Xinyi Chen","Sham M. Kakade","Karthik Narasimhan","Cyril Zhang","Yi Zhang"],"2593":["Robert \u00d6stling"],"2594":["Edouard Grave","Armand Joulin","Nicolas Usunier"],"2595":["Xiaoqi Jiao","Yichun Yin","Lifeng Shang","Xin Jiang","Xiao Chen","Linlin Li","F. Wang","Qun Liu"],"2596":["Erik Elmroth","Francisco Hernandez","Johan Tordsson"],"2597":["J. Bilmes","K. Kirchhoff"],"2598":["Fran\u00e7oise Beaufays","Brian Strope"],"2599":["Christian Gillot","Christophe Cerisara"],"2600":["Yucong Duan","Christophe Cruz","Abdelrahman Osman Elfaki","Yang Bai","Wencai Du"],"2601":["Long Ouyang","Jeff Wu","Xu Jiang","Diogo Almeida","Carroll L. Wainwright","Pamela Mishkin","Chong Zhang","Sandhini Agarwal","Katarina Slama","Alex Ray","John Schulman","Jacob Hilton","Fraser Kelton","Luke Miller","Maddie Simens","Amanda Askell","Peter Welinder","Paul Christiano","Jan Leike","Ryan Lowe"],"2602":["Rostislav Nedelchev","Jens Lehmann","Ricardo Usbeck"],"2603":["Abigail See","Aneesh Pappu","Rohun Saxena","Akhila Yerukola","Christopher D. Manning"],"2604":["Giuseppe Riccardi","Alexandros Potamianos","Shrikanth S. Narayanan"],"2605":["Xiaoyong Liu","W. Bruce Croft"],"2606":["Umurzakova Bonuxon Azizovna"],"2607":["Taylor Webb","Keith J. Holyoak","Hongjing Lu"],"2608":["Jason Wei","Xuezhi Wang","Dale Schuurmans","Maarten Bosma","Ed Chi","Quoc Le","Denny Zhou"],"2609":["Thorsten Brants"],"2610":["Takanobu Oba","Takaaki Hori","Atsushi Nakamura","Akinori Ito"],"2611":["Thorsten Brants"],"2612":["Jiaying Zhang","Zhixing Zhang","Huanhuan Zhang","Zhiyuan Ma","Yangming Zhou","Ping He"],"2613":["Loic Maisonnasse","Gilles S\u00e9rasset","Jean-Pierre Chevallet"],"2614":["Julian Salazar","Davis Liang","Toan Q. Nguyen","Katrin Kirchhoff"],"2615":["Tom\u00e1s Brychc\u00edn"],"2616":["Branden Chan","Stefan Schweter","Timo M\u00f6ller"],"2617":["Wesley Holland","Julie Baca","Dhruva Duncan","Joseph Picone"],"2618":["V\u00edt Baisa"],"2619":["Arne S\u00f8lvberg","Terje Brasethvik"],"2620":["Stuart Kent"],"2621":["Stefan Besling","Hans-G\u00fcnter Meier"],"2622":["Vasily Pestun","Yiannis Vlassopoulos"],"2623":["Luiz Augusto Sangoi Pizzato","Diego Moll\u00e1"],"2624":["Michiel Bacchiani","Brian Roark"],"2625":["Kisuh Ahn","Eunsuk Lim"],"2626":["Dayu Yuan","Ryan Doherty","Julian Richardson","Colin Evans","Eric Altendorf"],"2627":["Ruth Garrett Millikan"],"2628":["Salma Jamoussi","David Langlois","Jean Paul Haton","Kamel Sma\u00efli"],"2629":["Xuan Liu","Di Cao","Kai Yu"],"2630":["Ciprian Chelba"],"2631":["Audris Kalnins","Janis Barzdins","Edgars Celms"],"2632":["Kazunori Ueda","Norio Kato"],"2633":["Octavian Patrascoiu"],"2634":["Ciprian Chelba"],"2635":["Ori Ram","Yoav Levine","Itay Dalmedigos","Dor Muhlgay","Amnon Shashua","Kevin Leyton-Brown","Yoav Shoham"],"2636":["Liping Mu","Terje Gj\\textbackslashos\u00e6ter","Andreas Prinz","Merete Skjelten Tveit"],"2637":["Edward J. Hu","Yelong Shen","Phillip Wallis","Zeyuan Allen-Zhu","Yuanzhi Li","Shean Wang","Lu Wang","Weizhu Chen"],"2638":["Sergey Edunov","Alexei Baevski","Michael Auli"],"2639":["Seunghak Yu","Nilesh Kulkarni","Haejun Lee","Jihie Kim"],"2640":["Catherine Kobus","G\u00e9raldine Damnati","Lionel Delphin-Poulat","Renato de Mori"],"2641":["Weiwei Liu","Taiqing Dong","Jianhua Zhou","Xiguang Wang","Yujian Tang","Lintao Li","Dong Wu","Zhikai Hu","Peng Zhao","Weihua Zhang","Weiqiang Zhang","Jia Liu"],"2642":["Sergey Edunov","Alexei Baevski","Michael Auli"],"2643":["Haizhou Li","Bin Ma"],"2644":["Zhangyin Feng","Daya Guo","Duyu Tang","Nan Duan","Xiaocheng Feng","Ming Gong","Linjun Shou","Bing Qin","Ting Liu","Daxin Jiang","Ming Zhou"],"2645":["Aisha Khatun","Anisur Rahman","Hemayet Ahmed Chowdhury","Md. Saiful Islam","Ayesha Tasnim"],"2646":["Hongyang Yang","Xiao-Yang Liu","Christina Dan Wang"],"2647":["Oren Kurland","Lillian Lee"],"2648":["Yanrong Ji","Zhihan Zhou","Han Liu","Ramana V Davuluri"],"2649":["Hangbo Bao","Li Dong","Furu Wei","Wenhui Wang","Nan Yang","Xiaodong Liu","Yu Wang","Songhao Piao","Jianfeng Gao","Ming Zhou","Hsiao-Wuen Hon"],"2650":["Falko Theisselmann","Doris Dransch","Joachim Fischer"],"2651":["Ryan Cotterell","S. J. Mielke","Jason Eisner","Brian Roark"],"2652":["Dennis Wagelaar"],"2653":["R. S. Berndt","A. Caramazza"],"2654":["Erdefi Rakun","Mohammad Ivan Fanany","I Wayan Wiprayoga Wisesa","Andros Tjandra"],"2655":["Jason Wei","Xuezhi Wang","Dale Schuurmans","Maarten Bosma","Brian Ichter","Fei Xia","Ed Chi","Quoc Le","Denny Zhou"],"2656":["Masayuki Suzuki","Nobuyasu Itoh","Tohru Nagano","Gakuto Kurata","Samuel Thomas"],"2657":["U. Schade","T. Berg"],"2658":["T. Berg","U. Schade"],"2659":["Amogh Kamat Tarcar","Aashis Tiwari","Dattaraj Rao","Vineet Naique Dhaimodker","Penjo Rebelo","Rahul Desai"],"2660":["Rene Pickhardt","Thomas Gottron","Martin K\u00f6rner","Paul Georg Wagner","Till Speicher","Steffen Staab"],"2661":["Robert John Freeman"],"2662":["S. J. Mielke","Ryan Cotterell","Kyle Gorman","Brian Roark","Jason Eisner"],"2663":["Tommi Jauhiainen","Heidi Jauhiainen","Krister Lind\u00e9n"],"2664":["Philip Clarkson","Tony Robinson"],"2665":["Niels Brouwers","Marc Hamilton","Ivan Kurtev","Yaping Luo"],"2666":["Terrence Martin","Sridha Sridharan"],"2667":["Hemayet Ahmed Chowdhury","Md. Azizul Haque Imon","Anisur Rahman","Aisha Khatun","Md. Saiful Islam"],"2668":["Rene Pickhardt","Thomas Gottron","Martin K\u00f6rner","Paul Georg Wagner","Till Speicher","Steffen Staab"],"2669":["Martin C. Emele","Zica Valsan","Yin Lam","Silke Goronzy"],"2670":["Zheng Chen","Kai-Fu Lee","Mingjing Li"],"2671":["Edward W. D. Whittaker","Bhiksha Raj"],"2672":["Jey Han Lau","Timothy Baldwin","Trevor Cohn"],"2673":["Quan Hung Tran","Ingrid Zukerman","Gholamreza Haffari"],"2674":["Paolo Atzeni","Letizia Tanca"],"2675":["Subhabrata Mukherjee","Stephan G\u00fcnnemann","Gerhard Weikum"],"2676":["Koen Deschacht","Jan De Belder","Marie-Francine Moens"],"2677":["J.-H. Jayez","P. Levasseur","M. Liscouet"],"2678":["Thouraya Bouabana-Tebibel"],"2679":["Christian Gillot","Christophe Cerisara","David Langlois","Jean Paul Haton"],"2680":["Xunying Liu","Mark J. F. Gales","Philip C. Woodland"],"2681":["Arlen Cox","Jason Leasure"],"2682":["Sungjin Ahn","Heeyoul Choi","Tanel P\u00e4rnamaa","Yoshua Bengio"],"2683":["Wenlin Wang","Zhe Gan","Wenqi Wang","Dinghan Shen","Jiaji Huang","Wei Ping","Sanjeev Satheesh","Lawrence Carin"],"2684":["Donghui Feng"],"2685":["Erin\u00e7 Dikici","Murat Saraclar"],"2686":["Jiaji Huang","Yi Li","Wei Ping","Liang Huang"],"2687":["David J. C. MacKay","Linda C. Bauman Peto"],"2688":["J. A. Moyne"],"2689":["J\u00e1n Koll\u00e1r","Peter V\u00e1clav\u00edk","Lubomir Wassermann"],"2690":["Boulos Harb","Ciprian Chelba","Jeffrey Dean","Sanjay Ghemawat"],"2691":["D. J. C. MacKay","L. Peto"],"2692":["Ebru Celikel"],"2693":["Peter Klein"],"2694":["Kevin Lano","Shekoufeh Kolahdouz Rahimi","Tony Clark"],"2695":["Kei Yuen Hung","Robert Wing Pong Luk","Daniel S. Yeung","Korris Fu-Lai Chung","Wenhao Shu"],"2696":["Bernhard Rumpe"],"2697":["Aditya Agrawal"],"2698":["Lukasz Brocki","Krzysztof Marasek","Danijel Korzinek"],"2699":["Amit Singh"],"2700":["Corby Rosset","Chenyan Xiong","Minh Phan","Xia Song","Paul N. Bennett","Saurabh Tiwary"],"2701":["Florian Heidenreich","Jakob Henriksson","Jendrik Johannes","Steffen Zschaler"],"2702":["Laurence Tratt"],"2703":["Andreas Merkel","Dietrich Klakow"],"2704":["Terje Gj\u00f8s\u00e6ter","Andreas Prinz"],"2705":["Lars Ackermann","Bernhard Volz"],"2706":["Ahmad Emami","Frederick Jelinek"],"2707":["Wenhao Zhu","Hongyi Liu","Qingxiu Dong","Jingjing Xu","Lingpeng Kong","Jiajun Chen","Lei Li","Shujian Huang"],"2708":["Gautier Izacard","Patrick Lewis","Maria Lomeli","Lucas Hosseini","Fabio Petroni","Timo Schick","Jane Dwivedi-Yu","Armand Joulin","Sebastian Riedel","Edouard Grave"],"2709":["Seyed-Mehdi-Reza Beheshti","Hamid R. Motahari Nezhad","Boualem Benatallah"],"2710":["Christopher Tensmeyer","Curtis Wigington","Brian L. Davis","Seth Stewart","Tony R. Martinez","William Barrett"],"2711":["Tianxiao Shen","Victor Quach","Regina Barzilay","Tommi Jaakkola"],"2712":["Stevan Ostrogonac","Dragisa Miskovic","Milan Secujski","Darko Pekar","Vlado Delic"],"2713":["Jan Christian Blaise Cruz","Charibeth Cheng"],"2714":["Tanel Alum\u00e4e","Toomas Kirt"],"2715":["Jakob Henriksson","Uwe A\u00dfmann"],"2716":["Peiyou Song","Anhei Shu","David Phipps","Mohit Tiwari","Dan S. Wallach","Jedidiah R. Crandall","George F. Luger"],"2717":["Liwei Wu","Youhua Wu","Fei Li","Tao Zheng"],"2718":["Michael A. Lund","Herbert Gish"],"2719":["Tommi Jauhiainen","Krister Lind\u00e9n","Heidi Jauhiainen"],"2720":["Yu Gu","Robert Tinn","Hao Cheng","Michael Lucas","Naoto Usuyama","Xiaodong Liu","Tristan Naumann","Jianfeng Gao","Hoifung Poon"],"2721":["Ibrahim Burak Ozyurt"],"2722":["Tevfik Bultan"],"2723":["Peiyou Song","Anhei Shu","David Phipps","Dan S. Wallach","Mohit Tiwari","Jedidiah R. Crandall","George F. Luger"],"2724":["Alexandru Trifan","Marilena Anghelus","Rodica Constantinescu"],"2725":["Tommi Jauhiainen","Krister Lind\u00e9n","Heidi Jauhiainen"],"2726":["Khurram Shahzad","Mturi Elias","Paul Johannesson"],"2727":["Janna Omeliyanenko","Albin Zehe","Lena Hettinger","Andreas Hotho"],"2728":["Stephanie C. Lin","Jacob Hilton","Owain Evans"],"2729":["Pablo G\u00f3mez-Abajo","Esther Guerra","Juan de Lara"],"2730":["Peng Xu","Ahmad Emami","Frederick Jelinek"],"2731":["Amir Feder","Nadav Oved","Uri Shalit","Roi Reichart"],"2732":["Maria Carmen Leonardi","Mar\u00eda Virginia Mauco","Laura Felice","German Montejano","Daniel Riesco","Narayan C. Debnath"],"2733":["Karolina Zurowska","Juergen Dingel"],"2734":["Dominique Fohr","Odile Mella"],"2735":["Shima Asaadi","Eugenie Giesbrecht","Sebastian Rudolph"],"2736":["Teruhisa Misu","Shigeki Matsuda","Etsuo Mizukami","Hideki Kashioka","Haizhou Li"],"2737":["Fr'ed'eric Jouault","Ivan Kurtev"],"2738":["Najeeb Abdulmutalib","Norbert Fuhr"],"2739":["Aarohi Srivastava","Abhinav Rastogi","Abhishek Rao","Abu Awal Md Shoeb","Abubakar Abid","Adam Fisch","Adam R. Brown","Adam Santoro","Aditya Gupta","Adri\u00e0 Garriga-Alonso","Agnieszka Kluska","Aitor Lewkowycz","Akshat Agarwal","Alethea Power","Alex Ray","Alex Warstadt","Alexander W. Kocurek","Ali Safaya","Ali Tazarv","Alice Xiang","Alicia Parrish","Allen Nie","Aman Hussain","Amanda Askell","Amanda Dsouza","Ambrose Slone","Ameet Rahane","Anantharaman S. Iyer","Anders Andreassen","Andrea Madotto","Andrea Santilli","Andreas Stuhlm\u00fcller","Andrew Dai","Andrew La","Andrew Lampinen","Andy Zou","Angela Jiang","Angelica Chen","Anh Vuong","Animesh Gupta","Anna Gottardi","Antonio Norelli","Anu Venkatesh","Arash Gholamidavoodi","Arfa Tabassum","Arul Menezes","Arun Kirubarajan","Asher Mullokandov","Ashish Sabharwal","Austin Herrick","Avia Efrat","Aykut Erdem","Ayla Karaka\u015f","B. Ryan Roberts","Bao Sheng Loe","Barret Zoph","Bart\u0142omiej Bojanowski","Batuhan \u00d6zyurt","Behnam Hedayatnia","Behnam Neyshabur","Benjamin Inden","Benno Stein","Berk Ekmekci","Bill Yuchen Lin","Blake Howald","Bryan Orinion","Cameron Diao","Cameron Dour","Catherine Stinson","Cedrick Argueta","C\u00e9sar Ferri Ram\u00edrez","Chandan Singh","Charles Rathkopf","Chenlin Meng","Chitta Baral","Chiyu Wu","Chris Callison-Burch","Chris Waites","Christian Voigt","Christopher D. Manning","Christopher Potts","Cindy Ramirez","Clara E. Rivera","Clemencia Siro","Colin Raffel","Courtney Ashcraft","Cristina Garbacea","Damien Sileo","Dan Garrette","Dan Hendrycks","Dan Kilman","Dan Roth","Daniel Freeman","Daniel Khashabi","Daniel Levy","Daniel Mosegu\u00ed Gonz\u00e1lez","Danielle Perszyk","Danny Hernandez","Danqi Chen","Daphne Ippolito","Dar Gilboa","David Dohan","David Drakard","David Jurgens","Debajyoti Datta","Deep Ganguli","Denis Emelin","Denis Kleyko","Deniz Yuret","Derek Chen","Derek Tam","Dieuwke Hupkes","Diganta Misra","Dilyar Buzan","Dimitri Coelho Mollo","Diyi Yang","Dong-Ho Lee","Dylan Schrader","Ekaterina Shutova","Ekin Dogus Cubuk","Elad Segal","Eleanor Hagerman","Elizabeth Barnes","Elizabeth Donoway","Ellie Pavlick","Emanuele Rodola","Emma Lam","Eric Chu","Eric Tang","Erkut Erdem","Ernie Chang","Ethan A. Chi","Ethan Dyer","Ethan Jerzak","Ethan Kim","Eunice Engefu Manyasi","Evgenii Zheltonozhskii","Fanyue Xia","Fatemeh Siar","Fernando Mart\u00ednez-Plumed","Francesca Happ\u00e9","Francois Chollet","Frieda Rong","Gaurav Mishra","Genta Indra Winata","Gerard de Melo","Germ\u00e1n Kruszewski","Giambattista Parascandolo","Giorgio Mariani","Gloria Wang","Gonzalo Jaimovitch-L\u00f3pez","Gregor Betz","Guy Gur-Ari","Hana Galijasevic","Hannah Kim","Hannah Rashkin","Hannaneh Hajishirzi","Harsh Mehta","Hayden Bogar","Henry Shevlin","Hinrich Sch\u00fctze","Hiromu Yakura","Hongming Zhang","Hugh Mee Wong","Ian Ng","Isaac Noble","Jaap Jumelet","Jack Geissinger","Jackson Kernion","Jacob Hilton","Jaehoon Lee","Jaime Fern\u00e1ndez Fisac","James B. Simon","James Koppel","James Zheng","James Zou","Jan Koco\u0144","Jana Thompson","Janelle Wingfield","Jared Kaplan","Jarema Radom","Jascha Sohl-Dickstein","Jason Phang","Jason Wei","Jason Yosinski","Jekaterina Novikova","Jelle Bosscher","Jennifer Marsh","Jeremy Kim","Jeroen Taal","Jesse Engel","Jesujoba Alabi","Jiacheng Xu","Jiaming Song","Jillian Tang","Joan Waweru","John Burden","John Miller","John U. Balis","Jonathan Batchelder","Jonathan Berant","J\u00f6rg Frohberg","Jos Rozen","Jose Hernandez-Orallo","Joseph Boudeman","Joseph Guerr","Joseph Jones","Joshua B. Tenenbaum","Joshua S. Rule","Joyce Chua","Kamil Kanclerz","Karen Livescu","Karl Krauth","Karthik Gopalakrishnan","Katerina Ignatyeva","Katja Markert","Kaustubh D. Dhole","Kevin Gimpel","Kevin Omondi","Kory Mathewson","Kristen Chiafullo","Ksenia Shkaruta","Kumar Shridhar","Kyle McDonell","Kyle Richardson","Laria Reynolds","Leo Gao","Li Zhang","Liam Dugan","Lianhui Qin","Lidia Contreras-Ochando","Louis-Philippe Morency","Luca Moschella","Lucas Lam","Lucy Noble","Ludwig Schmidt","Luheng He","Luis Oliveros Col\u00f3n","Luke Metz","L\u00fctfi Kerem \u015eenel","Maarten Bosma","Maarten Sap","Maartje ter Hoeve","Maheen Farooqi","Manaal Faruqui","Mantas Mazeika","Marco Baturan","Marco Marelli","Marco Maru","Maria Jose Ram\u00edrez Quintana","Marie Tolkiehn","Mario Giulianelli","Martha Lewis","Martin Potthast","Matthew L. Leavitt","Matthias Hagen","M\u00e1ty\u00e1s Schubert","Medina Orduna Baitemirova","Melody Arnaud","Melvin McElrath","Michael A. Yee","Michael Cohen","Michael Gu","Michael Ivanitskiy","Michael Starritt","Michael Strube","Micha\u0142 Sw\u0119drowski","Michele Bevilacqua","Michihiro Yasunaga","Mihir Kale","Mike Cain","Mimee Xu","Mirac Suzgun","Mitch Walker","Mo Tiwari","Mohit Bansal","Moin Aminnaseri","Mor Geva","Mozhdeh Gheini","Mukund Varma T","Nanyun Peng","Nathan A. Chi","Nayeon Lee","Neta Gur-Ari Krakover","Nicholas Cameron","Nicholas Roberts","Nick Doiron","Nicole Martinez","Nikita Nangia","Niklas Deckers","Niklas Muennighoff","Nitish Shirish Keskar","Niveditha S. Iyer","Noah Constant","Noah Fiedel","Nuan Wen","Oliver Zhang","Omar Agha","Omar Elbaghdadi","Omer Levy","Owain Evans","Pablo Antonio Moreno Casares","Parth Doshi","Pascale Fung","Paul Pu Liang","Paul Vicol","Pegah Alipoormolabashi","Peiyuan Liao","Percy Liang","Peter Chang","Peter Eckersley","Phu Mon Htut","Pinyu Hwang","Piotr Mi\u0142kowski","Piyush Patil","Pouya Pezeshkpour","Priti Oli","Qiaozhu Mei","Qing Lyu","Qinlang Chen","Rabin Banjade","Rachel Etta Rudolph","Raefer Gabriel","Rahel Habacker","Ramon Risco","Rapha\u00ebl Milli\u00e8re","Rhythm Garg","Richard Barnes","Rif A. Saurous","Riku Arakawa","Robbe Raymaekers","Robert Frank","Rohan Sikand","Roman Novak","Roman Sitelew","Ronan LeBras","Rosanne Liu","Rowan Jacobs","Rui Zhang","Ruslan Salakhutdinov","Ryan Chi","Ryan Lee","Ryan Stovall","Ryan Teehan","Rylan Yang","Sahib Singh","Saif M. Mohammad","Sajant Anand","Sam Dillavou","Sam Shleifer","Sam Wiseman","Samuel Gruetter","Samuel R. Bowman","Samuel S. Schoenholz","Sanghyun Han","Sanjeev Kwatra","Sarah A. Rous","Sarik Ghazarian","Sayan Ghosh","Sean Casey","Sebastian Bischoff","Sebastian Gehrmann","Sebastian Schuster","Sepideh Sadeghi","Shadi Hamdan","Sharon Zhou","Shashank Srivastava","Sherry Shi","Shikhar Singh","Shima Asaadi","Shixiang Shane Gu","Shubh Pachchigar","Shubham Toshniwal","Shyam Upadhyay","Shyamolima","Debnath","Siamak Shakeri","Simon Thormeyer","Simone Melzi","Siva Reddy","Sneha Priscilla Makini","Soo-Hwan Lee","Spencer Torene","Sriharsha Hatwar","Stanislas Dehaene","Stefan Divic","Stefano Ermon","Stella Biderman","Stephanie Lin","Stephen Prasad","Steven T. Piantadosi","Stuart M. Shieber","Summer Misherghi","Svetlana Kiritchenko","Swaroop Mishra","Tal Linzen","Tal Schuster","Tao Li","Tao Yu","Tariq Ali","Tatsu Hashimoto","Te-Lin Wu","Th\u00e9o Desbordes","Theodore Rothschild","Thomas Phan","Tianle Wang","Tiberius Nkinyili","Timo Schick","Timofei Kornev","Titus Tunduny","Tobias Gerstenberg","Trenton Chang","Trishala Neeraj","Tushar Khot","Tyler Shultz","Uri Shaham","Vedant Misra","Vera Demberg","Victoria Nyamai","Vikas Raunak","Vinay Ramasesh","Vinay Uday Prabhu","Vishakh Padmakumar","Vivek Srikumar","William Fedus","William Saunders","William Zhang","Wout Vossen","Xiang Ren","Xiaoyu Tong","Xinran Zhao","Xinyi Wu","Xudong Shen","Yadollah Yaghoobzadeh","Yair Lakretz","Yangqiu Song","Yasaman Bahri","Yejin Choi","Yichi Yang","Yiding Hao","Yifu Chen","Yonatan Belinkov","Yu Hou","Yufang Hou","Yuntao Bai","Zachary Seid","Zhuoye Zhao","Zijian Wang","Zijie J. Wang","Zirui Wang","Ziyi Wu"],"2740":["Wengong Jin","Tianxing He","Yanmin Qian","Kai Yu"],"2741":["Klaus-D. Engel","Richard F. Paige","Dimitrios S. Kolovos"],"2742":["Klaus D. Engel","Richard F. Paige","Dimitrios Kolovos"],"2743":["Esther Guerra","Juan de Lara","Dimitrios S. Kolovos","Richard F. Paige"],"2744":["Shikha Bordia","Samuel R. Bowman"],"2745":["Louis Martin","Benjamin M\u00fcller","Pedro Javier Ortiz Su\u00e1rez","Yoann Dupont","Laurent Romary","\u00c9ric de la Clergerie","Djam\u00e9 Seddah","Beno\u00eet Sagot"],"2746":["Anoop Deoras","Frederick Jelinek","Yi Su"],"2747":["Yuri A. Kosarev"],"2748":["Ignacio Lopez-Moreno","Daniel Ramos","Joaquin Gonzalez-Rodriguez","Doroteo Torre Toledano"],"2749":["Jen-Tzung Chien","Yuan-Chu Ku"],"2750":["Volkmar Frinken","Alicia Forn\u00e9s","Josep Llad\u00f3s","Jean-Marc Ogier"],"2751":["David Janiszek","Renato De Mori","Fr\u00e9d\u00e9ric B\u00e9chet"],"2752":["Lichi Yuan"],"2753":["Tomas Mikolov","Martin Karafi\u00e1t","Luk\u00e1s Burget","Jan Cernock\u00fd","Sanjeev Khudanpur"],"2754":["Frederick Jelinek","Carolina Parada"],"2755":["Evgeny Drukh","Yishay Mansour"],"2756":["Deniz Yuret"],"2757":["Jen-Tzung Chien"],"2758":["Laurence Tratt"],"2759":["Kuntharrgyal Khysru","Di Jin","Jianwu Dang"],"2760":["Germ\u00e1n Urrego-Giraldo"],"2761":["A. Mayer","S. Kliger","David Ohsie","Shaula Yemini"],"2762":["Vesa Siivola","Bryan L. Pellom"],"2763":["Tom\u00e1s Mikolov","Martin Karafi\u00e1t","Luk\u00e1\u0161 Burget","Jan \u010cernock\u00fd","Sanjeev Khudanpur"],"2764":["Patrick Neubauer"],"2765":["Ahmet Afsin Akin","Cemil Demir"],"2766":["Lucian P. Smith","Frank T. Bergmann","Deepak Chandran","Herbert M. Sauro"],"2767":["Taro Watanabe","Hajime Tsukada","Hideki Isozaki"],"2768":["Ahmet Serkan Karatas"],"2769":["Arthur H. M. ter Hofstede","Chun Ouyang","Marcello La Rosa","Liang Song","Jianmin Wang","Artem Polyvyanyy"],"2770":["G\u00f6khan T\u00fcr"],"2771":["Patrick Delfmann","Hanns-Alexander Dietrich","Jean-Marie Havel","Matthias Steinhorst"],"2772":["Ivan Bretan"],"2773":["Daniel Soutner","Zdenek Loose","Ludek M\u00fcller","Ales Praz\u00e1k"],"2774":["Yasumasa Miyamoto","Kyunghyun Cho"],"2775":["Yike Zhang","Pengyuan Zhang","Yonghong Yan"],"2776":["Ciprian Chelba","Frederick Jelinek"],"2777":["Jen-Tzung Chien","Che-Yu Kuo"],"2778":["Viktor Shynkarenko","Olena Kuropiatnyk"],"2779":["Kevin Duh","Katrin Kirchhoff"],"2780":["Qiuchi Li","Massimo Melucci","Prayag Tiwari"],"2781":["Jean-Marc J\u00e9z\u00e9quel","Olivier Barais","Franck Fleurey"],"2782":["Young-Suk Lee","Kishore Papineni","Salim Roukos","Ossama Emam","Hany Hassan"],"2783":["Micha Livne","Kevin Swersky","David J. Fleet"],"2784":["Mandy Guo","Zihang Dai","Denny Vrandecic","Rami Al-Rfou"],"2785":["Thomas Wolf","Julien Chaumond","Clement Delangue"],"2786":["Genevieve Brown","Taylor Curro","Ryan Gentry","Thomas Hoffman","Timothy Lortz","Joseph Murrey","Joshua Peters","Scott Rachlinski","Eric Zatcoff"],"2787":["A. Nayeemulla Khan","B. Yegnanarayana"],"2788":["Shinsuke Mori","Masafumi Nishimura","Nobuyasu Itoh"],"2789":["Uwe Ohler","Stefan Harbeck","Heinrich Niemann"],"2790":["Francisco J. Valverde-Albacete","Jos\u00e9 Manuel Pardo"],"2791":["Andrzej Bassara"],"2792":["Matthias Steinhorst"],"2793":["Lo\u00efc Maisonnasse","\u00c9ric Gaussier","Jean-Pierre Chevallet"],"2794":["Erin\u00e7 Dikici","Murat Saraclar"],"2795":["Jes\u00fas Manuel Almendros-Jim\u00e9nez","Luis Iribarne","Jes\u00fas J. L\u00f3pez-Fern\u00e1ndez","\u00c1ngel Mora Segura"],"2796":["Miliwan Xuehelaiti","Kai Liu","Wenbin Jiang","Tuergen Yibulayin"],"2797":["Louis M. Rose","Dimitrios S. Kolovos","Richard F. Paige","Fiona A. C. Polack","Simon M. Poulding"],"2798":["Junlin Zhang","Le Sun","Yongche Zhang","Yufang Sun"],"2799":["Frederic Morin","Yoshua Bengio"],"2800":["Y. Adachi"],"2801":["Reinhard Kneser","Jochen Peters","Dietrich Klakow"],"2802":["Pawel Batko","Marcin Kuta"],"2803":["Hong Feng Yin","Ju Wei Tai"],"2804":["Rongmei Li","Theo P. van der Weide"],"2805":["Rong Jin","Alexander G. Hauptmann","ChengXiang Zhai"],"2806":["Jianfeng Gao","Jian-Yun Nie","Guangyuan Wu","Guihong Cao"],"2807":["Alexandr A. Savinov"],"2808":["Hoo-Chang Shin","Yang Zhang","Evelina Bakhturina","Raul Puri","Mostofa Patwary","Mohammad Shoeybi","Raghav Mani"],"2809":["Kento Watanabe","Yuichiroh Matsubayashi","Satoru Fukayama","Masataka Goto","Kentaro Inui","Tomoyasu Nakano"],"2810":["Andriy Mnih","Geoffrey E Hinton"],"2811":["Yongzhe Shi","Weiqiang Zhang","Meng Cai","Jia Liu"],"2812":["Marcin Gorawski","Aleksander Chr\u00f3szcz"],"2813":["G. J. Baxter","R. A. Blythe","W. Croft","A. J. McKane"],"2814":["Omar Bahy Badreddin"],"2815":["Min Lu","Feilong Bao","Guanglai Gao"],"2816":["Jan Lehecka","Ales Praz\u00e1k"],"2817":["Brahim Hamid"],"2818":["Xiaoyi Wu","Yuji Matsumoto"],"2819":["Francisco Criado","Tamaz Gachechiladze","Hamlet Meladze","Guram Tsertsvadze"],"2820":["Philip Clarkson","Tony Robinson"],"2821":["Bartosz Zi\u00f3lko","Suresh Manandhar","Richard C. Wilson","Mariusz Zi\u00f3lko"],"2822":["Tanel Alum\u00e4e"],"2823":["Frederic Morin","Yoshua Bengio"],"2824":["Vidura Seneviratne","Steve J. Young"],"2825":["Matthias Steinhorst"],"2826":["Florian Heidenreich","Jendrik Johannes","Sven Karol","Mirko Seifert","Christian Wende"],"2827":["Raphael Scheible","Fabian Thomczyk","Patric Tippmann","Victor Jaravine","Martin Boeker"],"2828":["Abir Masmoudi","Rim Laatar","Mariem Ellouze","Lamia Hadrich Belguith"],"2829":["Sung-Chien Lin","Chi-Lung Tsai","Lee-Feng Chien","Keh-Jiann Chen","Lin-Shan Lee"],"2830":["Zenshiro Kawasaki","Keiji Takida","Masato Tajima"],"2831":["Fan-Keng Sun","Cheng-I Lai"],"2832":["David Alfter"],"2833":["Miroslaw Milewski","Graham Roberts"],"2834":["Lalit R. Bahl","Peter F. Brown","Peter V. de Souza","Robert L. Mercer"],"2835":["Zhenyu Lv","Wenju Liu","Zhanlei Yang"],"2836":["Wataru Takano","Yoshihiko Nakamura"],"2837":["Miliwan Xuehelaiti","Kai Liu","Wenbin Jiang","Tuergen Yibulayin"],"2838":["Arnar Thor Jensson","Edward W. D. Whittaker","Koji Iwano","Sadaoki Furui"],"2839":["Hajnal Andr\u00e9ka","Tam\u00e1s Gergely","Istv\u00e1n N\u00e9meti"],"2840":["Josef Chaloupka"],"2841":["Chengrun Yang","Xuezhi Wang","Yifeng Lu","Hanxiao Liu","Quoc V. Le","Denny Zhou","Xinyun Chen"],"2842":["Samira Abnar","Lisa Beinborn","Rochelle Choenni","Willem Zuidema"],"2843":["Rylan Schaeffer","Brando Miranda","Sanmi Koyejo"],"2844":["Kristie Seymore","Stanley F. Chen","Ronald Rosenfeld"],"2845":["Kristie Seymore","Stanley Chen","Ronald Rosenfeld"],"2846":["Hsuan-Sheng Chin","Berlin Chen"],"2847":["Robert Hackman","Joanne M. Atlee","Alistair Finn Hackett","Michael W. Godfrey"],"2848":["Haoshuo Huang","Vihan Jain","Harsh Mehta","Jason Baldridge","Eugene Ie"],"2849":["Shixiang Lu","Wei Wei","Xiaoyin Fu","Bo Xu"],"2850":["Ying Li","Pascale Fung"],"2851":["Marco Ferretti","Giulio Maltese","Stefano Scarci"],"2852":["D.W. Shipman"],"2853":["Ahmad Emami","Peng Xu","Frederick Jelinek"],"2854":["John Miller","Fil Alleva"],"2855":["Shuanhu Bai","Chien-Lin Huang","Bin Ma","Haizhou Li"],"2856":["Zihang Dai","Zhilin Yang","Yiming Yang","Jaime Carbonell","Quoc V. Le","Ruslan Salakhutdinov"],"2857":["Ramya Rasipuram","Mathew Magimai-Doss"],"2858":["Xiaoyan Li"],"2859":["Shunyu Yao","Jeffrey Zhao","Dian Yu","Nan Du","Izhak Shafran","Karthik Narasimhan","Yuan Cao"],"2860":["Nikita Nangia","Clara Vania","Rasika Bhalerao","Samuel R. Bowman"],"2861":["Minjeong Kim","Gyuwan Kim","Sang-Woo Lee","Jung-Woo Ha"],"2862":["Shixiang Lu","Wei Wei","Xiaoyin Fu","Lichun Fan","Bo Xu"],"2863":["Siripong Potisuk"],"2864":["Chen Huang","Xiaoqing Ding","Yan Chen"],"2865":["Jian Gong","Martin Cooke","Mar\u00eda Luisa Garc\u00eda Lecumberri"],"2866":["Neel Kant","Raul Puri","Nikolai Yakovenko","Bryan Catanzaro"],"2867":["M. Selvam","A. M. Natarajan","R. Thangarajan"],"2868":["Jan Nouza","Jindra Drabkova"],"2869":["Peng Xu","Mostofa Patwary","Mohammad Shoeybi","Raul Puri","Pascale Fung","Anima Anandkumar","Bryan Catanzaro"],"2870":["Ying Li","Pascale Fung"],"2871":["Jacob Devlin","Ming-Wei Chang","Kenton Lee","Kristina Toutanova"],"2872":["Mahdi Namazifar","Alexandros Papangelis","G\u00f6khan T\u00fcr","Dilek Hakkani-T\u00fcr"],"2873":["Ali Orkan Bayer","Giuseppe Riccardi"],"2874":["Dan Song","Keqing He","Peng Liang","Wudong Liu"],"2875":["Yumeng Zhang","Xuanmin Lu","Bei Quan","Yuanyuan Wei"],"2876":["F. Song","W. Croft"],"2877":["Colin Matheson","Fergus R. McInnes"],"2878":["Kristie Seymore","Ronald Rosenfeld"],"2879":["Dietrich Klakow"],"2880":["Jianfeng Gao","Mingjing Li","Kai-Fu Lee"],"2881":["Woosung Kim","Sanjeev Khudanpur","Jun Wu"],"2882":["Yuan Yang"],"2883":["Tomas Mikolov","Geoffrey Zweig"],"2884":["Nikolaos Malandrakis","Alexandros Potamianos","Kean J. Hsu","Kalina N. Babeva","Michelle C. Feng","Gerald C. Davison","Shrikanth S. Narayanan"],"2885":["Gyuwan Kim"],"2886":["Hao Peng","Roy Schwartz","Noah A. Smith"],"2887":["A. Nur Zincir-Heywood","Tien D. Phan"],"2888":["Ciprian Chelba","Milind Mahajan"],"2889":["Joshua T. Goodman"],"2890":["Giuliano Antoniol","Fabio Brugnara","Mauro Cettolo","Marcello Federico"],"2891":["Juan Mart\u00ednez-Romo","Lourdes Araujo"],"2892":["Felix Stahlberg","James Cross","Veselin Stoyanov"],"2893":["Kenneth Heafield"],"2894":["G\u00fcnter Neumann"],"2895":["Michael Connor","Yael Gertner","Cynthia Fisher","Dan Roth"],"2896":["Kenneth Heafield","Ivan Pouzyrevsky","Jonathan H. Clark","Philipp Koehn"],"2897":["Liqi Gao","Yu Zhang","Ting Liu","Guiping Liu"],"2898":["Janez Zibert","Jerneja Gros","Simon Dobrisek","France Mihelic"],"2899":["Tatiana Likhomanenko","Qiantong Xu","Jacob Kahn","Gabriel Synnaeve","Ronan Collobert"],"2900":["Dilshodbek Kuryazov"],"2901":["Paul Buchheit"],"2902":["Pedro A. Torres-Carrasquillo","Douglas A. Reynolds","John R. Deller Jr."],"2903":["Wudong Liu","Keqing He","Yingshi","Hui Xu","Yixin Jing"],"2904":["Harald St\u00f6rrle"],"2905":["Alexandra Espich\u00e1n-Linares","Arturo Oncevay-Marcos"],"2906":["R. W. Matzen","K. M. George","George E. Hedrick"],"2907":["Shinji Watanabe","Tomoharu Iwata","Takaaki Hori","Atsushi Sako","Yasuo Ariki"],"2908":["Andreas van Cranenburgh","Rens Bod"],"2909":["Lorenzo De Mattei","Michele Cafagna","Felice Dell'Orletta","Malvina Nissim","Marco Guerini"],"2910":["F. Jelinek","B. Merialdo","S. Roukos","M. Strauss"],"2911":["Tsuyoshi Morioka","Tomoharu Iwata","Takaaki Hori","Tetsunori Kobayashi"],"2912":["Ekraam Sabir","Stephen Rawls","Prem Natarajan"],"2913":["Ke Sun","Xiaolong Wang","Chengjie Sun","Lei Lin"],"2914":["Pierre Duquesne","Ciar\u00e1n Bryce"],"2915":["Maryam Karimzadehgan","ChengXiang Zhai","Miles Efron"],"2916":["Fabio Brugnara","Mauro Cettolo"],"2917":["Fabio Brugnara","Marcello Federico"],"2918":["Xiaoshan Fang","Jianfeng Gao","Jianfeng Li","Huanye Sheng"],"2919":["Miriam Taverniers"],"2920":["Mallory Selfridge"],"2921":["Chuang-Hua Chueh","Jen-Tzung Chien"],"2922":["Object Management Group (OMG)"],"2923":["Helena Galhardas","Daniela Florescu","Dennis Shasha","Eric Simon","Cristian Saita"],"2924":["Benjamin Snyder","Regina Barzilay","Kevin Knight"],"2925":["P. Niyogi","R. C. Berwick"],"2926":["Anh Tuan Nguyen","Tien N. Nguyen"],"2927":["Thomas Cherian","Akshay Badola","Vineet Padmanabhan"],"2928":["Lipeng Zhang","Peng Zhang","Xindian Ma","Shuqin Gu","Zhan Su","Dawei Song"],"2929":["Tao Tao","Xuanhui Wang","Qiaozhu Mei","ChengXiang Zhai"],"2930":["Andr\u00e1s A. Bencz\u00far","Istv\u00e1n B\u00edr\u00f3","K\u00e1roly Csalog\u00e1ny","M\u00e1t\u00e9 Uher"],"2931":["Amittai Axelrod"],"2932":["John Lansdown"],"2933":["Elvira I. Sicilia-Garcia","Ji Ming","Francis Jack Smith"],"2934":["Dominique Massoni\u00e9","Pascal Nocera","Georges Linar\u00e8s"],"2935":["Harald St\u00f6rrle"],"2936":["Germ\u00e1n Colom\u00e1"],"2937":["John R. Anderson","Kline","Lewis"],"2938":["Nijol\u00e9 Merkien\u00e9"],"2939":["Michael Atherton","Debra A. Lelewer"],"2940":["Jiyou Jia","Youfu Ye","Klaus Mainzer"],"2941":["Chaohong Tan","Zhenhua Ling"],"2942":["A. V. Babichev","Valentin G. Lebedev"],"2943":["Alex B. Fine","Austin F. Frank","T. Florian Jaeger","Benjamin Van Durme"],"2944":["Bo-June Paul Hsu","James R. Glass"],"2945":["Paola Glavan","Dean Rosenzweig"],"2946":["Alessio Miaschi","Dominique Brunato","Felice Dell'Orletta","Giulia Venturi"],"2947":["Sanaa A. Alwidian"],"2948":["Michael H. B\u00f6hlen","Christian S. Jensen"],"2949":["Kuansan Wang","Ye-Yi Wang","Alex Acero"],"2950":["Toru Ishida","Yohei Murakami","Yoko Kubota","Rieko Inaba"],"2951":["Wei Zhang","Youyuan Lin","Ruoran Ren","Xiaodong Wang","Zhenshuang Liang","Zhen Huang"],"2952":["George Tambouratzis","Vasiliki Pouli"],"2953":["Gijs Kant","Alfons Laarman","Jeroen Meijer","Jaco van de Pol","Stefan Blom","Tom van Dijk"],"2954":["Abdullah Akce","Timothy Bretl"],"2955":["Tom Bulatewicz","Janice E. Cuny"],"2956":["Jen-Tzung Chien","Chuang-Hua Chueh"],"2957":["Octavian Popescu"],"2958":["Frederick Jelinek","Bernard M\u00e9rialdo","Salim Roukos","M. Strauss"],"2959":["Tao Tao","Xuanhui Wang","Qiaozhu Mei","ChengXiang Zhai"],"2960":["Ciprian Chelba","Milind Mahajan"],"2961":["Kenneth C. Arnold","Kai-Wei Chang","Adam Kalai"],"2962":["M. G. J. van den Brand"],"2963":["Yoshihisa Shinozawa","Akito Sakurai"],"2964":["Gui-Rong Xue","Jie Han","Yong Yu","Qiang Yang"],"2965":["Pieter Delobelle","Thomas Winters","Bettina Berendt"],"2966":["Alex B. Fine","Austin F. Frank","T. Florian Jaeger","Benjamin Van Durme"],"2967":["Moses Ganardi"],"2968":["Tajana Ban Kirigin","Ana Mestrovic","Sanda Martincic-Ipsic"],"2969":["Kelvin Guu","Kenton Lee","Zora Tung","Panupong Pasupat","Ming-Wei Chang"],"2970":["Wentao Ma","Yiming Cui","Chenglei Si","Ting Liu","Shijin Wang","Guoping Hu"],"2971":["Dietrich Stauffer","Xavier Castello","Victor M. Eguiluz","Maxi San Miguel"],"2972":["Tae-Gil Noh","Sebastian Pad\u00f3"],"2973":["Jerome R. Bellegarda"],"2974":["Hirofumi Yamamoto","Shuntaro Isogai","Yoshinori Sagisaka"],"2975":["Ernest Pusateri","Christophe Van Gysel","Rami Botros","Sameer Badaskar","Mirko Hannemann","Youssef Oualil","Ilya Oparin"],"2976":["Zhiyuan Tang","Dong Wang","Yixiang Chen","Lantian Li","Andrew Abel"],"2977":["Tomas Mikolov","Stefan Kombrink","Luk\u00e1s Burget","Jan Cernock\u00fd","Sanjeev Khudanpur"],"2978":["Mikko Kurimo","Krista Lagus"],"2979":["Rogardt Heldal","Fredrik Hultin"],"2980":["Khalid Choukri","Victoria Arranz"],"2981":["Raymond W. M. Ng","Cheung-Chi Leung","Tan Lee","Bin Ma","Haizhou Li"],"2982":["Anirudh Raju","Behnam Hedayatnia","Linda Liu","Ankur Gandhe","Chandra Khatri","Angeliki Metallinou","Anu Venkatesh","Ariya Rastrow"],"2983":["Audris Kalnins","Elina Kalnina","Agris Sostaks","Edgars Celms","Ivans Tabernakulovs"],"2984":["Jan Svec","Lubos Sm\u00eddl","Pavel Ircing"],"2985":["Vincent Wan","Thomas Hain"],"2986":["Ali Haznedaroglu","Levent M. Arslan"],"2987":["Dietrich Klakow"],"2988":["Ye Bai","Jianhua Tao","Jiangyan Yi","Zhengqi Wen","Cunhang Fan"],"2989":["Lucy Vasserman","Ben Haynor","Petar S. Aleksic"],"2990":["Shi-Yu Liang","Chen Yong","Zhi-Chong Li"],"2991":["Javier Dieguez-Tirado","Carmen Garc\u00eda-Mateo","Antonio Cardenal L\u00f3pez"],"2992":["Woosung Kim","Sanjeev Khudanpur"],"2993":["Saeedeh Momtazi","Friedrich Faubel","Dietrich Klakow"],"2994":["Marcello Federico"],"2995":["Ciprian Chelba","Frederick Jelinek"],"2996":["Antonio Est\u00e9vez","Javier Padr\u00f3n","E. Victor S\u00e1nchez Rebull","Jos\u00e9 L. Roda"],"2997":["William A. Sherden"],"2998":["Vasian Cepa","Mira Mezini"],"2999":["Sander Tichelaar","St\u00e9phane Ducasse","Serge Demeyer","Oscar Nierstrasz"],"3000":["Gilad Mishne","David Carmel","Ronny Lempel"],"3001":["Andr\u00e1s A. Bencz\u00far","Istv\u00e1n B\u00edr\u00f3","K\u00e1roly Csalog\u00e1ny","M\u00e1t\u00e9 Uher"],"3002":["Phan Thu Nhat Vo","Maria Spichkova"],"3003":["Qin Li","Huibiao Zhu","Jifeng He"],"3004":["Michael Lienhardt","Ivan Lanese","Mario Bravetti","Davide Sangiorgi","Gianluigi Zavattaro","Yannick Welsch","Jan Sch\u00e4fer","Arnd Poetzsch-Heffter"],"3005":["Yuming Zeng","Yongcheng Wang","Fangfang Wu"],"3006":["Fabian Friedrich","Jan Mendling","Frank Puhlmann"],"3007":["Jianfeng Gao","Hisami Suzuki","Wei Yuan"],"3008":["Piotr Wisniewski","Krzysztof Stencel"],"3009":["Partha Niyogi","Robert C. Berwick"],"3010":["Jeremy H. Wright","Gareth J. F. Jones","Harvey Lloyd-Thomas"],"3011":["J. P. Stemberger"],"3012":["Richard Antonello","Javier Turek","Alexander Huth"],"3013":["Antoine Rauzy"],"3014":["Shenghua Bao","Lei Zhang","Erdong Chen","Min Long","Rui Li","Yong Yu"],"3015":["M. D. Dennis","Alan M. Wallington","Geoffrey Sampson"],"3016":["Brent Martin","Amanda Nicholas"],"3017":["Jakub Kanis"],"3018":["Jinglei Zhao","Yeogirl Yun"],"3019":["Rong Jin","Luo Si","Alexander G. Hauptmann","James P. Callan"],"3020":["Chalermpol Tapsai","Phayung Meesad","Choochart Haruechaiyasak"],"3021":["Ronald C. Turner"],"3022":["Fei Song","W. Bruce Croft"],"3023":["Aaron Jaech","Mari Ostendorf"],"3024":["Robert C. Moore","William D. Lewis"],"3025":["J\u00e1n Stas","Daniel Hl\u00e1dek","Mat\u00fas Pleva","Jozef Juh\u00e1r"],"3026":["Jorge Mendes","Jo\u00e3o Saraiva"],"3027":["P\u00e9ter Feh\u00e9r","L\u00e1szl\u00f3 Lengyel"],"3028":["Benjamin Snyder","Regina Barzilay","Kevin Knight"],"3029":["Max Garagnani","Thomas Wennekers","Friedemann Pulverm\u00fcller"],"3030":["Adria A. Martinez-Villaronga","Miguel A. del Agua","Jes\u00fas Andr\u00e9s-Ferrer","Alfons Juan"],"3031":["Jo Calder"],"3032":["Anastasia Giachanou","Francisco M. Rangel Pardo","Fabio Crestani","Paolo Rosso"],"3033":["Helena Galhardas","Daniela Florescu","Dennis E. Shasha","Eric Simon","Cristian-Augustin Saita"],"3034":["OMG"],"3035":["Graham Neubig","Masato Mimura","Shinsuke Mori","Tatsuya Kawahara"],"3036":["Kristie Seymore","Ronald Rosenfeld"],"3037":["Kazem Taghva","Jeffrey S. Coombs","Ray Pereda","Thomas A. Nartker"],"3038":["Federico Tomassetti","Antonio Vetro","Marco Torchiano","Markus Voelter","Bernd Kolb"],"3039":["Viljami Venekoski","Jouko Vankka"],"3040":["Philip Langer","Manuel Wimmer","Jeff Gray","Gerti Kappel","Antonio Vallecillo"],"3041":["Xiaoyong Liu","W. Bruce Croft"],"3042":["Hoa Khanh Dam","Truyen Tran","Trang Pham"],"3043":["Nabil Khoufi","Chafik Aloulou","Lamia Hadrich Belguith"],"3044":["Kevin Humphreys","Mike Calcagno","David Weise"],"3045":["Yeha Lee","Seung-Hoon Na","Jong-Hyeok Lee"],"3046":["Aitao Chen","Yiping Zhou","Anne Zhang","Gordon Sun"],"3047":["John A. Keane","Ming Q. Xu"],"3048":["Helena Galhardas","Daniela Florescu","Dennis Shasha","Eric Simon","Cristian-Augustin Saita"],"3049":["Geraldine Chaudhri","Jim Cater","Brad Kizzort"],"3050":["Ales Praz\u00e1k","Josef V. Psutka","Jan Hoidekr","Jakub Kanis","Ludek M\u00fcller","Josef Psutka"],"3051":["Chuang-Hua Chueh","Jen-Tzung Chien"],"3052":["Vojtech Merunka","Oldrich Nouza","Jir\u00ed Brozek"],"3053":["Ahmad Emami"],"3054":["Genqing Wu","Thomas Fang Zheng","Ling Jin","Wenhu Wu"],"3055":["Wolfgang Reichl"],"3056":["Oliver Gabel","Lothar Litz"],"3057":["Hai Son Le","Ilya Oparin","Alexandre Allauzen","Jean-Luc Gauvain","Fran\u00e7ois Yvon"],"3058":["Robert W. Blanning"],"3059":["Irene Giakoumi","Christos Makris","Yiannis Plegas"],"3060":["Nobuo Satake"],"3061":["Fumihiro Kumeno","Yasuyuki Tahara","Akihiko Ohsuga","Shinichi Honiden"],"3062":["Qin Shi","Stephen M. Chu","Wen Liu","Hong-Kwang Jeff Kuo","Yi Liu","Yong Qin"],"3063":["Krzysztof Wolk","Agnieszka Wolk","Krzysztof Marasek"],"3064":["Hasim Sak","Fran\u00e7oise Beaufays","Kaisuke Nakajima","Cyril Allauzen"],"3065":["Langzhou Chen","Jean-Luc Gauvain","Lori Lamel","Gilles Adda"],"3066":["Ruiqiang Zhang","Ezra Black","Andrew M. Finch","Yoshinori Sagisaka"],"3067":["P. Srinivasa Rao","Michael D. Monkowski","Salim Roukos"],"3068":["Yifu Sun","Haoming Jiang"],"3069":["Obeida ElJundi","Wissam Antoun","Nour El Droubi","Hazem M. Hajj","Wassim El-Hajj","Khaled B. Shaban"],"3070":["Ciprian Chelba","Frederick Jelinek"],"3071":["Jingfang Xu","Yangbo Zhu","Xing Li"],"3072":["Mallory Selfridge"],"3073":["Min Ma","Michael Nirschl","Fadi Biadsy","Shankar Kumar"],"3074":["Bahman Zamani","Greg Butler"],"3075":["Patrick Dohrmann"],"3076":["Zhenjun Yue","Siyuan Gu","Chanzhen Rong","Yuan Wang"],"3077":["Zhenjun Yue","Chuanzhen Rong","Yuan Wang","Yu Yang"],"3078":["Masaaki Sibuya","Tetsunosuke Fujisaki","Yoichi Takao"],"3079":["Daniel Duran","Jagoda Bruni","Michael Walsh","Grzegorz Dogil"],"3080":["Xiao Wu","Ming Li","Hongbin Suo","Yonghong Yan"],"3081":["Junlin Zhang","Le Sun","Weimin Qu","Yufang Sun"],"3082":["Dong Ho Kim","Keun Ho Ryu","Hong Soo Kim"],"3083":["Haoruo Peng","Qiang Ning","Dan Roth"],"3084":["Viet Bac Le","Brigitte Bigi","Laurent Besacier","Eric Castelli"],"3085":["Hajime Morita","Daisuke Kawahara","Sadao Kurohashi"],"3086":["Peilu Wang","Ruihua Sun","Hai Zhao","Kai Yu"],"3087":["Jerzy Sas","Andrzej Zolnierek"],"3088":["Arnar Thor Jensson","Koji Iwano","Sadaoki Furui"],"3089":["Lee-Feng Chien","Keh-Jiann Chen","Lin-Shan Lee"],"3090":["Yuan Zhou","Bradley J. Gram-Hansen","Tobias Kohn","Tom Rainforth","Hongseok Yang","Frank Wood"],"3091":["Brandon T. Willard","R\u00e9mi Louf"],"3092":["Yukun Ma","Patrick H. Chen","Cho-Jui Hsieh"],"3093":["Chitra Babu","D. Janaki Ram"],"3094":["Kengo Ohta","Masatoshi Tsuchiya","Seiichi Nakagawa"],"3095":["Srinivas Bangalore","Owen Rambow"],"3096":["Kosuke Takahashi","Katsuhito Sudoh","Satoshi Nakamura"],"3097":["Mohsen Rashwan"],"3098":["Hirofumi Inaguma","Jaejin Cho","Murali Karthick Baskar","Tatsuya Kawahara","Shinji Watanabe"],"3099":["Roland Petrasch"],"3100":["Brij Mohan Lal Srivastava","Hari Krishna Vydana","Anil Kumar Vuppala","Manish Shrivastava"],"3101":["Chae-Gyun Lim","Ho-Jin Choi"],"3102":["Hirofumi Inaguma","Jaejin Cho","Murali Karthick Baskar","Tatsuya Kawahara","Shinji Watanabe"],"3103":["Xiaolin Wang","Hui Li","Yingwei Luo"],"3104":["Mirjam Sepesy Maucec","Tomaz Rotovnik","Zdravko Kacic","Janez Brest"],"3105":["Ryan Price"],"3106":["Cheng-I Lai","Yung-Sung Chuang","Hung yi Lee","Shang wen Li","James R. Glass"],"3107":["Christophe Feltus","Micha\u00ebl Petit","Eric Dubois"],"3108":["Noriaki Kawamae"],"3109":["Jos Warmer","Anneke Kleppe"],"3110":["Michael A. Klug"],"3111":["Tanel Alum\u00e4e"],"3112":["Mirjam Sepesy Maucec","Zdravko Kacic","Bogomir Horvat"],"3113":["Rose E Wang","Esin Durmus","Noah Goodman","Tatsunori Hashimoto"],"3114":["Stephanie Long","Alexandre Pich\u00e9","Valentina Zantedeschi","Tibor Schuster","Alexandre Drouin"],"3115":["Naranker Dulay","Emil Lupu","Morris Sloman","Nicodemos Damianou"],"3116":["Robert T. Johnson","James B. Morris"],"3117":["Michal Smialek","Andrzej Kardas"],"3118":["Shun-Po Chuang","Tzu-Wei Sung","Hung yi Lee"],"3119":["Yi Yang","Mark Christopher Siy Uy","Allen Huang"],"3120":["Philip C. Jackson Jr."],"3121":["Tatsuya Kawahara","Toshihiko Munetsugu","Norihide Kitaoka","Shuji Doshita"],"3122":["Giampiero Salvi"],"3123":["Oscar Chan","Roberto Togneri"],"3124":["Tomoyosi Akiba","Katsunobu Itou"],"3125":["Hitoshi Yamamoto","Ken Hanazawa","Kiyokazu Miki","Koichi Shinoda"],"3126":["Xunying Liu","Mark J. F. Gales","Philip C. Woodland"],"3127":["Till Mossakowski"],"3128":["Bernd Amann","Michel Scholl"],"3129":["Pinghai Yuan","Qingkai Zeng","Yao Liu"],"3130":["Ludvig Kihlman"],"3131":["Xinghua Yao","Jie Zhou"],"3132":["Antoine Laurent","William Hartmann","Lori Lamel"],"3133":["David Demeter","Doug Downey"],"3134":["Alon Talmor","Yanai Elazar","Yoav Goldberg","Jonathan Berant"],"3135":["Lucas Ondel","Luk\u00e1s Burget","Jan Cernock\u00fd","Santosh Kesiraju"],"3136":["Lukasz Brocki","Krzysztof Marasek","Danijel Korzinek"],"3137":["Hui Zhao","Ludovic Apvrille","Fr\u00e9d\u00e9ric Mallet"],"3138":["Kenneth Heafield","Philipp Koehn","Alon Lavie"],"3139":["Yang Zhao","Zhiyuan Luo","Akiko Aizawa"],"3140":["Brendon Boldt"],"3141":["William Schuler","Samir AbdelRahman","Timothy A. Miller","Lane Schwartz"],"3142":["Michael Gasser","Michael G. Dyer"],"3143":["J. Bian"],"3144":["Hans Gr\u00f6nniger","Jan Oliver Ringert","Bernhard Rumpe"],"3145":["Di Cao","Kai Yu"],"3146":["Wonjin Yoon","Jinhyuk Lee","Donghyeon Kim","Minbyul Jeong","Jaewoo Kang"],"3147":["Aneta Poniszewska-Maranda","Gilles Goncalves","Fred Hemery"],"3148":["Julian Eisenschlos","Sebastian Ruder","Piotr Czapla","Marcin Kardas","Sylvain Gugger","Jeremy Howard"],"3149":["Arantza Casillas","Amparo Varona","In\u00e9s Torres"],"3150":["Francis K. H. Quek"],"3151":["Xinyan Lou","Yang Liu","Xiaohui Yu"],"3152":["Pawel Batko","Marcin Kuta"],"3153":["Frances Gillis-Webber","Sabine Tittel","C. Maria Keet"],"3154":["Ramez Elmasri","Vram Kouramajian"],"3155":["Aditya Agrawal","Gabor Karsai","Sandeep Neema","Feng Shi","Attila Vizhanyo"],"3156":["Vlad Acretoaie","Harald St\u00f6rrle","Daniel Str\u00fcber"],"3157":["Yang Chen","Yaochu Jin","Xiaoyan Sun"],"3158":["Richard A. Blythe"],"3159":["Hui Ma"],"3160":["David Cheng-Han Chiang","Sung-Feng Huang","Hung yi Lee"],"3161":["Kenneth Heafield","Hieu Hoang","Philipp Koehn","Tetsuo Kiso","Marcello Federico"],"3162":["Jian Cheng","Brent Townshend"],"3163":["Aditya Agrawal","G'abor Karsai","Sandeep Neema","Feng Shi","Attila Vizhanyo"],"3164":["Shahid Alam","Samuel A. Ajila"],"3165":["Tran Hoai Nam","Chitta Baral"],"3166":["Wissam Antoun","Fady Baly","Hazem M. Hajj"],"3167":["Zhiyu Chen","Mohamed Trabelsi","Jeff Heflin","Yinan Xu","Brian D. Davison"],"3168":["Bin Zou","Vasileios Lampos","Shangsong Liang","Zhaochun Ren","Emine Yilmaz","Ingemar J. Cox"],"3169":["Hang Le","Lo\u00efc Vial","Jibril Frej","Vincent Segonne","Maximin Coavoux","Benjamin Lecouteux","Alexandre Allauzen","Beno\u00eet Crabb\u00e9","Laurent Besacier","Didier Schwab"],"3170":["Michael Oudshoorn","Chris D. Marlin"],"3171":["Y. Caseau"],"3172":["M.J. Carey","D.J. DeWitt","S.L. Vandenberg"],"3173":["C. Grover","A. Holt","E. Klein","M. Moens"],"3174":["Germ\u00e1n Colom\u00e1"],"3175":["William Schuler","Timothy A. Miller"],"3176":["Anne-Marie Rassinoux","Robert H. Baud","Patrick Ruch","B\u00e9atrice Trombert Paviot","Jean Marie Rodrigues"],"3177":["Md. Akmal Haidar","Douglas D. O'Shaughnessy"],"3178":["Alexander Richard","Juergen Gall"],"3179":["Ming-Wei Chang","Kristina Toutanova","Kenton Lee","Jacob Devlin"],"3180":["Lo\u00efc Maisonnasse","\u00c9ric Gaussier","Jean-Pierre Chevallet"],"3181":["Jun Xu","Ruifeng Xu","Xiaolong Wang"],"3182":["Richard A. Blythe"],"3183":["Guanming Shao","Yosuke Kobayashi","Jay Kishigami"],"3184":["Ankur Gandhe","Ariya Rastrow","Bj\u00f6rn Hoffmeister"],"3185":["Henrik Leopold","Jan Mendling","Artem Polyvyanyy"],"3186":["Karsten Konrad"],"3187":["Eugenie Giesbrecht"],"3188":["Xusheng Li","Chengcheng Fu","Ran Zhong","Duo Zhong","Tingting He","Xingpeng Jiang"],"3189":["Yoshio Izumida","Hiroshi Ishikawa","Toshiaki Yoshino","Tadashi Hoshiai","Akifumi Makinouchi"],"3190":["Pat Langley"],"3191":["David Huggins-Daines","Alexander I. Rudnicky"],"3192":["Nestor Rodriguez","Sergio Rojas Galeano"],"3193":["Samaneh Karimi","Azadeh Shakery"],"3194":["Avigdor Gal","Opher Etzion","Arie Segev"],"3195":["Kazunori Ueda","Norio Kato"],"3196":["Mohammad Sadegh Rasooli","Sarangarajan Parthasarathy"],"3197":["Li Lin","Jin Liu","Zhenkai Gu","Zelun Zhang","Haoliang Ren"],"3198":["Wenhan Xiong","Jingfei Du","William Yang Wang","Veselin Stoyanov"],"3199":["Annedore Paeseler","Hermann Ney"],"3200":["E. DeGiuli"],"3201":["Saeedeh Momtazi","Dietrich Klakow"],"3202":["Marie M. Vaughan","David D. McDonald"],"3203":["Christoph Seidl","Ina Schaefer","Uwe A\u00dfmann"],"3204":["Iz Beltagy","Kyle Lo","Arman Cohan"],"3205":["Yunyi Zhang","Jiaming Shen","Jingbo Shang","Jiawei Han"],"3206":["Jerzy Sas","Andrzej Zolnierek"],"3207":["Welly Naptali","Masatoshi Tsuchiya","Seiichi Nakagawa"],"3208":["Ahmad Emami","Frederick Jelinek"],"3209":["Dietrich Klakow"],"3210":["Shufan Zhou","Hao Zhong","Beijun Shen"],"3211":["Min Li","Hua Wang"],"3212":["Petri Selonen"],"3213":["Brigitte Bigi","Yan Huang","Renato de Mori"],"3214":["Joshua Goodman","Jianfeng Gao"],"3215":["G\u00e9rard Becher","Fran\u00e7oise Cl\u00e9rin-Debart","Patrice Enjalbert"],"3216":["Germ\u00e1n Sanchis-Trilles","Mauro Cettolo","Nicola Bertoldi","Marcello Federico"],"3217":["Qi Sun","Jinguo Yao","Junyu Niu"],"3218":["Faguo Zhou","Xingang Yu"],"3219":["Qiu-Feng Wang","Fei Yin","Cheng-Lin Liu"],"3220":["Joseph P. Stemberger"],"3221":["Guzen Erozel","Nihan Kesim Cicekli","Ilyas Cicekli"],"3222":["Valeriy I. Nenov","Michael G. Dyer"],"3223":["Sergio Pozo","Rafael Ceballos","Rafael M. Gasca"],"3224":["Huixing Jiang","Xiaojie Wang"],"3225":["Lei Wu","Nenghai Yu","Jing Liu","Mingjing Li"],"3226":["Zhiyu Chen","Harini Eavani","Yinyin Liu","William Yang Wang"],"3227":["Cyril Allauzen","Michael Riley"],"3228":["Daniel K. C. Chan","Jochem Vonk","Gabriel Sanchez","Paul W. P. J. Grefen","Peter M. G. Apers"],"3229":["Jes\u00fas Manuel Almendros-Jim\u00e9nez","Luis Iribarne"],"3230":["Sheng Zhang","Xianli Wu"],"3231":["Assaf Hoogi","Arjun Mishra","Francisco Gimenez","Jeffrey Dong","Daniel L. Rubin"],"3232":["Mudit Verma","Arun Balaji Buduru"],"3233":["Ankur Gandhe","Ariya Rastrow"],"3234":["Alastair F. Donaldson","Alice Miller","David Parker"],"3235":["Ye Bai","Jiangyan Yi","Jianhua Tao","Zhengqi Wen","Cunhang Fan"],"3236":["Karen da Silva Figueiredo","Viviane Torres da Silva"],"3237":["Akinori Ito","Masaki Kohda","Mari Ostendorf"],"3238":["Jerry T. Ball"],"3239":["Bo-June (Paul) Hsu","James Glass"],"3240":["Cong Zhang","Amol Bakshi","Viktor K. Prasanna"],"3241":["Victor H. Yngve"],"3242":["Ciprian Chelba","D. Engle","F. Jelinek","V. Jimenez","S. Khudanpur","L. Mangu","H. Printz","E. S. Ristad","R. Rosenfeld","A. Stolcke","D. Wu"],"3243":["Kakeung Wong","Man-Hung Siu"],"3244":["Luc Lussier","Edward W. D. Whittaker","Sadaoki Furui"],"3245":["Christopher M. White","Ariya Rastrow","Sanjeev Khudanpur","Frederick Jelinek"],"3246":["Diamantino Caseiro","Isabel Trancoso"],"3247":["Langzhou Chen","Jean-Luc Gauvain","Lori Lamel","Gilles Adda","Martine Adda-Decker"],"3248":["Camila Bezerra da Silva"],"3249":["Jerry T. Ball"],"3250":["Anelly Kremenska"],"3251":["Pierre-Antoine Jean","S\u00e9bastien Harispe","Sylvie Ranwez","Patrice Bellot","Jacky Montmain"],"3252":["Youzheng Wu","Kazuhiko Abe","Paul R. Dixon","Chiori Hori","Hideki Kashioka"],"3253":["Bo-June Paul Hsu","James R. Glass"],"3254":["Cedric G. P. Auzanne","John S. Garofolo","William M. Fisher","Jonathan G. Fiscus"],"3255":["Karsten Konrad"],"3256":["Taewoo Lee","Min-Joong Lee","Tae Gyoon Kang","Seokyeong Jung","Minseok Kwon","Yeona Hong","Jungin Lee","Kyoung-Gu Woo","Ho-Gyeong Kim","Jiseung Jeong","Jihyun Lee","Hosik Lee","Young Sang Choi"],"3257":["Yunping Huang","Le Sun","Jian-Yun Nie"],"3258":["Georg Hinkel"],"3259":["Zhiyu Chen","Harini Eavani","Wenhu Chen","Yinyin Liu","William Yang Wang"],"3260":["Qi Liu","Yanmin Qian","Kai Yu"],"3261":["Ying Xie","Vijay V. Raghavan","Andrew Young"],"3262":["Kevin Marth","Shangping Ren"],"3263":["Gakuto Kurata","Abhinav Sethy","Bhuvana Ramabhadran","Ariya Rastrow","Nobuyasu Itoh","Masafumi Nishimura"],"3264":["Sahar Kokaly"],"3265":["Victor Kromer"],"3266":["Ilham Firdausi Putra","Ayu Purwarianti"],"3267":["Ivan Kurtev"],"3268":["Ying Xie","Vijay V. Raghavan","Andrew Young"],"3269":["Nilesh N. Dalvi","Ravi Kumar","Bo Pang","Andrew Tomkins"],"3270":["Xin Li","Piji Li","Wei Bi","Xiaojiang Liu","Wai Lam"],"3271":["Yixiao Yang","Yu Jiang","Ming Gu","Jiaguang Sun","Jian Gao","Han Liu"],"3272":["Ehsan Hosseini-Asl","Bryan McCann","Chien-Sheng Wu","Semih Yavuz","Richard Socher"],"3273":["Naoto Iwahashi"],"3274":["Daisuke Okanohara","Jun'ichi Tsujii"],"3275":["Feifan Liu","Yang Liu"],"3276":["Stephen W. Liddle","David W. Embley","Scott N. Woodfield"],"3277":["Jeff Mitchell"],"3278":["Nhat-Hoa Tran","Yuki Chiba","Toshiaki Aoki"],"3279":["Luis Quesada","Fernando Berzal","Juan C. Cubero"],"3280":["Yuanrui Zhang","Yujing Ma","Yixiang Chen"],"3281":["Justin Scheiner","Ian Williams","Petar S. Aleksic"],"3282":["Jason Hepburn"],"3283":["Mariem Mefteh","Nadia Bouassida","Han\u00eane Ben-Abdallah"],"3284":["G\u00fcnes Erkan"],"3285":["Michael Subotin"],"3286":["Juan Carlos Franco","Thamar Solorio"],"3287":["Mohamed Amine Menacer","Abdelfetah Boumerdas","Chahnez Zakaria","Kamel Sma\u00efli"],"3288":["Massimo Tisi","Fr\u00e9d\u00e9ric Jouault","J\u00e9r\u00f4me Delatour","Zied Saidi","Hassene Choura"],"3289":["Mark Levene","Alexandra Poulovassilis"],"3290":["Bo Geng","Linjun Yang","Chao Xu"],"3291":["Rens Bod"],"3292":["Burr Settles","Brendan Meeder"],"3293":["Per-\u00c5ke Larson"],"3294":["Ronan Cummins"],"3295":["Timo Niemi","Lasse Hirvonen","Kalervo J\u00e4rvelin"],"3296":["Paul Buchheit"],"3297":["Pengfei Liu","Xipeng Qiu","Xuanjing Huang"],"3298":["Mohamed Trabelsi","Jin Cao","Jeff Heflin"],"3299":["Kevin Lano","S. Fang","M. A. Umar","Sobhan Yassipour-Tehrani"],"3300":["Larry Hoyle","Joachim Wackerow"],"3301":["Wataru Takano","Minoru Kanazawa","Yoshihiko Nakamura"],"3302":["Janis Barzdins","Juris Barzdins","Edgars Rencis","Agris Sostaks"],"3303":["Zbigniew Michalewicz"],"3304":["P. Broeder","J. Murre"],"3305":["Shamkant B. Navathe","Rafi Ahmed"],"3306":["Brian E. Pangburn","Robert C. Mathews","S. Sitharama Iyengar","Jonathan P. Ayo"],"3307":["P. Niyogi","R. C. Berwick"],"3308":["Nick Waegner","Steve J. Young"],"3309":["Ciprian Chelba","David Engle","Frederick Jelinek","Victor Jimenez","Sanjeev Khudanpur","Lidia Mangu","Harry Printz","Eric Ristad","Ronald Rosenfeld","Andreas Stolcke","Dekai Wu"],"3310":["Andreas Merkel","Dietrich Klakow"],"3311":["Wen Wang","Andreas Stolcke"],"3312":["Iz Beltagy","Kyle Lo","Arman Cohan"],"3313":["Wenhan Xiong","Jingfei Du","William Yang Wang","Veselin Stoyanov"],"3314":["Ankur Gandhe","Ariya Rastrow"],"3315":["Bingjie Wei","Shuai Zhang","Rui Li","Bin Wang"],"3316":["Tadashi Nomoto"],"3317":["Jia Xu","Geliang Chen"],"3318":["Stuart Bradley"],"3319":["Thora Tenbrink","Werner Kuhn"],"3320":["Monica Palmirani","Raffaella Brighi"],"3321":["Leonor Becerra-Bonache","Maria Dolores Jim\u00e9nez-L\u00f3pez"],"3322":["Yisel Clavel Quintero","Leticia Arco Garc\u00eda"],"3323":["Luis Villase\u00f1or Pineda","Manuel Montes y G\u00f3mez","Manuel Alberto P\u00e9rez-Couti\u00f1o","Dominique Vaufreydaz"],"3324":["Till Mossakowski"],"3325":["Tung Thanh Nguyen","Anh Tuan Nguyen","Hoan Anh Nguyen","Tien N. Nguyen"],"3326":["Michael J. Carey","David J. DeWitt","Scott L. Vandenberg"],"3327":["Ankur P. Parikh","Oscar T\u00e4ckstr\u00f6m","Dipanjan Das","Jakob Uszkoreit"],"3328":["Yirong Pan","Xiao Li","Yating Yang","Rui Dong"],"3329":["Stefan Heinrich","Yuan Yao","Tobias Hinz","Zhiyuan Liu","Thomas Hummel","Matthias Kerzel","Cornelius Weber","Stefan Wermter"],"3330":["Hisaki Ikebata","Kenta Hongo","Tetsu Isomura","Ryo Maezono","Ryo Yoshida"],"3331":["Masayuki Tsuji","Teijiro Isokawa","Takayuki Yumoto","Nobuyuki Matsui","Naotake Kamiura"],"3332":["Weirui Kong","Hyeju Jang","Giuseppe Carenini","Thalia Shoshana Field"],"3333":["Lance De Vine","Guido Zuccon","Bevan Koopman","Laurianne Sitbon","Peter Bruza"],"3334":["Bo Li","\u00c9ric Gaussier"],"3335":["Svetlana Sheremetyeva","Sergei Nirenburg"],"3336":["Youzheng Wu","Xugang Lu","Hitoshi Yamamoto","Shigeki Matsuda","Chiori Hori","Hideki Kashioka"],"3337":["Vera Demberg"],"3338":["Matthew G. Snover","Bonnie J. Dorr","Richard M. Schwartz"],"3339":["Qi Liu","Yanmin Qian","Kai Yu"],"3340":["Wunna Soe","Yadana Theins"],"3341":["Angelo Gargantini","Elvinia Riccobene","Patrizia Scandurra"],"3342":["Guangpu Huang","Meng Joo Er"],"3343":["Wayne H. Ward","Sunil Issar"],"3344":["Jose J. Padilla","David Shuttleworth","Kevin O'Brien"],"3345":["Bernd Amann","Michel Scholl"],"3346":["Sa'ed Abed","Kamran Hussain","Otmane A\u00eft Mohamed"],"3347":["Saebyeok Lee","Heuiseok Lim"],"3348":["Marjorie McShane"],"3349":["Tapas Kanungo","Song Mao"],"3350":["Thomas Niesler","Daniel Willett"],"3351":["Lawrence H. Reeve","Hyoil Han","Ari D. Brooks"],"3352":["Heshaam Faili","Hadi Ravanbakhsh"],"3353":["Jeremy Howard","Sebastian Ruder"],"3354":["Lo\u00efc Gammaitoni","Pierre Kelsen"],"3355":["Ivan Bulyko","Spyros Matsoukas","Richard M. Schwartz","Long Nguyen","John Makhoul"],"3356":["Mehrdad Farahani","Mohammad Gharachorloo","Marzieh Farahani","Mohammad Manthouri"],"3357":["Shady Elbassuoni","Maya Ramanath","Gerhard Weikum"],"3358":["Chedi Bechikh Ali","Hatem Haddad"],"3359":["Tsuyoshi Okita","Andy Way"],"3360":["Ying Li","Meng Xi","Hui Chen","Jianwei Yin"],"3361":["Makoto Yasuhara","Toru Tanaka","Jun ya Norimatsu","Mikio Yamamoto"],"3362":["Bo-June Paul Hsu","James R. Glass"],"3363":["Alessandro Di Bari","Guido Vetere","Kateryna Tymoshenko"],"3364":["Ranu Sewada","Ashwani Jangid","Piyush Kumar","Neha Mishra"],"3365":["Alejandro Barredo Arrieta","Natalia D\u00edaz Rodr\u00edguez","J. Ser","Adrien Bennetot","S. Tabik","A. Barbado","S. Garc\u00eda","S. Gil-Lopez","D. Molina","Richard Benjamins","Raja Chatila","Francisco Herrera"],"3366":["E. Topol"],"3367":["Roberto Mart\u00ednez-Maldonado","H. U. Hoppe","John Benedict du Boulay"],"3368":["Michael R. King","chatGPT"],"3369":["Michael Moor","O. Banerjee","Zahra F H Abad","H. Krumholz","J. Leskovec","E. Topol","P. Rajpurkar"],"3370":["J. Pavlik"],"3371":["Michele Salvagno","F. Taccone","A. Gerli"],"3372":["Tim Miller"],"3373":[],"3374":["P. Krausman"],"3375":["M. Simion","Christoph Kelp"],"3376":["Ashish Sarraju","Dennis Bruemmer","E. V. Van Iterson","L. Cho","Fatima Rodriguez","L. Laffin"],"3377":["Hanchen Wang","Tianfan Fu","Yuanqi Du","Wenhao Gao","Kexin Huang","Ziming Liu","P. Chandak","Shengchao Liu","Peter Van Katwyk","Andreea Deac","Anima Anandkumar","K. Bergen","Carla P. Gomes","Shirley Ho","Pushmeet Kohli","Joan Lasenby","J. Leskovec","Tie-Yan Liu","A. Manrai","Debora S. Marks","Bharath Ramsundar","Le Song","Jimeng Sun","Jian Tang","Petar Velickovic","Max Welling","Linfeng Zhang","Connor W. Coley","Y. Bengio","M. Zitnik"],"3378":["Lin Li","Lixin Qin","Zeguo Xu","Youbing Yin","Xin Wang","B. Kong","Junjie Bai","Yi Lu","Zhenghan Fang","Q. Song","K. Cao","Daliang Liu","Guisheng Wang","Qizhong Xu","Xisheng Fang","Shiqin Zhang","J. Xia","Jun Xia"],"3379":[],"3380":["Yogesh Kumar","Apeksha Koul","Ruchika Singla","M. Ijaz"],"3381":["Imran Ahmed","Gwanggil Jeon","F. Piccialli"],"3382":["Davinder Kaur","Suleyman Uslu","Kaley J. Rittichier","A. Durresi"],"3383":[],"3384":["Thien Huynh-The","Viet Quoc Pham","Xuan-Qui Pham","Thanh Thi Nguyen","Zhu Han","Dong-Seong Kim"],"3385":["Gwo-jen Hwang","Shu-Yun Chien"],"3386":["Fabio Urbina","Filippa Lentzos","C\u00e9dric Invernizzi","S. Ekins"],"3387":["Neha Gupta","S. Gupta","R. K. Pathak","Vanita Jain","P. Rashidi","J. Suri"],"3388":["F. Shi","Jun Wang","Jun Shi","Zi-xiang Wu","Qian Wang","Zhenyu Tang","Kelei He","Yinghuan Shi","D. Shen"],"3389":["Daniel D. Raj","Mr. Karthiban"],"3390":[],"3391":["Maad M. Mijwil"],"3392":["M. Krenn","R. Pollice","S. Guo","Matteo Aldeghi","Alba Cervera-Lierta","Pascal Friederich","Gabriel dos Passos Gomes","Florian Hase","A. Jinich","AkshatKumar Nigam","Zhenpeng Yao","Al\u00e1n Aspuru-Guzik"],"3393":["Kun\u2010Hsing Yu","Andrew Beam","I. Kohane"],"3394":["A. Kilani","A. Hamida","H. Hamam"],"3395":["Yuchen Jiang","Xiang Li","Hao Luo","Shen Yin","O. Kaynak"],"3396":["T. Davenport","R. Kalakota"],"3397":["Rohan Gupta","Devesh Srivastava","Mehar Sahu","Swati Tiwari","R. K. Ambasta","Pravir Kumar"],"3398":["D. Vrontis","M. Christofi","V. Pereira","S. Tarba","Anna Makrides","Eleni Trichina"],"3399":["Zhi Zhou","Xu Chen","En Li","Liekang Zeng","Ke Luo","Junshan Zhang"],"3400":["K. Letaief","Yuanming Shi","Jianmin Lu","Jianhua Lu"],"3401":["S. G. Finlayson","Adarsh Subbaswamy","Karandeep Singh","John Bowers","Annabel Kupke","Jonathan Zittrain","I. Kohane","S. Saria"],"3402":["Konstantinos C. Siontis","P. Noseworthy","Z. Attia","P. Friedman"],"3403":["P. Angelov","E. Soares","Richard Jiang","Nicholas I. Arnold","Peter M. Atkinson"],"3404":["Silvana Secinaro","D. Calandra","A. Secinaro","V. Muthurangu","P. Biancone"],"3405":["Ella Glikson","A. Woolley"],"3406":["Jordi Laguarta","F. Hueto","B. Subirana"],"3407":["Arun Das","P. Rad"],"3408":["Yue Pan","Limao Zhang"],"3409":["Caiming Zhang","Yang Lu"],"3410":["Yongjun Xu","Qi Wang","Zhulin An","Fei Wang","Libo Zhang","Yanjun Wu","Fengliang Dong","Cheng-Wei Qiu","Xin Liu","Junjun Qiu","K. Hua","Wentao Su","Huiyu Xu","Yong Han","Xinya Cao","En-ju Liu","C. Fu","Zhigang Yin","Miao Liu","R. Roepman","S. Dietmann","M. Virta","F. Kengara","Changping Huang","Ze Zhang","Lifu Zhang","Taolan Zhao","Jianwei Dai","Jialiang Yang","L. Lan","Ming Luo","Tao Huang","Zhaofeng Liu","Sen Qian","T. An","Xingchen Liu","Bin Zhang","Xiaolei He","Shan Cong","Xiaohong Liu","Wei Zhang","Fang Wang","Chuan\u2010Qi Lu","Zhipeng Cai","James P. Lewis","J. Tiedje","Jia-bao Zhang"],"3411":["A. Hosny","C. Parmar","John Quackenbush","L. Schwartz","H. Aerts"],"3412":["Ming-Hui Huang","R. Rust"],"3413":["M. Cui","David Zhang"],"3414":["David Gunning","D. Aha"],"3415":["K. Bera","Nathaniel Braman","Amit Gupta","V. Velcheti","A. Madabhushi"],"3416":["Kai Guo","Zhenze Yang","Chi-Hua Yu","M. Buehler"],"3417":["John T Mongan","L. Moy","C. E. Kahn"],"3418":["Dang Minh","H. X. Wang","Y. Li","Tan N. Nguyen"],"3419":["B. Bhinder","Coryandar Gilvary","Neel S. Madhukar","O. Elemento"],"3420":["Teo Lombardo","M. Duquesnoy","Hassna El-Bouysidy","Fabian \u00c5r\u00e9n","Alfonso Gallo-Bueno","P. B. J\u00f8rgensen","A. Bhowmik","Arnaud Demorti\u00e8re","E. Ayerbe","F. Alcaide","M. Reynaud","Javier Carrasco","A. Grimaud","Chao Zhang","T. Vegge","P. Johansson","A. Franco"],"3421":["Zhihan Lv","Yang Han","A. Singh","Gunasekaran Manogaran","Haibin Lv"],"3422":["Wei Hu"],"3423":["Ida Merete Enholm","Emmanouil Papagiannidis","Patrick Mikalef","J. Krogstie"],"3424":["J. He","Sally L. Baxter","Jie Xu","Jiming Xu","Xingtao Zhou","Kang Zhang"],"3425":["R. J. Kline","J. Kline","Sam Kline"],"3426":["Dhaya Sindhu Battina"],"3427":["Erico Tjoa","Cuntai Guan"],"3428":["Raju Vaishya","M. Javaid","I. Khan","Abid Haleem"],"3429":["B. Shneiderman"],"3430":["M. Mitchell"],"3431":["X. Mei","Hao-Chih Lee","Kai-yue Diao","Mingqian Huang","Bin Lin","Chenyu Liu","Zongyu Xie","Yixuan Ma","P. Robson","M. Chung","Adam Bernheim","V. Mani","C. Calcagno","Kunwei Li","Shaolin Li","H. Shan","Jian Lv","Tongtong Zhao","Junli Xia","Q. Long","Sharon Steinberger","A. Jacobi","T. Deyer","M. Luksza","Fang Liu","B. Little","Z. Fayad","Yang Yang"],"3432":["B. Shastri","A. Tait","T. F. D. Lima","W. Pernice","H. Bhaskaran","C. Wright","P. Prucnal"],"3433":["Anna Jobin","M. Ienca","E. Vayena"],"3434":["Sebastian Raisch","Sebastian Krakowski"],"3435":["Gordon Wetzstein","A. Ozcan","S. Gigan","S. Fan","D. Englund","M. Solja\u010di\u0107","C. Denz","D. Miller","D. Psaltis"],"3436":["Peng Zhang","Zhiling Guo","S. Ullah","G. Melagraki","A. Afantitis","I. Lynch"],"3437":["Joseph Aylett-Bullock","A. Luccioni","K. H. Pham","C. Lam","M. Luengo-Oroz"],"3438":["Jos\u00e9 Jim\u00e9nez-Luna","F. Grisoni","G. Schneider"],"3439":["J. Amann","A. Blasimme","E. Vayena","D. Frey","V. Madai"],"3440":["Eirini Ntoutsi","P. Fafalios","U. Gadiraju","Vasileios Iosifidis","W. Nejdl","Maria-Esther Vidal","S. Ruggieri","F. Turini","S. Papadopoulos","Emmanouil Krasanakis","I. Kompatsiaris","K. Kinder-Kurlanda","Claudia Wagner","F. Karimi","Miriam Fern\u00e1ndez","Harith Alani","Bettina Berendt","Tina Kruegel","C. Heinze","Klaus Broelemann","G. Kasneci","T. Tiropanis","Steffen Staab"],"3441":["Ravi Manne","S. Kantheti"],"3442":["Zixin Hu","Q. Ge","Shudi Li","Li Jin","M. Xiong"],"3443":["Xiangao Jiang","M. Coffee","Anasse Bari","Junzhang Wang","Xinyue Jiang","Jianping Huang","Jichan Shi","J. Dai","Jiong Cai","Tianxiao Zhang","Zheng-xing Wu","Guiqing He","Yitong Huang"],"3444":["Jing Zhang","D. Tao"],"3445":["R. Vinuesa","Hossein Azizpour","Iolanda Leite","Madeline Balaam","Virginia Dignum","S. Domisch","Anna Fell\u00e4nder","S. Langhans","Max Tegmark","F. F. Nerini"],"3446":["J. Korteling","G. V. D. Boer-Visschedijk","R. Blankendaal","R. Boonekamp","A. Eikelboom"],"3447":["Meng Hao","Hongwei Li","Xizhao Luo","Guowen Xu","Haomiao Yang","Sen Liu"],"3448":["S. Cruz Rivera","Xiaoxuan Liu","A. Chan","A. Denniston","M. Calvert"],"3449":["J. Scheetz","Philip Rothschild","M. McGuinness","X. Hadoux","H. Soyer","M. Janda","J. Condon","L. Oakden-Rayner","L. Palmer","S. Keel","P. van Wijngaarden"],"3450":["Shuai Zhao","F. Blaabjerg","Huai Wang"],"3451":["F. Jiang","Yong Jiang","Hui Zhi","Yi Dong","Hao Li","Sufeng Ma","Yilong Wang","Q. Dong","Haipeng Shen","Yongjun Wang"],"3452":["Ming-Hui Huang","R. Rust"],"3453":["Xiaoxuan Liu","S. Cruz Rivera","D. Moher","M. Calvert","A. Denniston","An-Wen Ara Christopher Christopher Hutan Jonathan J. Lavi Chan Darzi Holmes Yau Ashrafian Deeks Ferrante di ","An-Wen Ara Christopher Christopher Hutan Jonathan J. Lavi Chan Darzi Holmes Yau Ashrafian Deeks Ferrante di ","An-Wen Chan","A. Darzi","Christopher Holmes","Christopher Yau","H. Ashrafian","Jonathan J. Deeks","Lavinia Ferrante di Ruffano","Livia Faes","P. Keane","Sandra Jeanne Vollmer","Aaron Y. Adrian Andre Andrew L. An-Wen Maria Beatrice Ce Lee Jonas Esteva Beam Chan Panico Lee Haug Kelly Y","Aaron Lee","Adrian Jonas","Andre Esteva","Andrew Beam","M. Panico","Cecilia S. Lee","Charlotte Haug","Christopher J. Kelly","C. Mulrow","Cyrus Espinoza","John Fletcher","Dina Paltoo","Elaine Manna","G. Price","Gary S. Collins","Hugh Harvey","James Matcham","Jo\u00e3o Monteiro","M. ElZarrad","L. Oakden-Rayner","M. McCradden","Richard Savage","R. Golub","Rupa Sarkar","Samuel Rowley"],"3454":["A. Kaplan","M. Haenlein"],"3455":["Theo B. Araujo","N. Helberger","S. Kruikemeier","C. Vreese"],"3456":["Songkun Yu"],"3457":["T. Davenport","Abhijit Guha","Dhruv Grewal","Timna Bre\u00dfgott"],"3458":["Y. Y. Aung","D. C. Wong","D. Ting"],"3459":["Z. Attia","D. Harmon","E. Behr","P. Friedman"],"3460":["Araz Taeihagh"],"3461":["Shakir Mohamed","Marie-Therese Png","William S. Isaac"],"3462":["Olaf Zawacki-Richter","Victoria I. Mar\u00edn","Melissa Bond","Franziska Gouverneur"],"3463":["Zhimin Zhang","Huansheng Ning","Feifei Shi","Fadi Farha","Yang Xu","Jiabo Xu","Fan Zhang","K. Choo"],"3464":["K. Yeung"],"3465":["S. Puntoni","R. W. Reczek","M. Giesler","Simona Botti"],"3466":["O. Elemento","C. Leslie","Johan Lundin","G. Tourassi"],"3467":["Onur Asan","A. E. Bayrak","Avishek Choudhury"],"3468":["W. Naud\u00e9"],"3469":["G. Briganti","O. le Moine"],"3470":["F. Schwendicke","W. Samek","J. Krois"],"3471":["M. Jamshidi","A. Lalbakhsh","J. Talla","Z. Peroutka","F. Hadjilooei","Pedram Lalbakhsh","Morteza Jamshidi","L. L. Spada","M. Mirmozafari","Mojgan Dehghani","Asal Sabet","S. Roshani","S. Roshani","Nima Bayat-Makou","B. Mohamadzade","Zahra Malek","A. Jamshidi","S. Kiani","H. Hashemi\u2010Dezaki","Wahab Mohyuddin"],"3472":["Christopher J. Kelly","A. Karthikesalingam","Mustafa Suleyman","Greg C. Corrado","Dominic King"],"3473":["Agha Wafa Abbas Wafa","Muzammil Hussain Muzammil Hussain"],"3474":["Xiaodong Huang"],"3475":["Ana Jim\u00e9nez Pastor"],"3476":["R. Walters","Marko Novak"],"3477":["Giulia Vilone","L. Longo"],"3478":["W. Shi","Min Zhang","Rui Zhang","S. Chen","Zhao Zhan"],"3479":["Yuko Harayama","Michela Milano","Richard Baldwin","C\u00e9line Antonin","Janine Berg","Anousheh Karvar","Andrew Wyckoff"],"3480":["Cheng Jin","Weixiang Chen","Yukun Cao","Zhanwei Xu","Zimeng Tan","Xin Zhang","Lei Deng","C. Zheng","Jie Zhou","Heshui Shi","Jianjiang Feng"],"3481":["W. Bi","A. Hosny","M. Schabath","M. Giger","N. Birkbak","Alireza Mehrtash","Tavis Allison","O. Arnaout","C. Abbosh","I. Dunn","R. Mak","R. Tamimi","C. Tempany","C. Swanton","U. Hoffmann","L. Schwartz","R. Gillies","Raymond Y Huang","H. Aerts"],"3482":["Stuart Russell"],"3483":["M. Reyes","Raphael Meier","S\u00e9rgio Pereira","Carlos A. Silva","F. Dahlweid","H. von Tengg-Kobligk","R. Summers","R. Wiest"],"3484":["Omri Gillath","Ting Ai","M. Branicky","S. Keshmiri","Robert B. Davison","R. Spaulding"],"3485":["Yogesh Kumar Dwivedi","Laurie Hughes","Elvira Ismagilova","G. Aarts","C. Coombs","Tom Crick","Y. Duan","R. Dwivedi","J. Edwards","Aled Eirug","Vassilis Galanos","P. Ilavarasan","M. Janssen","Paul Jones","A. Kar","Hatice Kizgin","Bianca Kronemann","Banita Lal","B. Lucini","R. Medaglia","K. L. Meunier-FitzHugh","L. Meunier-FitzHugh","S. Misra","E. Mogaji","S. Sharma","Jang B. Singh","Vishnupriya Raghavan","R. Raman","N. Rana","Spyridon Samothrakis","Jak Spencer","K. Tamilmani","Annie Tubadji","P. Walton","Michael D. Williams"],"3486":["B. Kann","A. Hosny","H. Aerts"],"3487":["V. Dignum"],"3488":["V. Galaz","M. Centeno","Peter W. Callahan","Amar Causevic","Thayer S. Patterson","I. Brass","S. Baum","D. Farber","J. Fischer","David Garcia","T. McPhearson","Daniel Jimenez","B. King","P. Larcey","Karen Levy"],"3489":["Zaib Ullah","F. Al-turjman","L. Mostarda","R. Gagliardi"],"3490":["Chiara Longoni","Andrea Bonezzi","Carey K. Morewedge"],"3491":["Prasanna Tambe","P. Cappelli","V. Yakubovich"],"3492":["S. Singh","S. Rathore","J. Park"],"3493":["Andreas Holzinger","G. Langs","H. Denk","K. Zatloukal","Heimo M\u00fcller"],"3494":["K. Bera","K. Schalper","D. Rimm","V. Velcheti","A. Madabhushi"],"3495":["David Ria\u00f1o","Szymon Wilk","A. T. Teije"],"3496":["R. Challen","J. Denny","M. Pitt","Luke Gompels","Tom Edwards","K. Tsaneva-Atanasova"],"3497":["Iason Gabriel"],"3498":["Sebastian J. Vollmer","Bilal A. Mateen","Gergo Bohner","Franz J. Kir\u00e1ly","Rayid Ghani","Pall Jonsson","Sarah Cumbers","Adrian Jonas","Katherine S L McAllister","Puja Myles","David Grainger","Mark Birse","Richard Branson","K. Moons","Gary S. Collins","J. Ioannidis","Chris Holmes","Harry Hemingway"],"3499":["Tanha Talaviya","Dharam J. Shah","Nivedita Patel","Hiteshri Yagnik","Manan Shah"],"3500":["Yadi Zhou","Fei Wang","Jian Tang","R. Nussinov","F. Cheng"],"3501":["Shuiguang Deng","Hailiang Zhao","Jianwei Yin","S. Dustdar","Albert Y. Zomaya"],"3502":["R. Atkinson"],"3503":["A. Bohr","K. Memarzadeh"],"3504":["Anne Lauscher"],"3505":["Debleena Paul","G. Sanap","S. Shenoy","Dnyaneshwar Kalyane","K. Kalia","R. Tekade"],"3506":["Amisha","Paras Malik","Monika Pathania","V. Rathaur"],"3507":["M. Frank","David Autor","James Bessen","Erik Brynjolfsson","M. Cebri\u00e1n","D. Deming","M. Feldman","Matthew Groh","J. Lobo","E. Moro","Dashun Wang","Hyejin Youn","Iyad Rahwan"],"3508":["A. Ahuja"],"3509":["Xin Yang","Yifei Wang","Ryan Byrne","G. Schneider","Sheng-yong Yang"],"3510":["S. Gerke","T. Minssen","Glenn Cohen"],"3511":["P. Schneider","W. P. Walters","A. Plowright","Norman Sieroka","J. Listgarten","Robert A. Goodnow","Jasmin Fisher","Johanna M. Jansen","Jos\u00e9 S. Duca","Thomas S. Rush","M. Zentgraf","John Edward Hill","Elizabeth Krutoholow","Matthias Kohler","J. Blaney","K. Funatsu","Chris Luebkemann","G. Schneider"],"3512":["N. Misra","Y. Dixit","A. Al-Mallahi","Manreet Bhullar","R. Upadhyay","A. Martynenko"],"3513":["Giulia Vilone","L. Longo"],"3514":["A. Rodr\u00edguez-Ruiz","E. Krupinski","J. Mordang","K. Schilling","S. Heywang-K\u00f6brunner","I. Sechopoulos","R. Mann"],"3515":["Haoxiang Zhong","Chaojun Xiao","Cunchao Tu","T. Zhang","Zhiyuan Liu","Maosong Sun"],"3516":["V. C. M\u00fcller"],"3517":["Yifang Ma","Zhenyu Wang","Hong Yang","Lin Yang"],"3518":["W. Naud\u00e9"],"3519":["M. Ryan"],"3520":["T. Schaffter","D. Buist","Christoph I. Lee","Yaroslav Nikulin","D. Ribli","Y. Guan","William Lotter","Zequn Jie","Hao Du","Sijia Wang","Jiashi Feng","Mengling Feng","Hyo-Eun Kim","F. Albiol","A. Albiol","Stephen Morrell","Z. Wojna","M. Ahsen","U. Asif","Antonio Jos\u00e9 Jimeno Yepes","Shivanthan A. C. Yohanandan","S. Rabinovici-Cohen","Darvin Yi","B. Hoff","Thomas V Yu","E. Chaibub Neto","D. Rubin","Peter Lindholm","L. Margolies","R. McBride","J. Rothstein","W. Sieh","Rami Ben-Ari","S. Harrer","A. Trister","S. Friend","Thea C. Norman","B. Sahiner","F. Strand","J. Guinney","G. Stolovitzky","Lester W. Mackey","Joyce Cahoon","Li Shen","J. Sohn","H. Trivedi","Yiqiu Shen","L. Buturovic","Jos\u00e9 Costa Pereira","Jaime S. Cardoso","Eduardo Castro","K. T. Kalleberg","Obioma Pelka","Imane Nedjar","Krzysztof J. Geras","F. Nensa","Ethan Goan","S. Koitka","Luis Caballero","David D. Cox","Pavitra Krishnaswamy","G. Pandey","C. Friedrich","Dimitri Perrin","C. Fookes","Bibo Shi","Gerard Cardoso Negrie","M. Kawczynski","Kyunghyun Cho","Can Son Khoo","Joseph Y. Lo","A. Sorensen","Hwejin Jung"],"3521":["R. Verganti","Luca Vendraminelli","M. Iansiti"],"3522":["Kai-Cheng Yang","Onur Varol","Clayton A. Davis","Emilio Ferrara","A. Flammini","F. Menczer"],"3523":["D. Garc\u00eda-P\u00e9rez","Juan Angel Lorenzo del Castillo","Yahya Al-Hazmi","Josep Martrat","K. Kavoussanakis","Alastair C. Hume","Celia Velayos L\u00f3pez","G. Landi","T. Wauters","M. Gienger","D. Margery"],"3524":["Gwo-jen Hwang","Haoran Xie","B. Wah","D. Ga\u0161evi\u0107"],"3525":["V. Kaul","Sarah Enslin","S. Gross"],"3526":["Payal Dhar"],"3527":["Yuqi Guo","Zhichao Hao","Shichong Zhao","Jiaqi Gong","Fan Yang"],"3528":["R. Cioffi","Marta Travaglioni","G. Piscitelli","A. Petrillo","F. De Felice"],"3529":["Hao Zhu"],"3530":["Michael Webb"],"3531":["Theyazn H. H. Aldhyani","M. Al-Yaari","Hasan Alkahtani","M. Maashi"],"3532":["R. Confalonieri","Ludovik \u00c7oba","Benedikt Wagner","Tarek R. Besold"],"3533":["Zeynep Akata","D. Balliet","M. de Rijke","F. Dignum","V. Dignum","Gusz Eiben","Antske Fokkens","D. Grossi","K. Hindriks","H. Hoos","Hayley Hung","C. Jonker","Christof Monz","Mark Antonius Neerincx","Frans Oliehoek","H. Prakken","S. Schlobach","Linda van der Gaag","F. van Harmelen","Herke van Hoof","Birna van Riemsdijk","A. van Wynsberghe","R. Verbrugge","B. Verheij","P. Vossen","M. Welling"],"3534":["B. Mesk\u00f3","M\u00e1rton G\u00f6r\u00f6g"],"3535":["Hideyuki Shimizu","K. Nakayama"],"3536":["Ivy Munoko","Helen L. Brown-Liburd","M. Vasarhelyi"],"3537":["Ragnar Fjelland"],"3538":["T. Sejnowski"],"3539":["Z. Allam","Zaynah A. Dhunny"],"3540":["M. Niazi","A. Parwani","M. Gurcan"],"3541":["Kirtan Jha","Aalap Doshi","Pooja Patel","Manan Shah"],"3542":["B. Haibe-Kains","George Adam","A. Hosny","F. Khodakarami","L. Waldron","Bo Wang","C. McIntosh","A. Goldenberg","A. Kundaje","C. Greene","Tamara Broderick","M. M. Hoffman","J. Leek","Keegan D. Korthauer","W. Huber","A. Brazma","Joelle Pineau","R. Tibshirani","T. Hastie","J. Ioannidis","John Quackenbush","H. Aerts"],"3543":["T. Shan","F. R. Tay","Lisha Gu"],"3544":["C. Prentice","Sergio Dominique Lopes","Xuequn Wang"],"3545":["Federico Cugurullo"],"3546":["P. Phillips","Carina A. Hahn","Peter C. Fontana","David A. Broniatowski","Mark A. Przybocki"],"3547":["E. Neri","F. Coppola","V. Miele","C. Bibbolino","R. Grassi"],"3548":["V. Venkatasubramanian"],"3549":["Pavitra Dhamija","Surajit Bag"],"3550":["Qian Zhang","Jie Lu","Yaochu Jin"],"3551":["Zhuomin Sun","M. Anbarasan","D. P. Kumar"],"3552":["Wenjuan Sun","P. Bocchini","Brian D. Davison"],"3553":["S. Dash","K. Vijayakumar","B. K. Panigrahi","Swagatam Das"],"3554":["Tara Qian Sun","R. Medaglia"],"3555":["Kit-Kay Mak","M. Pichika"],"3556":["E. Huynh","A. Hosny","C. Guthier","D. Bitterman","S. Petit","D. Haas-Kogan","B. Kann","H. Aerts","R. Mak"],"3557":["E. Neri","V. Miele","F. Coppola","R. Grassi"],"3558":["N. Anantrasirichai","D. Bull"],"3559":["D. Hashimoto","E. Witkowski","Lei Gao","O. Meireles","G. Rosman"],"3560":["Hong Chen","L. Li","Yong Chen"],"3561":["R. Nishant","M. Kennedy","J. Corbett"],"3562":["M. Woschank","E. Rauch","Helmut E. Zsifkovits"],"3563":["B. Cope","M. Kalantzis","Duane Searsmith"],"3564":["W. Samek","K. M\u00fcller"],"3565":["Philipp Schmidt","F. Biessmann","Timm Teubner"],"3566":["Ale\u0161 Zavr\u0161nik"],"3567":["S. Ali","B. Choi"],"3568":["G. Collins","K. Moons"],"3569":["D. Dey","P. Slomka","P. Leeson","D. Comaniciu","S. Shrestha","P. Sengupta","T. Marwick"],"3570":["R. Abdullah","Bahjat Fakieh"],"3571":["Xieling Chen","Haoran Xie","D. Zou","Gwo-jen Hwang"],"3572":["Heike Felzmann","E. Fosch-Villaronga","C. Lutz","Aurelia Tam\u00f3-Larrieux"],"3573":["Paul Henman"],"3574":["A. Rosenkranz","Max Marian","F. Profito","Nathan Aragon","Raj Shah"],"3575":["J. Soun","D. Chow","M. Nagamine","R. Takhtawala","C. Filippi","Wengui Yu","P. Chang"],"3576":["S. Larsson","F. Heintz"],"3577":["Ibrahim Habli","Tom Lawton","Zoe Porter"],"3578":["N. Schwalbe","B. Wahl"],"3579":["Andrea L. Guzman","S. Lewis"],"3580":["A. Kaplan","M. Haenlein"],"3581":["Fei Wu","Cewu Lu","Ming-Li Zhu","Hao Chen","Jun Zhu","Kai Yu","Lei Li","Ming Li","Qian Chen","Xi Li","Xudong Cao","Zhongyuan Wang","Zhengjun Zha","Yueting Zhuang","Yunhe Pan"],"3582":["M. Komorowski","L. Celi","Omar Badawi","A. Gordon","Aldo A. Faisal"],"3583":["M. Janssen","P. Brous","Elsa Estevez","L. Barbosa","T. Janowski"],"3584":["M. Mitchell"],"3585":["C. Le Berre","W. Sandborn","Sabeur Aridhi","M. Devignes","L. Fournier","Malika Sma\u00efl-Tabbone","S. Danese","L. Peyrin-Biroulet"],"3586":["O. Buklemishev"],"3587":["Yang Lu"],"3588":["S. Goldenberg","G. Nir","S. Salcudean"],"3589":["D. Belanche","L. Casal\u00f3","C. Flavi\u00e1n"],"3590":["S. Graham","C. Depp","Ellen E. Lee","Camille Nebeker","X. Tu","Ho-Cheol Kim","D. Jeste"],"3591":["S. Bhaskar","Si\u00e2n Bradley","S. Sakhamuri","Sebastian Moguilner","Vijay Kumar Chattu","S. Pandya","Starr Schroeder","Daniel Ray","Maciej Banach"],"3592":["B. Lake","T. Ullman","J. Tenenbaum","S. Gershman"],"3593":["R. Byrne"],"3594":["Vinay Kumar","B. Rajan","R. Venkatesan","Jim Lecinski"],"3595":["Y. Shrestha","Shiko M. Ben-Menahem","G. von Krogh"],"3596":["Helin Yang","A. Alphones","Zehui Xiong","D. Niyato","Jun Zhao","Kaishun Wu"],"3597":["F. Pedr\u00f3","Miguel Subosa","A. Rivas","Paula Valverde"],"3598":["A. Agrawal","J. Gans","Avi Goldfarb"],"3599":["Nisreen Ameen","A. Tarhini","Alexander E. Reppel","A. Anand"],"3600":["Xiao-Yun Zhou","Yao Guo","Mali Shen","Guang-Zhong Yang"],"3601":["D. Ho"],"3602":["D. Gunasekeran","D. Ting","G. Tan","T. Wong"],"3603":["Omer Ben-Porat","Lital Kuchy","Moshe Tennenholtz"],"3604":["Rahul Reddy Nadikattu"],"3605":["D. Ali","S. Frimpong"],"3606":["Yueyue Dai","Du Xu","Sabita Maharjan","Guanhua Qiao","Y. Zhang"],"3607":["Nicolas Sabouret"],"3608":["Guoguang Rong","Arnaldo Mendez","E. B. Assi","Bo Zhao","Mohamad Sawan"],"3609":["B. \u00c1cs","M. Rantalainen","J. Hartman"],"3610":["Margaret A. Goralski","Tay Keong Tan"],"3611":["David Gunning"],"3612":["B. Tran","G. Vu","G. H. Ha","Q. Vuong","Manh-Tung Ho","Thu-Trang Vuong","Viet-Phuong La","Manh-Toan Ho","Kien-Cuong P. Nghiem","Huong Lan Thi Nguyen","C. Latkin","W. Tam","Ngai-Man Cheung","H. Nguyen","Cyrus S. H. Ho","R. Ho"],"3613":["B. Ko\u00e7ak","E. S. Durmaz","Ece Ate\u015f","\u00d6. K\u0131l\u0131\u00e7kesmez"],"3614":["K. Paranjape","M. Schinkel","R. N. Nannan Panday","J. Car","P. Nanayakkara"],"3615":["Pratik Shah","F. Kendall","S. Khozin","Ryan Goosen","Jianying Hu","J. Laramie","Michael Ringel","N. Schork"],"3616":["Wenwu Zhu","Xin Wang","Wen Gao"],"3617":["Baobao Zhang","A. Dafoe"],"3618":["A. Miriyev","M. Kova\u010d"],"3619":["Y. Mintz","R. Brodie"],"3620":["Nathalie A. Smuha"],"3621":["Heike Felzmann","E. F. Villaronga","C. Lutz","Aurelia Tam\u00f3-Larrieux"],"3622":["A. Grzybowski","Piotr Brona","Gilbert Lim","Paisan Ruamviboonsuk","G. Tan","M. Abr\u00e0moff","D. Ting"],"3623":["Shilin Qiu","Qihe Liu","Shijie Zhou","Chunjiang Wu"],"3624":["T. Maddox","J. Rumsfeld","Philip R. O. Payne"],"3625":["Raquel Dias","A. Torkamani"],"3626":["T. Loftus","P. Tighe","A. Filiberto","P. Efron","S. Brakenridge","A. Mohr","Parisa Rashidi","G. Upchurch","A. Bihorac"],"3627":["S. Kulkarni","N. Seneviratne","M. S. Baig","A. H. A. Khan"],"3628":["Alberto Fern\u00e1ndez","F. Herrera","O. Cord\u00f3n","M. J. Jes\u00fas","F. Marcelloni"],"3629":["Mark Coeckelbergh"],"3630":["Jeannette Paschen","Jan H. Kietzmann","Tim C Kietzmann"],"3631":["M. Ridley"],"3632":["Y. Charalabidis"],"3633":["C. Draxl","M. Scheffler"],"3634":["Y. Yang","C. S. Bang"],"3635":["Filip Karlo Dosilovic","Mario Br\u010di\u010d","N. Hlupic"],"3636":["F. Galbusera","Gloria Casaroli","T. Bassani"],"3637":["James Shaw","Frank Rudzicz","T. Jamieson","Avi Goldfarb"],"3638":["Jagdip Singh","Karen Flaherty","R. Sohi","Dawn Deeter-Schmelz","Johannes Habel","Kenneth Le Meunier-Fitzhugh","A. Malshe","Ryan Mullins","V. Onyemah"],"3639":["A. K. Arshadi","Julia Webb","M. Salem","Emmanuel Cruz","Stacie Calad-Thomson","N. Ghadirian","J. Collins","Elena Diez-Cecilia","Brendan Kelly","H. Goodarzi","J. Yuan"],"3640":["K. Janowicz","Song Gao","Grant McKenzie","Yingjie Hu","B. Bhaduri"],"3641":["M. I. Jordan"],"3642":["D. Ting","L. Pasquale","L. Peng","J. P. Campbell","Aaron Y. Lee","R. Raman","G. Tan","L. Schmetterer","P. Keane","T. Wong"],"3643":["J. Howard"],"3644":["Ravi B. Parikh","Stephanie Teeple","A. Navathe"],"3645":["D. Hashimoto","G. Rosman","D. Rus","O. Meireles"],"3646":["S. Park","Kyunghwa Han"],"3647":["F. Lopez\u2010Jimenez","Z. Attia","Adelaide M. Arruda-Olson","R. Carter","P. Chareonthaitawee","H. Jouni","S. Kapa","A. Lerman","C. Luong","J. Medina-Inojosa","P. Noseworthy","P. Pellikka","M. Redfield","V. Roger","G. Sandhu","Conor Senecal","P. Friedman"],"3648":["Xiaofeng Jin","Conghui Liu","Tailin Xu","L. Su","Xueji Zhang"],"3649":["Arnaud De Bruyn","Vijay Viswanathan","Y. Beh","J. Brock","Florian von Wangenheim"],"3650":["N. Bostrom"],"3651":["S. O\u2019Sullivan","Nathalie Nevejans","Colin Allen","Andrew Blyth","S. L\u00e9onard","U. Pagallo","K. Holzinger","Andreas Holzinger","M. Sajid","H. Ashrafian"],"3652":["Pei Wang"],"3653":["M. Hutson"],"3654":["Hamon Ronan","Junklewitz Henrik","S. Ignacio"],"3655":["W. Samek","T. Wiegand","K. M\u00fcller"],"3656":["J. Brag"],"3657":["Said Agrebi","A. Larbi"],"3658":["S. Ullman"],"3659":["D. Acemoglu","P. Restrepo"],"3660":["Wang Tong","Azhar Hussain","Wang Bo","Sabita Maharjan"],"3661":["Mei Chen","Michel D\u00e9cary"],"3662":["Mark O. Riedl"],"3663":["Justin B. Bullock"],"3664":["O. Adir","M. Poley","Gal Chen","Sahar Froim","N. Krinsky","J. Shklover","Janna Shainsky\u2010Roitman","T. Lammers","Avi Schroeder"],"3665":["S. Ellahham"],"3666":["Anupama Hoskoppa Sundaramurthy","Nitya Raviprakash","Divija Devarla","Asmitha Rathis"],"3667":["J. Bryson"],"3668":["Shihab Sarwar","A. Dent","K. Faust","M. Richer","Ugljesa Djuric","Randy van Ommeren","P. Diamandis"],"3669":["Shidan Wang","Donghan M. Yang","Ruichen Rong","Xiaowei Zhan","J. Fujimoto","Hongyu Liu","J. Minna","I. Wistuba","Yang Xie","Guanghua Xiao"],"3670":["Fei Wang","R. Kaushal","D. Khullar"],"3671":["M. H. Jarrahi"],"3672":["A. Filipa de Almeida","R. Moreira","T. Rodrigues"],"3673":["F. Kunz","A. Stellzig-Eisenhauer","F. Zeman","J. Boldt"],"3674":["J. Rudie","A. Rauschecker","R. Bryan","C. Davatzikos","S. Mohan"],"3675":["Ashley S. Deeks"],"3676":["N. C. Eli-Chukwu"],"3677":["Li-Qiang Zhou","Jia-Yu Wang","Song-Yuan Yu","Ge-Ge Wu","Qi Wei","Youbin Deng","Xingxing Wu","X. Cui","C. Dietrich"],"3678":[],"3679":["David Leslie"],"3680":["N. Schork"],"3681":["M. Frank","Dashun Wang","M. Cebri\u00e1n","Iyad Rahwan"],"3682":["Joseph R England","P. Cheng"],"3683":["E. Conant","A. Toledano","S. Periaswamy","S. V. Fotin","Jonathan Go","Justin E. Boatsman","J. Hoffmeister"],"3684":["S. Price"],"3685":["Randi Williams","Hae Won Park","C. Breazeal"],"3686":["Valentina Bellemo","Gilbert Lim","T. Rim","G. Tan","C. Cheung","S. Sadda","M. He","A. Tufail","M. Lee","W. Hsu","D. Ting"],"3687":["S. Harrer","Pratik Shah","B. Antony","Jianying Hu"],"3688":["Francisco-Javier Hinojo-Lucena","Inmaculada Aznar-D\u00edaz","M. C\u00e1ceres-Reche","Jos\u00e9-Mar\u00eda Romero-Rodr\u00edguez"],"3689":["M. Phillips","Helen Marsden","W. Jaffe","R. Matin","G. Wali","J. Greenhalgh","E. McGrath","Rob James","E. Ladoyanni","A. Bewley","G. Argenziano","I. Palamaras"],"3690":["Chunhao Wang","Xiaofeng Zhu","Julian C. Hong","D. Zheng"],"3691":["A. Kuzior","A. Kwili\u0144ski","V. Tkachenko"],"3692":["M. Uddin","Yujiang Wang","M. Woodbury-Smith"],"3693":["K. Masters"],"3694":["W. Price","S. Gerke","I. Cohen"],"3695":["T. Rajaee","H. Ebrahimi","Vahid Nourani"],"3696":["H. C. S. Chan","Hanbin Shan","T. Dahoun","H. Vogel","Shuguang Yuan","Shuguang Yuan"],"3697":["Krystyna Jarek","G. Mazurek"],"3698":["B. C. Smith"],"3699":["M. Perc","M. Ozer","Janja Hojnik"],"3700":["Miriam C. Buiten"],"3701":["Daniel T. Hogarty","John C. Su","K. Phan","Mohamed Attia","M. Hossny","S. Nahavandi","P. Lenane","F. Moloney","A. Yazdabadi"],"3702":["Huimin Lu","Yujie Li","Min Chen","Hyoungseop Kim","S. Serikawa"],"3703":["M. Salto\u2010Tellez","P. Maxwell","P. Hamilton"],"3704":["C. Connor"],"3705":["Xiaoxuan Samantha Cruz Livia Lavinia Christopher Pearse A.  Liu Rivera Faes Ferrante di Ruffano Yau Keane Ashr","Xiaoxuan Liu","Samantha Cruz Rivera","L. Faes","Lavinia Ferrante di Ruffano","C. Yau","P. Keane","H. Ashrafian","A. Darzi","S. Vollmer","J. Deeks","L. Bachmann","Christopher Holmes","A. Chan","D. Moher","M. Calvert","A. Denniston"],"3706":["Matej Moravc\u00edk","Martin Schmid","Neil Burch","V. Lis\u00fd","Dustin Morrill","Nolan Bard","Trevor Davis","K. Waugh","Michael Bradley Johanson","Michael H. Bowling"],"3707":["H. Rahmanifard","Tatyana Plaksina"],"3708":["Jing Pei","Lei Deng","Sen Song","Mingguo Zhao","Youhui Zhang","Shuang Wu","Guanrui Wang","Zhe Zou","Zhenzhi Wu","Wei He","Feng Chen","Ning Deng","Si Wu","Yu Wang","Yujie Wu","Zheyu Yang","Cheng Ma","Guoqi Li","Wentao Han","Huanglong Li","Huaqiang Wu","R. Zhao","Yuan Xie","Luping Shi"],"3709":["Renjie Wang","Wei Pan","L. Jin","Yuehan Li","Yudi Geng","Chun Gao","Gang Chen","Hui Wang","D. Ma","Shujie Liao"],"3710":["C. Lovejoy","V. Buch","M. Maruthappu"],"3711":["Harry Surden"],"3712":["O. Niel","P. Bastard"],"3713":["Dr. A. Pasumpon Pandian"],"3714":["F. Nensa","A. Demircio\u011flu","C. Rischpler"],"3715":["Manav Raj","Robert C. Seamans"],"3716":["Erik Brynjolfsson","Daniel Rock","C. Syverson"],"3717":["Ge-Ge Wu","Li-Qiang Zhou","Jianwei Xu","Jia-Yu Wang","Qi Wei","Youbin Deng","X. Cui","C. Dietrich"],"3718":["Avi Goldfarb","A. Agrawal","J. Gans"],"3719":["M. Taddeo","T. McCutcheon","L. Floridi"],"3720":["Shigao Huang","Jie Yang","S. Fong","Qi Zhao"],"3721":["Stefan A. D. Popenici","Sharon Kerr"],"3722":["Jun-Ho Huh","Yeong-Seok Seo"],"3723":["C. Kuziemsky","A. Maeder","Oommen John","S. Gogia","Arindam Basu","S. Meher","M\u00e1rcia Ito"],"3724":["A. Agrawal","J. Gans","Avi Goldfarb"],"3725":["Miles Brundage","S. Avin","Jack Clark","H. Toner","P. Eckersley","Ben Garfinkel","A. Dafoe","P. Scharre","Thomas Zeitzoff","Bobby Filar","H. Anderson","H. Roff","Gregory C. Allen","J. Steinhardt","Carrick Flynn","Se\u00e1n \u00d3 h\u00c9igeartaigh","S. Beard","Haydn Belfield","Sebastian Farquhar","Clare Lyle","Rebecca Crootof","Owain Evans","Michael Page","Joanna J. Bryson","Roman V. Yampolskiy","Dario Amodei"],"3726":["M. Gams","I. Gu","Aki H\u00e4rm\u00e4","A. Mu\u00f1oz","V. W. L. Tam"],"3727":["Mark E Whiting","J. Cagan","Philip LeDuc"],"3728":["J. Powell"],"3729":["N. Noorbakhsh-Sabet","R. Zand","Yanfei Zhang","V. Abedi"],"3730":["V. Dunjko","H. Briegel"],"3731":["Xiao Guo","Zhenjiang Shen","Yajing Zhang","Teng Wu"],"3732":["D. I. Patr\u00edcio","Rafael Rieder"],"3733":["Chengjie Zheng","T. Johnson","Aakriti Garg","Michael V. Boland"],"3734":["Qingjun Wang","Peng Lu"],"3735":["U. Schmidt-Erfurth","A. Sadeghipour","B. Gerendas","S. Waldstein","H. Bogunovi\u0107"],"3736":["H. Salehi","R. Burgue\u00f1o"],"3737":["Jay Lee","Hossein Davari","Jaskaran Singh","V. Pandhare"],"3738":["Bo-Hu Li","Baocun Hou","Wentao Yu","Xiaobing Lu","Chun-Wei Yang"],"3739":["Kipp W. Johnson","Jessica Torres Soto","Benjamin S. Glicksberg","K. Shameer","Riccardo Miotto","Mohsin Ali","E. Ashley","J. Dudley"],"3740":["E. Le","Y. Wang","Yuanlu Huang","S. Hickman","F. Gilbert"],"3741":["Z. S. Wong","Jiaqi Zhou","Qingpeng Zhang"],"3742":["Cristian Gonz\u00e1lez Garc\u00eda","Edward Rolando N\u00fa\u00f1ez-Vald\u00e9z","Vicente Garc\u00eda D\u00edaz","B. C. P. Garc\u00eda-Bustelo","J. M. C. Lovelle"],"3743":["M. Negnevitsky"],"3744":["Marian Mazzone","A. Elgammal"],"3745":["Fabian H Sinz","Xaq Pitkow","Jacob Reimer","M. Bethge","A. Tolias"],"3746":["E. Shortliffe","M. Sep\u00falveda"],"3747":["Rongpeng Li","Zhifeng Zhao","Xuan Zhou","Guoru Ding","Yan Chen","Zhongyao Wang","Honggang Zhang"],"3748":["Emilio Calvano","G. Calzolari","V. Denicol\u00f3","S. Pastorello"],"3749":["A. Basile","A. Yahi","N. Tatonetti"],"3750":["D. Santos","D. Giese","S. Brodehl","S. Chon","W. Staab","R. Kleinert","D. Maintz","B. Baessler"],"3751":["Peter Hahn"],"3752":["A. Minchol\u00e9","B. Rodr\u00edguez"],"3753":["I. Cockburn","R. Henderson","Scott Stern"],"3754":["Pablo Kogan","Tamir Tassa","Tal Grinshpoun"],"3755":["H. Tizhoosh","L. Pantanowitz"],"3756":["C. Langlotz"],"3757":["Spyros Makridakis"],"3758":["Chunli Qin","Demin Yao","Yonghong Shi","Zhijian Song"],"3759":["Georgios N. Yannakakis","J. Togelius"],"3760":["Iv\u00e1n Contreras","J. Veh\u00ed"],"3761":["Meredith Broussard","N. Diakopoulos","Andrea L. Guzman","Rediet Abebe","Michel Dupagne","C. Chuan"],"3762":["Corinne Cath"],"3763":["T. Panch","Peter Szolovits","R. Atun"],"3764":["A. Winfield","M. Jirotka"],"3765":["Virginia Dignum"],"3766":["Odd Erik Gundersen","Sigbj\u00f8rn Kjensmo"],"3767":["C. Krittanawong","Hongju Zhang","Zhen Wang","Mehmet Aydar","T. Kitai"],"3768":[],"3769":["D. Merk","Lukas Friedrich","F. Grisoni","G. Schneider"],"3770":["A. Fogel","J. Kvedar"],"3771":["D. Miller","E. Brown"],"3772":["Kan Yao","Rohit Unni","Yuebing Zheng"],"3773":["Han Yu","Zhiqi Shen","C. Miao","Cyril Leung","V. Lesser","Qiang Yang"],"3774":["V. Buch","Irfan Ahmed","M. Maruthappu"],"3775":["S. Reddy","John Fox","M. Purohit"],"3776":["J. A. Nichols","Hsien W. Herbert Chan","M. Baker"],"3777":["Songhee Oh","J. Kim","Sung-Woo Choi","Hee Jeong Lee","Jungrak Hong","S. Kwon"],"3778":["Susan Leavy"],"3779":["A. Annoni","P. Bencz\u00far","P. Bertoldi","Blagoj Delipetrev","G. Prato","C. Feij\u00f3o","Enrique Fern\u00e1ndez-Mac\u00edas","E. Guti\u00e9rrez","M. Portela","H. Junklewitz","M. L. Cobo","B. Martens","Susana Nascimento","S. Nativi","Alexandre P\u00f3lvora","Jose Ignacio Sanchez Martin","Songuel Tolan","I. Tuomi","Lucia Vesnic\u0301 Alujevic\u0301"],"3780":["A. Bundy"],"3781":["T. Davenport"],"3782":["J. Senders","O. Arnaout","A. Karhade","H. Dasenbrock","W. Gormley","M. Broekman","T. Smith"],"3783":["D. Izzo","Marcus M\u00e4rtens","Binfeng Pan"],"3784":["H. Chang","C. Jung","Junwoo Woo","Sanghun Lee","Joonyoung Cho","Sun Woo Kim","Tae-Yeong Kwak"],"3785":["W. Stead"],"3786":["Y. Mori","S. Kudo","M. Misawa","Yutaka Saito","H. Ikematsu","K. Hotta","K. Ohtsuka","F. Urushibara","S. Kataoka","Y. Ogawa","Y. Maeda","K. Takeda","H. Nakamura","K. Ichimasa","T. Kudo","Takemasa Hayashi","K. Wakamura","F. Ishida","H. Inoue","H. Itoh","M. Oda","K. Mori"],"3787":["P. Aghion","Benjamin F. Jones","C. I. Jones"],"3788":["Jian-hua Li"],"3789":["Jakub Olczak","Niklas Fahlberg","A. Maki","A. Razavian","Anthony Jilert","A. Stark","O. Sk\u00f6ldenberg","M. Gordon"],"3790":["K. Benke","G. Benke"],"3791":["Ashish Ghosh","Debasrita Chakraborty","Anwesha Law"],"3792":["K. Siau","Weiyu Wang"],"3793":["Prashansa Agrawal"],"3794":["G. Hessler","K. Baringhaus"],"3795":["Lars Kunze","Nick Hawes","T. Duckett","Marc Hanheide","T. Krajn\u00edk"],"3796":["A. Agrawal","J. Gans","Avi Goldfarb"],"3797":["J. Horvat"],"3798":["A. Upadhyay","Komal Khandelwal"],"3799":["K. Chapi","V. Singh","A. Shirzadi","H. Shahabi","D. Bui","B. Pham","K. Khosravi"],"3800":["H. Varian"],"3801":["Kun-Hsing Yu","A. L. Beam","I. Kohane"],"3802":["J. Stewart","P. Sprivulis","G. Dwivedi"],"3803":["YoungJu Jo","Hyungjoon Cho","Sang Yun Lee","Gunho Choi","Geon Kim","Hyun-Seok Min","Yongkeun Park"],"3804":["M. Alsharqi","W. Woodward","J. Mumith","D. C. Markham","R. Upton","P. Leeson"],"3805":["B. Mesk\u00f3","Gergely Het\u00e9nyi","Z. Gy\u0151rffy"],"3806":["Nicole Fleming"],"3807":["Jahanzaib Shabbir","T. Anwer"],"3808":["Arash Shaban-Nejad","Martin Michalowski","D. Buckeridge"],"3809":["J. Grudin","R. Jacques"],"3810":["Julia Kokina","T. Davenport"],"3811":["Kun\u2010Hsing Yu","I. Kohane"],"3812":["Anton Korinek","J. Stiglitz"],"3813":["Maud Chassignol","A. Khoroshavin","A. Klimova","Anna Bilyatdinova"],"3814":["M. Alagappan","J. G. Brown","Y. Mori","T. Berzin"],"3815":["Norbert Wirth"],"3816":["Zuiderveen Borgesius"],"3817":["M. Boden"],"3818":["Feisheng Zhong","Jing Xing","Xutong Li","Xiaohong Liu","Zunyun Fu","Zhaoping Xiong","D. Lu","Xiaolong Wu","Jihui Zhao","Xiaoqin Tan","Fei Li","Xiaomin Luo","Zhaojun Li","Kaixian Chen","M. Zheng","Hualiang Jiang"],"3819":["Li Li","Yi-Lun Lin","N. Zheng","Feiyue Wang","Yuehu Liu","Dongpu Cao","Kunfeng Wang","Wuling Huang"],"3820":["A. Verghese","N. Shah","R. Harrington"],"3821":["Erping Long","Haotian Lin","Zhenzhen Liu","Xiaohang Wu","Liming Wang","Jiewei Jiang","Yingying An","Zhuoling Lin","Xiao-yan Li","Jingjing Chen","J. Li","Q. Cao","Dongni Wang","Xiyang Liu","Weirong Chen","Yizhi Liu"],"3822":["M. Hutson"],"3823":["Matej Moravc\u00edk","Martin Schmid","Neil Burch","V. Lis\u00fd","Dustin Morrill","Nolan Bard","Trevor Davis","K. Waugh","Michael Bradley Johanson","Michael Bowling"],"3824":["Ryan Calo"],"3825":["Xueli Du","Wen-Bo Li","Bo-jie Hu"],"3826":["Yi Zeng","Enmeng Lu","Cunqing Huangfu"],"3827":["E. Popkova","V. Parakhina"],"3828":["Eug\u00e9nio C. Oliveira","Jo\u00e3o Gama","Zita Vale","Henrique Lopes Cardoso"],"3829":["Yaping Zang","Fengjiao Zhang","Chong\u2010an Di","Daoben Zhu"],"3830":["D. Bonderman"],"3831":["Bin Yu","Karl Kumbier"],"3832":["Max Tegmark"],"3833":["Matt Taddy"],"3834":["Mehrnaz Fahimirad","Sedigheh Shakib Kotamjani"],"3835":["Xianyu Zhang","X. Ming","Zhiwen Liu","Dao Yin","Zhihua Chen","Yuan Chang"],"3836":["Jiaxin Luo","Q. Meng","Yan-Song Cai"],"3837":["Charlene Liew"],"3838":["Lene Pettersen"],"3839":["Yongliang Shen","Kaitao Song","Xu Tan","D. Li","Weiming Lu","Y. Zhuang"],"3840":["Scott M. Lundberg","G. Erion","Hugh Chen","A. DeGrave","J. Prutkin","B. Nair","R. Katz","J. Himmelfarb","N. Bansal","Su-In Lee"],"3841":["David Baidoo-Anu","Leticia Owusu Ansah"],"3842":["Yihan Cao","Siyu Li","Yixin Liu","Zhiling Yan","Yutong Dai","Philip S. Yu","Lichao Sun"],"3843":["Junaid Qadir"],"3844":["Yuntao Bai","Saurav Kadavath","Sandipan Kundu","Amanda Askell","John Kernion","Andy Jones","A. Chen","Anna Goldie","Azalia Mirhoseini","C. McKinnon","Carol Chen","Catherine Olsson","C. Olah","Danny Hernandez","Dawn Drain","Deep Ganguli","Dustin Li","Eli Tran-Johnson","E. Perez","Jamie Kerr","J. Mueller","Jeff Ladish","J. Landau","Kamal Ndousse","Kamil\u0117 Luko\u0161i\u016bt\u0117","Liane Lovitt","Michael Sellitto","Nelson Elhage","Nicholas Schiefer","Noem'i Mercado","Nova DasSarma","R. Lasenby","Robin Larson","Sam Ringer","Scott Johnston","S. Kravec","S. E. Showk","Stanislav Fort","Tamera Lanham","Timothy Telleen-Lawton","Tom Conerly","T. Henighan","Tristan Hume","Sam Bowman","Zac Hatfield-Dodds","Benjamin Mann","Dario Amodei","Nicholas Joseph","Sam McCandlish","Tom B. Brown","Jared Kaplan"],"3845":["S. McKinney","M. Sieniek","Varun Godbole","Jonathan Godwin","Natasha Antropova","H. Ashrafian","T. Back","Mary Chesus","Greg C. Corrado","A. Darzi","M. Etemadi","Florencia Garcia-Vicente","F. Gilbert","M. Halling-Brown","D. Hassabis","Sunny Jansen","A. Karthikesalingam","Christopher J. Kelly","Dominic King","J. Ledsam","David S. Melnick","Hormuz Mostofi","L. Peng","J. Reicher","Bernardino Romera-Paredes","R. Sidebottom","Mustafa Suleyman","Daniel Tse","K. Young","J. Fauw","S. Shetty"],"3846":["Nash Anderson","D. Belavy","S. Perle","S. Hendricks","L. Hespanhol","E. Verhagen","A. Memon"],"3847":["AI Nist","Secretary Gina M. Raimondo","L. Locascio"],"3848":["M. Cannarsa"],"3849":["Qinglin Yang","Yetong Zhao","Huawei Huang","Zehui Xiong","Jiawen Kang","Zibin Zheng"],"3850":["Iqbal H. Sarker"],"3851":["E. Cambria","Qian Liu","S. Decherchi","Frank Xing","Kenneth Kwok"],"3852":["J. Acosta","G. Falcone","P. Rajpurkar","E. Topol"],"3853":["M. Chowdhury","Tawsifur Rahman","A. Khandakar","R. Mazhar","M. A. Kadir","Z. Mahbub","Khandakar R. Islam","Muhammad Salman Khan","A. Iqbal","N. Al-Emadi","M. Reaz"],"3854":["M. Savva","Abhishek Kadian","Oleksandr Maksymets","Yili Zhao","Erik Wijmans","Bhavana Jain","Julian Straub","Jia Liu","V. Koltun","Jitendra Malik","Devi Parikh","Dhruv Batra"],"3855":["Nithya Sambasivan","Shivani Kapania","H. Highfill","Diana Akrong","Praveen K. Paritosh","Lora Aroyo"],"3856":["Kate Crawford"],"3857":["Weixin Liang","G. Tadesse","Daniel Ho","Li Fei-Fei","M. Zaharia","Ce Zhang","James Zou"],"3858":["D. Long","Brian Magerko"],"3859":["Dario Amodei","C. Olah","J. Steinhardt","P. Christiano","John Schulman","Dandelion Man\u00e9"],"3860":["Yunfeng Zhang","Q. Liao","R. Bellamy"],"3861":["Gagan Bansal","Tongshuang Sherry Wu","Joyce Zhou","Raymond Fok","Besmira Nushi","Ece Kamar","Marco Tulio Ribeiro","Daniel S. Weld"],"3862":["Pandiaraj Manickam","Siva Ananth Mariappan","S. Murugesan","S. Hansda","A. Kaushik","Ravikumar Shinde","S. P. Thipperudraswamy"],"3863":["B. Vasey","M. Nagendran","Bruce Campbell","D. Clifton","Gary S. Collins","Spiros C. Denaxas","A. Denniston","L. Faes","B. Geerts","Mudathir Ibrahim","Xiaoxuan Liu","B. Mateen","P. Mathur","M. Mccradden","L. Morgan","Johan Ordish","Campbell Rogers","S. Saria","D. Ting","P. Watkinson","W. Weber","P. Wheatstone","P. McCulloch"],"3864":["K. Letaief","Wei Chen","Yuanming Shi","Jun Zhang","Y. Zhang"],"3865":["Pantelis Linardatos","Vasilis Papastefanopoulos","S. Kotsiantis"],"3866":["Md Meraj Ansari","Pawan Kumar Sharma","Bibhu Dash"],"3867":["Michael R. King"],"3868":["Philipp Moritz","Robert Nishihara","Stephanie Wang","Alexey Tumanov","Richard Liaw","Eric Liang","William Paul","Michael I. Jordan","I. Stoica"],"3869":["S. Amershi","Daniel S. Weld","Mihaela Vorvoreanu","Adam Fourney","Besmira Nushi","Penny Collisson","Jina Suh","Shamsi T. Iqbal","Paul N. Bennett","K. Quinn","J. Teevan","Ruth Kikin-Gil","E. Horvitz"],"3870":["B. Vasey","M. Nagendran","Bruce Campbell","D. Clifton","Gary S. Collins","Spiros C. Denaxas","A. Denniston","L. Faes","B. Geerts","Mudathir Ibrahim","Xiaoxuan Liu","B. Mateen","P. Mathur","M. Mccradden","L. Morgan","Johan Ordish","Campbell Rogers","S. Saria","D. Ting","P. Watkinson","W. Weber","P. Wheatstone","P. McCulloch"],"3871":["Kailong Liu","Zhongbao Wei","Chenghui Zhang","Yunlong Shang","R. Teodorescu","Qing\u2010Long Han"],"3872":["G. Collins","P. Dhiman","Constanza L. Andaur Navarro","Jie Ma","L. Hooft","J. Reitsma","Patricia Logullo","Andrew Beam","L. Peng","B. Van calster","M. van Smeden","R. Riley","K. Moons"],"3873":["Thilo Hagendorff"],"3874":["Ophir Gozes","Maayan Frid-Adar","H. Greenspan","P. Browning","Huangqi Zhang","W.-B. Ji","Adam Bernheim","E. Siegel"],"3875":["S. Udrescu","Max Tegmark"],"3876":["Patrick Mikalef","K. Conboy","J. Lundstr\u00f6m","Ale\u0161 Popovi\u010d"],"3877":["A. Monreale"],"3878":["Eric Kolve","Roozbeh Mottaghi","Winson Han","Eli VanderBilt","Luca Weihs","Alvaro Herrasti","Matt Deitke","Kiana Ehsani","Daniel Gordon","Yuke Zhu","Aniruddha Kembhavi","A. Gupta","Ali Farhadi"],"3879":["B. Mittelstadt"],"3880":["P. McEnroe","Shen Wang","Madhusanka Liyanage"],"3881":["A. Davies","Petar Velickovic","Lars Buesing","Sam Blackwell","Daniel Zheng","Nenad Toma\u0161ev","Richard Tanburn","P. Battaglia","C. Blundell","Andr\u00e1s Juh\u00e1sz","M. Lackenby","G. Williamson","D. Hassabis","Pushmeet Kohli"],"3882":["Hyesun Choung","Prabu David","Arun Ross"],"3883":["L. Floridi","Josh Cowls","Monica Beltrametti","R. Chatila","Patrice Chazerand","Virginia Dignum","C. Luetge","Robert Madelin","U. Pagallo","F. Rossi","Burkhard Schafer","P. Valcke","E. Vayena"],"3884":["Luke Munn"],"3885":["Jessica Fjeld","Nele Achten","Hannah Hilligoss","\u00c1d\u00e1m Nagy","Madhulika Srikumar"],"3886":["Alexandre Blanco-Gonzalez","Alfonso Cabez\u00f3n","Alejandro Seco-Gonzalez","Daniel Conde-Torres","Paula Antelo-Riveiro","\u00c1ngel Pi\u00f1eiro","R. Garc\u00eda\u2010Fandi\u00f1o"],"3887":["E. Wu","K. Wu","R. Daneshjou","David Ouyang","Daniel E. Ho","James Zou"],"3888":["Roy Schwartz","Jesse Dodge","Noah A. Smith","Oren Etzioni"],"3889":["A. DeGrave","Joseph D. Janizek","Su-In Lee"],"3890":["Henry A. Kautz"],"3891":["Noam Brown","T. Sandholm"],"3892":["W. Samek","G. Montavon","A. Vedaldi","L. K. Hansen","Klaus M\u00fcller"],"3893":["Martin Adam","Michael Wessel","Alexander Benlian"],"3894":["Michael A. Madaio","Luke Stark","Jennifer Wortman Vaughan","Hanna M. Wallach"],"3895":["Ming-Hui Huang","R. Rust"],"3896":["K. Santosh"],"3897":["P. Smolensky","R. Thomas McCoy","Roland Fernandez","Matthew A. Goldrick","Jia-Hao Gao"],"3898":["Dan Hendrycks","Collin Burns","Steven Basart","Andrew Critch","J. Li","D. Song","J. Steinhardt"],"3899":["J. Weston","Antoine Bordes","S. Chopra","Tomas Mikolov"],"3900":["Qian Yang","Aaron Steinfeld","C. Ros\u00e9","J. Zimmerman"],"3901":["K. Salah","M. H. Rehman","Nishara Nizamuddin","Ala I. Al-Fuqaha"],"3902":["Arun Rai"],"3903":["L. Floridi","Josh Cowls"],"3904":["Bo Wang","Shuo Jin","Qingsen Yan","Haibo Xu","Chuan Luo","Lai Wei","Wei Zhao","Xuexue Hou","Wenshuo Ma","Zhengqing Xu","Zhuozhao Zheng","Wenbo Sun","Lan Lan","Wei Zhang","Xiangdong Mu","C. Shi","Zhongxiao Wang","Jihae Lee","Jiahong Dong"],"3905":["Upol Ehsan","Q. Liao","Michael J. Muller","Mark O. Riedl","Justin D. Weisz"],"3906":["Paul Fyfe"],"3907":["J. Morley","L. Floridi","Libby Kinsey","Anat Elhalal"],"3908":["M. Abr\u00e0moff","P. Lavin","M. Birch","Nilay Shah","J. Folk"],"3909":["Matti M\u00e4ntym\u00e4ki","Matti Minkkinen","Teemu Birkstedt","M. Viljanen"],"3910":["Marwin H. S. Segler","M. Preuss","M. Waller"],"3911":["A. van Wynsberghe"],"3912":["Inioluwa Deborah Raji","Emily M. Bender","Amandalynne Paullada","Emily L. Denton","A. Hanna"],"3913":["Noam Brown","T. Sandholm"],"3914":["S. Gaube","Harini Suresh","M. Raue","Alexander Merritt","S. Berkowitz","E. Lermer","J. Coughlin","J. Guttag","E. Colak","M. Ghassemi"],"3915":["B. Mittelstadt","Chris Russell","Sandra Wachter"],"3916":["R. Bellamy","A. Mojsilovic","Seema Nagar","K. Ramamurthy","John T. Richards","Diptikalyan Saha","P. Sattigeri","Moninder Singh","Kush R. Varshney","Yunfeng Zhang","K. Dey","M. Hind","Samuel C. Hoffman","Stephanie Houde","Kalapriya Kannan","P. Lohia","Jacquelyn Martino","Shalin Mehta"],"3917":["E. Choi","M. T. Bahadori","A. Schuetz","W. Stewart","Jimeng Sun"],"3918":["Inioluwa Deborah Raji","Joy Buolamwini"],"3919":["Carole-Jean Wu","R. Raghavendra","Udit Gupta","Bilge Acun","Newsha Ardalani","Kiwan Maeng","Gloria Chang","Fiona Aga Behram","James Huang","Charles Bai","M. Gschwind","Anurag Gupta","Myle Ott","Anastasia Melnikov","Salvatore Candido","David Brooks","Geeta Chauhan","Benjamin Lee","Hsien-Hsin S. Lee","Bugra Akyildiz","Maximilian Balandat","Joe Spisak","R. Jain","M. Rabbat","K. Hazelwood"],"3920":["Xiaofei Wang","Yiwen Han","Chenyang Wang","Qiyang Zhao","Xu Chen","Min Chen"],"3921":["Iqbal H. Sarker","Md. Hasan Furhad","Raza Nowrozy"],"3922":["Isabella Seeber","E. Bittner","R. Briggs","Triparna de Vreede","G. Vreede","A. Elkins","R. Maier","A. B. Merz","Sarah Oeste-Reiss","Nils L. Randrup","G. Schwabe","Matthias S\u00f6llner"],"3923":["Michal Kempka","Marek Wydmuch","Grzegorz Runc","Jakub Toczek","Wojciech Ja\u015bkowski"],"3924":["Hollen Barmer","R. Dzombak","M. Gaston","Vijaykumar Palat","F. Redner","Carol J. Smith","Tanisha Smith"],"3925":["Q. Lu","Liming Zhu","Xiwei Xu","J. Whittle","Zhenchang Xing"],"3926":["Sina Mohseni","Niloofar Zarei","E. Ragan"],"3927":["Connor W. Coley","D. Thomas","Justin A M Lummiss","Jonathan N. Jaworski","Christopher P Breen","Victor Schultz","Travis Hart","Joshua S. Fishman","Luke Rogers","Hanyu Gao","Robert W. Hicklin","Pieter P. Plehiers","Joshua Byington","J. Piotti","W. Green","A. J. Hart","T. Jamison","K. Jensen"],"3928":["M. M. Rathore","Syed Attique Shah","Dhirendra Shukla","Elmahdi Bentafat","S. Bakiras"],"3929":["Shinjini Kundu"],"3930":["Matti Minkkinen","Anniina Niukkanen","Matti M\u00e4ntym\u00e4ki"],"3931":["V. Venkatesh"],"3932":["K. Grace","J. Salvatier","A. Dafoe","Baobao Zhang","Owain Evans"],"3933":["Andreas Holzinger","Chris Biemann","C. Pattichis","D. Kell"],"3934":["Jan J\u00f6hnk","Malte Wei\u00dfert","K. Wyrtki"],"3935":["eISSN 2178-938X","B. Reviews","Paul R. Daugherty","Rodrigo Brand\u00e3o"],"3936":["Bo Li","Peng Qi","Bo Liu","Shuai Di","Jingen Liu","Jiquan Pei","Jinfeng Yi","Bowen Zhou"],"3937":["Silverio Mart'inez-Fern'andez","J. Bogner","Xavier Franch","M. Oriol","Julien Siebert","Adam Trendowicz","Anna Maria Vollmer","S. Wagner"],"3938":["Vivian Lai","Chacha Chen","Q. Liao","Alison Smith-Renner","Chenhao Tan"],"3939":["Daniel T. Zhang","Saurabh Mishra","Erik Brynjolfsson","J. Etchemendy","Deep Ganguli","Barbara Grosz","Terah Lyons","J. Manyika","Juan Carlos Niebles","Michael Sellitto","Y. Shoham","Jack Clark","Ray Perrault"],"3940":["Jiafei Duan","Samson Yu","Tangyao Li","Huaiyu Zhu","Cheston Tan"],"3941":["Yuji Roh","Geon Heo","Steven Euijong Whang"],"3942":["Xuesong Zhai","Xiaoyan Chu","C. Chai","M. Jong","A. Isteni\u010d","Michael Spector","Jia-bao Liu","Jing Yuan","Yan Li"],"3943":["E. Cetinic","James She"],"3944":["Haochen Liu","Yiqi Wang","Wenqi Fan","Xiaorui Liu","Yaxin Li","Shaili Jain","Anil K. Jain","Jiliang Tang"],"3945":["Alon Jacovi","Ana Marasovi\u0107","Tim Miller","Yoav Goldberg"],"3946":["T. Nadarzynski","Oliver Miles","Aimee Cowie","D. Ridge"],"3947":["Gagan Bansal","Besmira Nushi","Ece Kamar","E. Horvitz","Daniel S. Weld"],"3948":["James Zou","L. Schiebinger"],"3949":["Miles Brundage","S. Avin","Jasmine Wang","Haydn Belfield","Gretchen Krueger","Gillian K. Hadfield","Heidy Khlaaf","Jingying Yang","H. Toner","Ruth Fong","Tegan Maharaj","Pang Wei Koh","Sara Hooker","Jade Leung","Andrew Trask","Emma Bluemke","Jonathan Lebensbold","Cullen O'Keefe","Mark Koren","T. Ryffel","JB Rubinovitz","T. Besiroglu","F. Carugati","Jack Clark","P. Eckersley","Sarah de Haas","Maritza L. Johnson","B. Laurie","A. Ingerman","Igor Krawczuk","Amanda Askell","Rosario Cammarota","A. Lohn","David Krueger","C. Stix","Peter Henderson","L. Graham","Carina E. A. Prunkl","Bianca Martin","Elizabeth Seger","Noa Zilberman","Se'an 'O h'Eigeartaigh","F. Kroeger","Girish Sastry","R. Kagan","Adrian Weller","Brian Tse","Elizabeth Barnes","A. Dafoe","P. Scharre","Ariel Herbert-Voss","Martijn Rasser","Shagun Sodhani","Carrick Flynn","T. Gilbert","Lisa Dyer","Saif Khan","Yoshua Bengio","Markus Anderljung"],"3950":["Yuanming Shi","Kai Yang","Tao Jiang","Jun Zhang","K. Letaief"],"3951":["Ziqi Huang","Yang Shen","Jiayi Li","M. Fey","C. Brecher"],"3952":["J. Bareis","Christian Katzenbach"],"3953":["Steven Umbrello","I. van de Poel"],"3954":[],"3955":["Kate Crawford","T. Paglen"],"3956":["Wen Wu","Conghao Zhou","Mushu Li","Huaqing Wu","Haibo Zhou","Ning Zhang","X. Shen","W. Zhuang"],"3957":["Viet Quoc Pham","Dinh C. Nguyen","Thien Huynh-The","W. Hwang","P. Pathirana"],"3958":["M. Huisman","E. Ranschaert","W. Parker","D. Mastrodicasa","M. Ko\u010d\u00ed","Daniel Pinto de Santos","F. Coppola","S. Morozov","M. Zins","C. Bohyn","U. Ko\u00e7","Jie Wu","Satyam Veean","D. Fleischmann","T. Leiner","M. Willemink"],"3959":["D. Castelvecchi"],"3960":["N. Rana","Sheshadri Chatterjee","Yogesh K. Dwivedi","Shahriar Akter"],"3961":["Matt Deitke","Winson Han","Alvaro Herrasti","Aniruddha Kembhavi","Eric Kolve","Roozbeh Mottaghi","Jordi Salvador","Dustin Schwenk","Eli VanderBilt","Matthew Wallingford","Luca Weihs","Mark Yatskar","Ali Farhadi"],"3962":["Josh Cowls","Andreas Tsamados","M. Taddeo","L. Floridi"],"3963":["S. Sundar"],"3964":["Rajasshrie Pillai","Brijesh Sivathanu"],"3965":["Carrie J. Cai","Samantha Winter","David F. Steiner","Lauren Wilcox","Michael Terry"],"3966":["J. Morley","Anat Elhalal","Francesca Garcia","Libby Kinsey","Jakob M\u00f6kander","L. Floridi"],"3967":["C. Flavi\u00e1n","A. P\u00e9rez-Rueda","D. Belanche","L. Casal\u00f3"],"3968":["David Piorkowski","Soya Park","A. Wang","Dakuo Wang","Michael J. Muller","Felix Portnoy"],"3969":["Lu Cheng","Kush R. Varshney","Huan Liu"],"3970":["Michael A. Madaio","Lisa Egede","Hariharan Subramonyam","Jennifer Wortman Vaughan","Hanna M. Wallach"],"3971":["Kevin B. Johnson","Wei-Qi Wei","D. Weeraratne","M. Frisse","K. Misulis","K. Rhee","Juan Zhao","J. Snowdon"],"3972":["Tarleton Gillespie"],"3973":["Serge-Lopez Wamba-Taguimdje","S. Wamba","Jean Robert Kala Kamdjoug","C. Wanko"],"3974":["A. Garcez","L. Lamb"],"3975":["Nanning Zheng","S. Du","Jianji Wang","He Zhang","Wenting Cui","Zijian Kang","Tao Yang","Bin Lou","Yuting Chi","Hong Long","Mei Ma","Q. Yuan","Shupei Zhang","Dong Zhang","F. Ye","J. Xin"],"3976":["Paul R. Daugherty"],"3977":["Irene A. Lee","Safinah Ali","Helen Zhang","Daniella DiPaola","C. Breazeal"],"3978":["Owain Evans","Owen Cotton-Barratt","Lukas Finnveden","Adam Bales","Avital Balwit","Peter Wills","Luca Righetti","W. Saunders"],"3979":["Tan Yigitcanlar","K. Desouza","Luke Butler","Farnoosh Roozkhosh"],"3980":["D. Touretzky","Christina Gardner-Mccune","F. Martin","Deborah W. Seehorn"],"3981":["Dakuo Wang","Liuping Wang","Zhan Zhang","Ding Wang","Haiyi Zhu","Yvonne Gao","Xiangmin Fan","Feng Tian"],"3982":["R. Zicari","J. Brusseau","S. Blomberg","H. Christensen","M. Coffee","M. B. Ganapini","S. Gerke","T. Gilbert","Eleanore Hickman","E. Hildt","Sune Holm","U. K\u00fchne","V. Madai","W. Osika","Andy Spezzatti","Eberhard Schnebel","Jesmin Jahan Tithi","Dennis Vetter","Magnus Westerlund","Rene\u00e9 C. Wurth","J. Amann","Vegard Antun","Valentina Beretta","Fr\u00e9d\u00e9rick Bruneault","Erik Campano","Boris D\u00fcdder","Alessio Gallucci","E. Goffi","C. Haase","Thilo Hagendorff","P. Kringen","Florian M\u00f6slein","D. Ottenheimer","M. Ozols","L. Palazzani","M. Petrin","Karin Tafur","J. T\u00f8rresen","H. Volland","G. Kararigas"],"3983":["Jeffrey T. Hancock","Mor Naaman","K. Levy"],"3984":["Dhruv Batra","Angel X. Chang","S. Chernova","A. Davison","Jia Deng","V. Koltun","S. Levine","J. Malik","Igor Mordatch","Roozbeh Mottaghi","M. Savva","Hao Su"],"3985":["V. Arya","R. Bellamy","Pin-Yu Chen","Amit Dhurandhar","M. Hind","Samuel C. Hoffman","Stephanie Houde","Q. Liao","Ronny Luss","A. Mojsilovic","Sami Mourad","Pablo Pedemonte","R. Raghavendra","John T. Richards","P. Sattigeri","Karthikeyan Shanmugam","Moninder Singh","Kush R. Varshney","Dennis Wei","Yunfeng Zhang"],"3986":["I. Nourbakhsh"],"3987":["Chafika Benzaid","T. Taleb"],"3988":["S. Reddy","S. Allan","S. Coghlan","P. Cooper"],"3989":["E. Popkova","B. Sergi"],"3990":["Gagan Bansal","Besmira Nushi","Ece Kamar","Walter S. Lasecki","Daniel S. Weld","E. Horvitz"],"3991":["Nolan Bard","Jakob N. Foerster","A. Chandar","Neil Burch","Marc Lanctot","H. F. Song","Emilio Parisotto","Vincent Dumoulin","Subhodeep Moitra","Edward Hughes","Iain Dunning","Shibl Mourad","H. Larochelle","Marc G. Bellemare","Michael H. Bowling"],"3992":["E. Payne","A. J. Dahl","J. Peltier"],"3993":["Haiyan Kong","Yue Yuan","Y. Baruch","N. Bu","Xinyu Jiang","Kang-Ting Wang"],"3994":["Feiyu Xu","H. Uszkoreit","Yangzhou Du","Wei Fan","Dongyan Zhao","Jun Zhu"],"3995":["T. Hernandez-Boussard","S. Bozkurt","J. Ioannidis","N. Shah"],"3996":["Upol Ehsan","Samir Passi","Q. Liao","Larry Chan","I-Hsiang Lee","Michael J. Muller","Mark O. Riedl"],"3997":["Shane T. Mueller","R. Hoffman","W. Clancey","Abigail Emrey","Gary Klein"],"3998":["Dakuo Wang","Justin D. Weisz","Michael J. Muller","P. Ram","Werner Geyer","Casey Dugan","Y. Tausczik","H. Samulowitz","Alexander G. Gray"],"3999":["Jacqui Ayling","Adriane P. Chapman"],"4000":["Wei Xu","M. Dainoff","Liezhong Ge","Zaifeng Gao"],"4001":["Cristina Trocin","Patrick Mikalef","Z. Papamitsiou","K. Conboy"],"4002":["Ming-Hui Huang","R. Rust","Vojislav Maksimovic"],"4003":["D. Acemoglu","P. Restrepo"],"4004":["Dinh C. Nguyen","Ming Ding","P. Pathirana","A. Seneviratne"],"4005":["L. Floridi","Josh Cowls","T. C. King","M. Taddeo"],"4006":["Pouyan Esmaeilzadeh"],"4007":["Daron Acemoglu","P. Restrepo"],"4008":["Micah Carroll","Rohin Shah","Mark K. Ho","T. Griffiths","S. Seshia","P. Abbeel","A. Dragan"],"4009":["L. Floridi"],"4010":["Nathalie A. Smuha"],"4011":["J. Morley","Libby Kinsey","Anat Elhalal","Francesca Garcia","M. Ziosi","L. Floridi"],"4012":["A. Dafoe","Edward Hughes","Y. Bachrach","Tantum Collins","Kevin R. McKee","Joel Z. Leibo","K. Larson","T. Graepel"],"4013":["David Mhlanga"],"4014":["Ryan Louie","Andy Coenen","Cheng-Zhi Anna Huang","Michael Terry","Carrie J. Cai"],"4015":["K. Siau","Weiyu Wang"],"4016":["V. Sounderajah","H. Ashrafian","R. Aggarwal","Jeffrey De Fauw","A. Denniston","F. Greaves","A. Karthikesalingam","Dominic King","Xiaoxuan Liu","S. Markar","M. McInnes","T. Panch","J. Pearson-Stuttard","D. Ting","R. Golub","D. Moher","P. Bossuyt","A. Darzi"],"4017":["Ai","Ashok K. Goel"],"4018":["C. Rudin","Joanna Radin"],"4019":["H. Fujita"],"4020":["Stephan Zheng","Alexander R. Trott","Sunil Srinivasa","N. Naik","Melvin Gruesbeck","David C. Parkes","R. Socher"],"4021":["M. Taddeo","L. Floridi"],"4022":["Nenad Toma\u0161ev","Julien Cornebise","F. Hutter","S. Mohamed","Angela Picciariello","Bec Connelly","D. Belgrave","Daphne Ezer","Fanny Cachat van der Haert","F. Mugisha","Gerald Abila","Hiromi Arai","Hisham Almiraat","Julia Proskurnia","Kyle Snyder","M. Otake-Matsuura","M. Othman","T. Glasmachers","W. D. Wever","Y. Teh","M. E. Khan","Ruben De Winne","T. Schaul","C. Clopath"],"4023":["Michele Colledanchise","Petter \u00d6gren"],"4024":["I. Poel"],"4025":["S. Kreps","Miles McCain","Miles Brundage"],"4026":["Gagan Bansal","Besmira Nushi","Ece Kamar","Daniel S. Weld","Walter S. Lasecki","E. Horvitz"],"4027":["Arun Rai","P. Constantinides","Saonee Sarker"],"4028":["B. Wirtz","Jan C. Weyerer","Benjamin Sturm"],"4029":["Andrey D. Ignatov","R. Timofte","William Chou","Ke Wang","Max Wu","Tim Hartley","L. Gool"],"4030":["F. L\u00e9cu\u00e9"],"4031":["Rafal Kocielnik","S. Amershi","Paul N. Bennett"],"4032":["Ehsan Toreini","M. Aitken","Kovila P. L. Coopamootoo","Karen Elliott","Carlos Vladimiro Gonzalez Zelaya","A. Moorsel"],"4033":["A. Ram","R. Prasad","Chandra Khatri","Anu Venkatesh","Raefer Gabriel","Qing Liu","J. Nunn","Behnam Hedayatnia","Ming Cheng","Ashish Nagar","Eric King","Kate Bland","Amanda Wartick","Yi Pan","Han Song","Sk Jayadevan","Gene Hwang","Art Pettigrue"],"4034":["Andrey D. Ignatov","R. Timofte","Andrei Kulik","Seungsoo Yang","Ke Wang","Felix Baum","Max Wu","Lirong Xu","L. Gool"],"4035":["Wei-Hao Chen","Kai-Xiang Li","Wei-Yu Lin","Kuo-Hsiang Hsu","Pin-Yi Li","Cheng-Han Yang","Cheng-Xin Xue","En-Yu Yang","Yen-Kai Chen","Yun-Sheng Chang","Tzu-Hsiang Hsu","Y. King","Chorng-Jung Lin","Ren-Shuo Liu","C. Hsieh","K. Tang","Meng-Fan Chang"],"4036":["Jeannette M. Wing"],"4037":["Ana\u00efs Ress\u00e9guier","Rowena Rodrigues"],"4038":["Nektaria Kaloudi","Jingyue Li"],"4039":["Mark Coeckelbergh"],"4040":["Thilo Hagendorff"],"4041":["Derek Doran","Sarah Schulz","Tarek R. Besold"],"4042":["A. Maedche","Christine Legner","Alexander Benlian","Benedikt Berger","Henner Gimpel","T. Hess","O. Hinz","Stefan Morana","Matthias S\u00f6llner"],"4043":["Hyunju Jeon","Ho-chang Youn","Sangyeon Ko","Tae-heon Kim"],"4044":["Elizabeth Bondi-Kelly","Lily Xu","Diana Acosta-Navas","J. Killian"],"4045":["J. Borenstein","A. Howard"],"4046":["Kenneth Holstein","V. Aleven"],"4047":["T. Panch","H. Mattie","L. Celi"],"4048":["B. Wahl","Aline Cossy-Gantner","S. Germann","N. Schwalbe"],"4049":["Lea Strohm","C. Hehakaya","E. Ranschaert","W. Boon","E. Moors"],"4050":["A. Hussain","Ouns Bouachir","F. Al-turjman","M. Aloqaily"],"4051":["Daniela Castillo","A. Canhoto","Emanuel Said"],"4052":["Oleg S. Pianykh","G. Langs","M. Dewey","D. Enzmann","C. Herold","S. Sch\u00f6nberg","J. Brink"],"4053":["J. Brock","Florian von Wangenheim"],"4054":["Rajasshrie Pillai","Brijesh Sivathanu"],"4055":["F. Doshi-Velez","Mason Kortz","Ryan Budish","Christopher T. Bavitz","S. Gershman","David O'Brien","Stuart Schieber","J. Waldo","David Weinberger","Adrian Weller","Alexandra Wood"],"4056":["Luis von Ahn","M. Blum","Nicholas Hopper","J. Langford"],"4057":["Tim Miller","P. Howe","L. Sonenberg"],"4058":["Veronica L. Thomas","Kendra Fowler"],"4059":["Shijie Jiang","Yi Zheng","D. Solomatine"],"4060":["Jess Whittlestone","Rune Nyrup","A. Alexandrova","S. Cave"],"4061":["Dakuo Wang","E. Churchill","P. Maes","Xiangmin Fan","B. Shneiderman","Yuanchun Shi","Qianying Wang"],"4062":["Urja Pawar","Donna O\u2019Shea","S. Rea","Ruairi O'Reilly"],"4063":["Kailas Vodrahalli","Tobias Gerstenberg","James Zou"],"4064":["M. Hind","S. Mehta","A. Mojsilovic","R. Nair","K. Ramamurthy","Alexandra Olteanu","Kush R. Varshney"],"4065":["L. Robert","Casey S. Pierce","Liz Morris","Sangmi Kim","Rasha Alahmad"],"4066":["M. McTear"],"4067":["A. Serag","A. Ion-Margineanu","H. Qureshi","Ryan McMillan","Marie-Judith Saint Martin","J. Diamond","P. O'Reilly","Peter Hamilton"],"4068":["Lefeng Cheng","Tao Yu"],"4069":["Ray Eitel-Porter"],"4070":["Nagaraj Samala","Bharath Shashanka Katkam","R. Bellamkonda","R. V. Rodriguez"],"4071":["Mireia Ribera","\u00c0gata Lapedriza"],"4072":["Yao Xie","Melody Chen","David Kao","Ge Gao","Xiang 'Anthony' Chen"],"4073":["Leila Ouchchy","Allen Coin","Veljko Dubljevi\u0107"],"4074":["S. Cave","Kanta Dihal"],"4075":["Hossein Hassani","E. Silva","Stephane Unger","Maedeh TajMazinani","Stephen Mac Feely"],"4076":["K. Gero","Zahra Ashktorab","Casey Dugan","Qian Pan","James M. Johnson","Werner Geyer","Maria Ruiz","Sarah Miller","D. Millen","Murray Campbell","Sadhana Kumaravel","Wei Zhang"],"4077":["Jihyun Kim","Kelly Merrill","Kun Xu","Deanna D. Sellnow"],"4078":["Arleen Salles","K. Evers","M. Farisco"],"4079":["J. Hatherley"],"4080":["S. Myers"],"4081":["Oscar Hengxuan Chi","D. Gursoy","C. Chi"],"4082":["Viet-thi Tran","C. Riveros","P. Ravaud"],"4083":["Andreas Holzinger"],"4084":["Fei Wang","A. Preininger"],"4085":["Mara Mills","M. Whittaker"],"4086":["Natalie Garrett","Nathan Beard","Casey Fiesler"],"4087":["S. McLennan","A. Fiske","L. Celi","Ruth M\u00fcller","Jana\u00edna Harder","Konstantin Ritt","S. Haddadin","A. Buyx"],"4088":["Dinh C. Nguyen","Peng Cheng","Ming Ding","D. L\u00f3pez-P\u00e9rez","P. Pathirana","Jun Li","A. Seneviratne","Yonghui Li","H. Poor"],"4089":["Matthew Arnold","Rachel K. E. Bellamy","Michael Hind","Stephanie Houde","Sameep Mehta","A. Mojsilovic","R. Nair","K. Ramamurthy","Alexandra Olteanu","David Piorkowski","Darrell Reimer","John T. Richards","Jason Tsay","Kush R. Varshney"],"4090":["F. Magrabi","E. Ammenwerth","J. B. McNair","N. D. de Keizer","H. Hypp\u00f6nen","P. Nyk\u00e4nen","M. Rigby","P. Scott","T. Vehko","Z. S. Wong","A. Georgiou"],"4091":["Thang N. Dinh","M. Thai"],"4092":["M. Stone","Eleni Aravopoulou","Y. Ekinci","Geraint Evans","Matt Hobbs","A. Labib","Paul Laughlin","J. Machtynger","Liz Machtynger"],"4093":["St\u00e9phan Vincent\u2010Lancrin","R. V. D. Vlies"],"4094":["R. Luckin","Wayne Holmes"],"4095":["Martin Brossard","A. Barrau","S. Bonnabel"],"4096":["Stefania Druga","Sarah T. Vu","Eesh Likhith","Tammy Qiu"],"4097":["J. Clune"],"4098":["T. Lysaght","H. Lim","Vicki Xafis","K. Ngiam"],"4099":["N. Houssami","Georgia Kirkpatrick-Jones","N. Noguchi","Christoph I. Lee"],"4100":["Krishna Gade","S. Geyik","K. Kenthapadi","Varun Mithal","Ankur Taly"],"4101":["R. Luckin","M. Cukurova"],"4102":["Merve Hickok"],"4103":["J. Leike","Miljan Martic","Victoria Krakovna","Pedro A. Ortega","Tom Everitt","Andrew Lefrancq","Laurent Orseau","S. Legg"],"4104":["R. Yuste","S. Goering","B. A. Y. Arcas","Guoqiang Bi","J. Carmena","A. Carter","J. Fins","Phoebe Friesen","J. Gallant","J. Huggins","J. Illes","P. Kellmeyer","E. Klein","Adam H. Marblestone","Christine Mitchell","E. Parens","Michelle Pham","Alan Rubel","N. Sadato","L. Sullivan","M. Teicher","D. Wasserman","Anna Wexler","M. Whittaker","J. Wolpaw"],"4105":["Helen Smith"],"4106":["Marieke M. M. Peeters","J. Diggelen","K. Bosch","A. Bronkhorst","Mark Antonius Neerincx","J. Schraagen","S. Raaijmakers"],"4107":["David Schneeberger","Karl St\u00f6ger","Andreas Holzinger"],"4108":["Andreas Holzinger","Peter Kieseberg","E. Weippl","A. Tjoa"],"4109":["K. Crawford","Vladan Joler"],"4110":["Mark Coeckelbergh"],"4111":["Jichen Zhu","Antonios Liapis","S. Risi","Rafael Bidarra","G. Youngblood"],"4112":["M. Pasquinelli","Vladan Joler"],"4113":["P. Keane","E. Topol"],"4114":["Maurice Jakesch","Megan French","Xiao Ma","Jeffrey T. Hancock","Mor Naaman"],"4115":["X. You","Chuan Zhang","X. Tan","Shi Jin","Hequan Wu"],"4116":["D. Ting","Yong Liu","P. Burlina","Xinxing Xu","N. Bressler","T. Wong"],"4117":["Wei Xu"],"4118":["Matthew J. Guzdial","N. Liao","Jonathan Chen","Shao-Yu Chen","Shukan Shah","Vishwa Shah","Josh Reno","Gillian Smith","Mark O. Riedl"],"4119":["D. Lauer"],"4120":["Scott Robbins"],"4121":["I. Stoica","D. Song","R. A. Popa","D. Patterson","Michael W. Mahoney","R. Katz","A. Joseph","Michael I. Jordan","J. Hellerstein","Joseph E. Gonzalez","Ken Goldberg","A. Ghodsi","David Culler","P. Abbeel"],"4122":["Sandra Wachter","B. Mittelstadt","L. Floridi"],"4123":["M. Johnson","A. Vera"],"4124":["Haydn Belfield"],"4125":["Hannah R Sullivan","Scott J. Schweikart"],"4126":["S. Cave","Katelyn Coughlan","Kanta Dihal"],"4127":["Urs Gasser","Virg\u00edlio A. F. Almeida"],"4128":["Anhong Guo","Ece Kamar","Jennifer Wortman Vaughan","Hanna M. Wallach","M. Morris"],"4129":["L. Alekseeva","Jos\u00e9 Azar","M. Gin\u00e9","S. Samila","Bledi Taska"],"4130":["Daniel S. Schiff","J. Biddle","J. Borenstein","Kelly Laas"],"4131":["K. Veeramachaneni","Ignacio Arnaldo","Vamsi Korrapati","Constantinos Bassias","Kuan-Ching Li"],"4132":["D. Roselli","Jeanna Neefe Matthews","Nisha Talagala"],"4133":["Kenneth Holstein","B. McLaren","V. Aleven"],"4134":["Gijs Overgoor","M. Chica","W. Rand","Anthony Weishampel"],"4135":["G. Irving","P. Christiano","Dario Amodei"],"4136":["Kate Crawford","Ryan Calo"],"4137":["R. McDougall"],"4138":["A. Preece"],"4139":["T. Davenport"],"4140":["B. Mittelstadt"],"4141":["P. van Esch","J. Stewart Black","D. Arli"],"4142":["E. Albert"],"4143":["A. Preece","Daniel Harborne","Dave Braines","Richard J. Tomsett","Supriyo Chakraborty"],"4144":["J. T\u00f8rresen"],"4145":["Diego Perez Liebana","Spyridon Samothrakis","J. Togelius","T. Schaul","S. Lucas"],"4146":["S. Alsheibani","Y. Cheung","C. Messom"],"4147":["P. Savadjiev","Jaron J. R. Chong","A. Dohan","M. Vakalopoulou","C. Reinhold","N. Paragios","B. Gallix"],"4148":["T. Rausch","W. Hummer","Vinod Muthusamy","A. Rashed","S. Dustdar"],"4149":["C. Conati","K. Porayska-Pomsta","M. Mavrikis"],"4150":["P. Krafft","Meg Young","Michael A. Katell","Karen Huang","Ghislain Bugingo"],"4151":["J. Hern\u00e1ndez-Orallo","Karina Vold"],"4152":["Thilo Hagendorff","Katharina Wezel"],"4153":["D. Touretzky","Christina Gardner-Mccune","C. Breazeal","F. Martin","Deborah W. Seehorn"],"4154":["Yueting Zhuang","Fei Wu","Chun Chen","Yunhe Pan"],"4155":["James Bessen"],"4156":["S. Kambhampati"],"4157":["Scott Robbins"],"4158":["Virginia Dignum"],"4159":["N. Codella","M. Hind","K. Ramamurthy","Murray Campbell","Amit Dhurandhar","Kush R. Varshney","Dennis Wei","A. Mojsilovic"],"4160":["M. Ghallab"],"4161":["Ece Kamar"],"4162":["T. Davenport","P. Michelman"],"4163":["Amy L. Ostrom","Darima Fotheringham","M. J. Bitner"],"4164":["Yoshua Bengio","Yann LeCun"],"4165":["Deborah G. Johnson","Mario Verdicchio"],"4166":["E. Blasch","R. Cruise","Alexander J. Aved","U. Majumder","T. Rovito"],"4167":["Odd Erik Gundersen","Y. Gil","D. Aha"],"4168":["F. Rossi","Nicholas Mattei"],"4169":["Yen-Lin Lee","Pei-Kuei Tsung","Max Wu"],"4170":["Noam Brown","T. Sandholm"],"4171":["Alexander Campolo","M. Sanfilippo","M. Whittaker","K. Crawford"],"4172":["Prithvijit Chattopadhyay","Deshraj Yadav","Viraj Prabhu","Arjun Chandrasekaran","Abhishek Das","Stefan Lee","Dhruv Batra","Devi Parikh"],"4173":["S. Cave","Se\u00e1n S. \u00d3h\u00c9igeartaigh"],"4174":["H. Anh","L. Pereira","T. Lenaerts"],"4175":["P. Voosen"],"4176":["Deborah G. Johnson","Mario Verdicchio"],"4177":["Dylan Hadfield-Menell","Gillian K. Hadfield"],"4178":["Deborah G. Johnson","Mario Verdicchio"],"4179":["Emily LaRosa","D. Danks"],"4180":["M. d\u2019Aquin","Pinelopi Troullinou","N. O'Connor","Aindrias Cullen","G. Faller","Louise Holden"],"4181":["M. Boden"],"4182":["Peter Clark","Oren Etzioni"],"4183":["Ashok K. Goel","David A. Joyner"],"4184":["Peter Clark"],"4185":["J. Hern\u00e1ndez-Orallo","Marco Baroni","J. Bieger","N. Chmait","D. Dowe","Katja Hofmann","Fernando Mart\u00ednez-Plumed","Claes Stranneg\u00e5rd","K. Th\u00f3risson"],"4186":["A. Guilherme"],"4187":["Stuart J. Russell","J. Bohannon"],"4188":["Mike Treanor","Alexander Zook","M. Eladhari","J. Togelius","Gillian Smith","Michael Cook","Tommy Thompson","Brian Magerko","J. Levine","Adam M. Smith"],"4189":["R. Murphy"],"4190":["Glen Robertson","I. Watson"],"4191":["Keith J. O'Hara","Douglas S. Blank","James B. Marshall"],"4192":["Jose D. Fern\u00e1ndez","F. Vico"],"4193":["S. Armstrong","Kaj Sotala","Se\u00e1n \u00d3 h\u00c9igeartaigh"],"4194":["N. Bostrom","Eliezer Yudkowsky"],"4195":["David J. Gunkel"],"4196":["Beverly Park Woolf","H. Chad Lane","V. Chaudhri","J. Kolodner"],"4197":["E. Benfenati","A. Manganaro","G. Gini"],"4198":["Roman V Yampolskiy"],"4199":["A. Kolobov"],"4200":["Steve Rabin"],"4201":["J. Togelius","Noor Shaker","S. Karakovskiy","Georgios N. Yannakakis"],"4202":["Georgios N. Yannakakis"],"4203":["S. Karakovskiy","J. Togelius"],"4204":["Trevor J. M. Bench-Capon","M. Araszkiewicz","Kevin D. Ashley","Katie Atkinson","Floris Bex","Filipe Borges","D. Bourcier","P. Bourgine","Jack G. Conrad","E. Francesconi","T. Gordon","Guido Governatori","Jochen L. Leidner","D. Lewis","R. Loui","L. McCarty","H. Prakken","Frank Schilder","E. Schweighofer","Paul Thompson","A. Tyrrell","B. Verheij","D. Walton","A. Wyner"],"4205":["S. Armstrong","A. Sandberg","N. Bostrom"],"4206":["Carlos S\u00e1nchez Quintana","F. Arcas","David Albarrac\u00edn Molina","Jose D. Fern\u00e1ndez","F. Vico"],"4207":[],"4208":["E. A. V. van Dis","J. Bollen","W. Zuidema","R. van Rooij","C. Bockting"],"4209":["Hsinchun Chen","David Zimbra"],"4210":["H. Thorp"],"4211":["Carmel Domshlak","Eyke H\u00fcllermeier","S. Kaci","H. Prade"],"4212":["Guillaume Chaslot","S. Bakkes","I. Szita","P. Spronck"],"4213":["Roman V Yampolskiy"],"4214":["M. Pollack"],"4215":["Debby R. E. Cotton","Peter A. Cotton","J. Shipway"],"4216":["Chris Stokel-Walker"],"4217":["W. Pieters"],"4218":["M. Cascella","J. Montomoli","Valentina Bellini","E. Bignami"],"4219":["Noor Shaker","J. Togelius","Georgios N. Yannakakis","B. Weber","Tomoyuki Shimizu","T. Hashiyama","N. Sorenson","Philippe Pasquier","Peter A. Mawhorter","Glen Takahashi","Gillian Smith","Robin Baumgarten"],"4220":["Holly Else"],"4221":["C. Aggarwal","Philip S. Yu"],"4222":["Jonathan H. Choi","Kristin E. Hickman","Amy B. Monahan","D. Schwarcz"],"4223":["G. Cooper"],"4224":["J. Laird","M. Lent"],"4225":["Xuemin Shen","Jie Gao","Wen Wu","Mushu Li","Conghao Zhou","W. Zhuang"],"4226":["Zhiheng Xi","Wenxiang Chen","Xin Guo","Wei He","Yiwen Ding","Boyang Hong","Ming Zhang","Junzhe Wang","Senjie Jin","Enyu Zhou","Rui Zheng","Xiaoran Fan","Xiao Wang","Limao Xiong","Qin Liu","Yuhao Zhou","Weiran Wang","Changhao Jiang","Yicheng Zou","Xiangyang Liu","Zhangyue Yin","Shihan Dou","Rongxiang Weng","Wensen Cheng","Qi Zhang","Wenjuan Qin","Yongyan Zheng","Xipeng Qiu","Xuanjing Huan","Tao Gui"],"4227":["H. Dreyfus"],"4228":["Yoshua Bengio","Aaron C. Courville","Pascal Vincent"],"4229":["L. Zadeh"],"4230":["Marc G. Bellemare","Yavar Naddaf","J. Veness","Michael Bowling"],"4231":["Piotr Faliszewski","A. Procaccia"],"4232":["Chin Hiong Tan","K. Tan","A. Tay"],"4233":["Stanley Kok","Parag Singla","Matthew Richardson","Pedro M. Domingos","Marc Sumner Hoifung Poon"],"4234":["Ninareh Mehrabi","Fred Morstatter","N. Saxena","Kristina Lerman","A. Galstyan"],"4235":["Roberto Navigli"],"4236":["Scott E. Reed","Zeynep Akata","Xinchen Yan","Lajanugen Logeswaran","B. Schiele","Honglak Lee"],"4237":["V. Sze","Yu-hsin Chen","Tien-Ju Yang","J. Emer"],"4238":["Jinghai Rao","Xiaomeng Su"],"4239":["P. Battaglia","Jessica B. Hamrick","V. Bapst","Alvaro Sanchez-Gonzalez","V. Zambaldi","Mateusz Malinowski","A. Tacchetti","David Raposo","Adam Santoro","Ryan Faulkner","\u00c7aglar G\u00fcl\u00e7ehre","H. F. Song","A. J. Ballard","J. Gilmer","George E. Dahl","Ashish Vaswani","Kelsey R. Allen","C. Nash","Victoria Langston","Chris Dyer","N. Heess","Daan Wierstra","Pushmeet Kohli","M. Botvinick","O. Vinyals","Yujia Li","Razvan Pascanu"],"4240":["M. Chiang","Tao Zhang"],"4241":["L. Rokach"],"4242":["Luis von Ahn","Laura A. Dabbish"],"4243":["Jim Austin"],"4244":["Xiaolong Xu","Haoyuan Li","Weijie Xu","Zhongjian Liu","L. Yao","Fei Dai"],"4245":["J. Adams"],"4246":["Eli Bingham","Jonathan P. Chen","M. Jankowiak","F. Obermeyer","Neeraj Pradhan","Theofanis Karaletsos","Rohit Singh","Paul A. Szerlip","Paul Horsfall","Noah D. Goodman"],"4247":["J. Togelius","S. Karakovskiy","Robin Baumgarten"],"4248":["Christopher Berner","Greg Brockman","Brooke Chan","Vicki Cheung","Przemyslaw Debiak","Christy Dennison","David Farhi","Quirin Fischer","Shariq Hashme","Christopher Hesse","R. J\u00f3zefowicz","S. Gray","Catherine Olsson","J. Pachocki","Michael Petrov","Henrique Pond\u00e9 de Oliveira Pinto","Jonathan Raiman","Tim Salimans","Jeremy Schlatter","Jonas Schneider","Szymon Sidor","I. Sutskever","Jie Tang","F. Wolski","Susan Zhang"],"4249":["Jochen Wirtz","P. Patterson","W. Kunz","Thorsten Gruber","V. Lu","Stefanie Paluch","Antje Martins"],"4250":["Mark Coeckelbergh"],"4251":["S. Amershi","Andrew Begel","C. Bird","R. Deline","H. Gall","Ece Kamar","Nachiappan Nagappan","Besmira Nushi","Thomas Zimmermann"],"4252":["S. Omohundro"],"4253":["Xingyi Yang","Jinyu Zhao","Yichen Zhang","Xuehai He","P. Xie"],"4254":["Yuxin Chen","E. Cand\u00e8s"],"4255":["M. Haenlein","A. Kaplan"],"4256":["D. McDermott"],"4257":["Abhishek Das","Samyak Datta","Georgia Gkioxari","Stefan Lee","Devi Parikh","Dhruv Batra"],"4258":["G. Kramer"],"4259":["R. Pfeifer","C. Scheier"],"4260":["C. Gao","Frederick M. Howard","N. Markov","E. Dyer","S. Ramesh","Yuan Luo","Alexander T. Pearson"],"4261":["S. Grigorescu","Bogdan Trasnea","Tiberiu T. Cocias","G. Macesanu"],"4262":["Molly K. Maskrey","Wallace Wang"],"4263":["A. Bakhtin","Noam Brown","Emily Dinan","Gabriele Farina","Colin Flaherty","Daniel Fried","Andrew Goff","Jonathan Gray","Hengyuan Hu","Athul Paul Jacob","Mojtaba Komeili","Karthik Konath","Minae Kwon","Adam Lerer","Mike Lewis","Alexander H. Miller","S. Mitts","Adithya Renduchintala","Stephen Roller","Dirk Rowe","Weiyan Shi","Joe Spisak","Alexander Wei","David J. Wu","Hugh Zhang","Markus Zijlstra"],"4264":["A. Tan","Han Yu","Li-zhen Cui","Qiang Yang"],"4265":["Artem Shmatko","N. Ghaffari Laleh","M. Gerstung","Jakob Nikolas Kather"],"4266":["Lefei Zhang","Liangpei Zhang"],"4267":["P. Spronck","M. Ponsen","I. Sprinkhuizen-Kuyper","E. Postma"],"4268":["Dinh C. Nguyen","Ming Ding","P. Pathirana","A. Seneviratne","Jun Li","F. I. H. Vincent Poor"],"4269":["Pei Wang"],"4270":["Steve Rabin"],"4271":["S. Bakkes","P. Spronck","J. Herik"],"4272":["Erik Nijkamp","Bo Pang","Hiroaki Hayashi","Lifu Tu","Haiquan Wang","Yingbo Zhou","S. Savarese","Caiming Xiong"],"4273":["Ross Taylor","Marcin Kardas","Guillem Cucurull","Thomas Scialom","A. Hartshorn","Elvis Saravia","Andrew Poulton","Viktor Kerkez","Robert Stojnic"],"4274":["Shijie Wu","Ozan Irsoy","Steven Lu","Vadim Dabravolski","Mark Dredze","Sebastian Gehrmann","P. Kambadur","D. Rosenberg","Gideon Mann"],"4275":["Xin Lai","Zhuotao Tian","Yukang Chen","Yanwei Li","Yuhui Yuan","Shu Liu","Jiaya Jia"],"4276":["Keqin Bao","Jizhi Zhang","Yang Zhang","Wenjie Wang","Fuli Feng","Xiangnan He"],"4277":["Andreas Kopf","Yannic Kilcher","Dimitri von Rutte","Sotiris Anagnostidis","Zhi Rui Tam","K. Stevens","Abdullah Barhoum","Nguyen Minh Duc","Oliver Stanley","Rich'ard Nagyfi","ES Shahul","Sameer Suri","David Glushkov","Arnav Dantuluri","Andrew Maguire","Christoph Schuhmann","Huu Nguyen","A. Mattick"],"4278":["Yunxiang Li","Zihan Li","Kai Zhang","Ruilong Dan","Steven Jiang","You Zhang"],"4279":["Shishir G. Patil","Tianjun Zhang","Xin Wang","Joseph E. Gonzalez"],"4280":["Woosuk Kwon","Zhuohan Li","Siyuan Zhuang","Ying Sheng","Lianmin Zheng","Cody Hao Yu","Joseph E. Gonzalez","Haotong Zhang","I. Stoica"],"4281":["Shilong Zhang","Pei Sun","Shoufa Chen","Min Xiao","Wenqi Shao","Wenwei Zhang","Kai Chen","Ping Luo"],"4282":["Jun Chen","Deyao Zhu","Xiaoqian Shen","Xiang Li","Zechun Liu","Pengchuan Zhang","Raghuraman Krishnamoorthi","Vikas Chandra","Yunyang Xiong","Mohamed Elhoseiny"],"4283":["Jiaxi Cui","Zongjia Li","Yang Yan","Bohua Chen","Li Yuan"],"4284":["Charlie Chen","Sebastian Borgeaud","G. Irving","Jean-Baptiste Lespiau","L. Sifre","J. Jumper"],"4285":["Lei Wang","Chengbang Ma","Xueyang Feng","Zeyu Zhang","Hao-ran Yang","Jingsen Zhang","Zhi-Yang Chen","Jiakai Tang","Xu Chen","Yankai Lin","Wayne Xin Zhao","Zhewei Wei","Ji-rong Wen"],"4286":["Wen Wang","Zhe Chen","Xiaokang Chen","Jiannan Wu","Xizhou Zhu","Gang Zeng","Ping Luo","Tong Lu","Jie Zhou","Y. Qiao","Jifeng Dai"],"4287":["Rui Yang","Lin Song","Yanwei Li","Sijie Zhao","Yixiao Ge","Xiu Li","Ying Shan"],"4288":["Paul K. Rubenstein","Chulayuth Asawaroengchai","D. Nguyen","Ankur Bapna","Zal\u00e1n Borsos","F. D. C. Quitry","Peter Chen","Dalia El Badawy","Wei Han","E. Kharitonov","Hannah Muckenhirn","D. Padfield","James Qin","Daniel Rozenberg","Tara N. Sainath","J. Schalkwyk","Matthew Sharifi","Michelle D. Tadmor","Ramanovich","M. Tagliasacchi","A. Tudor","Mihajlo Velimirovi'c","Damien Vincent","Jiahui Yu","Yongqiang Wang","V. Zayats","Neil Zeghidour","Yu Zhang","Zhishuai Zhang","Luk\u00e1s Zilka","C. Frank"],"4289":["Junjie Zhang","Ruobing Xie","Yupeng Hou","Wayne Xin Zhao","Leyu Lin","Ji-rong Wen"],"4290":["Allen Z. Ren","Anushri Dixit","Alexandra Bodrova","Sumeet Singh","Stephen Tu","Noah Brown","Peng Xu","L. Takayama","F. Xia","Jacob Varley","Zhenjia Xu","Dorsa Sadigh","Andy Zeng","Anirudha Majumdar"],"4291":["Ruipu Luo","Ziwang Zhao","Min Yang","Junwei Dong","Ming-Hui Qiu","Pengcheng Lu","Tao Wang","Zhongyu Wei"],"4292":["Jizhi Zhang","Keqin Bao","Yang Zhang","Wenjie Wang","Fuli Feng","Xiangnan He"],"4293":["Jieyi Long"],"4294":["Xue Jiang","Yihong Dong","Lecheng Wang","Qiwei Shang","Ge Li"],"4295":["Jinhao Jiang","Kun Zhou","Zican Dong","Keming Ye","Wayne Xin Zhao","Ji-rong Wen"],"4296":["Biao Zhang","B. Haddow","Alexandra Birch"],"4297":["Qinghao Ye","Haiyang Xu","Jiabo Ye","Mingshi Yan","Anwen Hu","Haowei Liu","Qi Qian","Ji Zhang","Fei Huang","Jingren Zhou"],"4298":["Zheng Liu","Aoxiao Zhong","Yiwei Li","Longtao Yang","Chao Ju","Zihao Wu","Chong Ma","Peng Shu","Cheng Chen","Sekeun Kim","Haixing Dai","Lin Zhao","Dajiang Zhu","Jun Liu","W. Liu","Dinggang Shen","Xiang Li","Quanzheng Li","Tianming Liu"],"4299":["R. Grosse","Juhan Bae","Cem Anil","Nelson Elhage","Alex Tamkin","Amirhossein Tajdini","Benoit Steiner","Dustin Li","Esin Durmus","Ethan Perez","Evan Hubinger","Kamil.e Lukovsiut.e","Karina Nguyen","Nicholas Joseph","Sam McCandlish","Jared Kaplan","Sam Bowman"],"4300":["D. Duong","B. D. Solomon"],"4301":["Hanyao Huang","Ou Zheng","Dongdong Wang","Jiayi Yin","Zijin Wang","Shengxuan Ding","H. Yin","Chuan Xu","R. Yang","Q. Zheng","B. Shi"],"4302":["D. Chalmers"],"4303":["Eric Schwitzgebel","David Schwitzgebel","A. Strasser"],"4304":["Xi Yang","Aokun Chen","Nima M. Pournejatian","Hoo-Chang Shin","Kaleb E. Smith","Christopher Parisien","Colin B. Compas","Cheryl Martin","Anthony B Costa","Mona G. Flores","Ying Zhang","Tanja Magoc","C. Harle","Gloria P. Lipori","Duane A. Mitchell","W. Hogan","E. Shenkman","Jiang Bian","Yonghui Wu"],"4305":["Tongshuang Sherry Wu","Ellen Jiang","Aaron Donsbach","J. Gray","A. Molina","Michael Terry","Carrie J. Cai"],"4306":["Jordan Hoffmann","Sebastian Borgeaud","A. Mensch","Elena Buchatskaya","Trevor Cai","Eliza Rutherford","Diego de Las Casas","Lisa Anne Hendricks","Johannes Welbl","Aidan Clark","Tom Hennigan","Eric Noland","Katie Millican","George van den Driessche","Bogdan Damoc","Aurelia Guy","Simon Osindero","K. Simonyan","Erich Elsen","O. Vinyals","Jack W. Rae","L. Sifre"],"4307":["Tongshuang Sherry Wu","Michael Terry","Carrie J. Cai"],"4308":["Allen Huang","Hui Wang","Yi Yang"],"4309":["Runxin Xu","Fuli Luo","Zhiyuan Zhang","Chuanqi Tan","Baobao Chang","Songfang Huang","Fei Huang"],"4310":["Yuanshun Yao","Xiaojun Xu","Yang Liu"],"4311":["Junnan Li","Dongxu Li","S. Savarese","Steven C. H. Hoi"],"4312":["Mark Chen","Jerry Tworek","Heewoo Jun","Qiming Yuan","Henrique Ponde","Jared Kaplan","Harrison Edwards","Yura Burda","Nicholas Joseph","Greg Brockman","Alex Ray","Raul Puri","Gretchen Krueger","Michael Petrov","Heidy Khlaaf","Girish Sastry","Pamela Mishkin","Brooke Chan","S. Gray","Nick Ryder","Mikhail Pavlov","Alethea Power","Lukasz Kaiser","Mohammad Bavarian","Clemens Winter","Philippe Tillet","F. Such","D. Cummings","Matthias Plappert","Fotios Chantzis","Elizabeth Barnes","Ariel Herbert-Voss","William H. Guss","Alex Nichol","Igor Babuschkin","S. Balaji","Shantanu Jain","A. Carr","J. Leike","Joshua Achiam","Vedant Misra","Evan Morikawa","Alec Radford","M. Knight","Miles Brundage","Mira Murati","Katie Mayer","P. Welinder","Bob McGrew","Dario Amodei","Sam McCandlish","I. Sutskever","Wojciech Zaremba"],"4313":["Deyao Zhu","Jun Chen","Xiaoqian Shen","Xiang Li","Mohamed Elhoseiny"],"4314":["Shunyu Yao","Dian Yu","Jeffrey Zhao","I. Shafran","T. Griffiths","Yuan Cao","Karthik Narasimhan"],"4315":["Can Xu","Qingfeng Sun","Kai Zheng","Xiubo Geng","Pu Zhao","Jiazhan Feng","Chongyang Tao","Daxin Jiang"],"4316":["Jinze Bai","Shuai Bai","Shusheng Yang","Shijie Wang","Sinan Tan","Peng Wang","Junyang Lin","Chang Zhou","Jingren Zhou"],"4317":["G. Li","H. Hammoud","Hani Itani","Dmitrii Khizbullin","Bernard Ghanem"],"4318":["Tiffany H. Kung","Morgan Cheatham","Arielle Medenilla","Czarina Sillos","Lorie De Leon","Camille Elepa\u00f1o","Maria Madriaga","Rimel Aggabao","Giezel Diaz-Candido","James Maningo","Victor Tseng"],"4319":["Shaden Smith","M. Patwary","Brandon Norick","P. LeGresley","Samyam Rajbhandari","J. Casper","Zhun Liu","Shrimai Prabhumoye","George Zerveas","V. Korthikanti","Elton Zhang","Rewon Child","Reza Yazdani Aminabadi","J. Bernauer","Xia Song","M. Shoeybi","Yuxiong He","Michael Houston","Saurabh Tiwary","Bryan Catanzaro"],"4320":["John Kirchenbauer","Jonas Geiping","Yuxin Wen","Jonathan Katz","Ian Miers","T. Goldstein"],"4321":["Jean-Baptiste Alayrac","Jeff Donahue","Pauline Luc","Antoine Miech","Iain Barr","Yana Hasson","Karel Lenc","A. Mensch","Katie Millican","Malcolm Reynolds","Roman Ring","Eliza Rutherford","Serkan Cabi","Tengda Han","Zhitao Gong","Sina Samangooei","Marianne Monteiro","Jacob Menick","Sebastian Borgeaud","Andy Brock","Aida Nematzadeh","Sahand Sharifzadeh","Mikolaj Binkowski","Ricardo Barreira","O. Vinyals","Andrew Zisserman","K. Simonyan"],"4322":["Zhiliang Peng","Wenhui Wang","Li Dong","Y. Hao","Shaohan Huang","Shuming Ma","Furu Wei"],"4323":["Danny Driess","F. Xia","Mehdi S. M. Sajjadi","Corey Lynch","Aakanksha Chowdhery","Brian Ichter","Ayzaan Wahid","Jonathan Tompson","Q. Vuong","Tianhe Yu","Wenlong Huang","Yevgen Chebotar","P. Sermanet","Daniel Duckworth","S. Levine","Vincent Vanhoucke","Karol Hausman","Marc Toussaint","Klaus Greff","Andy Zeng","Igor Mordatch","Peter R. Florence"],"4324":["D. Narayanan","M. Shoeybi","J. Casper","P. LeGresley","M. Patwary","V. Korthikanti","Dmitri Vainbrand","Prethvi Kashinkunti","J. Bernauer","Bryan Catanzaro","Amar Phanishayee","M. Zaharia"],"4325":["Teven Le Scao","Angela Fan","Christopher Akiki","Ellie Pavlick","Suzana Ili'c","Daniel Hesslow","Roman Castagn'e","A. Luccioni","Franccois Yvon","Matthias Gall\u00e9","J. Tow","Alexander M. Rush","Stella Biderman","Albert Webson","Pawan Sasanka Ammanamanchi","Thomas Wang","Beno\u00eet Sagot","Niklas Muennighoff","Albert Villanova del Moral","Olatunji Ruwase","Rachel Bawden","Stas Bekman","Angelina McMillan-Major","Iz Beltagy","Huu Nguyen","Lucile Saulnier","Samson Tan","Pedro Ortiz Suarez","Victor Sanh","Hugo Laurenccon","Yacine Jernite","Julien Launay","Margaret Mitchell","Colin Raffel","Aaron Gokaslan","Adi Simhi","Aitor Soroa Etxabe","Alham Fikri Aji","Amit Alfassy","Anna Rogers","Ariel Kreisberg Nitzav","Canwen Xu","Chenghao Mou","Chris C. Emezue","Christopher Klamm","Colin Leong","Daniel Alexander van Strien","David Ifeoluwa Adelani","Dragomir R. Radev","E. G. Ponferrada","Efrat Levkovizh","Ethan Kim","Eyal Natan","F. Toni","G\u00e9rard Dupont","Germ\u00e1n Kruszewski","Giada Pistilli","Hady ElSahar","Hamza Benyamina","H. Tran","Ian Yu","Idris Abdulmumin","Isaac Johnson","Itziar Gonzalez-Dios","Javier de la Rosa","Jenny Chim","Jesse Dodge","Jian Zhu","Jonathan Chang","Jorg Frohberg","Josephine L. Tobing","J. Bhattacharjee","Khalid Almubarak","Kimbo Chen","Kyle Lo","Leandro von Werra","Leon Weber","Long Phan","Loubna Ben Allal","Ludovic Tanguy","Manan Dey","M. Mu\u00f1oz","Maraim Masoud","Mar\u00eda Grandury","Mario vSavsko","Max Huang","Maximin Coavoux","Mayank Singh","Mike Tian-Jian Jiang","Minh Chien Vu","M. A. Jauhar","Mustafa Ghaleb","Nishant Subramani","Nora Kassner","Nurulaqilla Khamis","Olivier Nguyen","Omar Espejel","Ona de Gibert","Paulo Villegas","Peter Henderson","Pierre Colombo","Priscilla Amuok","Quentin Lhoest","Rheza Harliman","Rishi Bommasani","R. L'opez","Rui Ribeiro","Salomey Osei","Sampo Pyysalo","Sebastian Nagel","Shamik Bose","Shamsuddeen Hassan Muhammad","S. Sharma","S. Longpre","So-maieh Nikpoor","S. Silberberg","S. Pai","S. Zink","Tiago Timponi Torrent","Timo Schick","Tristan Thrush","V. Danchev","Vassilina Nikoulina","Veronika Laippala","Violette Lepercq","V. Prabhu","Zaid Alyafeai","Zeerak Talat","Arun Raja","Benjamin Heinzerling","Chenglei Si","Elizabeth Salesky","Sabrina J. Mielke","Wilson Y. Lee","Abheesht Sharma","Andrea Santilli","Antoine Chaffin","Arnaud Stiegler","Debajyoti Datta","Eliza Szczechla","Gunjan Chhablani","Han Wang","Harshit Pandey","Hendrik Strobelt","Jason Alan Fries","Jos Rozen","Leo Gao","Lintang Sutawika","M Saiful Bari","Maged S. Al-Shaibani","Matteo Manica","Nihal V. Nayak","Ryan Teehan","Samuel Albanie","Sheng Shen","Srulik Ben-David","Stephen H. Bach","Taewoon Kim","T. Bers","Thibault F\u00e9vry","Trishala Neeraj","Urmish Thakker","Vikas Raunak","Xiang Tang","Zheng-Xin Yong","Zhiqing Sun","Shaked Brody","Y. Uri","Hadar Tojarieh","Adam Roberts","Hyung Won Chung","Jaesung Tae","Jason Phang","Ofir Press","Conglong Li","D. Narayanan","Hatim Bourfoune","J. Casper","Jeff Rasley","Max Ryabinin","Mayank Mishra","Minjia Zhang","M. Shoeybi","Myriam Peyrounette","N. Patry","Nouamane Tazi","Omar Sanseviero","Patrick von Platen","Pierre Cornette","Pierre Franccois Lavall'ee","R. Lacroix","Samyam Rajbhandari","Sanchit Gandhi","Shaden Smith","S. Requena","Suraj Patil","Tim Dettmers","Ahmed Baruwa","Amanpreet Singh","Anastasia Cheveleva","Anne-Laure Ligozat","Arjun Subramonian","Aur'elie N'ev'eol","Charles Lovering","Daniel H Garrette","D. Tunuguntla","Ehud Reiter","Ekaterina Taktasheva","E. Voloshina","Eli Bogdanov","Genta Indra Winata","Hailey Schoelkopf","Jan-Christoph Kalo","Jekaterina Novikova","J. Forde","Xiangru Tang","Jungo Kasai","Ken Kawamura","Liam Hazan","Marine Carpuat","Miruna Clinciu","Najoung Kim","Newton Cheng","O. Serikov","Omer Antverg","Oskar van der Wal","Rui Zhang","Ruochen Zhang","Sebastian Gehrmann","Shachar Mirkin","S. Pais","Tatiana Shavrina","Thomas Scialom","Tian Yun","Tomasz Limisiewicz","Verena Rieser","Vitaly Protasov","V. Mikhailov","Yada Pruksachatkun","Yonatan Belinkov","Zachary Bamberger","Zdenvek Kasner","Zden\u011bk Kasner","A. Pestana","A. Feizpour","Ammar Khan","Amy Faranak","A. Santos","A. Hevia","Antigona Unldreaj","Arash Aghagol","Arezoo Abdollahi","A. Tammour","A. HajiHosseini","Bahareh Behroozi","Benjamin Ayoade Ajibade","B. Saxena","Carlos Mu\u00f1oz Ferrandis","Danish Contractor","D. Lansky","Davis David","Douwe Kiela","D. A. Nguyen","Edward Tan","Emi Baylor","Ezinwanne Ozoani","F. Mirza","Frankline Ononiwu","Habib Rezanejad","H.A. Jones","Indrani Bhattacharya","Irene Solaiman","Irina Sedenko","I. Nejadgholi","J. Passmore","Joshua Seltzer","Julio Bonis Sanz","Karen Fort","L\u00edvia Dutra","Mairon Samagaio","Maraim Elbadri","Margot Mieskes","Marissa Gerchick","Martha Akinlolu","Michael McKenna","Mike Qiu","M. Ghauri","Mykola Burynok","Nafis Abrar","Nazneen Rajani","Nour Elkott","N. Fahmy","Olanrewaju Samuel","Ran An","R. Kromann","Ryan Hao","S. Alizadeh","Sarmad Shubber","Silas L. Wang","Sourav Roy","S. Viguier","Thanh-Cong Le","Tobi Oyebade","T. Le","Yoyo Yang","Z. Nguyen","Abhinav Ramesh Kashyap","A. Palasciano","A. Callahan","Anima Shukla","Antonio Miranda-Escalada","A. Singh","Benjamin Beilharz","Bo Wang","C. Brito","Chenxi Zhou","Chirag Jain","Chuxin Xu","Cl\u00e9mentine Fourrier","Daniel Le'on Perin'an","Daniel Molano","Dian Yu","Enrique Manjavacas","Fabio Barth","Florian Fuhrimann","Gabriel Altay","Giyaseddin Bayrak","Gully Burns","Helena U. Vrabec","I. Bello","Isha Dash","J. Kang","John Giorgi","Jonas Golde","J. Posada","Karthi Sivaraman","Lokesh Bulchandani","Lu Liu","Luisa Shinzato","Madeleine Hahn de Bykhovetz","Maiko Takeuchi","Marc P\u00e0mies","M. A. Castillo","Marianna Nezhurina","Mario Sanger","M. Samwald","Michael Cullan","Michael Weinberg","M. Wolf","Mina Mihaljcic","Minna Liu","M. Freidank","Myungsun Kang","Natasha Seelam","N. Dahlberg","N. Broad","N. Muellner","Pascale Fung","Patricia Haller","R. Chandrasekhar","R. Eisenberg","Robert Martin","Rodrigo L. Canalli","Rosaline Su","Ruisi Su","Samuel Cahyawijaya","Samuele Garda","Shlok S Deshmukh","Shubhanshu Mishra","Sid Kiblawi","Simon Ott","Sinee Sang-aroonsiri","Srishti Kumar","Stefan Schweter","S. Bharati","Tanmay Laud","Th\u00e9o Gigant","Tomoya Kainuma","Wojciech Kusa","Yanis Labrak","Yashasvi Bajaj","Y. Venkatraman","Yifan Xu","Ying Xu","Yu Xu","Z. Tan","Zhongli Xie","Zifan Ye","M. Bras","Younes Belkada","Thomas Wolf"],"4326":["Rafael Rafailov","Archit Sharma","E. Mitchell","Stefano Ermon","Christopher D. Manning","Chelsea Finn"],"4327":["Aidan Gilson","C. Safranek","Thomas Huang","V. Socrates","Ling Chi","R. Taylor","David Chartash"],"4328":["Xinyun Chen","Maxwell Lin","Nathanael Sch\u00e4rli","Denny Zhou"],"4329":["Chaoyou Fu","Peixian Chen","Yunhang Shen","Yulei Qin","Mengdan Zhang","Xu Lin","Zhenyu Qiu","Wei Lin","Jinrui Yang","Xiawu Zheng","Ke Li","Xing Sun","Rongrong Ji"],"4330":["Freda Shi","Xinyun Chen","Kanishka Misra","Nathan Scales","David Dohan","E. Chi","Nathanael Scharli","Denny Zhou"],"4331":["Ali Madani","Ben Krause","E. Greene","Subu Subramanian","Benjamin P. Mohr","J. Holton","J. L. Olmos","Caiming Xiong","Zachary Z Sun","R. Socher","J. Fraser","N. Naik"],"4332":["Baolin Peng","Michel Galley","Pengcheng He","Hao Cheng","Yujia Xie","Yu Hu","Qiuyuan Huang","Lars Lid\u00e9n","Zhou Yu","Weizhu Chen","Jianfeng Gao"],"4333":["Ziyang Luo","Can Xu","Pu Zhao","Qingfeng Sun","Xiubo Geng","Wenxiang Hu","Chongyang Tao","Jing Ma","Qingwei Lin","Daxin Jiang"],"4334":["K. Singhal","Tao Tu","Juraj Gottweis","R. Sayres","Ellery Wulczyn","Le Hou","Kevin Clark","S. Pfohl","H. Cole-Lewis","Darlene Neal","M. Schaekermann","Amy Wang","Mohamed Amin","S. Lachgar","P. A. Mansfield","Sushant Prakash","Bradley Green","Ewa Dominowska","B. A. Y. Arcas","Nenad Toma\u0161ev","Yun Liu","Renee C Wong","Christopher Semturs","S. S. Mahdavi","J. Barral","D. Webster","G. Corrado","Yossi Matias","Shekoofeh Azizi","A. Karthikesalingam","Vivek Natarajan"],"4335":["Tianyi Zhang","Faisal Ladhak","Esin Durmus","Percy Liang","K. McKeown","Tatsunori Hashimoto"],"4336":["Peiyi Wang","Lei Li","Liang Chen","Dawei Zhu","Binghuai Lin","Yunbo Cao","Qi Liu","Tianyu Liu","Zhifang Sui"],"4337":["Yujia Qin","Shi Liang","Yining Ye","Kunlun Zhu","Lan Yan","Ya-Ting Lu","Yankai Lin","Xin Cong","Xiangru Tang","Bill Qian","Sihan Zhao","Runchu Tian","Ruobing Xie","Jie Zhou","Marc H. Gerstein","Dahai Li","Zhiyuan Liu","Maosong Sun"],"4338":["Yue Wang","Hung Le","Akhilesh Deepak Gotmare","Nghi D. Q. Bui","Junnan Li","Steven C. H. Hoi"],"4339":["Brady D. Lund","Ting Wang","Nishith Reddy Mannuru","Bing Nie","S. Shimray","Ziang Wang"],"4340":["Ying Sheng","Lianmin Zheng","Binhang Yuan","Zhuohan Li","Max Ryabinin","Daniel Y. Fu","Zhiqiang Xie","Beidi Chen","Clark W. Barrett","Joseph Gonzalez","Percy Liang","Christopher R\u00e9","I. Stoica","Ce Zhang"],"4341":["Dong Zhang","Shimin Li","Xin Zhang","Jun Zhan","Pengyu Wang","Yaqian Zhou","Xipeng Qiu"],"4342":["Viet Dac Lai","Nghia Trung Ngo","Amir Pouran Ben Veyseh","Hieu Man","Franck Dernoncourt","Trung Bui","Thien Huu Nguyen"],"4343":["Weiwei Sun","Lingyong Yan","Xinyu Ma","Pengjie Ren","Dawei Yin","Z. Ren"],"4344":["Maciej Besta","Nils Blach","Ale\u0161 Kub\u00ed\u010dek","Robert Gerstenberger","Lukas Gianinazzi","Joanna Gajda","Tomasz Lehmann","Michal Podstawski","H. Niewiadomski","P. Nyczyk","Torsten Hoefler"],"4345":["Chunyuan Li","Cliff Wong","Sheng Zhang","Naoto Usuyama","Haotian Liu","Jianwei Yang","Tristan Naumann","Hoifung Poon","Jianfeng Gao"],"4346":["Yupeng Hou","Junjie Zhang","Zihan Lin","Hongyu Lu","Ruobing Xie","Julian McAuley","Wayne Xin Zhao"],"4347":["Feilong Chen","Minglun Han","Haozhi Zhao","Qingyang Zhang","Jing Shi","Shuang Xu","Bo Xu"],"4348":["Bhargavi Paranjape","Scott M. Lundberg","Sameer Singh","Hannaneh Hajishirzi","Luke Zettlemoyer","Marco Tulio Ribeiro"],"4349":["Alireza Salemi","Sheshera Mysore","Michael Bendersky","Hamed Zamani"],"4350":["Benfeng Xu","An Yang","Junyang Lin","Quang Wang","Chang Zhou","Yongdong Zhang","Zhendong Mao"],"4351":["Denny Zhou","Nathanael Scharli","Le Hou","Jason Wei","Nathan Scales","Xuezhi Wang","D. Schuurmans","O. Bousquet","Quoc Le","E. Chi"],"4352":["Guanzhi Wang","Yuqi Xie","Yunfan Jiang","Ajay Mandlekar","Chaowei Xiao","Yuke Zhu","Linxi (Jim) Fan","Anima Anandkumar"],"4353":["Shun Zhang","Zhenfang Chen","Yikang Shen","Mingyu Ding","J. Tenenbaum","Chuang Gan"],"4354":["Shouyuan Chen","Sherman Wong","Liangjian Chen","Yuandong Tian"],"4355":["Yuqing Du","Olivia Watkins","Zihan Wang","C\u00e9dric Colas","Trevor Darrell","P. Abbeel","Abhishek Gupta","Jacob Andreas"],"4356":["Xinyin Ma","Gongfan Fang","Xinchao Wang"],"4357":["Xingxuan Li","Ruochen Zhao","Yew Ken Chia","Bosheng Ding","Lidong Bing","Shafiq R. Joty","Soujanya Poria"],"4358":["Raj Sanjay Shah","Kunal Chawla","Dheeraj Eidnani","Agam Shah","Wendi Du","S. Chava","Natraj Raman","Charese Smiley","Jiaao Chen","Diyi Yang"],"4359":["Shukang Yin","Chaoyou Fu","Sirui Zhao","Ke Li","Xing Sun","Tong Xu","Enhong Chen"],"4360":["Hao Sun","Zhexin Zhang","Jiawen Deng","Jiale Cheng","Minlie Huang"],"4361":["Xiaotian Zhang","Chun-yan Li","Yi Zong","Zhengyu Ying","Liang He","Xipeng Qiu"],"4362":["Deep Ganguli","Amanda Askell","Nicholas Schiefer","Thomas Liao","Kamil.e Lukovsiut.e","Anna Chen","Anna Goldie","Azalia Mirhoseini","Catherine Olsson","Danny Hernandez","Dawn Drain","Dustin Li","Eli Tran-Johnson","Ethan Perez","John Kernion","Jamie Kerr","J. Mueller","J. Landau","Kamal Ndousse","Karina Nguyen","Liane Lovitt","Michael Sellitto","Nelson Elhage","Noem'i Mercado","Nova DasSarma","R. Lasenby","Robin Larson","Sam Ringer","Sandipan Kundu","Saurav Kadavath","Scott Johnston","S. Kravec","S. E. Showk","Tamera Lanham","Timothy Telleen-Lawton","T. Henighan","Tristan Hume","Yuntao Bai","Zac Hatfield-Dodds","Benjamin Mann","Dario Amodei","Nicholas Joseph","Sam McCandlish","Tom B. Brown","C. Olah","Jack Clark","Sam Bowman","Jared Kaplan"],"4363":["T. Ullman"],"4364":["K. Singhal","Shekoofeh Azizi","T. Tu","S. Mahdavi","Jason Wei","Hyung Won Chung","Nathan Scales","A. Tanwani","H. Cole-Lewis","S. Pfohl","P. Payne","Martin G. Seneviratne","P. Gamble","C. Kelly","Nathaneal Scharli","Aakanksha Chowdhery","P. A. Mansfield","B. A. Y. Arcas","D. Webster","Greg S. Corrado","Yossi Matias","K. Chou","Juraj Gottweis","Nenad Toma\u0161ev","Yun Liu","A. Rajkomar","J. Barral","Christopher Semturs","A. Karthikesalingam","Vivek Natarajan"],"4365":["Liang Wang","Nan Yang","Furu Wei"],"4366":["Jakob Mokander","Jonas Schuett","Hannah Rose Kirk","Luciano Floridi"],"4367":["Yunzhi Yao","Peng Wang","Bo Tian","Siyuan Cheng","Zhoubo Li","Shumin Deng","Huajun Chen","Ningyu Zhang"],"4368":["Yotam Wolf","Noam Wies","Yoav Levine","A. Shashua"],"4369":["M. Shanahan","Kyle McDonell","Laria Reynolds"],"4370":["Cayque Monteiro Castro Nascimento","A. S. Pimentel"],"4371":["S. Dhuliawala","M. Komeili","Jing Xu","Roberta Raileanu","Xian Li","Asli Celikyilmaz","Jason Weston"],"4372":["Stella Biderman","USVSN Sai Prashanth","Lintang Sutawika","Hailey Schoelkopf","Quentin G. Anthony","Shivanshu Purohit","Edward Raf"],"4373":["Hang Zhang","Xin Li","Lidong Bing"],"4374":["Xiang Yue","Boshi Wang","Kai Zhang","Ziru Chen","Yu Su","Huan Sun"],"4375":["Sungdong Kim","Sanghwan Bae","Jamin Shin","Soyoung Kang","Donghyun Kwak","Kang Min Yoo","Minjoon Seo"],"4376":["Dale Schuurmans"],"4377":["Haiyan Zhao","Hanjie Chen","F. Yang","Ninghao Liu","Huiqi Deng","Hengyi Cai","Shuaiqiang Wang","Dawei Yin","Mengnan Du"],"4378":["Emilio Ferrara"],"4379":["Jacob Austin","Augustus Odena","Maxwell Nye","Maarten Bosma","H. Michalewski","David Dohan","Ellen Jiang","Carrie J. Cai","Michael Terry","Quoc V. Le","Charles Sutton"],"4380":["Yikang Pan","Liangming Pan","Wenhu Chen","Preslav Nakov","Min-Yen Kan","W. Wang"],"4381":["Zeming Lin","Halil Akin","Roshan Rao","B. Hie","Zhongkai Zhu","Wenting Lu","Nikita Smetanin","Robert Verkuil","Ori Kabeli","Y. Shmueli","Allan dos Santos Costa","Maryam Fazel-Zarandi","Tom Sercu","Salvatore Candido","Alexander Rives"],"4382":["Frank F. Xu","Uri Alon","Graham Neubig","V. Hellendoorn"],"4383":["Nicholas Carlini","Florian Tram\u00e8r","Eric Wallace","Matthew Jagielski","Ariel Herbert-Voss","Katherine Lee","Adam Roberts","Tom B. Brown","D. Song","\u00da. Erlingsson","Alina Oprea","Colin Raffel"],"4384":["Shibo Hao","Yi Gu","Haodi Ma","Joshua Jiahua Hong","Zhen Wang","D. Wang","Zhiting Hu"],"4385":["Yongchao Zhou","Andrei Ioan Muresanu","Ziwen Han","Keiran Paster","Silviu Pitis","Harris Chan","Jimmy Ba"],"4386":["D. Narayanan","M. Shoeybi","Jared Casper","P. LeGresley","M. Patwary","V. Korthikanti","Dmitri Vainbrand","Prethvi Kashinkunti","Julie Bernauer","Bryan Catanzaro","Amar Phanishayee","Matei Zaharia","Fiodar Kazhamiaka"],"4387":["Antonia Creswell","M. Shanahan","I. Higgins"],"4388":["Sami Sarsa","Paul Denny","Arto Hellas","Juho Leinonen"],"4389":["Ann Yuan","Andy Coenen","Emily Reif","Daphne Ippolito"],"4390":["W. Yu","Dan Iter","Shuohang Wang","Yichong Xu","Mingxuan Ju","Soumya Sanyal","Chenguang Zhu","Michael Zeng","Meng Jiang"],"4391":["Taylor W. Webb","K. Holyoak","Hongjing Lu"],"4392":["Nikhil Kandpal","H. Deng","Adam Roberts","Eric Wallace","Colin Raffel"],"4393":["S. Hegselmann","Alejandro Buendia","Hunter Lang","Monica Agrawal","Xiaoyi Jiang","D. Sontag"],"4394":["Gati Aher","RosaI. Arriaga","A. Kalai"],"4395":["Kushal Tirumala","Aram H. Markosyan","Luke Zettlemoyer","Armen Aghajanyan"],"4396":["Cem Anil","Yuhuai Wu","Anders Andreassen","Aitor Lewkowycz","Vedant Misra","V. Ramasesh","Ambrose Slone","Guy Gur-Ari","Ethan Dyer","Behnam Neyshabur"],"4397":["Antonia Creswell","M. Shanahan"],"4398":["Yixuan Weng","Minjun Zhu","Bin Li","Shizhu He","Kang Liu","Jun Zhao"],"4399":["Jacky Liang","Wenlong Huang","F. Xia","Peng Xu","Karol Hausman","Brian Ichter","Peter R. Florence","Andy Zeng"],"4400":["Xuechen Li","Florian Tram\u00e8r","Percy Liang","Tatsunori B. Hashimoto"],"4401":["Luca Beurer-Kellner","Marc Fischer","Martin T. Vechev"],"4402":["Joshua Robinson","Christopher Rytting","D. Wingate"],"4403":["Yue Zhao","Ishan Misra","Philipp Krahenbuhl","Rohit Girdhar"],"4404":["Nitarshan Rajkumar","Raymond Li","Dzmitry Bahdanau"],"4405":["Y. Gai","Liyi Zhou","Kaihua Qin","D. Song","Arthur Gervais"],"4406":["Daliang Li","A. Rawat","M. Zaheer","Xin Wang","M. Lukasik","Andreas Veit","Felix X. Yu","Surinder Kumar"],"4407":["Namgyu Ho","Laura Schmid","Se-Young Yun"],"4408":["Sachit Menon","Carl Vondrick"],"4409":["Sam Bowman","Jeeyoon Hyun","Ethan Perez","Edwin Chen","Craig Pettit","Scott Heiner","Kamil\u0117 Luko\u0161i\u016bt\u0117","Amanda Askell","Andy Jones","Anna Chen","Anna Goldie","Azalia Mirhoseini","C. McKinnon","C. Olah","D. Amodei","Dario Amodei","Dawn Drain","Dustin Li","Eli Tran-Johnson","John Kernion","Jamie Kerr","J. Mueller","Jeff Ladish","J. Landau","Kamal Ndousse","Liane Lovitt","Nelson Elhage","Nicholas Schiefer","Nicholas Joseph","Noem'i Mercado","Nova DasSarma","Robin Larson","Sam McCandlish","S. Kundu","Scott Johnston","S. Kravec","S. E. Showk","Stanislav Fort","Timothy Telleen-Lawton","Tom B. Brown","T. Henighan","Tristan Hume","Yuntao Bai","Zac Hatfield-Dodds","Benjamin Mann","Jared Kaplan"],"4410":["T. Sejnowski"],"4411":["S. Piantadosi","Felix Hill"],"4412":["Eldar Kurtic","Daniel Fernando Campos","Tuan Nguyen","Elias Frantar","Mark Kurtz","Ben Fineran","M. Goin","Dan Alistarh"],"4413":["Sean Trott","Cameron J. Jones","Tyler A. Chang","J. Michaelov","B. Bergen"],"4414":["Liang Xu","Xuanwei Zhang","Qianqian Dong"],"4415":["Gati Aher","RosaI. Arriaga","A. Kalai"],"4416":["Noelia Ferruz","Steffen Schmidt","B. H\u00f6cker"],"4417":["Zhengxiao Du","Yujie Qian","Xiao Liu","Ming Ding","J. Qiu","Zhilin Yang","Jie Tang"],"4418":["Zirui Wang","Jiahui Yu","Adams Wei Yu","Zihang Dai","Yulia Tsvetkov","Yuan Cao"],"4419":["Kunat Pipatanakul","Phatrasek Jirabovonvisut","Potsawee Manakul","Sittipong Sripaisarnmongkol","Ruangsak Patomwong","Pathomporn Chokchainant","Kasima Tharnpipitchai"],"4420":["Mina Lee","Percy Liang","Qian Yang"],"4421":["Zhengyan Zhang","Xu Han","Hao Zhou","Pei Ke","Yuxian Gu","Deming Ye","Yujia Qin","Yusheng Su","Haozhe Ji","Jian Guan","Fanchao Qi","Xiaozhi Wang","Yanan Zheng","Guoyang Zeng","Huanqi Cao","S. Chen","Daixuan Li","Zhenbo Sun","Zhiyuan Liu","Minlie Huang","Wentao Han","Jie Tang","Juan-Zi Li","Xiaoyan Zhu","Maosong Sun"],"4422":["Tianxiang Sun","Yunfan Shao","Hong Qian","Xuanjing Huang","Xipeng Qiu"],"4423":["Kelvin Guu","Kenton Lee","Zora Tung","Panupong Pasupat","Ming-Wei Chang"],"4424":["Teven Le Scao","Thomas Wang","Daniel Hesslow","Lucile Saulnier","Stas Bekman","Saiful Bari","Stella Biderman","Hady ElSahar","Niklas Muennighoff","Jason Phang","Ofir Press","Colin Raffel","Victor Sanh","Sheng Shen","Lintang Sutawika","Jaesung Tae","Zheng-Xin Yong","Julien Launay","Iz Beltagy"],"4425":["Abubakar Abid","Maheen Farooqi","James Y. Zou"],"4426":["Alex Tamkin","Miles Brundage","Jack Clark","Deep Ganguli"],"4427":["Laria Reynolds","Kyle McDonell"],"4428":["Naman Jain","Skanda Vaidyanath","Arun Shankar Iyer","Nagarajan Natarajan","Suresh Parthasarathy","S. Rajamani","Rahul Sharma"],"4429":["Yu Gu","Robert Tinn","Hao Cheng","Michael R. Lucas","Naoto Usuyama","Xiaodong Liu","Tristan Naumann","Jianfeng Gao","Hoifung Poon"],"4430":["Emily Reif","Daphne Ippolito","Ann Yuan","Andy Coenen","Chris Callison-Burch","Jason Wei"],"4431":["Dat Quoc Nguyen","Thanh Vu","A. Nguyen"],"4432":["H. Pearce","Benjamin Tan","Baleegh Ahmad","R. Karri","Brendan Dolan-Gavitt"],"4433":["Beliz Gunel","Jingfei Du","Alexis Conneau","Ves Stoyanov"],"4434":["Luyu Gao","Jamie Callan"],"4435":["Jiaji Huang","Yi Li","Wei Ping","Liang Huang"],"4436":["N. Keskar","Bryan McCann","L. Varshney","Caiming Xiong","R. Socher"],"4437":["Ziheng Wang","Jeremy Wohlwend","Tao Lei"],"4438":["Ai Ming Yang","Bin Xiao","Bingning Wang","Borong Zhang","Ce Bian","Chao Yin","Chenxu Lv","Da Pan","Dian Wang","Dong Yan","Fan Yang","Fei Deng","Feng Wang","Feng Liu","Guangwei Ai","Guosheng Dong","Hai Zhao","Hang Xu","Hao-Lun Sun","Hongda Zhang","Hui Liu","Jiaming Ji","Jian Xie","Juntao Dai","Kuncheng Fang","Lei Su","Liang Song","Lifeng Liu","Liyun Ru","Luyao Ma","Mang Wang","Mickel Liu","MingAn Lin","Nuolan Nie","Pei Guo","Ruiyang Sun","Zhang Tao","Tianpeng Li","Tianyu Li","Wei Cheng","Weipeng Chen","Xiangrong Zeng","Xiaochuan Wang","Xiaoxi Chen","Xin Men","Xin Yu","Xuehai Pan","Yan-Bin Shen","Yiding Wang","Yiyu Li","Youxin Jiang","Yuchen Gao","Yupeng Zhang","Zenan Zhou","Zhiying Wu"],"4439":["Yu Meng","Chenyan Xiong","Payal Bajaj","Saurabh Tiwary","Paul N. Bennett","Jiawei Han","Xia Song"],"4440":["Sam Witteveen","Martin Andrews"],"4441":["Hang Le","Lo\u00efc Vial","Jibril Frej","V. Segonne","Maximin Coavoux","B. Lecouteux","A. Allauzen","Benoit Crabb'e","L. Besacier","D. Schwab"],"4442":["Branden Chan","Stefan Schweter","Timo M\u00f6ller"],"4443":["Yi Yang","Mark Christopher Siy Uy","Allen Huang"],"4444":["Ning Ding","Yujia Qin","Guang Yang","Fu Wei","Zonghan Yang","Yusheng Su","Shengding Hu","Yulin Chen","Chi-Min Chan","Weize Chen","Jing Yi","Weilin Zhao","Xiaozhi Wang","Zhiyuan Liu","Haitao Zheng","Jianfei Chen","Y. Liu","Jie Tang","Juanzi Li","Maosong Sun"],"4445":["Jesse Vig","Yonatan Belinkov"],"4446":["S. Sun","Zhe Gan","Yu Cheng","Yuwei Fang","Shuohang Wang","Jingjing Liu"],"4447":["Zhaojiang Lin","Andrea Madotto","Pascale Fung"],"4448":["Samuel R. Bowman","Gabor Angeli","Christopher Potts","Christopher D. Manning"],"4449":["Dan Su","Yan Xu","Genta Indra Winata","Peng Xu","Hyeon-Jin Kim","Zihan Liu","Pascale Fung"],"4450":["Cheng-Yu Hsieh","Chun-Liang Li","Chih-Kuan Yeh","Hootan Nakhost","Yasuhisa Fujii","Alexander J. Ratner","Ranjay Krishna","Chen-Yu Lee","Tomas Pfister"],"4451":["Qingyang Wu","Yichi Zhang","Yu Li","Zhou Yu"],"4452":["Xi Chen","Xiao Wang","Soravit Changpinyo","A. Piergiovanni","Piotr Padlewski","Daniel M. Salz","Sebastian Goodman","Adam Grycner","Basil Mustafa","Lucas Beyer","Alexander Kolesnikov","J. Puigcerver","Nan Ding","Keran Rong","Hassan Akbari","Gaurav Mishra","Linting Xue","Ashish V. Thapliyal","James Bradbury","Weicheng Kuo","Mojtaba Seyedhosseini","Chao Jia","Burcu Karagol Ayan","C. Riquelme","A. Steiner","A. Angelova","Xiaohua Zhai","N. Houlsby","Radu Soricut"],"4453":["Yusong Wu","K. Chen","Tianyu Zhang","Yuchen Hui","Taylor Berg-Kirkpatrick","S. Dubnov"],"4454":["Shubham Toshniwal","Anjuli Kannan","C. Chiu","Yonghui Wu","Tara N. Sainath","Karen Livescu"],"4455":["Adam Goodkind","K. Bicknell"],"4456":["Yu Sun","Shuohuan Wang","Shikun Feng","Siyu Ding","Chao Pang","Junyuan Shang","Jiaxiang Liu","Xuyi Chen","Yanbin Zhao","Yuxiang Lu","Weixin Liu","Zhihua Wu","Weibao Gong","Jianzhong Liang","Zhizhou Shang","Peng Sun","Wei Liu","Ouyang Xuan","Dianhai Yu","Hao Tian","Hua Wu","Haifeng Wang"],"4457":["Chitwan Saharia","William Chan","Saurabh Saxena","Lala Li","Jay Whang","Emily L. Denton","Seyed Kamyar Seyed Ghasemipour","Burcu Karagol Ayan","S. S. Mahdavi","Raphael Gontijo Lopes","Tim Salimans","Jonathan Ho","David J. Fleet","Mohammad Norouzi"],"4458":["Aakanksha Chowdhery","Sharan Narang","Jacob Devlin","Maarten Bosma","Gaurav Mishra","Adam Roberts","Paul Barham","Hyung Won Chung","Charles Sutton","Sebastian Gehrmann","Parker Schuh","Kensen Shi","Sasha Tsvyashchenko","Joshua Maynez","Abhishek Rao","Parker Barnes","Yi Tay","Noam M. Shazeer","Vinodkumar Prabhakaran","Emily Reif","Nan Du","B. Hutchinson","Reiner Pope","James Bradbury","Jacob Austin","M. Isard","Guy Gur-Ari","Pengcheng Yin","Toju Duke","Anselm Levskaya","S. Ghemawat","Sunipa Dev","H. Michalewski","Xavier Garc\u00eda","Vedant Misra","Kevin Robinson","Liam Fedus","Denny Zhou","Daphne Ippolito","D. Luan","Hyeontaek Lim","Barret Zoph","A. Spiridonov","Ryan Sepassi","David Dohan","Shivani Agrawal","Mark Omernick","Andrew M. Dai","Thanumalayan Sankaranarayana Pillai","Marie Pellat","Aitor Lewkowycz","Erica Moreira","Rewon Child","Oleksandr Polozov","Katherine Lee","Zongwei Zhou","Xuezhi Wang","Brennan Saeta","Mark D\u00edaz","Orhan Firat","Michele Catasta","Jason Wei","K. Meier-Hellstern","D. Eck","J. Dean","Slav Petrov","Noah Fiedel"],"4459":["Alec Radford","Karthik Narasimhan"],"4460":["Andy Zou","Zifan Wang","J. Z. Kolter","Matt Fredrikson"],"4461":["Alexander Richard","Juergen Gall"],"4462":["Susan Zhang","Stephen Roller","Naman Goyal","Mikel Artetxe","Moya Chen","Shuohui Chen","Christopher Dewan","Mona T. Diab","Xian Li","Xi Victoria Lin","Todor Mihaylov","Myle Ott","Sam Shleifer","Kurt Shuster","Daniel Simig","Punit Singh Koura","Anjali Sridhar","Tianlu Wang","Luke Zettlemoyer"],"4463":["Shaohan Huang","Li Dong","Wenhui Wang","Y. Hao","Saksham Singhal","Shuming Ma","Tengchao Lv","Lei Cui","O. Mohammed","Qiang Liu","Kriti Aggarwal","Zewen Chi","Johan Bjorck","Vishrav Chaudhary","Subhojit Som","Xia Song","Furu Wei"],"4464":["A. Nguyen","T. Nguyen"],"4465":["Jiasen Lu","Christopher Clark","Rowan Zellers","Roozbeh Mottaghi","Aniruddha Kembhavi"],"4466":["Amanpreet Singh","Ronghang Hu","Vedanuj Goswami","Guillaume Couairon","Wojciech Galuba","Marcus Rohrbach","Douwe Kiela"],"4467":["Chengwei Qin","Aston Zhang","Zhuosheng Zhang","Jiaao Chen","Michihiro Yasunaga","Diyi Yang"],"4468":["Xin Eric Wang","Jiawei Wu","Junkun Chen","Lei Li","Yuan-fang Wang","William Yang Wang"],"4469":["Yilun Du","Shuang Li","A. Torralba","J. Tenenbaum","Igor Mordatch"],"4470":["Zhuosheng Zhang","Aston Zhang","Mu Li","Hai Zhao","G. Karypis","Alexander J. Smola"],"4471":["Hyung Won Chung","Le Hou","S. Longpre","Barret Zoph","Yi Tay","W. Fedus","Eric Li","Xuezhi Wang","Mostafa Dehghani","Siddhartha Brahma","Albert Webson","S. Gu","Zhuyun Dai","Mirac Suzgun","Xinyun Chen","Aakanksha Chowdhery","Dasha Valter","Sharan Narang","Gaurav Mishra","Adams Wei Yu","Vincent Zhao","Yanping Huang","Andrew M. Dai","Hongkun Yu","Slav Petrov","E. Chi","J. Dean","Jacob Devlin","Adam Roberts","Denny Zhou","Quoc V. Le","Jason Wei"],"4472":["M. Shoeybi","M. Patwary","Raul Puri","P. LeGresley","J. Casper","Bryan Catanzaro"],"4473":["Wei Zeng","Xiaozhe Ren","Teng Su","Hui Wang","Yi Liao","Zhiwei Wang","Xin Jiang","ZhenZhang Yang","Kaisheng Wang","Xiaoda Zhang","Chen Li","Ziyan Gong","Yifan Yao","Xinjing Huang","Jun Wang","Jianfeng Yu","Qiwei Guo","Yue Yu","Yan Zhang","Jin Wang","Heng Tao","Dasen Yan","Z. Yi","Fang Peng","Fan Jiang","Han Zhang","Lingfeng Deng","Yehong Zhang","Zhengping Lin","Chao Zhang","Shaojie Zhang","Mingyue Guo","Shanzhi Gu","Gaojun Fan","Yaowei Wang","Xuefeng Jin","Qun Liu","Yonghong Tian"],"4474":["Zhilin Yang","Zihang Dai","Yiming Yang","J. Carbonell","R. Salakhutdinov","Quoc V. Le"],"4475":["Zhenzhong Lan","Mingda Chen","Sebastian Goodman","Kevin Gimpel","Piyush Sharma","Radu Soricut"],"4476":["Anthony Brohan","Noah Brown","Justice Carbajal","Yevgen Chebotar","K. Choromanski","Tianli Ding","Danny Driess","Chelsea Finn","Peter R. Florence","Chuyuan Fu","Montse Gonzalez Arenas","K. Gopalakrishnan","Kehang Han","Karol Hausman","Alexander Herzog","Jasmine Hsu","Brian Ichter","A. Irpan","Nikhil J. Joshi","Ryan C. Julian","Dmitry Kalashnikov","Yuheng Kuang","Isabel Leal","S. Levine","H. Michalewski","Igor Mordatch","Karl Pertsch","Kanishka Rao","Krista Reymann","M. Ryoo","Grecia Salazar","P. Sanketi","P. Sermanet","Jaspiar Singh","Anika Singh","Radu Soricut","Huong Tran","Vincent Vanhoucke","Q. Vuong","Ayzaan Wahid","Stefan Welker","Paul Wohlhart","Ted Xiao","Tianhe Yu","Brianna Zitkovich"],"4477":["Wissam Antoun","Fady Baly","Hazem M. Hajj"],"4478":["Jerry W. Wei","Jason Wei","Yi Tay","Dustin Tran","Albert Webson","Yifeng Lu","Xinyun Chen","Hanxiao Liu","Da Huang","Denny Zhou","Tengyu Ma"],"4479":["Geunwoo Kim","P. Baldi","S. McAleer"],"4480":["Yuqi Huo","Manli Zhang","Guangzhen Liu","Haoyu Lu","Yizhao Gao","Guoxing Yang","Jing Wen","Heng Zhang","Baogui Xu","Weihao Zheng","Zongzheng Xi","Yueqian Yang","Anwen Hu","Jinming Zhao","Ruichen Li","Yida Zhao","Liang Zhang","Yuqing Song","Xin Hong","Wanqing Cui","Danyang Hou","Yingyan Li","Junyi Li","Peiyu Liu","Zheng Gong","Chu Jin","Yuchong Sun","Shizhe Chen","Zhiwu Lu","Zhicheng Dou","Qin Jin","Yanyan Lan","Wayne Xin Zhao","Ruihua Song","Ji-rong Wen"],"4481":["Mikel Artetxe","Shruti Bhosale","Naman Goyal","Todor Mihaylov","Myle Ott","Sam Shleifer","Xi Victoria Lin","Jingfei Du","Srini Iyer","Ramakanth Pasunuru","Giridhar Anantharaman","Xian Li","Shuohui Chen","H. Ak\u0131n","Mandeep Baines","Louis Martin","Xing Zhou","Punit Singh Koura","Brian O'Horo","Jeff Wang","Luke Zettlemoyer","Mona T. Diab","Zornitsa Kozareva","Ves Stoyanov"],"4482":["Wenlong Huang","Chen Wang","Ruohan Zhang","Yunzhu Li","Jiajun Wu","Li Fei-Fei"],"4483":["Michael Ahn","Anthony Brohan","Noah Brown","Yevgen Chebotar","Omar Cortes","Byron David","Chelsea Finn","K. Gopalakrishnan","Karol Hausman","Alexander Herzog","Daniel Ho","Jasmine Hsu","Julian Ibarz","Brian Ichter","A. Irpan","Eric Jang","Rosario Jauregui Ruano","Kyle Jeffrey","Sally Jesmonth","N. Joshi","Ryan C. Julian","Dmitry Kalashnikov","Yuheng Kuang","Kuang-Huei Lee","S. Levine","Yao Lu","Linda Luu","Carolina Parada","P. Pastor","Jornell Quiambao","Kanishka Rao","Jarek Rettinghouse","D. Reyes","P. Sermanet","Nicolas Sievers","Clayton Tan","Alexander Toshev","Vincent Vanhoucke","F. Xia","Ted Xiao","Peng Xu","Sichun Xu","Mengyuan Yan"],"4484":["Yizhong Wang","Yeganeh Kordi","Swaroop Mishra","Alisa Liu","Noah A. Smith","Daniel Khashabi","Hannaneh Hajishirzi"],"4485":["Jerret Ross","Brian M. Belgodere","V. Chenthamarakshan","Inkit Padhi","Youssef Mroueh","Payel Das"],"4486":["Chen Sun","Austin Myers","Carl Vondrick","K. Murphy","C. Schmid"],"4487":["T. Brants","Ashok Popat","P. Xu","F. Och","J. Dean"],"4488":["Jason Wei","Maarten Bosma","Vincent Zhao","Kelvin Guu","Adams Wei Yu","Brian Lester","Nan Du","Andrew M. Dai","Quoc V. Le"],"4489":["Zhuohan Li","Siyuan Zhuang","Shiyuan Guo","Danyang Zhuo","Hao Zhang","D. Song","I. Stoica"],"4490":["Peng Xu","M. Patwary","M. Shoeybi","Raul Puri","Pascale Fung","Anima Anandkumar","Bryan Catanzaro"],"4491":["Minae Kwon","Sang Michael Xie","Kalesha Bullard","Dorsa Sadigh"],"4492":["Gautier Izacard","Patrick Lewis","M. Lomeli","Lucas Hosseini","Fabio Petroni","Timo Schick","Jane A. Yu","Armand Joulin","Sebastian Riedel","Edouard Grave"],"4493":["Aitor Lewkowycz","Anders Andreassen","David Dohan","Ethan Dyer","H. Michalewski","V. Ramasesh","Ambrose Slone","Cem Anil","Imanol Schlag","Theo Gutman-Solo","Yuhuai Wu","Behnam Neyshabur","Guy Gur-Ari","Vedant Misra"],"4494":["Cheolhyoung Lee","Kyunghyun Cho","Wanmo Kang"],"4495":["Martin M\u00fcller","M. Salath\u00e9","P. Kummervold"],"4496":["Xiaozhi Wang","Tianyu Gao","Zhaocheng Zhu","Zhiyuan Liu","Juan-Zi Li","Jian Tang"],"4497":["Xiaodong Liu","Hao Cheng","Pengcheng He","Weizhu Chen","Yu Wang","Hoifung Poon","Jianfeng Gao"],"4498":["Jianfeng Wang","Zhengyuan Yang","Xiaowei Hu","Linjie Li","Kevin Lin","Zhe Gan","Zicheng Liu","Ce Liu","Lijuan Wang"],"4499":["Nicholas Carlini","Daphne Ippolito","Matthew Jagielski","Katherine Lee","Florian Tram\u00e8r","Chiyuan Zhang"],"4500":["Zal\u00e1n Borsos","Rapha\u00ebl Marinier","Damien Vincent","E. Kharitonov","O. Pietquin","Matthew Sharifi","Dominik Roblek","O. Teboul","David Grangier","M. Tagliasacchi","Neil Zeghidour"],"4501":["Kenneth Heafield","Ivan Pouzyrevsky","J. Clark","Philipp Koehn"],"4502":["J. Kaplan","Sam McCandlish","T. Henighan","Tom B. Brown","Benjamin Chess","Rewon Child","S. Gray","Alec Radford","Jeff Wu","Dario Amodei"],"4503":["Bo Li","Yuanhan Zhang","Liangyu Chen","Jinghao Wang","Jingkang Yang","Ziwei Liu"],"4504":["Jiasen Lu","Dhruv Batra","Devi Parikh","Stefan Lee"],"4505":["Loren Lugosch","M. Ravanelli","Patrick Ignoto","Vikrant Singh Tomar","Yoshua Bengio"],"4506":["Suchin Gururangan","Ana Marasovi\u0107","Swabha Swayamdipta","Kyle Lo","Iz Beltagy","Doug Downey","Noah A. Smith"],"4507":["Mehdi Cherti","Romain Beaumont","Ross Wightman","Mitchell Wortsman","Gabriel Ilharco","Cade Gordon","Christoph Schuhmann","Ludwig Schmidt","J. Jitsev"],"4508":["Jack W. Rae","Sebastian Borgeaud","Trevor Cai","Katie Millican","Jordan Hoffmann","Francis Song","John Aslanides","Sarah Henderson","Roman Ring","Susannah Young","Eliza Rutherford","Tom Hennigan","Jacob Menick","Albin Cassirer","Richard Powell","George van den Driessche","Lisa Anne Hendricks","M. Rauh","Po-Sen Huang","A. Glaese","Johannes Welbl","Sumanth Dathathri","Saffron Huang","J. Uesato","John F. J. Mellor","I. Higgins","Antonia Creswell","Nathan McAleese","Amy Wu","Erich Elsen","Siddhant M. Jayakumar","Elena Buchatskaya","D. Budden","Esme Sutherland","K. Simonyan","Michela Paganini","L. Sifre","Lena Martens","Xiang Lorraine Li","A. Kuncoro","Aida Nematzadeh","E. Gribovskaya","Domenic Donato","Angeliki Lazaridou","A. Mensch","Jean-Baptiste Lespiau","Maria Tsimpoukelli","N. Grigorev","Doug Fritz","Thibault Sottiaux","Mantas Pajarskas","Tobias Pohlen","Z. Gong","Daniel Toyama","Cyprien de Masson d'Autume","Yujia Li","Tayfun Terzi","Vladimir Mikulik","Igor Babuschkin","Aidan Clark","Diego de Las Casas","Aurelia Guy","Chris Jones","James Bradbury","Matthew G. Johnson","Blake A. Hechtman","Laura Weidinger","Iason Gabriel","William S. Isaac","Edward Lockhart","Simon Osindero","Laura Rimell","Chris Dyer","O. Vinyals","Kareem W. Ayoub","Jeff Stanway","L. Bennett","D. Hassabis","K. Kavukcuoglu","G. Irving"],"4509":["Canwen Xu","Daya Guo","Nan Duan","Julian McAuley"],"4510":["Junnan Li","Ramprasaath R. Selvaraju","Akhilesh Deepak Gotmare","Shafiq R. Joty","Caiming Xiong","S. Hoi"],"4511":["Kalpesh Krishna","Yapei Chang","J. Wieting","Mohit Iyyer"],"4512":["Fu-Ming Guo","Sijia Liu","F. Mungall","Xue Lin","Yanzhi Wang"],"4513":["F\u00e1bio Perez","Ian Ribeiro"],"4514":["Yifei Li","Zeqi Lin","Shizhuo Zhang","Qiang Fu","Bei Chen","Jian-Guang Lou","Weizhu Chen"],"4515":["Jacob Menick","Maja Trebacz","Vladimir Mikulik","John Aslanides","Francis Song","Martin Chadwick","Mia Glaese","Susannah Young","Lucy Campbell-Gillingham","G. Irving","Nathan McAleese"],"4516":["Or Honovich","Thomas Scialom","Omer Levy","Timo Schick"],"4517":["Freda Shi","Mirac Suzgun","Markus Freitag","Xuezhi Wang","Suraj Srivats","Soroush Vosoughi","Hyung Won Chung","Yi Tay","Sebastian Ruder","Denny Zhou","Dipanjan Das","Jason Wei"],"4518":["Xiujun Li","Xi Yin","Chunyuan Li","Xiaowei Hu","Pengchuan Zhang","Lei Zhang","Lijuan Wang","Houdong Hu","Li Dong","Furu Wei","Yejin Choi","Jianfeng Gao"],"4519":["Aditya Siddhant","Zachary Chase Lipton"],"4520":["William Chan","N. Jaitly","Quoc V. Le","O. Vinyals"],"4521":["Yixuan Su","Tian Lan","Huayang Li","Jialu Xu","Yan Wang","Deng Cai"],"4522":["Lucie Charlotte Magister","Jonathan Mallinson","Jakub Adamek","Eric Malmi","Aliaksei Severyn"],"4523":["Timo Schick","Hinrich Sch\u00fctze"],"4524":["Nan Du","Yanping Huang","Andrew M. Dai","Simon Tong","Dmitry Lepikhin","Yuanzhong Xu","M. Krikun","Yanqi Zhou","Adams Wei Yu","Orhan Firat","Barret Zoph","Liam Fedus","Maarten Bosma","Zongwei Zhou","Tao Wang","Yu Emma Wang","Kellie Webster","Marie Pellat","Kevin Robinson","K. Meier-Hellstern","Toju Duke","Lucas Dixon","Kun Zhang","Quoc V. Le","Yonghui Wu","Z. Chen","Claire Cui"],"4525":["Tim Althoff","Kevin Clark","J. Leskovec"],"4526":["Michihiro Yasunaga","Hongyu Ren","Antoine Bosselut","Percy Liang","J. Leskovec"],"4527":["Amanda Askell","Yuntao Bai","Anna Chen","Dawn Drain","Deep Ganguli","T. Henighan","Andy Jones","Nicholas Joseph","Benjamin Mann","Nova DasSarma","Nelson Elhage","Zac Hatfield-Dodds","Danny Hernandez","John Kernion","Kamal Ndousse","Catherine Olsson","Dario Amodei","Tom B. Brown","Jack Clark","Sam McCandlish","C. Olah","Jared Kaplan"],"4528":["Noam M. Shazeer","Azalia Mirhoseini","Krzysztof Maziarz","Andy Davis","Quoc V. Le","Geoffrey E. Hinton","J. Dean"],"4529":["Lewei Yao","Runhu Huang","Lu Hou","Guansong Lu","Minzhe Niu","Hang Xu","Xiaodan Liang","Zhenguo Li","Xin Jiang","Chunjing Xu"],"4530":["M. Lewis","Shruti Bhosale","Tim Dettmers","Naman Goyal","Luke Zettlemoyer"],"4531":["Zexuan Zhong","Tao Lei","Danqi Chen"],"4532":["James Caverlee","Steve Webb"],"4533":["Xiaodong Liu","Pengcheng He","Weizhu Chen","Jianfeng Gao"],"4534":["Anjuli Kannan","A. Datta","Tara N. Sainath","Eugene Weinstein","B. Ramabhadran","Yonghui Wu","Ankur Bapna","Z. Chen","Seungjin Lee"],"4535":["Alexander Wettig","Tianyu Gao","Zexuan Zhong","Danqi Chen"],"4536":["Erik Nijkamp","Jeffrey A. Ruffolo","Eli N. Weinstein","N. Naik","Ali Madani"],"4537":["Chenliang Li","Haiyang Xu","Junfeng Tian","Wei Wang","Ming Yan","Bin Bi","Jiabo Ye","Hehong Chen","Guohai Xu","Zheng-da Cao","Ji Zhang","Songfang Huang","Feiran Huang","Jingren Zhou","Luo Si"],"4538":["Jan Deriu","Aur\u00e9lien Lucchi","V. D. Luca","Aliaksei Severyn","Simon M\u00fcller","Mark Cieliebak","Thomas Hofmann","Martin Jaggi"],"4539":["Yongming Rao","Wenliang Zhao","Guangyi Chen","Yansong Tang","Zheng Zhu","Guan Huang","Jie Zhou","Jiwen Lu"],"4540":["Moin Nadeem","Anna Bethke","Siva Reddy"],"4541":["William Chan","Daniel S. Park","Chris Lee","Yu Zhang","Quoc V. Le","Mohammad Norouzi"],"4542":["Wenhui Wang","Hangbo Bao","Li Dong","Furu Wei"],"4543":["Rohan Anil","Badih Ghazi","Vineet Gupta","Ravi Kumar","Pasin Manurangsi"],"4544":["Zhengyan Zhang","Xu Han","Zhiyuan Liu","Xin Jiang","Maosong Sun","Qun Liu"],"4545":["H. Soltau","H. Liao","H. Sak"],"4546":["D. Hewlett","Alexandre Lacoste","Llion Jones","Illia Polosukhin","Andrew Fandrianto","Jay Han","Matthew Kelcey","David Berthelot"],"4547":["Sumanth Dathathri","Andrea Madotto","Janice Lan","Jane Hung","Eric Frank","Piero Molino","J. Yosinski","Rosanne Liu"],"4548":["Radim Rehurek","Petr Sojka"],"4549":["Dzmitry Bahdanau","J. Chorowski","Dmitriy Serdyuk","Philemon Brakel","Yoshua Bengio"],"4550":["Kaitao Song","Xu Tan","Tao Qin","Jianfeng Lu","Tie-Yan Liu"],"4551":["Linjie Li","Yen-Chun Chen","Yu Cheng","Zhe Gan","Licheng Yu","Jingjing Liu"],"4552":["Yang Liu","Chengjie Sun","Lei Lin","Xiaolong Wang"],"4553":["Yao-Hung Hubert Tsai","Shaojie Bai","P. Liang","J. Z. Kolter","Louis-Philippe Morency","R. Salakhutdinov"],"4554":["Irene Solaiman","Miles Brundage","Jack Clark","Amanda Askell","Ariel Herbert-Voss","Jeff Wu","Alec Radford","Jasmine Wang"],"4555":["Yixin Nie","Adina Williams","Emily Dinan","Mohit Bansal","J. Weston","Douwe Kiela"],"4556":["Zhuyun Dai","Jamie Callan"],"4557":["Luowei Zhou","H. Palangi","Lei Zhang","Houdong Hu","Jason J. Corso","Jianfeng Gao"],"4558":["Weijie Liu","Peng Zhou","Zhe Zhao","Zhiruo Wang","Qi Ju","Haotang Deng","Ping Wang"],"4559":["Ningyu Zhang","Luoqiu Li","Xiang Chen","Shumin Deng","Zhen Bi","Chuanqi Tan","Fei Huang","Huajun Chen"],"4560":["Suchin Gururangan","Swabha Swayamdipta","Omer Levy","Roy Schwartz","Samuel R. Bowman","Noah A. Smith"],"4561":["Chen Ju","Tengda Han","Kunhao Zheng","Ya Zhang","Weidi Xie"],"4562":["Di Qi","Lin Su","Jianwei Song","Edward Cui","Taroon Bharti","Arun Sacheti"],"4563":["Johannes Welbl","A. Glaese","J. Uesato","Sumanth Dathathri","John F. J. Mellor","Lisa Anne Hendricks","Kirsty Anderson","Pushmeet Kohli","Ben Coppin","Po-Sen Huang"],"4564":["Cewu Lu","Ranjay Krishna","Michael S. Bernstein","Li Fei-Fei"],"4565":["Qian Chen","Xiao-Dan Zhu","Zhenhua Ling","Si Wei","Hui Jiang","D. Inkpen"],"4566":["H. Sak","A. Senior","F. Beaufays"],"4567":["Yuan Yao","Ao Zhang","Zhengyan Zhang","Zhiyuan Liu","Tat-seng Chua","Maosong Sun"],"4568":["Bo Li","Ruoming Pang","Tara N. Sainath","Anmol Gulati","Yu Zhang","James Qin","Parisa Haghani","W. R. Huang","Min Ma","Junwen Bai"],"4569":["Tommaso Caselli","Valerio Basile","Jelena Mitrovi\u0107","M. Granitzer"],"4570":["Dat Quoc Nguyen","A. Nguyen"],"4571":["Haoming Jiang","Pengcheng He","Weizhu Chen","Xiaodong Liu","Jianfeng Gao","T. Zhao"],"4572":["Jiasen Lu","Vedanuj Goswami","Marcus Rohrbach","Devi Parikh","Stefan Lee"],"4573":["Prakhar Ganesh","Yao Chen","Xin Lou","Mohammad Ali Khan","Y. Yang","Deming Chen","M. Winslett","Hassan Sajjad","Preslav Nakov"],"4574":["Chen Zhu","Yu Cheng","Zhe Gan","S. Sun","T. Goldstein","Jingjing Liu"],"4575":["R. J\u00f3zefowicz","O. Vinyals","M. Schuster","Noam M. Shazeer","Yonghui Wu"],"4576":["Ruiqi Zhong","Kristy Lee","Zheng Zhang","D. Klein"],"4577":["Elizabet Spaepen","Marie Coppola","E. Spelke","S. Carey","S. Goldin\u2010Meadow"],"4578":["Dani Yogatama","Cyprien de Masson d'Autume","Lingpeng Kong"],"4579":["Xi Victoria Lin","Todor Mihaylov","Mikel Artetxe","Tianlu Wang","Shuohui Chen","Daniel Simig","Myle Ott","Naman Goyal","Shruti Bhosale","Jingfei Du","Ramakanth Pasunuru","Sam Shleifer","Punit Singh Koura","Vishrav Chaudhary","Brian O'Horo","Jeff Wang","Luke Zettlemoyer","Zornitsa Kozareva","Mona T. Diab","Ves Stoyanov","Xian Li"],"4580":["Naman Goyal","Jingfei Du","Myle Ott","Giridhar Anantharaman","Alexis Conneau"],"4581":["Sebastian Ruder","Matthew E. Peters","Swabha Swayamdipta","Thomas Wolf"],"4582":["Woojeong Jin","Yu Cheng","Yelong Shen","Weizhu Chen","Xiang Ren"],"4583":["Junxian He","Graham Neubig","Taylor Berg-Kirkpatrick"],"4584":["Zheng Yuan","Yijia Liu","Chuanqi Tan","Songfang Huang","Fei Huang"],"4585":["O. Vinyals","Lukasz Kaiser","Terry Koo","Slav Petrov","I. Sutskever","Geoffrey E. Hinton"],"4586":["Fabio Petroni","Patrick Lewis","Aleksandra Piktus","Tim Rockt\u00e4schel","Yuxiang Wu","Alexander H. Miller","Sebastian Riedel"],"4587":["Nadav Brandes","Dan Ofer","Yam Peleg","Nadav Rappoport","M. Linial"],"4588":["Rebecca Marvin","Tal Linzen"],"4589":["Ashish Vaswani","Yinggong Zhao","Victoria Fossum","David Chiang"],"4590":["Mor Geva","Ankit Gupta","Jonathan Berant"],"4591":["Daxiang Dong","Hua Wu","W. He","Dianhai Yu","Haifeng Wang"],"4592":["Hao Tan","Mohit Bansal"],"4593":["Benjamin Muller","Antonis Anastasopoulos","Beno\u00eet Sagot","Djam\u00e9 Seddah"],"4594":["S. Sun","Yu Cheng","Zhe Gan","Jingjing Liu"],"4595":["Tanja Bunk","Daksh Varshneya","Vladimir Vlasov","Alan Nichol"],"4596":["Patrick Lewis","Myle Ott","Jingfei Du","Veselin Stoyanov"],"4597":["Libo Qin","Wanxiang Che","Yangming Li","Haoyang Wen","Ting Liu"],"4598":["Christopher Olston","B. Reed","U. Srivastava","Ravi Kumar","A. Tomkins"],"4599":["O. Vinyals","Quoc V. Le"],"4600":["Pierre Lison","J. Tiedemann","Milen Kouylekov"],"4601":["Rami Al-Rfou","Dokook Choe","Noah Constant","Mandy Guo","Llion Jones"],"4602":["Trevor Strohman","Donald Metzler","Howard R. Turtle","W. B. Croft"],"4603":["K. Deschacht","Marie-Francine Moens"],"4604":["Colin B. Clement","Dawn Drain","Jonathan Timcheck","Alexey Svyatkovskiy","Neel Sundaresan"],"4605":["Denis Filimonov","M. Harper"],"4606":["Veselin Raychev","Martin T. Vechev","Eran Yahav"],"4607":["Po-Sen Huang","Huan Zhang","Ray Jiang","Robert Stanforth","Johannes Welbl","Jack W. Rae","Vishal Maini","Dani Yogatama","Pushmeet Kohli"],"4608":["Jonathan Ragan-Kelley","Connelly Barnes","Andrew Adams","Sylvain Paris","F. Durand","Saman P. Amarasinghe"],"4609":["H. B. McMahan","Eider Moore","Daniel Ramage","B. A. Y. Arcas"],"4610":["M. Sundermeyer","H. Ney","R. Schl\u00fcter"],"4611":["Jonas Pfeiffer","Ivan Vulic","Iryna Gurevych","Sebastian Ruder"],"4612":["Wissam Antoun","Fady Baly","Hazem M. Hajj"],"4613":["Urs-Viktor Marti","H. Bunke"],"4614":["Alexandra Chronopoulou","Christos Baziotis","A. Potamianos"],"4615":["Hilde Kuehne","A. B. Arslan","Thomas Serre"],"4616":["Richard Futrell","Ethan Gotlieb Wilcox","Takashi Morita","Peng Qian","Miguel Ballesteros","R. Levy"],"4617":["Young-suk Lee","K. Papineni","S. Roukos","O. Emam","Hany Hassan"],"4618":["B. Zhao","Matthias Eck","S. Vogel"],"4619":["Ahmad Emami","F. Jelinek"],"4620":["Lili Mou","Rui Men","Ge Li","Yan Xu","Lu Zhang","Rui Yan","Zhi Jin"],"4621":["Tomas Mikolov","Anoop Deoras","Daniel Povey","L. Burget","J. \u010cernock\u00fd"],"4622":["Wissam Antoun","Fady Baly","Hazem M. Hajj"],"4623":["Chenguang Wang","Mu Li","Alex Smola"],"4624":["Mario Giulianelli","J. Harding","Florian Mohnert","D. Hupkes","Willem H. Zuidema"],"4625":["Yishu Miao","Phil Blunsom"],"4626":["David E. Gay","P. Levis","R. von Behren","M. Welsh","E. Brewer","D. Culler"],"4627":["Marc'Aurelio Ranzato","Arthur Szlam","Joan Bruna","Micha\u00ebl Mathieu","Ronan Collobert","S. Chopra"],"4628":["Wietse de Vries","Andreas van Cranenburgh","Arianna Bisazza","Tommaso Caselli","Gertjan van Noord","M. Nissim"],"4629":["Maximilian Nickel","Douwe Kiela"],"4630":["Mingqing Chen","A. Suresh","Rajiv Mathews","Adeline Wong","Cyril Allauzen","F. Beaufays","M. Riley"],"4631":["Qizhe Xie","Guokun Lai","Zihang Dai","E. Hovy"],"4632":["Haotian Liu","Chunyuan Li","Qingyang Wu","Yong Jae Lee"],"4633":["Ciprian Chelba","Daniel M. Bikel","Maria Shugrina","Patrick Nguyen","Shankar Kumar"],"4634":["Siva Reddy","Mirella Lapata","Mark Steedman"],"4635":["K. Beyer","V. Ercegovac","Rainer Gemulla","Andrey Balmin","M. Eltabakh","C. Kanne","Fatma \u00d6zcan","E. Shekita"],"4636":["Adam Pauls","D. Klein"],"4637":["Jan A. Botha","Phil Blunsom"],"4638":["Jen-Tzung Chien","Y. Ku"],"4639":["Matthew E. Peters","Mark Neumann","Mohit Iyyer","Matt Gardner","Christopher Clark","Kenton Lee","Luke Zettlemoyer"],"4640":["Sewon Min","Xinxi Lyu","Ari Holtzman","Mikel Artetxe","M. Lewis","Hannaneh Hajishirzi","Luke Zettlemoyer"],"4641":["Menglin Jia","Luming Tang","Bor-Chun Chen","Claire Cardie","S. Belongie","Bharath Hariharan","S. Lim"],"4642":["C. Buck","Kenneth Heafield","B. V. Ooyen"],"4643":["J. Park","Joseph C. O'Brien","Carrie J. Cai","M. Morris","Percy Liang","Michael S. Bernstein"],"4644":["Xiang Lisa Li","Percy Liang"],"4645":["Jiahui Yu","Yuanzhong Xu","Jing Yu Koh","Thang Luong","Gunjan Baid","Zirui Wang","Vijay Vasudevan","Alexander Ku","Yinfei Yang","Burcu Karagol Ayan","B. Hutchinson","Wei Han","Zarana Parekh","Xin Li","Han Zhang","Jason Baldridge","Yonghui Wu"],"4646":["H. Liao","G. Pundak","O. Siohan","M. K. Carroll","N. Coccaro","Q. Jiang","Tara N. Sainath","A. Senior","F. Beaufays","M. Bacchiani"],"4647":["Baptiste Rozi\u00e8re","Jonas Gehring","Fabian Gloeckle","Sten Sootla","Itai Gat","Xiaoqing Tan","Yossi Adi","Jingyu Liu","Tal Remez","J. Rapin","Artyom Kozhevnikov","I. Evtimov","Joanna Bitton","Manish P Bhatt","Cristian Cant\u00f3n Ferrer","Aaron Grattafiori","Wenhan Xiong","Alexandre D'efossez","Jade Copet","F. Azhar","Hugo Touvron","Louis Martin","Nicolas Usunier","Thomas Scialom","Gabriel Synnaeve"],"4648":["A. Karpathy","Li Fei-Fei"],"4649":["Yang Liu","Dan Iter","Yichong Xu","Shuo Wang","Ruochen Xu","Chenguang Zhu"],"4650":["Victor Sanh","Albert Webson","Colin Raffel","Stephen H. Bach","Lintang Sutawika","Zaid Alyafeai","Antoine Chaffin","Arnaud Stiegler","Teven Le Scao","Arun Raja","Manan Dey","M Saiful Bari","Canwen Xu","Urmish Thakker","S. Sharma","Eliza Szczechla","Taewoon Kim","Gunjan Chhablani","Nihal V. Nayak","Debajyoti Datta","Jonathan D. Chang","Mike Tian-Jian Jiang","Han Wang","Matteo Manica","Sheng Shen","Zheng-Xin Yong","Harshit Pandey","Rachel Bawden","Thomas Wang","Trishala Neeraj","Jos Rozen","Abheesht Sharma","Andrea Santilli","Thibault F\u00e9vry","Jason Alan Fries","Ryan Teehan","Stella Biderman","Leo Gao","T. Bers","Thomas Wolf","Alexander M. Rush"],"4651":["Tim Brooks","Aleksander Holynski","Alexei A. Efros"],"4652":["Niklas Muennighoff","Thomas Wang","Lintang Sutawika","Adam Roberts","Stella Biderman","Teven Le Scao","M Saiful Bari","Sheng Shen","Zheng-Xin Yong","Hailey Schoelkopf","Xiangru Tang","Dragomir R. Radev","Alham Fikri Aji","Khalid Almubarak","Samuel Albanie","Zaid Alyafeai","Albert Webson","Edward Raff","Colin Raffel"],"4653":["Raymond Li","Loubna Ben Allal","Yangtian Zi","Niklas Muennighoff","Denis Kocetkov","Chenghao Mou","Marc Marone","Christopher Akiki","Jia Li","Jenny Chim","Qian Liu","Evgenii Zheltonozhskii","Terry Yue Zhuo","Thomas Wang","Olivier Dehaene","Mishig Davaadorj","J. Lamy-Poirier","Jo\u00e3o Monteiro","Oleh Shliazhko","Nicolas Gontier","Nicholas Meade","Armel Zebaze","Ming-Ho Yee","Logesh Kumar Umapathi","Jian Zhu","Benjamin Lipkin","Muhtasham Oblokulov","Zhiruo Wang","Rudra Murthy","J. Stillerman","S. Patel","Dmitry Abulkhanov","Marco Zocca","Manan Dey","Zhihan Zhang","N. Fahmy","Urvashi Bhattacharyya","W. Yu","Swayam Singh","Sasha Luccioni","Paulo Villegas","M. Kunakov","Fedor Zhdanov","Manuel Romero","Tony Lee","Nadav Timor","Jennifer Ding","Claire Schlesinger","Hailey Schoelkopf","Jana Ebert","Tri Dao","Mayank Mishra","A. Gu","Jennifer Robinson","Carolyn Jane Anderson","Brendan Dolan-Gavitt","Danish Contractor","Siva Reddy","Daniel Fried","Dzmitry Bahdanau","Yacine Jernite","Carlos Mu\u00f1oz Ferrandis","Sean M. Hughes","Thomas Wolf","Arjun Guha","Leandro von Werra","Harm de Vries"],"4654":["Chunting Zhou","Pengfei Liu","Puxin Xu","Srini Iyer","Jiao Sun","Yuning Mao","Xuezhe Ma","Avia Efrat","Ping Yu","L. Yu","Susan Zhang","Gargi Ghosh","M. Lewis","Luke Zettlemoyer","Omer Levy"],"4655":["K. Vertanen","P. Kristensson"],"4656":["Ce Zhou","Qian Li","Chen Li","Jun Yu","Yixin Liu","Guan Wang","Kaichao Zhang","Cheng Ji","Qi Yan","Lifang He","Hao Peng","Jianxin Li","Jia Wu","Ziwei Liu","P. Xie","Caiming Xiong","Jian Pei","Philip S. Yu","Lichao Sun Michigan State University","B. University","Lehigh University","M. University","Nanyang Technological University","University of California at San Diego","D. University","U. Chicago","S. Research"],"4657":["D. Gerz","Ivan Vulic","E. Ponti","Roi Reichart","A. Korhonen"],"4658":["Barret Zoph","Ashish Vaswani","Jonathan May","Kevin Knight"],"4659":["Hugo Laurenccon","Lucile Saulnier","Thomas Wang","Christopher Akiki","Albert Villanova del Moral","Teven Le Scao","Leandro von Werra","Chenghao Mou","E. G. Ponferrada","Huu Nguyen","Jorg Frohberg","Mario vSavsko","Quentin Lhoest","Angelina McMillan-Major","G\u00e9rard Dupont","Stella Biderman","Anna Rogers","Loubna Ben Allal","F. Toni","Giada Pistilli","Olivier Nguyen","So-maieh Nikpoor","Maraim Masoud","Pierre Colombo","Javier de la Rosa","Paulo Villegas","Tristan Thrush","S. Longpre","Sebastian Nagel","Leon Weber","M. Mu\u00f1oz","Jian Zhu","Daniel Alexander van Strien","Zaid Alyafeai","Khalid Almubarak","Minh Chien Vu","Itziar Gonzalez-Dios","Aitor Soroa Etxabe","Kyle Lo","Manan Dey","Pedro Ortiz Suarez","Aaron Gokaslan","Shamik Bose","David Ifeoluwa Adelani","Long Phan","H. Tran","I. Yu","S. Pai","Jenny Chim","Violette Lepercq","Suzana Ilic","Margaret Mitchell","Sasha Luccioni","Yacine Jernite"],"4660":["Vinodkumar Prabhakaran","B. Hutchinson","Margaret Mitchell"],"4661":["Ze Liu","Yutong Lin","Yue Cao","Han Hu","Yixuan Wei","Zheng Zhang","Stephen Lin","B. Guo"],"4662":["Jinze Bai","Shuai Bai","Yunfei Chu","Zeyu Cui","Kai Dang","Xiaodong Deng","Yang Fan","Wenhang Ge","Yu Han","Fei Huang","Binyuan Hui","Luo Ji","Mei Li","Junyang Lin","Runji Lin","Dayiheng Liu","Gao Liu","Chengqiang Lu","K. Lu","Jianxin Ma","Rui Men","Xingzhang Ren","Xuancheng Ren","Chuanqi Tan","Sinan Tan","Jianhong Tu","Peng Wang","Shijie Wang","Wei Wang","Shengguang Wu","Benfeng Xu","Jin Xu","An Yang","Hao Yang","Jian Yang","Jian Yang","Shusheng Yang","Shusheng Yang","Bowen Yu","Yu Bowen","Hongyi Yuan","Zheng Yuan","Jianwei Zhang","Xing Zhang","Yichang Zhang","Zhenru Zhang","Chang Zhou","Jingren Zhou","Xiaohuan Zhou","Tianhang Zhu"],"4663":["N. Locascio","Karthik Narasimhan","E. DeLeon","Nate Kushman","R. Barzilay"],"4664":["Veselin Raychev","Pavol Bielik","Martin T. Vechev"],"4665":["Miltiadis Allamanis","Charles Sutton"],"4666":["A. Genkin","D. Lewis","D. Madigan"],"4667":["Kevin Clark","Urvashi Khandelwal","Omer Levy","Christopher D. Manning"],"4668":["Telmo Pires","Eva Schlinger","Dan Garrette"],"4669":["Brian Lester","Rami Al-Rfou","Noah Constant"],"4670":["Yuan Yu","M. Isard","Dennis Fetterly","M. Budiu","\u00da. Erlingsson","P. Gunda","J. Currey"],"4671":["Renqian Luo","Liai Sun","Yingce Xia","Tao Qin","Sheng Zhang","Hoifung Poon","Tie-Yan Liu"],"4672":["Marcello Federico","N. Bertoldi","M. Cettolo"],"4673":["Jianmo Ni","Jiacheng Li","Julian McAuley"],"4674":["Syed Waqas Zamir","Aditya Arora","Salman Hameed Khan","Munawar Hayat","F. Khan","Ming-Hsuan Yang"],"4675":["J. Schemmel","Daniel Br\u00fcderle","Andreas Gr\u00fcbl","Matthias Hock","K. Meier","S. Millner"],"4676":["Nelson F. Liu","Matt Gardner","Yonatan Belinkov","Matthew E. Peters","Noah A. Smith"],"4677":["Pengcheng He","Jianfeng Gao","Weizhu Chen"],"4678":["S. Reed","Konrad Zolna","Emilio Parisotto","Sergio Gomez Colmenarejo","Alexander Novikov","Gabriel Barth-Maron","Mai Gimenez","Yury Sulsky","Jackie Kay","J. T. Springenberg","Tom Eccles","Jake Bruce","Ali Razavi","Ashley D. Edwards","N. Heess","Yutian Chen","R. Hadsell","O. Vinyals","Mahyar Bordbar","Nando de Freitas"],"4679":["Erik Nijkamp","Bo Pang","Hiroaki Hayashi","Lifu Tu","Haiquan Wang","Yingbo Zhou","S. Savarese","Caiming Xiong"],"4680":["E. Zelikman","Yuhuai Wu","Noah D. Goodman"],"4681":["H. B. McMahan","Eider Moore","Daniel Ramage","S. Hampson","B. A. Y. Arcas"],"4682":["Piotr Bojanowski","Edouard Grave","Armand Joulin","Tomas Mikolov"],"4683":["Yao Lu","Max Bartolo","Alastair Moore","S. Riedel","Pontus Stenetorp"],"4684":["Kevin Meng","Arnab Sharma","A. Andonian","Yonatan Belinkov","David Bau"],"4685":["Marcel Binz","Eric Schulz"],"4686":["Pengcheng He","Xiaodong Liu","Jianfeng Gao","Weizhu Chen"],"4687":["Patrick Lewis","Ethan Perez","Aleksandara Piktus","Fabio Petroni","Vladimir Karpukhin","Naman Goyal","Heinrich Kuttler","M. Lewis","Wen-tau Yih","Tim Rockt\u00e4schel","Sebastian Riedel","Douwe Kiela"],"4688":["Ron Mokady","Amir Hertz"],"4689":["Hao Hao Tan","Mohit Bansal"],"4690":["Philipp Koehn","F. Och","Daniel Marcu"],"4691":["Stephen Merity","Caiming Xiong","James Bradbury","R. Socher"],"4692":["Iulian Serban","Alessandro Sordoni","Yoshua Bengio","Aaron C. Courville","Joelle Pineau"],"4693":["Sainbayar Sukhbaatar","Arthur Szlam","J. Weston","R. Fergus"],"4694":["Zachary Chase Lipton"],"4695":["M. Bronstein","Joan Bruna","Yann LeCun","Arthur Szlam","P. Vandergheynst"],"4696":["Guillaume Lample","Ludovic Denoyer","Marc'Aurelio Ranzato"],"4697":["Yi Tay","Mostafa Dehghani","Dara Bahri","Donald Metzler"],"4698":["Nisan Stiennon","Long Ouyang","Jeff Wu","Daniel M. Ziegler","Ryan J. Lowe","Chelsea Voss","Alec Radford","Dario Amodei","Paul Christiano"],"4699":["Ikuya Yamada","Akari Asai","Hiroyuki Shindo","Hideaki Takeda","Yuji Matsumoto"],"4700":["W. Saunders","Catherine Yeh","Jeff Wu","Steven Bills","Ouyang Long","Jonathan Ward","J. Leike"],"4701":["Wenhui Wang","Furu Wei","Li Dong","Hangbo Bao","Nan Yang","Ming Zhou"],"4702":["Yizhong Wang","Swaroop Mishra","Pegah Alipoormolabashi","Yeganeh Kordi","Amirreza Mirzaei","Anjana Arunkumar","Arjun Ashok","Arut Selvan Dhanasekaran","Atharva Naik","David Stap","Eshaan Pathak","Giannis Karamanolakis","Haizhi Gary Lai","I. Purohit","Ishani Mondal","Jacob Anderson","Kirby Kuznia","Krima Doshi","Maitreya Patel","Kuntal Kumar Pal","M. Moradshahi","Mihir Parmar","Mirali Purohit","Neeraj Varshney","Phani Rohitha Kaza","Pulkit Verma","Ravsehaj Singh Puri","Rushang Karia","Shailaja Keyur Sampat","Savan Doshi","S. Mishra","Sujan Reddy","Sumanta Patro","Tanay Dixit","Xudong Shen","Chitta Baral","Yejin Choi","Noah A. Smith","Hannaneh Hajishirzi","Daniel Khashabi"],"4703":["Jeff Rasley","Samyam Rajbhandari","Olatunji Ruwase","Yuxiong He"],"4704":["Kobus Barnard","P. D. Sahin","D. Forsyth","Nando de Freitas","D. Blei","Michael I. Jordan"],"4705":["Angela Fan","Shruti Bhosale","Holger Schwenk","Zhiyi Ma","Ahmed El-Kishky","Siddharth Goyal","Mandeep Baines","Onur \u00c7elebi","Guillaume Wenzek","Vishrav Chaudhary","Naman Goyal","Tom Birch","Vitaliy Liptchinsky","Sergey Edunov","Edouard Grave","Michael Auli","Armand Joulin"],"4706":["Long Duong","Antonios Anastasopoulos","David Chiang","Steven Bird","Trevor Cohn"],"4707":["Hai Wang","Dian Yu","Kai Sun","Jianshu Chen","Dong Yu"],"4708":["Sewon Min","M. Lewis","Luke Zettlemoyer","Hannaneh Hajishirzi"],"4709":["Bryan McCann","James Bradbury","Caiming Xiong","R. Socher"],"4710":["Guillaume Lample","Myle Ott","Alexis Conneau","Ludovic Denoyer","Marc'Aurelio Ranzato"],"4711":["Charles Sutton","A. McCallum"],"4712":["Marwin H. S. Segler","T. Kogej","C. Tyrchan","M. Waller"],"4713":["Amit Goyal","Hal Daum\u00e9","Suresh Venkatasubramanian"],"4714":["Barret Zoph","Deniz Yuret","Jonathan May","Kevin Knight"],"4715":["Jiayuan Mao","Chuang Gan","Pushmeet Kohli","J. Tenenbaum","Jiajun Wu"],"4716":["J. Weston","Antoine Bordes","Oksana Yakhnenko","Nicolas Usunier"],"4717":["Hong Yu","V. Hatzivassiloglou"],"4718":["Arun Babu","Changhan Wang","Andros Tjandra","Kushal Lakhotia","Qiantong Xu","Naman Goyal","Kritika Singh","Patrick von Platen","Yatharth Saraf","J. Pino","Alexei Baevski","Alexis Conneau","Michael Auli"],"4719":["R. Lebret","David Grangier","Michael Auli"],"4720":["Han Fang","Pengfei Xiong","Luhui Xu","Yu Chen"],"4721":["Armen Aghajanyan","Anchit Gupta","Akshat Shrivastava","Xilun Chen","Luke Zettlemoyer","Sonal Gupta"],"4722":["Jakob Uszkoreit","T. Brants"],"4723":["Timo Schick","Sahana Udupa","Hinrich Sch\u00fctze"],"4724":["Xiaobo Liang","Lijun Wu","Juntao Li","Yue Wang","Qi Meng","Tao Qin","Wei Chen","M. Zhang","Tie-Yan Liu"],"4725":["Joe Davison"],"4726":["X. Lei","A. Senior","A. Gruenstein","Jeffrey Scott Sorensen"],"4727":["Shaojie Bai","J. Z. Kolter","V. Koltun"],"4728":["R. Chaiken","Bob Jenkins","P. Larson","Bill Ramsey","Darren Shakib","S. Weaver","Jingren Zhou"],"4729":["Chunli Wang","S. Shan","Wen Gao"],"4730":["Kun Liu","Wu-Jun Li","M. Guo"],"4731":["P. Brown"],"4732":["Holger Schwenk","J. Gauvain"],"4733":["Holger Schwenk","J. Gauvain"],"4734":["Iulia Turc","Ming-Wei Chang","Kenton Lee","Kristina Toutanova"],"4735":["Jiacheng Liu","Alisa Liu","Ximing Lu","S. Welleck","Peter West","Ronan Le Bras","Yejin Choi","Hannaneh Hajishirzi"],"4736":["Jonas Pfeiffer","Andreas R\u00fcckl\u00e9","Clifton A. Poth","Aishwarya Kamath","Ivan Vulic","Sebastian Ruder","Kyunghyun Cho","Iryna Gurevych"],"4737":["Akash Bharadwaj","David R. Mortensen","Chris Dyer","J. Carbonell"],"4738":["Ahmad Emami","K. Papineni","Jeffrey Scott Sorensen"],"4739":["Qian Chen","Zhu Zhuo","Wen Wang"],"4740":["M. Snover","B. Dorr","R. Schwartz"],"4741":["Joshua Feldman","Joe Davison","Alexander M. Rush"]},"abstract":{"0":"Artificial Intelligence is one of the emerging technologies in the field of agriculture which tries to simulate human reasoning in intelligent systems. It is making a revolution in agriculture by replacing inefficient traditional methods with more efficient AI based methods. AI is used in agriculture in various ways such as automation, robots, drones, soil and crop monitoring, and predictive analytics. This paper provides various applications of AI tools in agriculture. Matthew N. O. Sadiku | Sarhan M. Musa | Abayomi Ajayi-Majebi \u00c4rtificial Intelligence in Agriculture\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-2 , February 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38513.pdf Paper Url: https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/38513\/artificial-intelligence-in-agriculture\/matthew-n-o-sadiku","1":"this article demonstrate Disadvantage of artificial intelligence to a different field as well as benefits of artificial intelligence. Research to verify that's artificial intelligence is beneficial if its having risks aspect then it's also having advantage and it is safe. Influences virtual physiological state which guide behavior and learning, modulating the emotion that our artificial human \"\"feel\"\" and express. At the aspect of risks factor it may reach to the human level intelligence n may dominate to the human being. Seeta M. Chauhan \"\u00c4rtificial Intelligence: Benefit and Risks\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-2 , February 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd30232.pdf\r\n\r\nPaper Url : https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/30232\/artificial-intelligence-benefit-and-risks\/seeta-m-chauhan","2":"With increased competitiveness in power generation industries, more resources are directed in optimizing plant operation, including fault detection and diagnosis. One of the most powerful tools in faults detection and diagnosis is artificial intelligence AI . Faults should be detected early so correct mitigation measures can be taken, whilst false alarms should be eschewed to avoid unnecessary interruption and downtime. For the last few decades there has been major interest towards intelligent condition monitoring system ICMS application in power plant especially with AI development particularly in artificial neural network ANN . ANN is based on quite simple principles, but takes advantage of their mathematical nature, non linear iteration to demonstrate powerful problem solving ability. With massive possibility and room for improvement in AI, the inspiration for researching them are apparent, and literally, hundreds of papers have been published, discussing the findings of hybrid AI for condition monitoring purposes. In this paper, the studies of ANN and fuzzy logic application will be presented. P. Naveen | S. Nikitha | P. Sudeesh | V. Vaishnavi \u00c4rtificial Intelligence in Power Station\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-1 , December 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd29784.pdf  Paper URL: https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/29784\/artificial-intelligence-in-power-station\/p-naveen","3":"Training and placement cell in colleges is to carry out all the job interviews and skill development procedures of the candidates, all this procedures are carried out either manually or some sort of database software which is slow and sloppy, We hereby took a step forward to build an Artificial Intelligence based solution to the problem, we propose a system where admin and student can carry out all the training and placement related operations with the implementation of an Artificial Intelligence based environment where user can communicate with artificial intelligence based control system. Krishanu Deb | Pankaj Agrawal | Harish Nawale | Shradha Jadhav | Prof. Manisha Darak \u00c4rtificial Intelligence Based Training and Placement Management\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-1 , December 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd19180.pdf","4":"In an increasingly digitalized world, the utilization of Artificial Intelligence AI in digital marketing has emerged as a transformative force, redefining how businesses engage with their audiences and tailor their strategies. This research paper aims to elucidate the pivotal role of AI in enhancing various facets of digital marketing, including personalization, customer insights, campaign optimization, and predictive analysis. By examining case studies and empirical evidence, this paper underscores the advantages of AI driven digital marketing and its potential to revolutionize the landscape of consumer business interactions. Dr. Atul Kumar Mishra \"The Significance of Artificial Intelligence in Digital Marketing\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-4, August 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd59801.pdf Paper Url:https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59801\/the-significance-of-artificial-intelligence-in-digital-marketing\/dr-atul-kumar-mishra","5":"There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a \u2018good\u2019 explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology\/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.","6":"Both Simulation and Artificial Intelligence try to model reality for problem solving and decision making. In this paper, we propose a framework for integrating the two areas by uncovering fundamental similarities between them and opportunities for combining them which can be mutually useful. The framework also shows the potential gains for Simulation by applying Artificial Intelligence concepts (mainly expert systems) to assist in the simulation process and reveals in an organised way the potential gains for Artificial Intelligence by applying concepts derived from Simulation.","7":"Artificial Intelligence AI is a growing field at the intersection of computer science, mathematics, and engineering, focused on creating machines capable of intelligent behavior. Over the years, AI has evolved from rule based systems to data driven approaches, prominently leveraging machine learning and deep learning. This evolution has led to AI systems capable of complex tasks such as pattern recognition, natural language processing, and decision making. The applications of AI are vast and diverse, permeating industries like healthcare, finance, automotive, retail, and education. AI driven technologies enable efficient automation, precise data analysis, personalized experiences, and improved decision making. However, with these advancements come ethical and culture concerns, including biases, data privacy, job displacement, and the responsible development and deployment of AI. Striking a balance between AIs potential and its associated risks necessitates a holistic approach, incorporating transparency, fairness, robust regulations, and ongoing research. This abstract encapsulates AIs transformative potential, emphasizing the importance of responsible AI development to ensure a positive impact on society while mitigating risks. Manish Verma \u00c4rtificial Intelligence Role in Modern Science: Aims, Merits, Risks and Its Applications\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-5 , October 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd59910.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59910\/artificial-intelligence-role-in-modern-science-aims-merits-risks-and-its-applications\/manish-verma","8":"Art is arguably the most creative form of expression known to mankind. Artificial intelligence has advanced to occupy a position of paramount importance in Science, but seldom do we associate it with Human Creativity in general, and with Modern Art in particular. Creativity is one of the rudimentary constituents of the functioning of machines. By using algorithms, machines churn out representations of shapes, images and structures. Machines are perpetually expanding, redefining and reinventing creativity in their own right. This idea has triggered the birth of a new subfield in Artificial Intelligence known as Computational Creativity. This paper analyzes the diverse ways in which the integrated algorithms of Machine Learning and Artificial Intelligence are cut for producing breakthroughs in the field of 21st Century Modern Arts. Avani Goenka \u00c4rtificial Intelligence Catalyzes a Revolution for 21st Century Human Creativity and Modern Art\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-2 , February 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49150.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/49150\/artificial-intelligence-catalyzes-a-revolution-for-21st-century-human-creativity-and-modern-art\/avani-goenka","9":"Artificial Intelligence is not only about the machines that play an authoritative role in humans, but they both are working together. Machines provide the human with the ability of insight and perspective but the machines will not provide the decisive role of supplying judgement and creativity. There is a huge scope of artificial intelligence in this era. The combination of human creativity and technology together results in the excitement that can solve various problems and challenges related to the world. Deepak Kumar \u00c4rtificial Intelligence Empowering the Future of Digital Transformation\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-3 , April 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd30287.pdf","10":"Advanced and Rapid development in the field of technology in recent times has been led by Artificial Intelligence which has profoundly changed development and life status of the world. Based on background, this paper traces the development in technology with Artificial Intelligence and the result of the application introduced for development and thus affected work culture and life in India. This paper studies the achievements in Artificial Intelligence based on technical aspects, studies the market of artificial intelligence and its development features, studies structural trend of application in the field of artificial intelligence and development with artificial intelligence and studies of competitions and associated patterns in the field of artificial intelligence. The focus remains on the outline of initiatives undertaken for advancement in the field of artificial Intelligence. Finally, to highlight the applications associated with artificial Intelligence, several examples of industrial and societal development are expounded and prospected. Chinmaya Naik | Sonali Jain | Jai Sehgal \u00c4nalysis and Comparative Study of the Development of Technology with Artificial Intelligence in India\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31249.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31249\/analysis-and-comparative-study-of-the-development-of-technology-with-artificial-intelligence-in-india\/chinmaya-naik","11":"The pharmaceutical industry stands to be transformed by Artificial Intelligence AI , particularly in areas such as drug discovery, clinical trials, and personalized medicine. However, there are several obstacles to implementing AI in this industry, including limited familiarity with the technology, inadequate IT infrastructure, and the difficulty of extracting valuable data from patients records. One specific application of AI in the pharmaceutical field involves the development of small peptides with antimicrobial properties, which can serve as novel antibiotics to combat superbugs that are resistant to multiple drugs. AI can assist in determining the effectiveness and potency of these peptides, facilitating the development of powerful antibiotics. Despite these challenges, AI holds tremendous potential in the pharmaceutical industry, enabling accelerated innovation, time and cost savings, and ultimately, saving lives. In conclusion, although there are limitations to adopting AI in pharma, there are numerous promising future prospects that could revolutionize the industry and enhance patient outcomes. Shaikh Sameer Salim | Manoj Kumar \"Impact of Artificial Intelligence in the Pharmaceutical World- A Review\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-3 , June 2023, URL: https:\/\/www.ijtsrd.com.com\/papers\/ijtsrd57564.pdf Paper URL: https:\/\/www.ijtsrd.com.com\/computer-science\/artificial-intelligence\/57564\/impact-of-artificial-intelligence-in-the-pharmaceutical-world-a-review\/shaikh-sameer-salim","12":"Artificial intelligence AI is the intelligence exhibited by an artificial entity, generally assumed to be a computer. It has been involved with gaming since day one. It is progressively being widely used in the gaming industry. AI in games is commonly used for creating players opponents. It is the foundation of all video games. Games like Nim, checkers, or chess took advantage of smart algorithms to beat human players. AI based games are based on a finite set of actions or reactions whose sequence can be easily predicted by expert players. This paper provides an introduction on the applications of AI in different games. Matthew N. O. Sadiku | Sarhan M. Musa | Abayomi Ajayi-Majebi \u00c4rtificial Intelligence in Gaming\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-2 , February 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38516.pdf Paper Url: https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/38516\/artificial-intelligence-in-gaming\/matthew-n-o-sadiku","13":"Computers are becoming ubiquitous and are playing significant roles in our lives. Domestic digital devices for leisure and entertainment are becoming increasingly important. To be usable, every computing device must allow for some form of interaction with its user. The human computer interaction is the point of communication between the human user and the computer. AI has been gradually being incorporated into human computer interaction HCI . As AI systems become more and more ubiquitous, it is imperative to understand those systems from a human perspective. This paper provides an introduction to the \u00e2\u20ac\u0153marriage\u00e2\u20ac\u009d between HCI and AI. Matthew N. O. Sadiku | Uwakwe C. Chukwu | Abayomi Ajayi-Majebi | Sarhan M. Musa \u00c4rtificial Intelligence and Human-Computer Interaction\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-6 , October 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd47491.pdf Paper URL : https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/47491\/artificial-intelligence-and-humancomputer-interaction\/matthew-n-o-sadiku","14":"The Artificial intelligence field continues to be plagued by what can only be described as 'bold promises for the future syndrome'. Research in AI has built upon the tools and techniques of many different disciplines, including formal logic, probability theory, decision theory, management science, linguistic philosophy. However, the application of these disciplines in artificial intelligence has necessitated the development of many enhancements and extensions. Since the invention of computers or machines, their capability to perform various tasks went on growing exponentially. Humans have developed the power of computer systems in terms of their diverse working domains, their increasing speed and reducing size with respect to time. Durgesh Raghuvanshi \u00c4rtificial Intelligence: Basics and Terminology\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-6 , October 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd18909.pdf","15":"Several developments in the transfer of data through the internet make it easier to transfer the data faster and accurately to the destination. But in this, anyone can misuse and modify the critical information through hacking. Video steganography is a technique which is used to hide the message and to transfer the message inside a video. Video is an application of many frames of audio, text, and images. The segmentation is known as the advanced technology that provides rich information of an image. The purpose of this paper is to propose a new technique to hide the data using video steganography with the help of artificial intelligence and DWT. This paper focuses on analyzing the various video steganography techniques which were proposed for securing the data transmission. In this paper, artificial intelligence is applied in order to improve the integrity and security of data transfer. The performance of the proposed method is evaluated on the basis of Bit error, mean square error, and PSNR metrics. Shivani Gupta | Gargi Kalia | Preeti Sondhi \"\"Video Steganography using Discrete Wavelet Transform and Artificial Intelligence\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-4 , June 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd25067.pdf\r\n\r\nPaper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/25067\/video-steganography-using-discrete-wavelet-transform-and-artificial-intelligence\/shivani-gupta","16":"In complex environments, it is challenging to learn enough about the underlying characteristics of transactions so as to design the best institutions to efficiently generate gains from trade. In recent years, Artificial Intelligence has emerged as an important tool that allows market designers to uncover important market fundamentals, and to better predict fluctuations that can cause friction in markets. This paper offers some recent examples of how Artificial Intelligence helps market designers improve the operations of markets, and outlines directions in which it will continue to shape and influence market design. Mrs. Pushpalata S. Patil\u00c4rtificial Intelligence and Machine Learning's Impact on Market Design\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-4 , June 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd14534.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/14534\/artificial-intelligence-and-machine-learning's-impact-on-market-design\/mrs-pushpalata-s-patil","17":"In the history of the quest for human-level artificial intelligence, a number of rival paradigms have vied for supremacy. Symbolic artificial intelligence was dominant for much of the 20th century, but currently a connectionist paradigm is in the ascendant, namely machine learning with deep neural networks. However, both paradigms have strengths and weaknesses, and a significant challenge for the field today is to effect a reconciliation. A central tenet of the symbolic paradigm is that intelligence results from the manipulation of abstract compositional representations whose elements stand for objects and relations. If this is correct, then a key objective for deep learning is to develop architectures capable of discovering objects and relations in raw data, and learning how to represent them in ways that are useful for downstream processing. This short review highlights recent progress in this direction.","18":"This paper provides detailed understanding about the role of ethics in artificial intelligence. The author will be dealing with the given title by dividing it into two branches: roboethics and machine ethics along with their moral values and role in technology. In the roboethics section, the author clearly explains about the characteristics and behaviour of a robot and the problems caused by it and how it poses as a threat to human beings. In the second part, the author differentiates between roboethics and machine ethics. Machine ethics only refers to computed machine which are designed by humans and their moral behaviour. Moreover, the author has discussed in detail about societal moral values of artificial intelligence and how it tends to threaten human and future technology in terms of philosophy and law. Only moral values and the ethics behind the technology governing the artificial intelligence exists in the present and there is no law to formally punish crimes that deals with AI. To revolutionise the current scenario, the author widely takes the topic into several dimensions regarding governance and other ethical aspects of artificial intelligence. One way to overcome the difficulty in governing artificial intelligence, as suggested by the author, is to obtain patents for the inventions and eventually this would fall under the category of intellectual property rights. Many countries like the UK, USA and Japan have proposed to include artificial intelligence under intellectual property and this process is in progression. Few other countries are at the developing stages for devising laws that concern artificial intelligence, because this branch of engineering is an emerging one. By moving on to the second part of this paper, the author describes AI' influence on human race. The super intelligent machines created by a human may put an end to another human' life. Next comes the issue of employment disability which will be illustrated elaborately. This paper concludes by reviewing both positive and negative aspects concerning AI: how it aids in improving technology and bringing a developed society and how it steals jobs that employed humans and the danger it causes Rishikesh R\"Role of Ethics in Artificial Intelligence\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-5 , August 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd17135.pdf http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/17135\/role-of-ethics-in-artificial-intelligence\/rishikesh-r","19":"An autonomous vehicle is one that uses a combination of sensors, cameras, radar, and artificial intelligence AI to travel between destinations without a human operator. It is designed to be able to detect objects on the road, maneuver through the traffic without human intervention, and get to the destination safely. It is fitted with AI based functional systems such as voice and speech recognition, gesture controls, eye tracking, and other driving monitoring systems. Several companies have announced their plan to get involved in autonomous or driverless and electric vehicle technology. This paper presents uses of AI technology in autonomous vehicles. Matthew N. O. Sadiku | Sarhan M. Musa | Abayomi Ajayi-Majebi \u00c4rtificial Intelligence in Autonomous Vehicles\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-2 , February 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38514.pdf Paper Url: https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/38514\/artificial-intelligence-in-autonomous-vehicles\/matthew-n-o-sadiku","20":"There are emerging concerns about the Fairness, Accountability, Transparency, and Ethics (FATE) of educational interventions supported by the use of Artificial Intelligence (AI) algorithms. One of the emerging methods for increasing trust in AI systems is to use eXplainable AI (XAI), which promotes the use of methods that produce transparent explanations and reasons for decisions AI systems make. Considering the existing literature on XAI, this paper argues that XAI in education has commonalities with the broader use of AI but also has distinctive needs. Accordingly, we first present a framework, referred to as XAI-ED, that considers six key aspects in relation to explainability for studying, designing and developing educational AI tools. These key aspects focus on the stakeholders, benefits, approaches for presenting explanations, widely used classes of AI models, human-centred designs of the AI interfaces and potential pitfalls of providing explanations within education. We then present four comprehensive case studies that illustrate the application of XAI-ED in four different educational AI tools. The paper concludes by discussing opportunities, challenges and future research needs for the effective incorporation of XAI in education.","21":"Technology is evolving every day and the latest trends in Artificial Intelligence have made dental procedures less time consuming and minimally invasive. With their application the decision making processes have become easier and faster. Patients are more comfortable and dentists are more confident about their work. This advent of technology into medical science has made both practitioners and their patients comfortable and confident about the treatment rendered and received.  The use of machine learning has made the decision making process easier by stimulating human intelligence into the machines that are programmed to think like humans and mimic their actions. Artificial Intelligence has proved itself to be a boon in the field of dentistry. In future AI based comprehensive care systems are expected to have high quality patient care and help researchers know and treat more about diseases. Even though misconceptions and certain limitations about Artificial Intelligence prevails, it still continues to flourish due to its advantages in providing precision.","22":"This paper discusses the European Parliament\u2019s Resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on Robotics (European Parliament 2017). It provides a brief summary of the content of the Resolution and looks at its basic principles and raison d\u2019\u00eatre. It also touches on the issue of defining robots and their liability. In so doing, it suggests a twofold shift in the rationale of Parliament\u2019s recommendations. Using a prospective approach and taking into consideration the views of scholars who are specialised in analysing robotics and artificial intelligence, this paper proposes that Parliament\u2019s recommendations could go further by addressing a much broader spectrum of artificial agents and artificial intelligence, instead of focusing on specific categories of robots. It then looks at the responsibility, visibility and liability of those who have decision-making powers over the design, development and deployment of robots and artificial intelligence, including designers and developers.","23":"HR professionals need to get to grips with artificial intelligence and the way it's changing the world of work. From using natural language processing to ensure job adverts are free from bias and gendered language to implementing chatbots to enhance the employee experience, AI has created a variety of opportunities for the HR function. Artificial Intelligence for HR empowers HR professionals to leverage this potential and use AI to improve efficiency and develop a talented and productive workforce. Outlining the current technology landscape as well as the latest AI developments, this book ensures that HR professionals fully understand what AI is and what it means for HR in practice. Covering everything from recruitment and retention to employee engagement and learning and development, Artificial Intelligence for HR outlines the value AI can add to HR. It also features discussions on the challenges that can arise from AI and how to deal with them, including data privacy, algorithmic bias and how to develop the skills of a workforce with the rise of automation, robotics and machine learning in order to make it more human, not less. Packed with practical advice, research and case studies from global organizations including Uber, IBM and Unilever, this book will equip HR professionals with the knowledge they need to leverage AI to recruit and develop a successful workforce and help their businesses thrive in the future.","24":"In today's digital era \u00c4rtificial intelligence\" has been used in nearly all industries. AI has many applications in various different sectors like healthcare, aviation, IT, manufacturing etc. AI helps in reducing human errors, risks, best results with accuracy, does risky and or hazardous jobs like bomb defusing, saves training cost. But at the same time AI has few risk factors like human intelligence can be replicated up to certain limit only, lesser jobs, hurt learners, affects human thinking powers. Mrs. Jyoti M Bohra | Ms. Bhagyashri G Joshi \u00c4rtificial Intelligence & its Role in Industry\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Special Issue | International Conference on Digital Economy and its Impact on Business and Industry , October 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd18705.pdfhttp:\/\/www.ijtsrd.com\/management\/research-method\/18705\/artificial-intelligence-and-its-role-in-industry\/mrs-jyoti-m-bohra","25":"Artificial intelligence and machine learning are poised to influence nearly every aspect of the human condition, and cardiology is not an exception to this trend. This paper provides a guide for clinicians on relevant aspects of artificial intelligence and machine learning, reviews selected applications of these methods in cardiology to date, and identifies how cardiovascular medicine could incorporate artificial intelligence in the future. In particular, the paper first reviews predictive modeling concepts relevant to cardiology such as feature selection and frequent pitfalls such as improper dichotomization. Second, it discusses common algorithms used in supervised learning and reviews selected applications in cardiology and related disciplines. Third, it describes the advent of deep learning and related methods collectively called unsupervised learning, provides contextual examples both in general medicine and in cardiovascular medicine, and\u00a0then explains how these methods could be applied to enable precision cardiology and improve patient outcomes.","26":"Banking is the most important sector of any economy because it connects the most with government and public at large and also it protects the economy from any crises. Technology has brought tremendous change, it has made both positive and negative impact on every sector and banking sector is the most dynamic in technological transformation. Among the various technological transformations of recent, the birth of Artificial intelligence is particularly remarkable. AI is fast evolving as the go to technology for banking sectors across the world to personalise experience for individuals. With data analytics, block chain and machine learning, banks are advancing their services and offerings. The technology itself is getting better and smarter day by day, allowing more and newer banks to adopt the AI for various applications. Banking sector is becoming one of the first adopters of AI. And just like other segments, banks are exploring and implementing the technology in various ways. AI refers to computers having cognitive skills similar to humans, which could result in immense efficiency gains for banks and their clients alike. It is important to understand how AI can influence the Banking sector hence this paper is an attempt to understand the opportunities and challenges of Artificial intelligence for Indian banking sector. Prof. Mohammed Nawaz | Prof. Triveni. K | Prof. Bharathi. G. R \u00c4pplication of Artificial Intelligence in Indian Banking-Opportunities and Challenges\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-1 , December 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd37964.pdf Paper URL : https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/37964\/application-of-artificial-intelligence-in-indian-bankingopportunities-and-challenges\/prof-mohammed-nawaz","27":"Trends in computer science show that various aspects of Artificial Intelligence are emerging and other trends show that these advances are being applied to create intelligent in formation systems. In recent days artificial intelligence is changing the ways in which computers are usable as problem solving tools. The talent of humans is thus smartly creating and operating tools are indeed a feature of human based brainpower. This technology is now adapted by various E Commerce websites in order to identify the customer preference pervious purchases frequent checks etc. Google and Microsoft are also investing in artificial intelligence through various forms in order to enhance better customer service. The main aim of the study is to analysis and explores the various applications and impact of artificial intelligence in E Commerce industry. This study analyses and concludes that by replacement of human expert with artificial intelligence systems in E Commerce industry can significantly speedup and cheapens the production or service process. Prof. Lakshmi Narayan. N | Naveena. N \u00c4 Study on the Applications and Impact of Artificial Intelligence in E-Commerce Industry\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd) ISSN: 2456-6470 Volume-3 | Issue-5  August 2019 URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd26374.pdfPaper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/26374\/a-study-on-the-applications-and-impact-of-artificial-intelligence-in-e-commerce-industry\/prof-lakshmi-narayan-n","28":"Computation has become essential part of our life. The input devices and user interface changes from each generation. But still many users are unable to access information from reliable sources in a convenient way. In this paper, we are implementing digital assistants on mini drones using raspberry pi. This adds mobility and portability to the device. By this model, users can easily obtain responses through voice commands. The output is received from the cloud through Text-to-Speech (TTS) converter. The Arduino acts as flight controller and it is serially interfaced with raspberry using python and hence the drone can also be controlled through voice commands. In order to prevent collision, we are implementing obstacle avoidance system through ultrasonic sensors. Mr. A. Kishorekumar | Mr. E. Ezhilarasan | Mr. R. Parthiban\"Intelligent Drone based Personal Assistant using Artificial Intelligence (AI)\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd11482.pdf  http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/11482\/intelligent-drone-based-personal-assistant-using-artificial-intelligence-ai\/mr-a-kishorekumar","29":"The smart grid is an electrical power grid that is integrated with an AI enabled, two way communication network providing energy and information. It is a technology that enables instantaneous feedback from various sensors and devices on the operation of the power grid. Although AI is relatively new, it is poised to revolutionize the way we produce, transmit, and consume energy. AI will constitute the brain of future smart grid. The power sector has started to use AI and related technologies for communication between smart grids, smart meters, and Internet of things devices. This paper presents some applications of AI in smart grid. Matthew N. O. Sadiku | Uwakwe C. Chukwu | Abayomi Ajayi-Majebi | Sarhan M. Musa \u00c4rtificial Intelligence in Smart Grid\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-5 , August 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd50563.pdf Paper URL: https:\/\/www.ijtsrd.com\/engineering\/other\/50563\/artificial-intelligence-in-smart-grid\/matthew-n-o-sadiku","30":"AI - Artificial Intelligence has a considerably different meaning, unique things and diverse functions. Each person looks at it as a distinct theory, an artificial human created core and computer development program, and a system that requires human intelligence and performs special tasks beyond human capacity, such as decision-making, speech recognition, visual perception, and translation of languages.The theory and practice of AI have an origin dating back to 1950 and thereafter, a gradual progress was made to substantiate its precise meaning and performance. Considering the rising number of virtual world providing an implicit significance to its theory and practice for the last several decades, an acute interest in AI has now reignited. The dynamic and rapid technological and scientific development pace of AI has created difficulty to predict precisely what path we should take to enable us to alter this world in ways we need to comprehend it, resulting in policies and laws to remain one step at the back of all the technological development.The human beings are very sophisticated and robust intellectual living gadgets on the mother earth, as we have our own intelligence to take decisions, and therefore, we ruled and controlled other living creatures of this planet. In the course of time, we learned and acquired various dynamic skills, necessary for survival, yet, our search process went further to explore more than what we have and wanted. The infinite intelligence having no boundaries started to visualize and invent means to help us save more time and to ensure better health, more security and safety and thus ventured to formulate and devise machines that can extend our intellectual capacity for the multitask activities, and memorizing more information than available presently. Hence, while imagining the ways to improve our functioning, the first Analytical Engine, Turing-Complete computing machine was invented in 1833 by Charles Babbage, and from that time, the computer machines have transformed Analog era towards the Digital Era. In short, AI is a computational human behavior model, which programs and externally behaves like humans. It is a human extended thought of the computational model, with specific programs that internally operates like humans do. Prachi Shah | Praveen Gautam\"Transformational Planning for Artificial Intelligence\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd11666.pdf  http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/11666\/transformational-planning-for-artificial-intelligence\/prachi-shah","31":"The hazard of fungal and bacterial crop syndrome can be predicted using risk models with exact ecological parameters such as temperature, relative humidity, solar radiation, wind speed, and leaf wetness duration. The ecological Parameter has recognized as key in the management of crop disease. Air temperature and moisture pressure the preponderance of fungal place diseases. In ecological factors mainly condensation also impacts pest populations, as well as contamination deposits. a lot of parameters are well unspoken, readily defined, and effortlessly measured. The trouble and vagueness connected with monitoring ecological aspects at the local leaf balance and the complication of up scaling to the crop stage stop obtainable disease risk models from life form used with consistency. One nonparametric arithmetical move toward in receipt of scant notice for the modeling of crop paddy syndrome forecast is that of artificial intelligence. In this project AIs estimate this key environmental variable at local crop scales, using local and regional weather station data and site-specific sensing data. The ultimate goal is to embed the AI into a highly-portable tool, designed to predict leaf wetness duration in conjunction with local weather stations, and as input to real-time decision support systems. M. Juno Isabel Susinthra | S. Vinitha\u00c4rtificial Intelligence Assisted Weather Based Plant Disease Forecasting System\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd12734.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/12734\/artificial-intelligence-assisted-weather-based-plant-disease-forecasting-system\/m-juno-isabel-susinthra","32":"The huge development of technology has greatly affected human life. Recent technologies are being used in all sectors. Artificial intelligence is a modern science that aims to create a machine that imitates human intelligence. It is used in many domains like finance, robotics, healthcare, and marketing. The goal of this paper is to study the impact of artificial intelligence applications on e marketing and its competitive advantage for Iraq\u2019s marketing companies. The authors used a descriptive analytical approach. It considered three dimensions of artificial intelligence natural language processing, expert systems, kinetic navigation used in email marketing, and the application of automated robots. To investigate this impact, a questionnaire with 28 items is being developed. The research community consisted of marketers and employees of marketing companies in Iraq, where the researcher distributed 300 questionnaires to a convenience sample, and 288 valid questionnaires were retrieved for research purposes. The collected data is examined using SPSS. The results prove the presence of a statistically significant impact of artificial intelligence applications on e marking and its competitive advantage. Nour Sadeq | Ghalia Nassreddine | Joumana Younis \"Impact of Artificial Intelligence on E-marketing\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-1 , February 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd53850.pdf Paper URL: https:\/\/www.ijtsrd.com\/other-scientific-research-area\/other\/53850\/impact-of-artificial-intelligence-on-emarketing\/nour-sadeq","33":"The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields.","34":"The concept of innateness is rarely discussed in the context of artificial\r\nintelligence. When it is discussed, or hinted at, it is often the context of\r\ntrying to reduce the amount of innate machinery in a given system. In this\r\npaper, I consider as a test case a recent series of papers by Silver et al\r\n(Silver et al., 2017a) on AlphaGo and its successors that have been presented\r\nas an argument that a \"even in the most challenging of domains: it is possible\r\nto train to superhuman level, without human examples or guidance\", \"starting\r\ntabula rasa.\"\r\n  I argue that these claims are overstated, for multiple reasons. I close by\r\narguing that artificial intelligence needs greater attention to innateness, and\r\nI point to some proposals about what that innateness might look like.","35":"We exist in a surrounding where, humans prefer a computer-oriented application to buy clothes or food, rather than going to the nearest store to buy all the amenities. The change of preference was due to a vast emerging concept called \u00c4U\"\u009d. AU is the use or introduction of automatic equipment in the process of manufacturing or other processes and facility. Artificial Intelligence (AI) is the concept that evolved from AU. Artificial Intelligence is the process of simulation of human intelligence in machines. Humans are unaware of the fact that their job opportunities are being taken over by a machine simulated by human intelligence. In recent years, many law firms started to engage their firms with an AI in their legal research. Our paper traces the evolution of the relationship between technology and human productivity from a contemporary perspective with special reference to HR as well as speculates on the possible answers to emerging problems. The main question is whether an AI taking over the legal sector or any other sectors affects the basic HR? HR is a right which is believed to belong to every person. Here, in this case, the basic right of getting a proper livelihood is being affected. Krishna Ramanujam | Nethra Mohanram\u00c4rtificial Intelligence in Accordance to Human Rights\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-5 , August 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd18312.pdf http:\/\/www.ijtsrd.com\/economics\/development-economics\/18312\/artificial-intelligence-in-accordance-to-human-rights\/krishna-ramanujam","36":"This research investigates the potential benefits and challenges associated with the implementation of Artificial Intelligence AI in the education sector. Through a qualitative content analysis of scholarly articles and educational policy documents, this study explores perceptions of AIs transformative role in education, ethical considerations, and recommendations for managing risks and challenges. Findings suggest that while AI offers numerous advantages such as personalized learning, immediate feedback, and improved administrative efficiency, it also raises concerns related to data privacy, educational inequity, dehumanization, and the need for teacher training. Ethical issues concerning data privacy and security, fairness and bias, transparency, and accountability were also identified. The study concludes by underscoring the need for comprehensive policy guidance and further research to ensure that AI is implemented responsibly and equitably in education. Suman Roy | Sujit Kumar Paul \"Revolutionizing Education: How Artificial Intelligence is transforming the Learning Landscape\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-4, August 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd59781.pdf Paper Url:https:\/\/www.ijtsrd.com\/humanities-and-the-arts\/education\/59781\/revolutionizing-education-how-artificial-intelligence-is-transforming-the-learning-landscape\/suman-roy","37":"In todays world, we are experiencing tremendous growth in the research and application of Artificial intelligence. Power plants are a vast sector where there is a scope of using AI to rectify the faults and optimize the overall running of the plants. The use of AI will help in reducing human dependence, and during a breakdown, will assist in rectifying the problem by determining the cause quickly. This paper focusses on proposing numerous methods to implement AI in various power plants and how it will help in the same. Anshika Gupta \"Techniques to Apply Artificial Intelligence in Power Plants\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-5 , August 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33050.pdf Paper Url :https:\/\/www.ijtsrd.com\/engineering\/information-technology\/33050\/techniques-to-apply-artificial-intelligence-in-power-plants\/anshika-gupta","38":"The creation of artificial intelligence has enriched the market of works and has triggered thinking about the copyright protection of artificial intelligence creations. This article clarifies the copyright ownership of artificial intelligence, Combined with the status quo of the attribution of artificial intelligence creations to analysis. Using the analytical methods of law and economics, transaction cost theory concludes that user ownership can reduce transaction costs more. Compared with investors, the positive externality benefits exerted by the ownership of user rights in the externality theory are more obvious. From the perspective of game theory, the rights of artificial intelligence creations belong to the users, which are more conducive to promoting the development of artificial intelligence technology, it is more practical and feasible to attribute the rights of artificial intelligence creations to users. Changjun Wu | Fuyao Xiang \"Law and Economic Analysis of the Ownership of Artificial Intelligence Creation\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-5 , August 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33044.pdf Paper Url :https:\/\/www.ijtsrd.com\/management\/law-and-management\/33044\/law-and-economic-analysis-of-the-ownership-of-artificial-intelligence-creation\/changjun-wu","39":"Artificial intelligence (AI) is the theory and development of computer systems able to perform tasks normally requiring human intelligence. With teeming competition and increasing demand in the food industry, the industry has begun to embrace AI technologies in a bid to maximize profits and explore new ways to reach and serve the consumers. AI has been successfully deployed for applications such as sorting fresh produce, managing supply chain, food safety compliance monitoring, effective cleaning in place systems, anticipating consumer preference and new product development with greater efficiency and savings on time and resources. However, there are challenges to adoption of AI technologies which include cost, cultural changes, expert skill requirements, transparency issues and one track minds. Despite these challenges, researches are on-going on optimized production process using AI but it is important to note that the benefits of AI application in food industry greatly outweigh its challenges.","40":"Suicide has been major death cause from an over decade now. The ideation of suicide and its thought process leads to a very dangerous after effect. Since social media is very trending factor now a day\u2019s, it can be used to detect suicide ideation. Artificial intelligence which is used for tasks like problem solving, spam filter, speech recognition etc. can also be used for detect suicides over social media. This study is aimed in designing an automated classifier which can detect suicidal thoughts with the help of algorithmic approach by studying various patterns in the discussions or communications. This paper also studies the previous attempts made for the design of such architecture. At the end we conclude the feasibility and practicability of the approach that we are going to use for the detection of suicide attempts on social media. Shrivallabh Walkade | Rajesh Sonnakula | Deepak Kumar | Shreyas Nambiar \"Stride for Detecting Suicidal Thoughts Using Artificial Intelligence\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-2 , February 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49182.pdf Paper URL: https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/49182\/stride-for-detecting-suicidal-thoughts-using-artificial-intelligence\/shrivallabh-walkade","41":"Supporting transnational worker organizing should be at the center of the fight for \u201cethical AI.\u201d","42":null,"43":"Authors like Isaac Asimov have for long imagined a future where machines that can govern themselves to help mankind. It was perhaps this imagination by the artists that led science on the path of inventing artificial intelligence in the first place. However, as things have advanced, science and AI have proven themselves to be much more than merely helpful for mankind so far. Machines have a long way to go before they reach the point envisioned by Asimov as they are still dependent on people to make moral judgment. Among the latest patrons of AI are the HR departments that have realized the wisdom and impact of assisted decision making about the most valuable resource of an organization - the people. The present paper stresses upon the impact of Artificial Intelligence on Human resource management. The paper goes on discussing the areas of Human Resource wherein Artifi Dr. Amol Murgai\"Role of Artificial Intelligence in Transforming Human Resource Management\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd11127.pdf  http:\/\/www.ijtsrd.com\/management\/general-management\/11127\/role-of-artificial-intelligence-in-transforming-human-resource-management\/dr-amol-murgai","44":"PURPOSE OF REVIEW: Despite the impressive results of recent artificial\r\nintelligence (AI) applications to general ophthalmology, comparatively less\r\nprogress has been made toward solving problems in pediatric ophthalmology using\r\nsimilar techniques. This article discusses the unique needs of pediatric\r\nophthalmology patients and how AI techniques can address these challenges,\r\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\r\nfuture directions in the field.\r\n  RECENT FINDINGS: The most significant advances involve the automated\r\ndetection of retinopathy of prematurity (ROP), yielding results that rival\r\nexperts. Machine learning (ML) has also been successfully applied to the\r\nclassification of pediatric cataracts, prediction of post-operative\r\ncomplications following cataract surgery, detection of strabismus and\r\nrefractive error, prediction of future high myopia, and diagnosis of reading\r\ndisability via eye tracking. In addition, ML techniques have been used for the\r\nstudy of visual development, vessel segmentation in pediatric fundus images,\r\nand ophthalmic image synthesis.\r\n  SUMMARY: AI applications could significantly benefit clinical care for\r\npediatric ophthalmology patients by optimizing disease detection and grading,\r\nbroadening access to care, furthering scientific discovery, and improving\r\nclinical efficiency. These methods need to match or surpass physician\r\nperformance in clinical trials before deployment with patients. Due to\r\nwidespread use of closed-access data sets and software implementations, it is\r\ndifficult to directly compare the performance of these approaches, and\r\nreproducibility is poor. Open-access data sets and software implementations\r\ncould alleviate these issues, and encourage further AI applications to\r\npediatric ophthalmology.\r\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\r\ndeep learning","45":"\u201cMaid in India\u201d is more important than \u201cMade in India\u201d in modern society. Nonetheless, in the hustle and bustle of the present world, cleanliness has been ignored. To make life of humankind easier, with assistance of machines and innovations a floor cleaner is introduced. TIDDY, the floor cleaning robot is both a self sufficient and manual controlled cleaning machine used to accomplish the act of cleaning by methods of its wet and dry modes. Keeping the surrounding clean is just not an aspiration but an action. Hence, TIDDY helps to keep the surrounding neat and tidy. Cleanliness is most important for physical well being and a healthy environment. It has bearing on public and personal hygiene. This is made budget friendly keeping in mind the different economical classes in society. With the additional features of artificial intelligence and voice recognition, it is made cost effective. Bhumika T J | Harshitha U | Meghana S | Dr. Ganashree T S \"TIDDY - An Artificial Intelligence Based Floor Cleaning Robot\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-5 , August 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31847.pdf Paper Url :https:\/\/www.ijtsrd.com\/engineering\/telecommunications\/31847\/tiddy--an-artificial-intelligence-based-floor-cleaning-robot\/bhumika-t-j","46":"This paper proposes a modular ACTOR architecture and definitional method for artificial intelligence that is conceptually based on a single kind of object: actors or, if you will, virtual processors, activation frames, or streams. The formalism makes no presuppositions about the representation of primitive data structures and control structures. Such structures can be programmed, micro-coded, or hard wired in a uniform modular fashion. In fact it is impossible to determine whether a given object is \"really\" represented as a list, a vector, a hash table, a function, or a process. The architecture will efficiently run the coming generation of PLANNER-like artificial intelligence languages including those requiring a high degree of parallelism. The efficiency is gained without loss of programming generality because it only makes certain actors more efficient; it does not change their behavioral characteristics. The architecture is general with respect to control structure and does not have or need goto, interrupt, or semaphore primitives. The formalism achieves the goals that the disallowed constructs are intended to achieve by other more structured methods.","47":"A constant and solid supply of power is essential for the working of the present current and advanced society. A large portion of the exertion in control frameworks investigation has gotten some distance from the system of formal scientific demonstrating which originated from the territories of tasks look into, control hypothesis, and numerical examination to the less thorough and less tedious methods of artificial intelligence AI . AI Methods have turned out to be popular for taking care of various issues in control frameworks like control, arranging, forecast, scheduling, and so forth. These strategies can manage troublesome assignments looked by applications in present day extensive power frameworks with significantly more interconnections introduced to meet the increasing load demand. The real goal of this paper is to show how computerized reasoning procedures may assume an essential part in displaying and expectation of the execution of sun based vitality frameworks. The paper traces a comprehension of how expert systems and neural systems work by method for exhibiting various issues in the diverse orders of sun based vitality designing. Prof. Vijay Aithekar | Mr. Hitesh \"Review on Solar Power System with Artificial Intelligence\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-1 , December 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd47866.pdf Paper URL: https:\/\/www.ijtsrd.com\/physics\/engineering-physics\/47866\/review-on-solar-power-system-with-artificial-intelligence\/prof-vijay-aithekar","48":"\u2018Whoever becomes the ruler of AI will become the ruler of the world,\u2019 said Vladimir Putin in September 2017. The USA, Russia and China are all adamant that artificial intelligence (AI) will be the key technology underpinning their national power in the future. What place, then, is there for Europe in this context? The European Commission has recently set out a European initiative on AI which focuses on boosting the EU's technological and industrial capacity, developing an innovation ecosystem, ensuring the establishment of an appropriate legal and ethical framework, and preparing for socio-economic changes. This edition of the Foresight Brief presents the results of a mapping exercise on AI\u2019s impact on the world of work. It looks at the issues of work organisation and infrastructure, introduces the idea of \u2018AI literacy\u2019 for the workforce (as a necessary complement to technical reskilling), and details several AI risks for companies and workers. It also looks at aspects related to algorithmic decision making and the necessary establishment of an ethical and legal framework.","49":"This study incorporates the use of Artificial Intelligence in the monitoring of atmospheric distillation unit of large scale refining operation using Google AutoML tables, Jupyter, and Python software. The process involved training, evaluation, improvement, and deployment of the models based on the input data. The predicted yield (vol %) for the models were: Auto ML model: liquefied petroleum gas (LPG) - 1.41 , straight run gasoline (SRG)\u2013 4.96, straight run naphtha (SRN) \u2013 17.87, straight run kerosene (SRK) \u2013 14.5, light diesel oil (LDO) \u2013 26.47, heavy diesel oil (HDO) \u2013 2.7, and atmospheric residue (AR) \u201330.03; Jupyter Model: LPG \u2013 (0.93), SRG \u2013 (4.69), SRN \u2013 (17.24), SRK \u2013 (14.39), LDO \u2013 (26.43), HDO \u2013 (2.7), and AR \u2013 (30.18); and Python Model:LPG \u2013 (1.66) , SRG \u2013 (7.58), SRN \u2013 (11.68), SRK \u2013 (14.92), LDO \u2013 (24.77), HDO \u2013 (4.59), and AR \u2013 (24.59). The coefficient of determination (R2) values of 0.99981, 0.99943, and 0.93078 and Standard Error values of 0.240918, 0.419291, 3.536064, were obtained for the 3 models, respectively. All the software gave good predictions of the actual yield, although the Google Auto ML Table gave the best prediction. The training of the model is fundamental to its performance and precision.","50":"This review discusses the impact of artificial intelligence AI on pharmaceutical technology. AI has the potential to revolutionize the pharmaceutical industry by improving drug discovery  development  and delivery. By using AI powered tools  pharmaceutical companies can accelerate the drug discovery process  improve clinical trials  and optimize drug formulation. Additionally  AI can be used to monitor patient outcomes  identify adverse effects  and personalize treatment plans. The use of AI in the pharmaceutical industry can result in cost savings  enhanced patient care  and profitable innovation. Despite the many advantages  there are limitations to the adoption of AI in pharma technology  including data privacy concerns and the need for highly skilled personnel. Nevertheless  the future of pharma technology lies in the integration of AI  and it is expected that 50 of global healthcare companies will actively execute AI strategies and adopt this technology by 2025. Tirumala Durvasula | Pooja Rathod \"Impact of Artificial Intelligence on Pharm Technology- A Review\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd)  ISSN: 2456-6470  Volume-7 | Issue-2   April 2023  URL: https:\/\/www.ijtsrd.com.com\/papers\/ijtsrd55095.pdf Paper URL: https:\/\/www.ijtsrd.com.com\/pharmacy\/other\/55095\/impact-of-artificial-intelligence-on-pharm-technology-a-review\/tirumala-durvasula","51":"The stock market is highly volatile and complex in nature. However, notion of stock price predictability is typical, many researchers suggest that the Buy and Sell prices are predictable and investor can make above average profits using efficient Technical Analysis TA .Most of the earlier prediction models predict individual stocks and the results are mostly influenced by company\u2019s reputation, news, sentiments and other fundamental issues while stock indices are less affected by these issues. In this work, an effort is made to predict the Buy and Sell decisions of stocks, trends of stock by utilizing Stock Technical Indicators STIs As a part of prediction model the Long Short Term Memory LSTM , Support Virtual Machine SVM Artificial intelligence algorithms will be used with Stock Technical Indicators STIs. The project will be carried on National Stock Exchange NSE Stocks of India. Mr. Ketan Ashok Bagade | Yogini Bagade \u00c4rtificial Intelligence Based Stock Market Prediction Model using Technical Indicators\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-2 , April 2023, URL: https:\/\/www.ijtsrd.com.com\/papers\/ijtsrd53854.pdf Paper URL: https:\/\/www.ijtsrd.com.com\/management\/other\/53854\/artificial-intelligence-based-stock-market-prediction-model-using-technical-indicators\/mr-ketan-ashok-bagade","52":null,"53":"A breakthrough era that holds enormous promise for increasing patient care, lowering healthcare costs, and improving overall healthcare outcomes has arrived with the integration of Artificial Intelligence AI in healthcare. This in depth analysis examines the several ways in which AI is transforming healthcare, including diagnosis, treatment, drug research, patient management, and administrative procedures. To lay a strong foundation for understanding AI, machine learning, and deep learning applications in healthcare, the examination begins with clarifying their core principles. It explores how AI might be used to analyze large scale, intricate medical datasets including electronic health records EHRs , medical imaging, and genomes, enabling the early detection of disease, precise diagnosis, and tailored therapy recommendations. Additionally, AI driven technologies like natural language processing NLP have demonstrated considerable potential in extracting important insights from unstructured clinical notes and research literature, supporting clinical decision support and medical research. AI powered robotics and automation have also begun to play crucial roles in rehabilitation and minimally invasive surgery, lowering the invasiveness of operations and speeding up patient recovery. The review emphasizes the efforts that are still being made to create AI driven drug discovery systems that hasten the identification of new treatments and enhance the layouts of clinical trials. By examining trends and patterns in healthcare data, it also examines AIs function in predictive analytics, predicting disease outbreaks, and enhancing population health management. Furthermore, in the context of optimizing healthcare operations and lowering administrative duties, the contribution of AI to administrative tasks such as medical billing, fraud detection, and resource allocation is considered. The review emphasizes the significance of privacy, transparency, and responsible AI deployment while highlighting the ethical and regulatory concerns involved with AI in healthcare. In order to fully realize the potential of AI, it also analyzes potential adoption barriers and the necessity of interdisciplinary cooperation between healthcare experts, data scientists, and legislators. In conclusion, this in depth analysis offers a complete overview of how AI is altering healthcare and provides insights into its present successes and potential in the future. This effort intends to spur innovation, educate stakeholders, and open the door for a more effective, patient centered, and accessible healthcare ecosystem by shedding light on the revolutionary effects of AI on healthcare. Kajal Gohane | Roshini S | Komal Pode \"The Role of Artificial Intelligence in Revolutionizing Healthcare: A Comprehensive Review\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-5 , October 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd59874.pdf Paper Url: https:\/\/www.ijtsrd.com\/biological-science\/microbiology\/59874\/the-role-of-artificial-intelligence-in-revolutionizing-healthcare-a-comprehensive-review\/kajal-gohane","54":"My initial tasks in this paper are, first, to delimit the boundaries\n\tof artificial intelligence, then, to justify calling it a science:\n\tis AI science, or is it engineering, or some combination of these?\n\tAfter arguing that it is (at least) a science, I will consider how\n\tit is best pursued: in particular, the respective roles for experiment\n\tand theory in developing AI.\n\t\n\tI will rely more on history than on speculation, for our actual experience\n\tin advancing the field has much to tell us about how we can continue\n\tand accelerate that advance. Many of my examples will be drawn from\n\twork with which I have been associated, for I can speak with greater\n\tconfidence about what motivated that work and its methods (and about\n\tits defects) than I can about the work of others. My goal, however,\n\tis not to give you a trip through history, but to make definite proposals\n\tfor our future priorities, using history, where relevant, as evidence\n\tfor my views.","55":"The newly established cluster of excellence CoTeSys investigates the realization of cognitive capabilities such as perception, learning, reasoning, planning, and execution for technical systems including humanoid robots, flexible manufacturing systems, and autonomous vehicles. In this paper we describe cognitive technical systems using a sensor-equipped kitchen with a robotic assistant as an example. We will particularly consider the role of Artificial Intelligence in the research enterprise. Key research foci of Artificial Intelligence research in CoTeSys include (a) symbolic representations grounded in perception and action, (b) first-order probabilistic representations of actions, objects, and situations, (c) reasoning about objects and situations in the context of everyday manipulation tasks, and (d) the representation and revision of robot plans for everyday activity.","56":"AI is changing the exceptionally nature of work and information science is no special case. Will the more high demand specialized aptitudes of nowadays be required ten a long time from presently. How will the information science teach advance to meet the trade needs of a commercial center with ever increasing applications of AI. Mussaratjahan Korpali | Akshata Walikar | Kaveri Parshuram Vijapur \u00c4rtificial Intelligence: A Study of Automation, and Its Impact on Data Science\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-2 , February 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49316.pdf Paper URL: https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/49316\/artificial-intelligence-a-study-of-automation-and-its-impact-on-data-science\/mussaratjahan-korpali","57":"This study proposes Artificial Intelligence AI based path loss prediction models for the suburban areas of Abuja, Nigeria. The AI based models were created on the bases of two deep learning networks, namely the Adaptive Neuro Fuzzy Inference System ANFIS and the Generalized Radial Basis Function Neural network RBF NN . These prediction models were created, trained, validated and tested for path loss prediction using path loss data recorded at 1800MHz from multiple Base Transceiver Stations BTSs distributed across the areas under investigation. Results indicate that the ANFIS and RBF NN based models with Root Mean Squared Error RMSE values of 5.30dB and 5.31dB respectively, offer greater prediction accuracy over the widely used empirical COST 231 Hata, which has an RMSE of 8.18dB. Deme C. Abraham \"\u00c4n Artificial Intelligence Approach to Ultra-High Frequency Path Loss Modelling of the Suburban Areas of Abuja, Nigeria\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-2 , February 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd30227.pdf\r\n\r\nPaper Url : https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/30227\/an-artificial-intelligence-approach-to-ultra-high-frequency-path-loss-modelling-of-the-suburban-areas-of-abuja-nigeria\/deme-c-abraham","58":"During the past century, energy consumption have increased drastically due to a wide variety of factors including both technological and population based. Therefore, increasing our energy efficiency is of great importance in order to achieve overall sustainability. Forecasting the building energy consumption is important for a wide variety of applications including planning, management, optimization, and conservation. Data driven models for energy forecasting have grown significantly within the past few decades due to their increased performance, robustness and ease of deployment. Amongst the many different types of models, among the most popular data driven approaches applied to date. This paper offers a review of Electricity consumption forecasting in office buildings an artificial intelligence approach for forecasting building energy use and demand, with a particular focus on reviewing the applications, data, forecasting models, and performance metrics used in model evaluations. Based on this review, existing research gaps are identified and presented. Aditya Sonar | Vinita Galande \"Review on Electricity Consumption Forecasting in Buildings: using Artificial Intelligence\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31303.pdf Paper Url :https:\/\/www.ijtsrd.com\/management\/management-development\/31303\/review-on-electricity-consumption-forecasting-in-buildings-using-artificial-intelligence\/aditya-sonar","59":"Artificial intelligence (AI) researchers have been developing and refining\r\nlarge language models (LLMs) that exhibit remarkable capabilities across a\r\nvariety of domains and tasks, challenging our understanding of learning and\r\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\r\nunprecedented scale of compute and data. In this paper, we report on our\r\ninvestigation of an early version of GPT-4, when it was still in active\r\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\r\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\r\nexhibit more general intelligence than previous AI models. We discuss the\r\nrising capabilities and implications of these models. We demonstrate that,\r\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\r\nspan mathematics, coding, vision, medicine, law, psychology and more, without\r\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\r\nperformance is strikingly close to human-level performance, and often vastly\r\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\r\ncapabilities, we believe that it could reasonably be viewed as an early (yet\r\nstill incomplete) version of an artificial general intelligence (AGI) system.\r\nIn our exploration of GPT-4, we put special emphasis on discovering its\r\nlimitations, and we discuss the challenges ahead for advancing towards deeper\r\nand more comprehensive versions of AGI, including the possible need for\r\npursuing a new paradigm that moves beyond next-word prediction. We conclude\r\nwith reflections on societal influences of the recent technological leap and\r\nfuture research directions.","60":"Technology has changed the organizations are done and in the course of the last couple of years. Sector wise, innovation is riding the pony of fortune and managing organizations to productive development, effortlessly. There is a developing need to utilize Artificial Intelligence AI and the Indian banking Sector is steadily moving itself towards utilizing AI. On the off chance that one discussions about the banking segment, the reception has been continuous, when contrasted with different areas. This can be expected to the reality that banking is as yet a labor driven part, with tasks that require human inclusion .Yet the Indian Banking part comprehends the need to chop down expense and the consumption on excess errands. The Indian financial area is investigating the ways by which it can saddle the intensity of AI to improve the procedures and upgrade the Customer Service in the long run. The paper looks to investigate the regions where the AI is being utilized in the Banking Sector whats more, its suggestion in the top banks in India. Prof. Lakshminarayana. N | Ms. Deepthi B. R. \"\u00c4dvent of Artificial Intelligence and its Impact on Top Leading Commercial Banks in India \u00e2\u20ac\u201c Case Study\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-4 , June 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd23850.pdf\r\n\r\n Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23850\/advent-of-artificial-intelligence-and-its-impact-on-top-leading-commercial-banks-in-india-\u00e2\u20ac\u201c-case-study\/prof-lakshminarayana-n","61":"Employees are the most valuable resource in any organization. It is the main cause of the success of a business. The evaluation of employee performance is a very important task as it shows how the employees skills are improving. The HR department is responsible for this task. The employee performance evaluation is complex and time consuming. In the last decade, information technology tools have undergone a huge evolution and can help with this complex task. Artificial Intelligence AI is a recent science that consists of building systems that imitate human behavior. It is widely used in many applications like healthcare, banking, and finance. This thesis focuses on studying the impact of artificial intelligence on the employee evaluation process. An analytical descriptive methodology was used. Intelligence is considered an independent variable. However, employee performance evaluation is the dependent variable DV . Four dimensions of the DV were considered objectives and key results, skills gap analysis, tracking training completion, and project or task management tools. A questionnaire of 34 items was built and distributed to the IT employees of the Babylon Education Directorate. 295 answers were collected. SPSS software was used to examine the answers. The MANOVA test was applied to validate the study hypotheses. The result shows that AI has a significant positive impact on the evaluation of employees performance in all dimensions. Ali Al Imari | Saeed Abdallah \"The Role of Artificial Intelligence Applications in Managing the Employee Performance Evaluation: An Iraqi Case Study\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-3 , June 2023, URL: https:\/\/www.ijtsrd.com.com\/papers\/ijtsrd57454.pdf Paper URL: https:\/\/www.ijtsrd.com.com\/computer-science\/other\/57454\/the-role-of-artificial-intelligence-applications-in-managing-the-employee-performance-evaluation-an-iraqi-case-study\/ali-al-imari","62":"There is no good evidence that facial expressions reveal a person\u2019s feelings. But big tech companies want you to believe otherwise.","63":"How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.","64":"Table of Contents\n\n                 Artificial Intelligence and Geography.\n\n                 A Brief History of Artificial Intelligence.\n\n                 Heuristic Search in Geography.\n\n                 Expert Systems and Intelligent Knowledge-based\n                 Systems.\n\n                 Neurocomputing.\n\n                 Applying Artificial Neural Networks.\n\n                 Evolutionary Computation, Genetic Algorithms, Evolution\n                 Strategies and Genetic Programming.\n\n                 Artificial Life.\n\n                 Fuzzy Logic, Fuzzy Systems and Soft\n                 Computing.\n\n                 Conclusions and Epilogue.\n\n                 Index.","65":"In a democracy, the link between government and society is fundamental to prevent corruption and to ensure the functioning of mechanisms for transparency, accountability of the guaranteeing bodies and access to information. This is why the State implements public policies on transparency. However, corruption has been a phenomenon that is present in the Public Administration and has not been able to diminish with these policies. The objective of this study is to analyze the variables on the perception of corruption in society using the Bayesian method. For this, the data were obtained from the National Institute of Statistics and Geography in the part of its portal National Survey on Quality and Government Impact 2015, in the section on corruption. For its later Bayesian analysis using the software ELVIRA through the algorithm K2. Erica Pascual-Garcia | Guillermo De la Torre-Gea\"Bayesian Analysis to the experiences of corruption through Artificial Intelligence\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-2 , February 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd2443.pdf  http:\/\/www.ijtsrd.com\/computer-science\/data-miining\/2443\/bayesian-analysis-to-the-experiences-of-corruption-through-artificial-intelligence\/erica-pascual-garcia","66":null,"67":"The artificial immune system (AIS) community has been vibrant and active for a number of years now, producing a prolific amount of research ranging from modeling the natural immune system, to tackling real world applications, using an equally diverse set of immune inspired algorithms. We review the current immune applications of the AIS approach, and propose a number of suggestions to the AIS community that can be undertaken to help move the area forward. Despite many successes of AIS techniques, there remain some open issues which have to be addressed in order to make the AIS a real-world problem solving technique.","68":"With the development of computing and information processing techniques, artificial intelligence (AI) has been extensively applied in education. Artificial intelligence in education (AIEd) opens new opportunities, potentials, and challenges in educational practices. In its short history, AIEd has been undergoing several paradigmatic shifts, which are characterized into three paradigms in this position paper: AI-directed, learner-as-recipient, AI-supported, learner-as-collaborator, and AI-empowered, learner-as-leader. In three paradigms, AI techniques are used to address educational and learning issues in varied ways. AI is used to represent knowledge models and direct cognitive learning while learners are recipients of AI service in Paradigm One; AI is used to support learning while learners work as collaborators with AI in Paradigm Two; AI is used to empower learning while learners take agency to learn in Paradigm Three. Overall, the development trend of AIEd has been developing to empower learner agency and personalization, enable learners to reflect on learning and inform AI systems to adapt accordingly, and lead to an iterative development of the learner-centered, data-driven, personalized learning.","69":"In the past twenty years, much time, effort, and money has been expended on designing an unambiguous representation of natural language to make them accessible to computer processing, These efforts have centered around creating schemata designed to parallel logical relations with relations expressed by the syntax and semantics of natural languages, which are clearly cumbersome and ambiguous in their function as vehicles for the transmission of logical data. Understandably, there is a widespread belief that natural languages are unsuitable for the transmission of many ideas that artificial languages can render with great precision and mathematical rigor. But this dichotomy, which has served as a premise underlying much work in the areas of linguistics and artificial intelligence, is a false one. There is at least one language, Sanskrit, which for the duration of almost 1000 years was a living spoken language with a considerable literature of its own. Besides works of literary value, there was a long philosophical and grammatical tradition that has continued to exist with undiminished vigor until the present century. Among the accomplishments of the grammarians can be reckoned a method for paraphrasing Sanskrit in a manner that is identical not only in essence but in form with current work in Artificial Intelligence. This article demonstrates that a natural language can serve as an artificial language also, and that much work in AI has been reinventing a wheel millenia old. First, a typical Knowledge Representation Scheme (using Semantic Nets) will be laid out, followed by an outline of the method used by the ancient Indian grammarians to analyze sentences unambiguously. Finally, the clear parallelism between the two will be demonstrated, and the theoretical implications of this equivalence will be given.","70":"This book is one of the oldest and most popular introductions to artificial intelligence. An accomplished artificial intelligence (AI) scientist, Winston heads MIT's Artificial Intelligence Laboratory, and his hands-on AI research experience lends authority to what he writes. Winston provides detailed pseudo-code for most of the algorithms discussed, so you will be able to implement and test the algorithms immediately. The book contains exercises to test your knowledge of the subject and helpful introductions and summaries to guide you through the material.","71":"The field of education has experienced a transformation as artificial intelligence (AI) becomes increasingly applicable for learning purposes. AI has the potential to transform the social interactions in educational contexts among learners, teachers, and technologies. In this systematic mapping review, we focus on mapping and framing trends for educational applications of AI in simulation-based learning. Fifty-nine studies met the inclusion and exclusion criteria. We coded and analyzed six mapped categories in this literature review: (1) the year-of-study trend, (2) methods, (3) AI technologies, (4) simulation, (5) study trends, and (6) learning principles and theories. To provide nuanced details from the included literature, we also synthesized three thematic trends: (1) AI built in virtual agents for simulation-based learning, (2) AI infused in simulation-based learning with affective computing, and (3) AI leveraged in simulation-based learning for assessments. Trend One builds on a general acknowledgement of virtual agents as a guide for situated learning. Trend Two posits the role of affective states in learning trajectories and suggests the related machine learning approaches. Trend Three discusses machine learning techniques and multimodal computing used for assessment and feedback. The paper concludes with implications and suggestions for research and practice in AI in education using simulation-based learning.","72":null,"73":"The influence of artificial intelligence (A.I) has played significant roles in the emerging world. Its applications have rapidly expanded even in the aerospace research, we can find influences of its concept. Hence the present reports briefly highlight the aerospace and its dependency on artificial intelligence. Kaveriappa. T 2018. The concept of artificial intelligence in aerospace .\u00a0International Journal on Integrated Education. 1, 1 (Dec. 2018), 58-61. DOI:https:\/\/doi.org\/10.31149\/ijie.v1i1.534 Pdf Url : https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/534\/509 Paper Url : https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/534","74":"Google is opening this platform to the world  which gives us an equal opportunity to peek in and see how the company thinks about developing machine learning systems. Internally  Google has spent the last five years building a massive platform for artificial intelligence and now they're unleashing it on the world. Although Google would prefer you call it machine intelligence they feel that the word artificial intelligence carries too many connotations and fundamentally they're trying to create genuine intelligence just in machine. The internet makes things faster  but faster is not always necessarily. Better we don't want to sacrifice our critical thinking and uniqueness for the attainment of a modest amount of additional productivity our cognitive thinking is one of our greatest possessions and we should take this extra time to preserve it. Prof. Rekha D. M | Sandhya G N \u00c4 Study on Google is an Artificial Encyclopedia Affecting Human Intelligence  An Empirical Study\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd)  ISSN: 2456-6470  Volume-3 | Issue-5   August 2019  URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd27923.pdfPaper URL: https:\/\/www.ijtsrd.com\/humanities-and-the-arts\/education\/27923\/a-study-on-google-is-an-artificial-encyclopedia-affecting-human-intelligence-%E2%80%93-an-empirical-study\/prof-rekha-d-m","75":"The term \u201csupply chain\u201d refers to a network of facilities that includes a variety of companies. To minimise the entire cost of the supply chain, these entities must collaborate. This research focuses on the use of Artificial Intelligence techniques in supply chain management. It includes supply chain management examples like as\r\ndemand forecasting, supply forecasting, text analytics, pricing panning, and more to help companies improve their processes, lower costs and risk, and boost revenue. It gives us a quick rundown of all the key principles of economics and how to comprehend and use them effectively.","76":"Artificial intelligence is part of almost every business today; it facilitates business operations, increases productivity, and offers a variety of ways to speed up communication processes. Artificial intelligence and software (or software applications installed on it), as well as automation through AI systems, perform many of the tasks previously performed by employees and workers. Switching to an automated working environment has resulted in a lot of unnecessary business expenses, substantial time savings and a gradual increase in profits. The automation through AI of various business processes has taken many companies and organizations to the next level in terms of production and management.So, this article explains the role of artificial intelligence, machine learning and cloud computing in business. Dr. Pawan Whig 2019. Artificial Intelligence and Machine Learning In Business.\u00a0International Journal on Integrated Education. 2, 2 (Jun. 2019). Pdf Url : https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/516\/493 Paper Url : https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/516","77":"Given the foreseeable pervasiveness of Artificial Intelligence in modern societies, it is legitimate and necessary to ask the question how this new technology must be shaped to support the maintenance and strengthening of constitutional democracy. This paper first describes the four core elements of today's digital power concentration, which need to be seen in cumulation and which, seen together, are both a threat to democracy and to functioning markets. It then recalls the experience with the lawless internet and the relationship between technology and the law as it has developed in the internet economy and the experience with GDPR before it moves on to the key question for AI in democracy, namely which of the challenges of AI can be safely and with good conscience left to ethics, and which challenges of AI need to be addressed by rules which are enforceable and encompass the legitimacy of democratic process, thus laws. The paper closes with a call for a new culture of incorporating the principles of Democracy, Rule of law and Human Rights by design in AI and a three level technological impact assessment for new technologies like AI as a practical way forward for this purpose.","78":"The world has gradually embraced Cryptocurrency at various levels of capacity and it is not governed or regulated by any Control system. It has over time been used by participants as a means to invest despite the volatility of the market. Though Government does not have the means to outright stop the virtual marketing deals of Cryptocurrency, recently Nigerian government called for deposit money banks to close all corresponding accounts which are perceived to be involved in Cryptocurrency trading. The Crypto market has evolved as the digital and internet world exponentially evolved. Sadly, Cryptocurrency has been misused by illegal participants and exploited wrongly which has caused a lot of worrisome issues around the world like loss of funds, hacking of financial databases, terrorist financing, and identity fraud. This paper aims to identify the methodology surrounding the world of Cryptocurrency and how advancement in technology through artificial intelligence can help solve ethical issues related to cryptocurrency trading. Based on these appraisals, we highlighted procedures to provide a safer, secure, and relatable market for every interested individual.","79":"Context is the challenge for the coming years in Artificial\nIntelligence (AI). In the companion paper, we present a view of\nhow context is considered through the literature in various\ndomains. In this paper, we present the main results of\ndiscussions at some workshops and the first conference focusing\non the notion of context. We point out the opposition between two\nviewpoints on context, namely the Engineering and the Cognitive\nones. We show that this opposition is only superficial because\nthey do not consider context at the same level, one is at the\nlevel of the knowledge representation, the other at the level of\nthe interaction between two agents.","80":"Presents a guide to artificial intelligence, covering such topics as intelligent agents, problem-solving, logical agents, planning, uncertainty, learning, and robotics.","81":"Artificial intelligence (AI) is becoming ubiquitous in the lives of both researchers and non-researchers, but AI models often lack transparency. To make well-informed and trustworthy decisions based on these models, people require explanations that indicate how to interpret the model outcomes. This paper presents our ongoing research in explainable AI, which investigates how visual analytics interfaces and visual explanations, tailored to the target audience and application domain, can make AI models more transparent and allow interactive steering based on domain expertise. First, we present our research questions and methods, contextualised by related work at the intersection of AI, human-computer interaction, and information visualisation. Then, we discuss our work so far in healthcare, agriculture, and education. Finally, we share our research ideas for additional studies in these domains.","82":"The branch of science and machinery which is concerned with computational understanding of an intelligent human behaviour is called Artificial Intelligence. This blend of science and machinery has proven to be of great benefit to human activities and medical science. Our aim is to accentuate that can antimicrobial resistance be fought by Artificial Intelligence. The various studies indicate that Artificial Intelligence can be a solution to the issue of antimicrobial resistance which is rising at a great pace throughout the world.","83":null,"84":"Recent research in artificial intelligence and machine learning has largely\r\nemphasized general-purpose learning and ever-larger training sets and more and\r\nmore compute. In contrast, I propose a hybrid, knowledge-driven,\r\nreasoning-based approach, centered around cognitive models, that could provide\r\nthe substrate for a richer, more robust AI than is currently possible.","85":"Context is the challenge for the coming years in Artificial\nIntelligence. In the companion paper, we present the main results\nof discussions at two workshops and at the first conference\nfocusing on the notion of context. In this paper, we present a\nview of how context is considered in knowledge acquisition,\nmachine learning, communication, and databases and ontologies. We\ndescribe the way in which context is modeled and represented in\nthe logic formalism and a rulebased formalism. We present briefly\nafter some of the other approaches, and sum up the different\npoints that may be of interest for modeling effectively context.","86":"It has been developed a traditional model based on Artificial Intelligence for natural language programming pursuant to RNN and a big dataset. The model makes its greatest contribution by improving the accuracy in the translation of informal language. The methodology consists on studying the training model that appy each of them (Goole Translate, Deepl and Bing). To do this, we have designed a dataset which includes more than 200 registers of poetic verses and 230 popular sentences - with differences between folk sayings and proverbs. Our aim is training a model and validating the results obtained from machine translators.","87":null,"88":null,"89":"Explains how it is possible for computers to reason and perceive, thus introducing the field called Artificial Intelligence. This book helps in learning why the field is important, both as a branch of engineering and as a science. It is aimed at computer scientists, engineers, psychologists, biologists, linguists and philosophers.","90":"The term Genetic Programming describes a research area\n                 within the general field of Artificial Intelligence\n                 that deals with the evolution of computer code. This\n                 area is an outgrowoftwo independent efforts in\n                 Artificial Intelligence, namely automatic programming\n                 and machine learning. Automatic programming is\n                 concerned with the induction of computer code which\n                 precisely fulfills certain functions, whereas machine\n                 learning studies improvement of computer programs\n                 through training and experience.","91":null,"92":"Fault diagnosis of rotating machinery plays a significant role for the reliability and safety of modern industrial systems. As an emerging field in industrial applications and an effective solution for fault recognition, artificial intelligence (AI) techniques have been receiving increasing attention from academia and industry. However, great challenges are met by the AI methods under the different real operating conditions. This paper attempts to present a comprehensive review of AI algorithms in rotating machinery fault diagnosis, from both the views of theory background and industrial applications. A brief introduction of different AI algorithms is presented first, including the following methods: k-nearest neighbour, naive Bayes, support vector machine, artificial neural network and deep learning. Then, a broad literature survey of these AI algorithms in industrial applications is given. Finally, the advantages, limitations, practical implications of different AI algorithms, as well as some new research trends, are discussed.","93":null,"94":"Renewable Energy Sources are those energy sources which are not destroyed when their energy is harnessed. The model of wind turbine is based on the steady state power characteristics of the turbine. The stiffness of the drive train is infinite and the friction factor and the inertia of the turbine must be combined with those of the generator coupled to the turbine. In this paper, variable speed wind turbine driving permanent magnet synchronous generator with current controlled voltage source inverter has been proposed. Detailed modeling and control strategies of the overall system has been developed. It is found that under the proposed control strategy the system runs smoothly under randomly and quickly varying wind condition. During both symmetrical and unsymmetrical fault conditions, the system is found stable. Results shows that by using Current Source Inverter CSI stability of wind power can be achieved. By using CSI based Controller voltage source inverter or any separate storage energy system is not required. A model is used to determine the behavior of the wind turbine, induction generator and load. Arshid Mehraj | Harjit Singh | Onkar Singh \"Design and Analysis of Artificial Intelligence Based Approach for Control of Wind Turbine\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-1 , December 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38107.pdf Paper URL : https:\/\/www.ijtsrd.com\/engineering\/electronics-and-communication-engineering\/38107\/design-and-analysis-of-artificial-intelligence-based-approach-for-control-of-wind-turbine\/arshid-mehraj","95":null,"96":null,"97":"This study investigates the affect of Artificial Intelligence AI and Internet of Things IoT in the transformation of E Business Segment. AI and IoT are starting to shape the longer term of many industries universally by creating an unprecedented amount of information. The objective of this study isnt to reproduce tests, but to explore and quantify the affect AI and IoT has within the transformative process of alter within the E Business segment. This ponder utilized a qualitative inquire about approach and information was collected through a orderly writing audit utilizing the snowballing look strategy. 18 peer looked into papers were identified and analyzed in connection to their pertinence to the study. Neha Bhujbal | Prof. Shrikant Nagure \u00c4 Review on the Impact of Artificial Intelligence and Internet of Things in the Transformation of E-Business Sector\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31305.pdf Paper Url :https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/31305\/a-review-on-the-impact-of-artificial-intelligence-and-internet-of-things-in-the-transformation-of-ebusiness-sector\/neha-bhujbal","98":"Coordination, the process by which an agent reasons about its local\n\tactions and the (anticipated) actions of others to try and ensure\n\tthe community acts in a coherent manner, is perhaps the key problem\n\tof the discipline of Distributed Artificial Intelligence (DAI). In\n\torder to make advances it is important that the theories and principles\n\twhich guide this central activity are uncovered and analysed in a\n\tsystematic and rigourous manner. To this end, this paper models agent\n\tcommunities using a distributed goal search formalism, and argues\n\tthat commitments (pledges to undertake a specific course of action)\n\tand conventions (means of monitoring commitments in changing circumstances)\n\tare the foundation of coordination in all DAI systems.","99":"Recent decades have witnessed the emergence of artificial intelligence as a serious science and engineering discipline. This textbook AI using a coherent framework to study the design of intelligent computational agents. By showing how basic approaches fit into a multidimensional design space, readers can learn the fundamentals without losing sight of the bigger picture. The book balances theory and experiment, showing how to link them intimately together, and develops the science of AI together with its engineering applications. AI is a rapidly developing field: this book encapsulates the latest results without being exhaustive and encyclopedic. The text is supported by an online learning environment, AIspac.org, so that students can experiment with the main AI algorithms plus problems, animations, lecture slides, and a knowledge representation system, AIlog, for experimentation and problem solving.","100":null,"101":"Recently, Artificial Intelligence approaches have been used to solve the problem of constraint based scheduling. The aim is to find a feasible solution which satisfies a maximum number of constraints within a reasonable amount of time. An AI system in this domain should not replicate the human scheduler but extend his capabilities by doing more problem solving than was manually possible. The constraints are used to help reduce the search space. This paper reviews various AI techniques used in constraint based scheduling, by comparing existing systems. The aim is to help future research concerning the application of AI to constraint based scheduling, by evaluating the progress that has been made in this area.","102":"There is no doubt, that Artificial Intelligent (AI) has a lot of contributions to the overall well-being of humanity and improving productivity, it is worth noting that there is also a negative impact associated with the use of the technology on ethical grounds ranging from privacy issue to lack of transparency. Quite a lot of work has been done to provide best practices in the development and use of these systems to gain the acceptance of the public; this is very essential for nations to exploit the full potential of AI. This work brings to the frontlines some of the risks\/issues associated with AI from different pieces of literature and provides commonplace or leading principles and frameworks required to develop responsible AI or autonomous systems. The guidelines encourage stakeholders in the industry of developing AI systems to incorporate facilities that support transparency, privacy protection, accountability, and contestability. These guidelines are not new, but they are contextualized to suit ethics in AI. Above all, AI systems should not be seen as being equal to or greater than man, but rather be seen as augmenting intelligent systems to assist human who is the only moral agents.","103":"This study examines the effect of information and communication technology ICT on human resource management performance through the mediating role represented by artificial intelligence AI within human resource departments and the IT division. Using survey data including 179 respondents and a factor analysis framework, we find that ICT engagement, as measured by management decision support systems, enterprise resource planning ERP , data access and analysis DAA technologies, process support and improvement PSI technologies, and communication technologies, has a positive and statistically significant effect on human resource management performance. Similarly, we find that artificial intelligence and information and communication technologies are positively associated with human resource management performance. The results imply that companies should do their best to promote and facilitate the engagement of ICT and AI to improve their HRM performance as well as their information system, which will produce positive results for the company structure. Kourda Hayat | Pr. Trebucq St\u00c3\u00a9phane \"The Impact of Information and Communication Technologies on the Performance of Human Resources Management and the Mediating Role of Artificial Intelligence\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42380.pdf Paper URL: https:\/\/www.ijtsrd.commanagement\/management-development\/42380\/the-impact-of-information-and-communication-technologies-on-the-performance-of-human-resources-management-and-the-mediating-role-of-artificial-intelligence\/kourda-hayat","104":"Artificial Intelligence based on Deep Learning (DL) is opening new horizons in biomedical research and promises to revolutionize the microscopy field. It is now transitioning from the hands of experts in computer sciences to biomedical researchers. Here, we introduce recent developments in DL applied to microscopy, in a manner accessible to non-experts. We give an overview of its concepts, capabilities and limitations, presenting applications in image segmentation, classification and restoration. We discuss how DL shows an outstanding potential to push the limits of microscopy, enhancing resolution, signal and information content in acquired data. Its pitfalls are discussed, along with the future directions expected in this field.","105":"This chapter traces the ethical issues around applying artificial intelligence (AI) in education from the early days of artificial intelligence in education in the 1970s to the current state of this field, including the increasing sophistication of the system interfaces and the rise in data use and misuse. While in the early days most tools were largely learner-facing, now there are tools that are teacher-facing, supporting their management of the classroom, and administrator-facing, assisting in their management of cohorts of students. Learner-facing tools now take into account the affective and motivational aspects of learning as well as the cognitive. The rise of data collection and its associated analytic tools has enabled the development of dashboards for the dynamic management and reflective understanding of learners, teachers, and administrators. Ethical issues hardly figured in the early days of the field but now they loom large. This is because of the legitimate fears that learners' and teachers' autonomy will be compromised, that learner data will be collected and potentially misappropriated for other purposes, and that AI will introduce extra biases into educational decisions and increase existing inequity and also because of the scary reputation that AI has in general.","106":"We have been using robots in our artificial intelligence course since fall 2000. We have been using the robots for open-laboratory projects. The projects are designed to emphasize high-level knowledge-based AI algorithms. After three offerings of the course, we paused to analyze the collected data and to see if we could answer the following questions: (i) Are robot projects effective at helping students learn AI concepts? (ii) What advantages, if any, can be attributed to using robots for AI projects? (iii) What are the downsides of using robots for traditional projects in AI? In this article we discuss the results of our evaluation and list the lessons learned.","107":"'Decision Theory = Probability + Utility Theory' plus 'Universal Induction = Ockham + Bayes + Turing' results in '= A Unified View of Artificial Intelligence'. This book presents sequential decision theory from a novel algorithmic information theory perspective. While the former is suited for active agents in known environments, the latter is suited for passive prediction in unknown environments. The book introduces these two well-known but very different ideas and removes the limitations by unifying them to one parameter-free theory of an optimal reinforcement learning agent embedded in an arbitrary unknown environment. Most if not all AI problems can easily be formulated within this theory, which reduces the conceptual problems to pure computational ones. Considered problem classes include sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations to other approaches to AI. One intention of this book is to excite a broader AI audience about abstract algorithmic information theory concepts, and conversely to inform theorists about exciting applications to AI.","108":"Ethics is a central and growing concern in all applications utilizing artificial intelligence (AI). Earth observation (EO) and remote sensing (RS) research relies heavily on both big data and AI or machine learning (ML). While this reliance is not new, with increasing image resolutions and the growing number of EO\/RS use cases that have a direct impact on governance, policy, and the lives of people, ethical issues are taking center stage. In this article, we provide scientists engaged with AI for EO (AI4EO) research, 1) a practically useful overview of the key ethical issues emerging in this field, with concrete examples from within EO\/RS to explain these issues, and 2) a first road map (flowchart) that scientists can use to identify ethical issues in their ongoing research. With this, we aim to sensitize scientists to these issues and create a bridge to facilitate constructive and regular communication among scientists engaged in AI4EO research, on the one hand, and ethics research, on the other hand. The article also provides detailed illustrations from four AI4EO research fields to explain how scientists can redesign research questions to more effectively grab ethical opportunities to address real-world problems that are otherwise akin to ethical dilemmas with no win-win solution in sight. The article concludes by providing recommendations to institutions that want to support ethically mindful AI4EO research and provides suggestions for future research in this field.","109":"Artificial intelligence and machine learning capabilities are growing at an unprecedented rate. These technologies have many widely beneficial applications, ranging from machine translation to medical image analysis. Countless more such applications are being developed and can be expected over the long term. Less attention has historically been paid to the ways in which artificial intelligence can be used maliciously. This report surveys the landscape of potential security threats from malicious uses of artificial intelligence technologies, and proposes ways to better forecast, prevent, and mitigate these threats. We analyze, but do not conclusively resolve, the question of what the long-term equilibrium between attackers and defenders will be. We focus instead on what sorts of attacks we are likely to see soon if adequate defenses are not developed.\r\nThe report builds on a 2 day workshop held in Oxford, UK, in February 2017. More information can be found in Appendix A of the report.","110":"We automatically create enormous, free and multilingual silver-standard training annotations for named entity recognition (ner) by exploiting the text and structure of Wikipedia. Most ner systems rely on statistical models of annotated data to identify and classify names of people, locations and organisations in text. This dependence on expensive annotation is the knowledge bottleneck our work overcomes.\r\n\r\nWe first classify each Wikipedia article into named entity (ne) types, training and evaluating on 7200 manually-labelled Wikipedia articles across nine languages. Our cross-lingual approach achieves up to 95% accuracy.\r\n\r\nWe transform the links between articles into ne annotations by projecting the target article\u02bcs classifications onto the anchor text. This approach yields reasonable annotations, but does not immediately compete with existing gold-standard data. By inferring additional links and heuristically tweaking the Wikipedia corpora, we better align our automatic annotations to gold standards.\r\n\r\nWe annotate millions of words in nine languages, evaluating English, German, Spanish, Dutch and Russian Wikipedia-trained models against conll shared task data and other gold-standard corpora. Our approach outperforms other approaches to automatic ne annotation (Richman and Schone, 2008 61, Mika et al., 2008 46) competes with gold-standard training when tested on an evaluation corpus from a different source; and performs 10% better than newswire-trained models on manually-annotated Wikipedia text.","111":null,"112":"Astrophysical processes such as feedback from supernovae and active galactic\r\nnuclei modify the properties and spatial distribution of dark matter, gas, and\r\ngalaxies in a poorly understood way. This uncertainty is one of the main\r\ntheoretical obstacles to extract information from cosmological surveys. We use\r\n2,000 state-of-the-art hydrodynamic simulations from the CAMELS project\r\nspanning a wide variety of cosmological and astrophysical models and generate\r\nhundreds of thousands of 2-dimensional maps for 13 different fields: from dark\r\nmatter to gas and stellar properties. We use these maps to train convolutional\r\nneural networks to extract the maximum amount of cosmological information while\r\nmarginalizing over astrophysical effects at the field level. Although our maps\r\nonly cover a small area of $(25~h^-1Mpc)^2$, and the different fields\r\nare contaminated by astrophysical effects in very different ways, our networks\r\ncan infer the values of $\u00d8mega_m$ and $\\sigma_8$ with a few percent\r\nlevel precision for most of the fields. We find that the marginalization\r\nperformed by the network retains a wealth of cosmological information compared\r\nto a model trained on maps from gravity-only N-body simulations that are not\r\ncontaminated by astrophysical effects. Finally, we train our networks on\r\nmultifields -- 2D maps that contain several fields as different colors or\r\nchannels -- and find that not only they can infer the value of all parameters\r\nwith higher accuracy than networks trained on individual fields, but they can\r\nconstrain the value of $\u00d8mega_m$ with higher accuracy than the maps from\r\nthe N-body simulations.","113":null,"114":null,"115":null,"116":null,"117":null,"118":"Artificial intelligence research has thrived in the years since this best-selling AI classic was first published. The revision encompasses these advances by adapting its coding to Common Lisp, the well-documented language standard, and by bringing together even more useful programming tools. Today's programmers in AI will find this volume's superior coverage of programming techniques and easily applicable style anything but common.","119":"The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence.","120":null,"121":"Science funding agencies (NASA, DOE, and NSF), the science community, and the\r\nUS taxpayer have all benefited enormously from the several-decade series of\r\nNational Academies Decadal Surveys. These Surveys are one of the primary means\r\nwhereby these agencies may align multi-year strategic priorities and funding to\r\nguide the scientific community. They comprise highly regarded subject matter\r\nexperts whose goal is to develop a set of science and program priorities that\r\nare recommended for major investments in the subsequent 10+ years. They do this\r\nusing both their own professional knowledge and by synthesizing details from\r\nmany thousands of existing and solicited documents.\r\n  Congress, the relevant funding agencies, and the scientific community have\r\nplaced great respect and value on these recommendations. Consequently, any\r\nsignificant changes to the process of determining these recommendations should\r\nbe scrutinized carefully. That said, we believe that there is currently\r\nsufficient justification for the National Academies to consider some changes.\r\nWe advocate that they supplement the established survey process with\r\npredictions of promising science priorities identified by application of\r\ncurrent Artificial Intelligence (AI) techniques These techniques are being\r\napplied elsewhere in long-range planning and prioritization.\r\n  We present a proposal to apply AI to aid the Decadal Survey panel in\r\nprioritizing science objectives. We emphasize that while AI can assist a mass\r\nreview of papers, the decision-making remains with humans. In our paper below\r\nwe summarize the case for using AI in this manner and suggest small inexpensive\r\ndemonstration trials, including an AI\/ML assessment of the white papers\r\nsubmitted to Astro2020 and backcasting to evaluate AI in making predictions for\r\nthe 2010 Decadal Survey.","122":"Data integration is a problem at the intersection of the fields of Artificial Intelligence and Database Systems. The goal of a data integration system is to provide a uniform interface to a multitude of data sources, whether they are within one enterprise or on the World-Wide Web. The key challenges in data integration arise because the data sources being integrated have been designed independently for autonomous applications, and their contents are related in subtle ways. As a result, a data integration system requires rich formalisms for describing contents of data sources and relating between contents of different sources. This paper discusses works aimed at applying techniques from...","123":null,"124":"This paper sheds light on the role of digital platform labour in the development of today\u2019s artificial intelligence, predicated on data-intensive machine learning algorithms. Focus is on the specific ways in which outsourcing of data tasks to myriad \u2018micro-workers\u2019, recruited and managed through specialized platforms, powers virtual assistants, self-driving vehicles and connected objects. Using qualitative data from multiple sources, we show that micro-work performs a variety of functions, between three poles that we label, respectively, \u2018artificial intelligence preparation\u2019, \u2018artificial intelligence verification\u2019 and \u2018artificial intelligence impersonation\u2019. Because of the wide scope of application of micro-work, it is a structural component of contemporary artificial intelligence production processes \u2013 not an ephemeral form of support that may vanish once the technology reaches maturity stage. Through the lens of micro-work, we prefigure the policy implications of a future in which data technologies do not replace human workforce but imply its marginalization and precariousness.","125":"Previous treatments of Artificial Intelligence (AI) divide the subject into its major areas of application, namely, natural language processing, automatic programming, robotics, machine vision, automatic theorem proving, intelligent data retrieval systems, etc. The major difficulty with this approach is that these application areas are now so extensive, that each could, at best, be only superficially treated in a book of this length. Instead, I have attempted here to describe fundamental AI ideas that underlie many of these applications. My organization of these ideas is not, then, based on the subject matter of their application, but is, instead, based on general computational concepts involving the kinds of data structures used, the types of operations performed on these data structures, and the properties of control strategies used by AI systems. I stress, in particular, the important roles played in AI by generalized production systems and the predicate calculus. The notes on which the book is based evolved in courses and seminars at Stanford University and at the University of Massachusetts at Amherst. Although certain topics treated in my previous book, Problem-solving Methods in Artificial Intelligence, are covered here as well, this book contains many additional topics such as rule-based systems, robot problem-solving systems, and structured-object representations.","126":"Summarizing a text, differentiating and learning actions taken by subjets, is something remains out beyond the reach of any artificial intelligence? Because machines cannot reason but combine data and order them according to the preestabished algorithms. The study referred to an analysis about how machine translations based on Artificial Intelligence hold influence over Spanish language. It is studied for a possible mechanization of traditions (assigned roles to occupations), cultural and gender biases, even language discrimination (formal and informal). The work of machine translators become incompetent. Based on the results, we understand that the most accurate translator to interpret Spanish language is DeepL.","127":null,"128":"The living cell exists by virtue of thousands of nonlinearly interacting processes. This complexity greatly impedes its understanding. The standard approach to the calculation of the behaviour of the living cell, or part thereof, integrates all the rate equations of the individual processes. If successful extremely intensive calculations often lead the calculation of coherent, apparently simple, cellular \"decisions\" taken in response to a signal: the complexity of the behavior of the cell is often smaller than it might have been. The \"decisions\" correspond to the activation of entire functional units of molecular processes, rather than individual ones. The limited complexity of signal and response suggests that there might be a simpler way to model at least some important aspects of cell function. In the field of Artificial Intelligence, such simpler modelling methods for complex systems have been developed. In this paper, it is shown how the Artificial Intelligence description method for deliberative agents functioning on the basis of beliefs, desires and intentions as known in Artificial Intelligence, can be used successfully to describe essential aspects of cellular regulation. This is demonstrated for catabolite repression and substrate induction phenomena in the bacterium Escherichia coli. The method becomes highly efficient when the computation is automated in a Prolog implementation. By defining in a qualitative way the food supply of the bacterium, the make-up of its catabolic pathways is readily calculated for cases that are sufficiently complex to make the traditional human reasoning tedious and error prone.","129":"Software has partially or fully displaced many former human activities, such as catching speeders or flying airplanes, and proven itself able to surpass humans in certain contests, like Chess and Go. What are the prospects for the displacement of human courts as the centerpiece of legal decisionmaking? Based on the case study of hate speech control on major tech platforms, particularly on Twitter and Facebook, this Essay suggests displacement of human courts remains a distant prospect, but suggests that hybrid machine\u2013human systems are the predictable future of legal adjudication, and that there lies some hope in that combination, if done well.","130":null,"131":"In the last years, Artificial Intelligence (AI) has achieved a notable\r\nmomentum that may deliver the best of expectations over many application\r\nsectors across the field. For this to occur, the entire community stands in\r\nfront of the barrier of explainability, an inherent problem of AI techniques\r\nbrought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not\r\npresent in the last hype of AI. Paradigms underlying this problem fall within\r\nthe so-called eXplainable AI (XAI) field, which is acknowledged as a crucial\r\nfeature for the practical deployment of AI models. This overview examines the\r\nexisting literature in the field of XAI, including a prospect toward what is\r\nyet to be reached. We summarize previous efforts to define explainability in\r\nMachine Learning, establishing a novel definition that covers prior conceptual\r\npropositions with a major focus on the audience for which explainability is\r\nsought. We then propose and discuss about a taxonomy of recent contributions\r\nrelated to the explainability of different Machine Learning models, including\r\nthose aimed at Deep Learning methods for which a second taxonomy is built. This\r\nliterature analysis serves as the background for a series of challenges faced\r\nby XAI, such as the crossroads between data fusion and explainability. Our\r\nprospects lead toward the concept of Responsible Artificial Intelligence,\r\nnamely, a methodology for the large-scale implementation of AI methods in real\r\norganizations with fairness, model explainability and accountability at its\r\ncore. Our ultimate goal is to provide newcomers to XAI with a reference\r\nmaterial in order to stimulate future research advances, but also to encourage\r\nexperts and professionals from other disciplines to embrace the benefits of AI\r\nin their activity sectors, without any prior bias for its lack of\r\ninterpretability.","132":"Neuro-symbolic artificial intelligence refers to a field of research and applications that combines machine learning methods based on artificial neural networks, such as deep learning, with symbolic approaches to computing and artificial intelligence (AI), as can be found for example in the AI subfield of knowledge representation and reasoning. Neuro-symbolic AI has a long history; however, it remained a rather niche topic until recently, when landmark advances in machine learning\u2014prompted by deep learning\u2014caused a significant rise in interest and research activity in combining neural and symbolic methods. In this overview, we provide a rough guide to key research directions, and literature pointers for anybody interested in learning more about the field.","133":"This article examines the impact and effectiveness of artificial intelligence in the management of technological enterprises improving technological processes at enterprises. Artificial intelligence analyzes the use of artificial intelligence systems in various areas of enterprise management, the role of artificial intelligence in the sustainable development of enterprises.","134":"Artificial intelligence (AI) is a technology that can learn, understand and act on the information it receives. AI identifies and prioritizes risks, giving IT professionals the ability to instantly identify malware on their networks and develop an incident response strategy. AI has many applications in information security management, in particular; incident response, disruption prediction, performance monitoring and inventory management. The problems faced by AI in its application for information security management can be divided into digital, physical and political, while the methods of applying AI are artificial narrow intelligence (ANI), artificial general intelligence (AGI) and artificial superintelligence (ASI). ). The paper discusses the applications of artificial intelligence (AI) in information security management, discusses its benefits and challenges, and recommends areas for future research.","135":"Over the coming decades, Artificial Intelligence will profoundly impact the way we live, work, wage war, play, seek a mate, educate our young, and care for our elderly. It is likely to greatly increase our aggregate wealth, but it will also upend our labor markets, reshuffle our social order, and strain our private and public institutions. Eventually it may alter how we see our place in the universe, as machines pursue goals independent of their creators and outperform us in domains previously believed to be the sole dominion of humans. Whether we regard them as conscious or unwitting, revere them as a new form of life or dismiss them as mere clever appliances, is beside the point. They are likely to play an increasingly critical and intimate role in many aspects of our lives. The emergence of systems capable of independent reasoning and action raises serious questions about just whose interests they are permitted to serve, and what limits our society should place on their creation and use. Deep ethical questions that have bedeviled philosophers for ages will suddenly arrive on the steps of our courthouses. Can a machine be held accountable for its actions? Should intelligent systems enjoy independent rights and responsibilities, or are they simple property? Who should be held responsible when a self-driving car kills a pedestrian? Can your personal robot hold your place in line, or be compelled to testify against you? If it turns out to be possible to upload your mind into a machine, is that still you? The answers may surprise you.","136":null,"137":null,"138":null,"139":null,"140":null,"141":null,"142":null,"143":"The third edition of this best-selling guide to Prolog and Artificial Intelligence has been updated to include key developments in the field while retaining its lucid approach to these topics. Divided into two parts, the first part of the book introduces the programming language Prolog, while the second part teaches Artificial Intelligence using Prolog as a tool for the implementation of AI techniques. Prolog has its roots in logic, however the main aim of this book is to teach Prolog as a practical programming tool. This text therefore concentrates on the art of using the basic mechanisms of Prolog to solve interesting problems. The third edition has been fully revised and extended to provide an even greater range of applications, which further enhance its value as a self-contained guide to Prolog, AI or AI Programming for students and professional programmers alike.","144":"Artificial intelligence (AI)-based methods have emerged as powerful tools to transform medical care. Although machine learning classifiers (MLCs) have already demonstrated strong performance in image-based diagnoses, analysis of diverse and massive electronic health record (EHR) data remains challenging. Here, we show that MLCs can query EHRs in a manner similar to the hypothetico-deductive reasoning used by physicians and unearth associations that previous statistical methods have not found. Our model applies an automated natural language processing system using deep learning techniques to extract clinically relevant information from EHRs. In total, 101.6 million data points from 1,362,559 pediatric patient visits presenting to a major referral center were analyzed to train and validate the framework. Our model demonstrates high diagnostic accuracy across multiple organ systems and is comparable to experienced pediatricians in diagnosing common childhood diseases. Our study provides a proof of concept for implementing an AI-based system as a means to aid physicians in tackling large amounts of data, augmenting diagnostic evaluations, and to provide clinical decision support in cases of diagnostic uncertainty or complexity. Although this impact may be most evident in areas where healthcare providers are in relative shortage, the benefits of such an AI system are likely to be universal.","145":"In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.","146":"According to this fact that educational system is the base of constant development in every country and this\r\nsystem educates human-forces and this forces,are accelerators and a factor, of achieving the goals of\r\ndevelopment,the educational system can play, Major role in the context economic behavior, in this context\r\nsome concepts are regarded as behavioral targets and performance.In educational system, handling\r\nartificial intelligence, in teaching and learning process, had a surprising evolution through educational\r\nadvantages, making job, respecting customers rights and customer relationship management, to assist\r\npriority and citizenship, correct investment through formal markets. Science-Based economy, resistible\r\neconomy and a positive view to job and Iran capital,including concepts which can be institutionalize in to\r\nthe educational system. In this paper it is decided to pose a new method, creating a proper cultural and\r\nscientific bed, this helps.That the educational system behavioral goals, better and stable being achieved.\r\nThe method presented in this paper is general and based on handling artificial intelligence, information\r\ntechnology and electronic content management that means in an intelligent educational system.The\r\neducational goals can be better achieved and managed by new technology of education.","147":"Applications of artificial intelligence in education (AIEd) are emerging and are new to researchers and practitioners alike. Reviews of the relevant literature have not examined how AI technologies have been integrated into each of the four key educational domains of learning, teaching, assessment, and administration. The relationships between the technologies and learning outcomes for students and teachers have also been neglected. This systematic review study aims to understand the opportunities and challenges of AIEd by examining the literature from the last 10 years (2012\u20132021) using matrix coding and content analysis approaches. The results present the current focus of AIEd research by identifying 13 roles of AI technologies in the key educational domains, 7 learning outcomes of AIEd, and 10 major challenges. The review also provides suggestions for future directions of AIEd research.","148":"Artificial Intelligence as a field of research and also its criticism is dominated by notions such as \u2018intelligence\u2019, \u2018learning\u2019 or \u2018neuronal\u2019. This paper discusses how the use of anthropomorphising language is fueling AI hype. AI hype involves many promises, such as that \u2018AI can be creative\u2019, or \u2018AI can solve world hunger\u2019. This hype is problematic since it covers up the negative consequences of AI use. Instead, the author proposes to use alternative terminology such as: \u2018Automated Pattern Recognition\u2019, \u2018Machine Conditioning\u2019, or \u2018Weighted Network\u2019.","149":"This presentation is about a potential shortcut to\n                 artificial intelligence by trading mind-design for\n                 world-design using artificial evolution. Evolutionary\n                 algorithms are a pump for turning CPU cycles into\n                 brain designs. With exponentially increasing CPU\n                 cycles while our understanding of intelligence is\n                 almost a flat-line, the evolutionary route to AI is a\n                 centerpiece of most Kurzweilian singularity scenarios.\n                 This talk introduces the Polyworld artificial life\n                 simulator as well as results from our ongoing attempt\n                 to evolve artificial intelligence and further the\n                 Singularity. Polyworld is the brain child of Apple\n                 Computer Distinguished Scientist Larry Yaeger, who\n                 remains the primary developer of Polyworld:\n                 http:\/\/www.beanblossom.in.us\/larryy\/P... Speaker:\n                 Virgil Griffith Virgil Griffith is a first year\n                 graduate student in Computation and Neural Systems at\n                 the California Institute of Technology. On weekdays he\n                 studies evolution, computational neuroscience, and\n                 artificial life. He did computer security work until\n                 his first year of university when his work got him sued\n                 for sedition and espionage. He then decided that\n                 security was probably not safest field to be in and he\n                 turned his life to science.","150":"The aim of Artificial Intelligence is to develop the machines to perform the tasks in a better way than the humans. Another aim of Artificial Intelligence is to understand the actions whether it occurs in humans, machines or animals. As a result, Artificial Intelligence is gaining Importance in science and engineering fields. The use of Artificial Intelligence in medical diagnosis too is becoming increasingly common and has been used widely in the diagnosis of cancers, tumors, hepatitis, lung diseases, etc... The main aim of this paper is to build an Artificial Intelligent System that after analysis of certain parameters can predict that whether a person is diabetic or not. Diabetes is the name used to describe a metabolic condition of having higher than normal blood sugar levels. Diabetes is becoming increasingly more common throughout the world, due to increased obesity - which can lead to metabolic syndrome or pre-diabetes leading to higher incidences of type 2 diabetes. Authors have identified 10 parameters that play an important role in diabetes and prepared a rich database of training data which served as the backbone of the prediction algorithm. Keeping in view this training data authors developed a system that uses the artificial neural networks algorithm to serve the purpose. These are capable of predicting new observations (on specific variables) from previous observations (on the same or other variables) after executing a process of so-called learning from existing training data (Haykin1998).The results indicate that the ANN is the best predictor with the accuracy of about 96%. This system can be used to assist medical programs especially in geographically remote areas where expert human diagnosis not possible with an advantage of minimal expenses and faster results. Abid Sarwar\"Diagnosis of hyperglycemia using Artificial Neural Networks\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-1 , December 2017, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd7045.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/7045\/diagnosis-of-hyperglycemia-using--artificial-neural-networks\/abid-sarwar","151":null,"152":null,"153":"A scientist who has spent a career developing Artificial Intelligence takes a realistic look at the technological challenges and assesses the likely effect of AI on the future.\u00a0How will Artificial Intelligence (AI) impact our lives? Toby Walsh, one of the leading AI researchers in the world, takes a critical look at the many ways in which \"thinking machines\" will change our world.Based on a deep understanding of the technology, Walsh describes where Artificial Intelligence is today, and where it will take us. \u00a0\u00a0\u00a0\u2022\u00a0Will automation take away most of our jobs? \u00a0\u00a0\u00a0\u2022\u00a0Is a \"technological singularity\" near? \u00a0\u00a0\u00a0\u2022\u00a0What is the chance that robots will take over? \u00a0\u00a0\u00a0\u2022\u00a0How do we best prepare for this future? The author concludes that, if we plan well, AI could be our greatest legacy, the last invention human beings will ever need to make.","154":"Has an identifiable core of activities called AI been established, during the AI boom in the eighties? Is AI already in a \"paradigmatic\" phase? There has been a lot of disagreement among commentators and specialists about the nature of artificial intelligence as a speciality. This makes AI an interesting case of an emerging speciality. We use aggregated journal-journal citations for describing artificial intelligence as sets of journals; factor analytic techniques are used to analyze the development of AI in terms of (an emerging) stability and coherency of the journal sets during the period 1982-1992. The analysis teaches us that AI has emerged as a set of journals with the characteristics of a discipline only since 1988. The thereafter relatively stable set of journals includes both fundamental and applied AI journals, and journals with a focus on expert systems. Additionally, specialities related to artificial intelligence (like pattern analysis, computer science, cognitive psychology) are identified. Neural network research is a part neither of AI nor of its direct citation environment. Information science is related to AI only in the early eighties. The citation environment of AI is more stable than AI itself","155":null,"156":"Considerable buzz surrounds artificial intelligence, and, indeed, AI is all around us. As with any software-based technology, it is also prone to vulnerabilities. Here, the author examines how we determine whether AI is sufficiently reliable to do its job and how much we should trust its outcomes.","157":null,"158":"In 2017 a very powerful AI (artificial intelligence) tool has been established for predicting lysine phosphoglycerylation sites in proteins, one of the most important post modifications in proteins","159":"In this paper, we review the role of probabilistic graphical models\r\n\tin artificial intelligence. We start by giving an account of the\r\n\tearly years when there was important controversy about the suitability\r\n\tof probability for intelligent systems. We then discuss the main\r\n\tmilestones for the foundations of graphical models starting with\r\n\tPearl's pioneering work. Some of the main techniques for problem\r\n\tsolving (abduction, classification, and decision-making) are briefly\r\n\texplained. Finally, we propose some important challenges for future\r\n\tresearch and highlight relevant applications (forensic reasoning,\r\n\tgenomics and the use of graphical models as a general optimization\r\n\ttool).","160":"The artificial evolution of intelligence is discussed\n                 with respect to current methods. An argument for\n                 withdrawal of the traditional \ufffdfitness function\ufffd in\n                 genetic algorithms is given on the grounds that this\n                 would better enable the emergence of intelligence,\n                 necessary because we cannot specify what intelligence\n                 is. A modular developmental system is constructed to\n                 aid the evolution of neural structures and a simple\n                 virtual world with many of the properties believed\n                 beneficial is set up to test these ideas. Resulting\n                 emergent properties are given, along with a brief\n                 discussion.","161":"Artificial intelligence (AI) has infiltrated our everyday lives, and there have been extremely promising applications of AI in the area of health in the previous decade, including medical imaging, in vitro diagnostics, intelligent rehabilitation, and prognosis. Breast cancer is one of the most prevalent malignant tumors in women, and it poses a major danger to both physical and emotional health. Early detection of breast cancer by mammography, ultrasound, and magnetic resonance imaging (MRI) may greatly improve patients' prognoses. AI has shown exceptional performance in picture recognition tasks and has been extensively researched in breast cancer screening. This study discusses the history of artificial intelligence (AI) and its applications in breast medical imaging (mammography, ultrasound, and MRI), such as lesion recognition, segmentation, and classification; breast density evaluation; and breast cancer risk assessment. In addition, we examine the limitations and future prospects of using AI in medical imaging of the breast.","162":"The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors' duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students' assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students' needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.","163":"Blockchain is an essentially distributed database recording all transactions\r\nor digital events among participating parties. Each transaction in the records\r\nis approved and verified by consensus of the participants in the system that\r\nrequires solving a hard mathematical puzzle, which is known as proof-of-work.\r\nTo make the approved records immutable, the mathematical puzzle is not trivial\r\nto solve and therefore consumes substantial computing resources. However, it is\r\nenergy-wasteful to have many computational nodes installed in the blockchain\r\ncompeting to approve the records by just solving a meaningless puzzle. Here, we\r\npose proof-of-work as a reinforcement-learning problem by modeling the\r\nblockchain growing as a Markov decision process, in which a learning agent\r\nmakes an optimal decision over the environment's state, whereas a new block is\r\nadded and verified. Specifically, we design the block verification and\r\nconsensus mechanism as a deep reinforcement-learning iteration process. As a\r\nresult, our method utilizes the determination of state transition and the\r\nrandomness of action selection of a Markov decision process, as well as the\r\ncomputational complexity of a deep neural network, collectively to make the\r\nblocks not easy to recompute and to preserve the order of transactions, while\r\nthe blockchain nodes are exploited to train the same deep neural network with\r\ndifferent data samples (state-action pairs) in parallel, allowing the model to\r\nexperience multiple episodes across computing nodes but at one time. Our method\r\nis used to design the next generation of public blockchain networks, which has\r\nthe potential not only to spare computational resources for industrial\r\napplications but also to encourage data sharing and AI model design for common\r\nproblems.","164":"This is an overview of classical artificial intelligence (AI) programming via\nactual implementation of landmark systems (case studies). For the student\ninterested in AI, \\_Paradigms of Artificial Intelligence Programming\\_ is an\ninvaluable history lesson. Even the programmer who is relatively uninterested\nin AI will find value in the book's basic introduction to Lisp and case\nstudies written in Lisp. But perhaps the book's best feature is its\ninformation on efficiency considerations in Lisp. \\_Paradigms of Artificial\nIntelligence Programming\\_ is worth purchasing for these discussions alone,\nwhich provide a wealth of useful guidelines for optimizing your code.","165":"Deep learning is a collection of machine learning algorithms utilizing multiple layers, with which higher levels of raw data are slowly removed. For example, lower layers can recognize edges in image processing whereas higher layers may define concepts for humans such as numbers or letters or faces. In this paper we have done a literature survey of some other papers to know how useful is Deep Learning and how to define other Artificial Intelligence things using Deep Learning. Anirban Chakraborty \u00c4 Study of Deep Learning Applications\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31629.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31629\/a-study-of-deep-learning-applications\/anirban-chakraborty","166":"Social insects--ants, bees, termites, and wasps--can be viewed as powerful problem-solving systems with sophisticated collective intelligence. Composed of simple interacting agents, this intelligence lies in the networks of interactions among individuals and between individuals and the<br>environment. A fascinating subject, social insects are also a powerful metaphor for artificial intelligence, and the problems they solve--finding food, dividing labor among nestmates, building nests, responding to external challenges--have important counterparts in engineering and computer science. <br> <br>This book provides a detailed look at models of social insect behavior and how to apply these models in the design of complex systems. The book shows how these models replace an emphasis on control, preprogramming, and centralization with designs featuring autonomy, emergence, and distributed<br>functioning. These designs are proving immensely flexible and robust, able to adapt quickly to changing environments and to continue functioning even when individual elements fail. In particular, these designs are an exciting approach to the tremendous growth of complexity in software and<br>information. Swarm Intelligence draws on up-to-date research from biology, neuroscience, artificial intelligence, robotics, operations research, and computer graphics, and each chapter is organized around a particular biological example, which is then used to develop an algorithm, a multiagent<br>system, or a group of robots. The book will be an invaluable resource for a broad range of disciplines.","167":"As the artificial intelligence (AI) systems in military simulations\n\tand computer games become more complex, their actions become increasingly\n\tdifficult for users to understand. Expert systems for medical diagnosis\n\thave addressed this challenge though the addition of explanation\n\tgeneration systems that explain a system\u00e2\u20ac\u2122s internal processes. This\n\tpaper describes the AI architecture and associated explanation capability\n\tused by Full Spectrum Command, a training system developed for the\n\tU.S. Army by commercial game developers and academic researchers.","168":null,"169":"This book provides a detailed understanding of the broad issues in artificial intelligence and a useful survey of current AI technology. The author delivers broad coverage of innovative representational techniques, including neural networks, image processing, and probabilistic reasoning, alongside the traditional methods of symbolic reasoning. AI algorithms are described in detailed prose in the text and fully implemented in LISP at the ends of chapters. A stand-alone LISP chapter makes an excellent reference and refresher. Each chapter includes a detailed description of an AI application.","170":"Automating digital systems in healthcare plays a significant role in transforming the quality-of-care\r\nservices delivered to patients across the board. This role is anticipated to be accomplished by the \r\ndevelopment and implementation of artificial intelligence in healthcare which has the potential to impact \r\nthe provision of healthcare services. This paper sought to investigate the impact of adopting and \r\nimplementing artificial intelligence on the automation of digital health systems within the different levels of healthcare. The general objective of the research study was to investigate the impact of artificial \r\nintelligence in the automation of digital health systems.","171":"The concept of `social situatedness', i.e. the idea that the development of\r\nindividual intelligence requires a social (and cultural) embedding, has\r\nrecently received much attention in cognitive science and artificial\r\nintelligence research, in particular work on social or epigenetic robotics.\r\nThe work of Lev Vygotsky who put forward this view already in the\r\n1920s has influenced the discussion to some degree, but still remains far\r\nfrom well known. This paper therefore aims to give an overview of his\r\ncognitive development theory and discuss its relation to more recent\r\nwork in primatology and socially situated artificial intelligence, in\r\nparticular humanoid robotics.","172":"Artificial Intelligence (AI) is everywhere, yet it causes damage to society in ways that can\u2019t be fixed. Instead of helping to address our current crises, AI causes divisions that limit people\u2019s life chances, and even suggests fascistic solutions to social problems. This book provides an analysis of AI\u2019s deep learning technology and its political effects and traces the ways that it resonates with contemporary political and social currents, from global austerity to the rise of the far right. Dan McQuillan calls for us to resist AI as we know it and restructure it by prioritising the common good over algorithmic optimisation. He sets out an anti-fascist approach to AI that replaces exclusions with caring, proposes people\u2019s councils as a way to restructure AI through mutual aid and outlines new mechanisms that would adapt to changing times by supporting collective freedom.","173":"Traditionally science is done using the reductionism paradigm. Artificial intelligence does not make an exception and it follows the same strategy. At the same time, network science tries to study complex systems as a whole. This synopsis presents my PhD thesis which takes an alternative approach to the reductionism strategy, with the aim to advance both fields, advocating that major breakthroughs can be made when these two are combined. The thesis illustrates this bidirectional relation by: (1) proposing a new method which uses artificial intelligence to improve network science algorithms (i.e. a new centrality metric which computes fully decentralized the nodes and links importance, on the polylogarithmic scale with respect to the number of nodes in the network); and (2) proposing two methods which take inspiration from network science to improve artificial intelligence algorithms (e.g. quadratic acceleration in terms of memory requirements and computational speed of artificial neural network fully connected layers during both, training and inference).","174":null,"175":null,"176":"Recounts the history of artificial intelligence and explores the central problems of philosophy and linguistics involved in developing a machine that can formulate new information on the basis of experience","177":null,"178":"This research\u2019s primary objective is to investigate the impact of artificial intelligence, big data analytics, and business intelligence on digital transformation in UAE telecommunications companies. Following the completion of the sample checking procedure, 200 samples were collected. The Amos program was used to process all the collected data in the research study. The findings of the research demonstrate a set of relationships and linkages that can enhance digital transformation. Moreover, a summary of the findings revealed that all three hypotheses H1, H2, and H3 were found to be valid and significant. This study concluded that artificial intelligence, big data analytics, and business intelligence have a positive impact on developing and enhancing for digital transformation.","179":null,"180":null,"181":null,"182":"Artificial intelligence (AI) is the use of mathematical algorithms to simulate human cognitive skills and to handle challenging healthcare concerns such as cancer. The exponential progress of AI in the recent decade demonstrates its promise as a platform for optimum decisionmaking by superintelligence, since the human mind is constrained to processing large amounts of data in a short period of time. Cancer is a complex disease with hundreds of genetic and epigenetic variants. AI-based techniques show significant potential for identifying these genetic alterations and abnormal protein interactions at an early stage. Modern biomedical research is also focused on bringing AI technologies to clinics in a safe and ethical manner. AI-based help to pathologists and clinicians might be a huge step forward in disease risk, diagnosis, prognosis, and therapy prediction. AI and Machine Learning (ML) clinical applications in cancer diagnosis and therapy are the future of medical guidance toward speedier mapping of a new treatment for each person. Researchers may cooperate in realtime and exchange information digitally by employing an AI base system method, which has the potential to treat millions. We concentrated on presenting gamechanging technologies of the future in clinics by linking biology with Artificial Intelligence and explaining how AIbased aid helps oncologists for accurate therapy in this evaluation.","183":null,"184":"Onboarding has always emphasized personal contact with new employees. Excellent onboarding can extend employee retention and improve loyalty. Even in a physical setting, the onboarding process is demanding for both the newcomer and the onboarding organization. Remote work, in contrast, has made this process even more challenging by forcing a rapid shift from offline to online onboarding practices. Organizations are adopting new technologies like artificial intelligence (AI) to support work processes, such as hiring processes or innovation facilitation, which could shape a new era of work practices. However, it has not been studied how AI applications can or should support onboarding. Therefore, our research conducts a literature review on current onboarding practices and uses expert interviews to evaluate AI's potential and pitfalls for each action. We contribute to the literature by presenting a holistic picture of onboarding practices and assessing potential application areas of AI in the onboarding process.","185":"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","186":"By far the greatest danger of Artificial Intelligence is that people conclude too early that they understand it. Of course this problem is not limited to the field of AI. Jacques Monod wrote: 'A curious aspect of the theory of evolution is that everybody thinks heunderstands it'. Nonetheless the problem seems to be unusually acute in Artificial Intelligence. The field of AI has a reputation for making huge promises and then failing to deliver on them. Most observers conclude that AI is hard; as indeed it is. But the embarrassment does not stem from the difficulty. It is difficult to build a star from hydrogen, but the field of stellar astronomy does not have a terrible reputation for promising to build stars and then failing. The critical inference is not that AI is hard, but that, for some reason, it is very easy for people to think they know far more about Artificial Intelligence than they actually do. I think it is possible for mere fallible humans to succeed on the challenge of building Friendly AI. But only if intelligence ceases to be a sacred mystery to us, as life was a sacred mystery to Lord Kelvin. Intelligence must cease to be any kind of mystery whatever, sacred or not. We must execute the creation of Artificial Intelligence as the exact application of an exact art. And maybe then we can win.","187":"In this work, we present and analyze reported failures of artificially\r\nintelligent systems and extrapolate our analysis to future AIs. We suggest that\r\nboth the frequency and the seriousness of future AI failures will steadily\r\nincrease. AI Safety can be improved based on ideas developed by cybersecurity\r\nexperts. For narrow AIs safety failures are at the same, moderate, level of\r\ncriticality as in cybersecurity, however for general AI, failures have a\r\nfundamentally different impact. A single failure of a superintelligent system\r\nmay cause a catastrophic event without a chance for recovery. The goal of\r\ncybersecurity is to reduce the number of successful attacks on the system; the\r\ngoal of AI Safety is to make sure zero attacks succeed in bypassing the safety\r\nmechanisms. Unfortunately, such a level of performance is unachievable. Every\r\nsecurity system will eventually fail; there is no such thing as a 100% secure\r\nsystem.","188":null,"189":null,"190":"An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ``normal'' examples with only a small percentage of ``abnormal'' or    ``interesting'' examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.","191":null,"192":null,"193":null,"194":null,"195":"It is essential to utilize deep-learning algorithms based on big data for the implementation of the new generation of artificial intelligence. Effective utilization of deep learning relies considerably on the number of labeled samples, which restricts the application of deep learning in an environment with a small sample size. In this paper, we propose an approach based on a generative adversarial network (GAN) combined with a deep neural network (DNN). First, the original samples were divided into a training set and a test set. The GAN was trained with the training set to generate synthetic sample data, which enlarged the training set. Next, the DNN classifier was trained with the synthetic samples. Finally, the classifier was tested with the test set, and the effectiveness of the approach for multi-classification with a small sample size was validated by the indicators. As an empirical case, the approach was then applied to identify the stages of cancers with a small labeled sample size. The experimental results verified that the proposed approach achieved a greater accuracy than traditional methods. This research was an attempt to transform the classical statistical machine-learning classification method based on original samples into a deep-learning classification method based on data augmentation. The use of this approach will contribute to an expansion of application scenarios for the new generation of artificial intelligence based on deep learning, and to an increase in application effectiveness. This research is also expected to contribute to the comprehensive promotion of new-generation artificial intelligence.","196":null,"197":null,"198":"In November 2020, OpenAI officially launched their new generation of generative artificial intelligence, ChatGPT, based on large language models. ChatGPT garnered the attention and usage of over a billion people in less than two months, making it the fastest consumer level application to reach this milestone. As a generative AI, ChatGPT not only achieved remarkable success in the market but also contributed to the advancements in the field of generative AI, with its representation of 2022s top ten scientific breakthroughs in the international reputable journal, Science. Generative artificial intelligence is having a profound impact on our lives and work, with one of the most critical aspects being its algorithms. These algorithms directly influence the learning performance of the entire intelligent system and the quality of recommendations. However, with the widespread application of algorithms and the continuous evolution of artificial intelligence, a series of regulatory and management issues have arisen. Many countries worldwide are continually strengthening algorithm oversight to ensure transparency, fairness, and security. This project, part of a university student innovation training program, will primarily focus on researching personal information security risks among the potential risks associated with generative artificial intelligence. The following will provide an introduction to what generative artificial intelligence is, the personal information security risks and preventive measures involved in generative artificial intelligence. Chen Jiaqi | Zhen Yunuo | Guo Simeng \"Legal Risks and Preventive Measures in ChatGPT Applications\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-6 , December 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd60109.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/60109\/legal-risks-and-preventive-measures-in-chatgpt-applications\/chen-jiaqi","199":null,"200":"My 1971 Turing Award Lecture was entitled \"Generality in Artificial\n\tIntelligence.\" The topic turned out to have been overambitious in\n\tthat I discovered I was unable to put my thoughts on the subject\n\tin a satisfactory written form at that time. It would have been better\n\tto have reviewed my previous work rather than attempt something new,\n\tbut such was not my custom at that time. I am grateful to ACM for\n\tthe opportunity to try again. Unfortunately for our science, although\n\tperhaps fortunately for this project, the problem of generality in\n\tartificial intelligence (AI) is almost as unsolved as ever, although\n\twe now have many ideas not available in 1971. This paper relies heavily\n\ton such ideas, but it is far from a full 1987 survey of approaches\n\tfor achieving generality. Ideas are therefore discussed at a length\n\tproportional to my familiarity with them rather than according to\n\tsome objective criterion. It was obvious in 1971 and even in 1958\n\tthat AI programs suffered from a lack of generality. It is still\n\tobvious; there are many more details. The first gross symptom is\n\tthat a small addition to the idea of a program often involves a complete\n\trewrite beginning with the data structures. Some progress has been\n\tmade in modularizing data structures, but small modifications of\n\tthe search strategies are even less likely to be accomplished without\n\trewriting. Another symptom is no one knows how to make a general\n\tdatabase of commonsense knowledge that could be used by any program\n\tthat needed the knowledge. Along with other information, such a database\n\twould contain what a robot would need to know about the effects of\n\tmoving objects around, what a person can be expected to know about\n\this family, and the facts about buying and selling. This does not\n\tdepend on whether the knowledge is to be expressed in a logical language\n\tor in some other formalism. When we take the logic approach to AI,\n\tlack of generality shows up in that the axioms we devise to express\n\tcommonsense knowledge are too restricted in their applicability for\n\ta general commonsense database. In my opinion, getting a language\n\tfor expressing general commonsense knowledge for inclusion in a general\n\tdatabase is the key problem of generality in AI. Here are some ideas\n\tfor achieving generality proposed both before and after 1971. I repeat\n\tmy disclaimer of comprehensiveness.","201":null,"202":null,"203":null,"204":"Artificial Intelligence, as we see it, is a collection of multiple technologies that enable machines to sense, comprehend and act---and learn, either on their own or to augment human activities. Compelling data reveal a discouraging truth about growth today. There has been a marked decline in the ability of traditional levers of production---capital investment and labor---to propel economic growth. Yet, the numbers tell only part of the story. Artificial intelligence (AI) is a new factor of production and has the potential to introduce new sources of growth, changing how work is done and reinforcing the role of people to drive growth in business. Accenture research on the impact of AI in 12 developed economies reveals that AI could double annual economic growth rates in 2035 by changing the nature of work and creating a new relationship between man and machine. The impact of AI technologies on business is projected to increase labor productivity by up to 40 percent and enable people to make more efficient use of their time.","205":null,"206":null,"207":null,"208":"In 30 years, will robots do all the unpleasant work for us? Or will they subjugate us to become submissive slaves? The debates on how Artificial Intelligence (AI) will change our lives move between these extremes. There is no doubt that the change will be dramatic. Maybe now is just the right time to start interfering. This pioneering comic essay on AI invites you on an illustrated journey through the dimensions and implications of the groundbreaking technology. Discussing important chances and risks associated with AI, this work is a creative stimulus for insiders of the subject as well as an invitation for newbies to get informed and join the debate. With a doctorate in economics, Julia Schneider appreciates data and code as tools for solving complex puzzles \u2013 and loves comics as a medium for telling complex stories. Coming from the opposite direction, artist Lena Kadriye Ziyal loves encrypting complexity with associations and thereby expands the meaning of a theme with her perspective.","209":"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system's models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","210":"Artificial intelligence (AI) has growing impact on working life. Risks of job losses, discrimination, data protection violations, surveillance pressure and health hazards require legal regulation. But which legal framework is necessary to effectively protect the rights and interests of workers?\r\nWorkplace co-determination can help to minimize risks and reap the benefits of AI systems. It is therefore a problem that collective, co-determinated solutions do not seem to be considered in the drafting of the AI Act at EU level. This does not live up to the aspirations currently expressed in the European Pillar of Social Rights, and some member states go decidedly further, too. Fair AI requires involvement and empowerment of the workforce. Otherwise, the intended \"human-centered\" approach to AI in the world of work remains just a phrase.","211":"An intelligent agent interacting with the real world will encounter individual people, courses, test results, drugs prescriptions, chairs, boxes, etc., and needs to reason about properties of these individuals and relations among them as well as cope with uncertainty. Uncertainty has been studied in probability theory and graphical models, and relations have been studied in logic, in particular in the predicate calculus and its extensions. This book examines the foundations of combining logic and probability into what are called relational probabilistic models. It introduces representations, inference, and learning techniques for probability, logic, and their combinations. The book focuses on two representations in detail: Markov logic networks, a relational extension of undirected graphical models and weighted first-order predicate calculus formula, and Problog, a probabilistic extension of logic programs that can also be viewed as a Turing-complete relational extension of Bayesian networks.","212":null,"213":"Given the foreseeable pervasiveness of artificial intelligence (AI) in modern societies, it is legitimate and necessary to ask the question how this new technology must be shaped to support the maintenance and strengthening of constitutional democracy. This paper first describes the four core elements of today's digital power concentration, which need to be seen in cumulation and which, seen together, are both a threat to democracy and to functioning markets. It then recalls the experience with the lawless Internet and the relationship between technology and the law as it has developed in the Internet economy and the experience with GDPR before it moves on to the key question for AI in democracy, namely which of the challenges of AI can be safely and with good conscience left to ethics, and which challenges of AI need to be addressed by rules which are enforceable and encompass the legitimacy of democratic process, thus laws. The paper closes with a call for a new culture of incorporating the principles of democracy, rule of law and human rights by design in AI and a three-level technological impact assessment for new technologies like AI as a practical way forward for this purpose.","214":null,"215":null,"216":null,"217":null,"218":"Ensuring that AI empowers educators and learners, not over-empowers them, and that future developments and practices are truly for the common good.\r\n\r\nArtificial intelligence (Al) is increasingly having an impact on education, bringing opportunities as well as numerous challenges. These observations were noted by the Council of Europe\u2019s Committee of Ministers in 2019 and led to the commissioning of this report, which sets out to examine the connections between Al and education (AI&ED). In particular, the report presents an overview of AI&ED seen through the lens of the Council of Europe values of human rights, democracy and the rule of law; and it provides a critical analysis of the academic evidence and the myths and hype.\r\n\r\nThe Covid-19 pandemic school shutdowns triggered a rushed adoption of educational technology, which increasingly includes AI-assisted classrooms tools (AIED). This AIED, which by definition is designed to influence child development, also impacts on critical issues such as privacy, agency and human dignity \u2013 all of which are yet to be fully explored and addressed. But AI&ED is not only about teaching and learning with AI, but also teaching and learning about AI (AI literacy), addressing both the technological dimension and the often-forgotten human dimension of AI.\r\n\r\nThe report concludes with a provisional needs analysis \u2013 the aim being to stimulate further critical debate by the Council of Europe\u2019s member states and other stakeholders and to ensure that education systems respond both proactively and effectively to the numerous opportunities and challenges introduced by AI&ED.","219":"Artificial Intelligence is set to influence every aspect of our lives, not least the way production is organized. AI, as a technology platform, can automate tasks previously performed by labor or create new tasks and activities in which humans can be productively employed. Recent technological change has been biased towards automation, with insufficient focus on creating new tasks where labor can be productively employed. The consequences of this choice have been stagnating labor demand, declining labor share in national income, rising inequality and lower productivity growth. The current tendency is to develop AI in the direction of further automation, but this might mean missing out on the promise of the \u201cright\u201d kind of AI with better economic and social outcomes.","220":"Invoice processing and payment activities are crucial financial operations for businesses. Traditionally, these tasks involved manual data entry and validation, which is time-consuming and prone to errors. With recent advances in artificial intelligence (AI), intelligent systems can automate and streamline invoice and payment processing, leading to higher efficiency and cost savings. This paper explores the applications of AI in financial document processing, data extraction, verification, reconciliation, and payment execution. The capabilities of AI methods, including computer vision, natural language processing, and machine learning, are analyzed in the context of digitizing financial workflows. Challenges such as the lack of standardized data, the need for integration with legacy systems, and data security considerations are also discussed. The paper concludes that AI-based tools can enable intelligent invoice management, smart approvals, and automated payment processing to transform financial operations.","221":null,"222":null,"223":"Artificial Intelligence (AI) is an increasingly popular field that seeks to create machines that can perform tasks that would normally require human intelligence. AI systems are known for their ability to learn and make decisions using data and other inputs, and their capabilities have expanded rapidly in recent years. Some of the capabilities of AI include computer vision, natural language processing, predictive modeling, and decision-making. These capabilities have led to the development of applications such as self-driving cars, virtual assistants, and fraud detection systems. However, the rapid advancement of AI has also raised ethical and social concerns, including issues around job displacement, privacy, and bias. This paper provides an overview of the capabilities of AI and highlights the need for responsible and ethical development and implementation of this technology.","224":null,"225":null,"226":null,"227":"On behalf of the German Federal Ministry of Economic Affairs and Climate Action, DIN and DKE started work on the second edition of the German Standardization Roadmap Artificial Intelligence in January 2022. With the broad participation and involvement of more than 570 experts from industry, science, the public sector and civil society, the strategic Roadmap for AI standardization was thus further developed. This work was coordinated and accompanied by a high-level coordination group for AI standardization and conformity \r\n\r\nThe standardization roadmap implements a measure of the German government\u2019s AI Strategy and thus makes a significant contribution to \u201cAI \u2013 Made in Germany\u201d. \r\n\r\nStandardization is part of the AI Strategy and is a strategic instrument for strengthening the innovation and competitiveness of the German and European economies. Not least for this reason, standardization plays a special role in the planned European legal framework for AI, the Artificial Intelligence Act. \r\n\r\nThis Standardization Roadmap AI identifies the requirements in standardization, formulates concrete recommendations and thus creates the basis for initiating standardization work at national level, and especially at European and international level, at an early stage. In doing so, the Roadmap makes a significant contribution to the European Commission\u2019s Artificial Intelligence Act, supporting its implementation. \r\n\r\nThe Standardization Roadmap AI focuses on nine key topics, which are addressed in Chapter 4: \r\n\r\n\u2192 The Roadmap begins with the basic topics, such as terminologies and definitions, classifications and ethical issues. They are the basis for AI discussions and are thus the central core of the Roadmap.\r\n\r\n\u2192 The security\/safety of AI systems plays a crucial role in widespread use of AI solutions. Only a more in-depth consideration of requirements for operational safety and information security, for example, can enable the comprehensive use of AI systems in business and society.\r\n\r\n\u2192 Another key topic, and the basis for the broad market success of AI, is testing and certification. This requires reliable quality criteria and reproducible test methods that can be used to verify the properties of AI systems. They are a key prerequisite for assessing the quality of AI-based applications and contribute significantly to explainability and traceability \u2013 two factors that build trust and acceptance.\r\n\r\n\u2192 Another challenge in the use of AI, especially for small and medium-sized enterprises, is the integration of AI technologies in organizations. The focus here is on sociotechnical aspects such as human-technology interaction, humane work design, and requirements for business structures and processes, which are all examined in the Roadmap. \r\n\r\n\u2192 The fields of application of AI are extremely diverse. AI technologies are used in almost all business and application areas and offer great potential. To cover a broad spectrum of applications, the Roadmap considers industry-specific challenges for the following five sectors in particular, in addition to the cross-cutting issues mentioned above: Industrial Automation, Mobility, Medicine, Financial Services and Energy \/ Environment. \r\n\r\nThe present Roadmap outlines the work and discussion results for all nine key topics and provides a comprehensive overview of the status quo, requirements, and needs for action.","228":null,"229":"This paper aims at filling some gaps in the mainstream debate on automation, the introduction of new technologies at the workplace and the future of work. This debate has concentrated, so far, on how many jobs will be lost as a consequence of technological innovation. This paper examines instead issues related to the quality of jobs in future labour markets. It addresses the detrimental effects on workers of awarding legal capacity and rights and obligation to robots. It examines the implications of practices such as People Analytics and the use of big data and artificial intelligence to manage the workforce. It stresses on an oft-neglected feature of the contract of employment, namely the fact that it vests the employer with authority and managerial prerogatives over workers. It points out that a vital function of labour law is to limit these authority and prerogatives to protect the human dignity of workers. In light of this, it argues that even if a Universal Basic Income were introduced, the existence of managerial prerogatives would still warrant the existence of labour regulation since this regulation is about much more than protecting workers\u2019 income. It then highlights the benefits of human-rights based approaches to labour regulation to protect workers\u2019 privacy against invasive electronic monitoring. It concludes by highlighting the crucial role of collective regulation and social partners in governing automation and the impact of technology at the workplace. It stresses that collective dismissal regulation and the involvement of workers\u2019 representatives in managing and preventing job losses is crucial and that collective actors should actively participate in the governance of technology-enhanced management systems, to ensure a vital \u201chuman-in-command\u201d approach.","230":null,"231":null,"232":"This paper deals with the concept of Emotional Intelligence and its importance in various fields. Emotional Intelligence having the capacity to wind up mindful of even unobtrusive changes in ones and others emotional tones and to control them, to try to avoid panicking amidst weight, to start and keep up sound associations with others, and to keep up an idealistic viewpoint towards life. Later on in this paper the focus is on the importance of emotional intelligence in various fields. Jyoti Shikha | Dr. Sanjeev Singh \u00c4 Study and Analysis of Emotional Intelligence and its Impacts\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-6 , October 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd29136.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/29136\/a-study-and-analysis-of-emotional-intelligence-and-its-impacts\/jyoti-shikha","233":null,"234":"SummaryObjectiveFormalizing clinical practice guidelines (CPGs) for a subsequent computer-supported processing is a challenging, but burdensome and time-consuming task. Existing methods and tools to support this task demand detailed medical knowledge, knowledge about the formal representations, and a manual modeling. Furthermore, formalized guideline documents mostly fall far short in terms of readability and understandability for the human domain modeler.Methods and materialWe propose a new multi-step approach using information extraction methods to support the human modeler by both automating parts of the modeling process and making the modeling process traceable and comprehensible. This paper addresses the first steps to obtain a representation containing processes which is independent of the final guideline representation language.ResultsWe have developed and evaluated several heuristics without the need to apply natural language understanding and implemented them in a framework to apply them to several guidelines from the medical subject of otolaryngology. Findings in the evaluation indicate that using semi-automatic, step-wise information extraction methods are a valuable instrument to formalize CPGs.ConclusionOur evaluation shows that a heuristic-based approach can achieve good results, especially for guidelines with a major portion of semi-structured text. It can be applied to guidelines irrespective to the final guideline representation format.","235":"Communicative Machines Laboratories (CMLabs) specializes in communication between humans and computers, using solutions from artificial intelligence to create interactive experiences and enable machines to perceive and act in a variety of environments. Based on research spanning more than a decade, the Psyclone platform has been in development by CMLabs for over three years, with the express goal of creating a new foundation for simulating complex phenomena. Psyclone is being used in several advanced development projects in Europe and the U.S. in areas including robotics, computer vision, computer graphics and animation.","236":"For the control of the forming process it is necessary\n                 to know as precisely as possible the flow curve of the\n                 material formed. The paper presents the determination\n                 of the equation for the flow curve with the artificial\n                 intelligence approach. The genetic programming method\n                 (GP) was used. It is an evolutionary optimisation\n                 technique based on the Darwinian natural selection and\n                 the survival of the fittest organisms. The comparison\n                 between the experimental results, the analytical\n                 solution and the solution obtained genetically clearly\n                 shows that the genetic programming method is a very\n                 promising approach.","237":null,"238":"The aim of the paper is to define Artificial Intelligence (AI) for librarians by examining general definitions of AI, analysing the umbrella of technologies that make up AI, defining types of use case by area of library operation, and then reflecting on the implications for the profession, including from an equality, diversity and inclusion perspective. The paper is a conceptual piece based on an exploratory literature review, targeting librarians interested in AI from a strategic rather than a technical perspective. Five distinct types of use cases of AI are identified for libraries, each with its own underlying drivers and barriers, and skills demands. They are applications in library back-end processes, in library services, through the creation of communities of data scientists, in data and AI literacy and in user management. Each of the different applications has its own drivers and barriers. It is hard to anticipate the impact on professional work but as information environment becomes more complex it is likely that librarians will continue to have a very important role, especially given AI\u2019s dependence on data. However, there could be some negative impacts on equality, diversity and inclusion if AI skills are not spread widely.","239":"The foundations for biometrics or identification systems were laid long ago. Today these developments have contributed to the identification of people, access to private sites and all places that need security and order with the help of computerized computers that perform biometric facial recognition, exclusively based on images of human faces for their function. With the extraction of facial midst characteristics of each person provides information used for the detection of the face. This communication also addresses the different processes, stages and methods of feature extraction operated by facial recognition systems. Including the positive and negative aspects of the implementation of these, the advantages and disadvantages, peoples criteria in this respect. Tovbaev Sirojiddin | Karshiboev Nizomiddin \"Image Based Facial Recognition\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31330.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31330\/image-based-facial-recognition\/tovbaev-sirojiddin","240":"This Note briefly describes the method of `co-word analysis', and presents some results of its application to the scientific literature in the field of Artificial Intelligence (AI). A sample of AI literature, published in 1984 and 1985, was drawn from the CNRS\/INIST database in Paris. The co-word analysis of this sample suggests that AI is a field characterized by relative intellectual coherence in a specifiable `core'. In this, AI seems to differ from scientific studies of the acidification of the environment: a co-word analysis of the latter field reveals no evidence of the likely emergence of a unifying general theory. The policy implications of such co-word studies are discussed.","241":null,"242":null,"243":"We present new constraints on the masses of the halos hosting the Milky Way\r\nand Andromeda galaxies derived using graph neural networks. Our models, trained\r\non thousands of state-of-the-art hydrodynamic simulations of the CAMELS\r\nproject, only make use of the positions, velocities and stellar masses of the\r\ngalaxies belonging to the halos, and are able to perform likelihood-free\r\ninference on halo masses while accounting for both cosmological and\r\nastrophysical uncertainties. Our constraints are in agreement with estimates\r\nfrom other traditional methods.","244":null,"245":null,"246":"This article highlights and substantiates the methods of using artificial intelligence in the effective organization of the activities of insurance organizations and in order to reduce excessive costs, convenience and prospects of maintaining an insurance portfolio in electronic form.","247":null,"248":null,"249":"This paper introduces a new approach for the forecasting of mean hourly global solar radiation received by a horizontal surface. In addition to the traditional linear methods, several artificial-intelligence-based techniques are studied. These include linear, feed-forward, recurrent Elman and Radial Basis neural networks alongside the adaptive neuro-fuzzy inference scheme. The problem is examined initially for the univariate case, and is extended to include additional meteorological parameters in the process of estimating the optimum model. The results indicate that the developed artificial intelligence models predict the solar radiation time series more effectively compared to the conventional procedures based on the clearness index. The forecasting ability of some models can be further enhanced with the use of additional meteorological parameters.","250":null,"251":null,"252":null,"253":"Machine learning is a fascinating topic its astonishing how a small change in the evaluation values may result in an unfathomable number of outcomes. The goal of this study is to develop a model that uses image processing to identify skin cancer. We will later use the model in real life through an android application. Sunami Dasgupta | Soham Das | Sayani Hazra Pal \"Skin Cancer Detection using Image-Processing in Real-Time\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-6 , October 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd46384.pdf Paper URL : https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/46384\/skin-cancer-detection-using-imageprocessing-in-realtime\/sunami-dasgupta","254":null,"255":"AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using the rules to reach approximate or definite conclusions), and self-correction. As a result, Artificial Intelligence is gaining Importance in science and engineering fields. The use of Artificial Intelligence in medical diagnosis too is becoming increasingly common and has been used widely in the diagnosis of cancers, tumors, hepatitis, lung diseases, etc... The main aim of this paper is to build an Artificial Intelligent System that after analysis of certain parameters can predict that whether a person is diabetic or not. Diabetes is the name used to describe a metabolic condition of having higher than normal blood sugar levels. Diabetes is becoming increasingly more common throughout the world, due to increased obesity - which can lead to metabolic syndrome or pre-diabetes leading to higher incidences of type 2 diabetes. Authors have identified 10 parameters that play an important role in diabetes and prepared a rich database of training data which served as the backbone of the prediction algorithm. Keeping in view this training data authors developed a system that uses the artificial neural networks algorithm to serve the purpose. These are capable of predicting new observations (on specific variables) from previous observations (on the same or other variables) after executing a process of so-called learning from existing training data (Haykin 1998).The results indicate that the performance of KNN method when compared with the medical diagnosis system was found to be 91%. This system can be used to assist medical programs especially in geographically remote areas where expert human diagnosis not possible with an advantage of minimal expenses and faster results. Abid Sarwar\"K-Nearest Neighbours based diagnosis of hyperglycemia\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-1 , December 2017, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd7046.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/7046\/k-nearest-neighbours-based-diagnosis-of-hyperglycemia\/abid-sarwar","256":null,"257":"In April 2021, the European Commission published its first draft of the Proposal for a Regulation on Artificial Intelligence. Since AI in the work context has increasingly become important in organising work and managing workers, the AI Act will undoubtedly have an impact on EU and national labour law systems. One aim of the proposal is to guarantee \u2018consistency with existing Union legislation applicable to sectors where high-risk Artificial Intelligence systems are already used or likely to be used in the near future\u2019, which includes the EU social acquis. It could be argued that ensuring true consistency with EU law means guaranteeing that the way the AI Act will be implemented and applied will still allow the other pieces of EU labour law to fulfil their purpose. It is undeniable that the implementation of the AI Act will overlap with various fields of EU law, especially considering the increasing use of AI technology at work. Thus, this article seeks to identify ways to refine the AI Act, insofar as it impacts work. The contribution discusses the current AI Act as proposed in April 2021, thereby focusing on two particular areas, EU non-discrimination law and EU law on occupational health and safety (OSH), as these two areas are, more or less explicitly, addressed as legal fields in the AI Act. The article starts with taking the perspective of EU labour law influencing the development of AI systems used in the employment context. We argue that providers should respect EU labour law throughout the development of the AI system (section 2). Then, the areas where EU labour law and the AI overlap are identified, thereby viewing it from an employer's perspective, i.e., the user of the AI system (section 3). Using two specific EU labour law areas (the right not to be discriminated against and the right to healthy and safe working conditions) the article provides a first assessment of how the AI Act might influence work and the regulation thereof (section 4). Finally, the conclusion critically explores whether and to what extent AI in employment situations warrants particular attention (section 5).","258":null,"259":"We use a generic formalism designed to search for relations in\r\nhigh-dimensional spaces to determine if the total mass of a subhalo can be\r\npredicted from other internal properties such as velocity dispersion, radius,\r\nor star-formation rate. We train neural networks using data from the Cosmology\r\nand Astrophysics with MachinE Learning Simulations (CAMELS) project and show\r\nthat the model can predict the total mass of a subhalo with high accuracy: more\r\nthan 99% of the subhalos have a predicted mass within 0.2 dex of their true\r\nvalue. The networks exhibit surprising extrapolation properties, being able to\r\naccurately predict the total mass of any type of subhalo containing any kind of\r\ngalaxy at any redshift from simulations with different cosmologies,\r\nastrophysics models, subgrid physics, volumes, and resolutions, indicating that\r\nthe network may have found a universal relation. We then use different methods\r\nto find equations that approximate the relation found by the networks and\r\nderive new analytic expressions that predict the total mass of a subhalo from\r\nits radius, velocity dispersion, and maximum circular velocity. We show that in\r\nsome regimes, the analytic expressions are more accurate than the neural\r\nnetworks. We interpret the relation found by the neural network and\r\napproximated by the analytic equation as being connected to the virial theorem.","260":null,"261":"This paper describes the recently developed 'genetic\n                 programming' paradigm which genetically breeds\n                 populations of computer programs to solve problems. In\n                 genetic programming, the individuals in the population\n                 are hierarchical computer programs of various sizes and\n                 shapes. Three applications to problems in artificial\n                 intelligence are presented.","262":null,"263":null,"264":"The objective of this article is to explain the problems of using the facial recognition functions in current mobile devices, as well as to give a possible solution based on a client server design. Sirojiddin Tavboev | Tavboev Islom \"Face Recognition and Increased Reality System for Mobile Devices\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31384.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31384\/face-recognition-and-increased-reality-system-for-mobile-devices\/sirojiddin-tavboev","265":null,"266":"Objectives: Electronic health records (EHRs) are only a first step in\r\ncapturing and utilizing health-related data - the challenge is turning that\r\ndata into useful information. Furthermore, EHRs are increasingly likely to\r\ninclude data relating to patient outcomes, functionality such as clinical\r\ndecision support, and genetic information as well, and, as such, can be seen as\r\nrepositories of increasingly valuable information about patients' health\r\nconditions and responses to treatment over time. Methods: We describe a case\r\nstudy of 423 patients treated by Centerstone within Tennessee and Indiana in\r\nwhich we utilized electronic health record data to generate predictive\r\nalgorithms of individual patient treatment response. Multiple models were\r\nconstructed using predictor variables derived from clinical, financial and\r\ngeographic data. Results: For the 423 patients, 101 deteriorated, 223 improved\r\nand in 99 there was no change in clinical condition. Based on modeling of\r\nvarious clinical indicators at baseline, the highest accuracy in predicting\r\nindividual patient response ranged from 70-72% within the models tested. In\r\nterms of individual predictors, the Centerstone Assessment of Recovery Level -\r\nAdult (CARLA) baseline score was most significant in predicting outcome over\r\ntime (odds ratio 4.1 + 2.27). Other variables with consistently significant\r\nimpact on outcome included payer, diagnostic category, location and provision\r\nof case management services. Conclusions: This approach represents a promising\r\navenue toward reducing the current gap between research and practice across\r\nhealthcare, developing data-driven clinical decision support based on\r\nreal-world populations, and serving as a component of embedded clinical\r\nartificial intelligences that \"learn\" over time.","267":"This paper describes a fully operational AI-CAI system (accessible\n\tover the ARPANET) which incorporates Artificial Intelligence techniques\n\tto perform question answering, hypothesis verification and theory\n\tformation activities in the domain of electronic troubleshooting.\n\tMuch of its logical or inferencing capabilities are derived from\n\tuses of simulation models in conjunction with numerous procedural\n\tspecialists. The system also includes a highly tuned structural parser\n\tfor allowing the student to communicate in natural language. Although\n\tthe system is extremely large it is sufficiently fast to be thoroughly\n\texercised in a training or classroom environment.","268":null,"269":null,"270":null,"271":"Smarter applications are making better use of the insights gleaned from data,\r\nhaving an impact on every industry and research discipline. At the core of this\r\nrevolution lies the tools and the methods that are driving it, from processing\r\nthe massive piles of data generated each day to learning from and taking useful\r\naction. Deep neural networks, along with advancements in classical ML and\r\nscalable general-purpose GPU computing, have become critical components of\r\nartificial intelligence, enabling many of these astounding breakthroughs and\r\nlowering the barrier to adoption. Python continues to be the most preferred\r\nlanguage for scientific computing, data science, and machine learning, boosting\r\nboth performance and productivity by enabling the use of low-level libraries\r\nand clean high-level APIs. This survey offers insight into the field of machine\r\nlearning with Python, taking a tour through important topics to identify some\r\nof the core hardware and software paradigms that have enabled it. We cover\r\nwidely-used libraries and concepts, collected together for holistic comparison,\r\nwith the goal of educating the reader and driving the field of Python machine\r\nlearning forward.","272":null,"273":null,"274":"The MIT Artificial Intelligence Project has a variety of goals all bound together by search for principles of intelligent behavior. Among our immediate goals are to develop systems with practical applications for: Visually-controlled automatic manipulation and physical world problem-solving, machine understanding of natural language text and narrative, and advanced applied mathematics. The long-range goals are concerned with simplifying, unifying and extending the techniques of heuristic programming. We expect the results of our work to: make it easier to write and debug large heuristic programs, develop packaged collections of knowledge about many different kinds of things, lending to programs with more resourcefulness, understanding and common sense\", and identify and sharpen certain principles for programming intelligence.","275":null,"276":null,"277":"This article proposes ways to prevent cyber-attacks and emergency situations with a high probability of occurrence using the capabilities of artificial intelligence (AI) in the early detection of threats to the information systems of enterprise organizations. The article presents the important elements for the development of a smart protection system that includes the analysis of the possible origin of natural and artificial threats, threat isolation, level determination, and decision-making functions.","278":null,"279":"Text is one of the traditional ways of communication between people. With the growing availability of text data in electronic form, handling and analysis of text by means of computers gained popularity. Handling text data with machine learning methods brought interesting challenges to the area that got further extended by incorporation of some natural language specifics. As the methods were capable of addressing more complex problems related to text data, the expectations become bigger calling for more sophisticated methods, in particular a combination of methods from different research areas including information retrieval, machine learning, statistical data analysis, data mining, natural language processing, semantic technologies. Automatic text analysis become an integral part of many systems, pushing boundaries of research capabilities towards what one can refer to as an artificial intelligence dream - never ending learning from text aiming at mimicking ways of human learning. The paper presents development of text analysis research in Slovenian that we have been personally involved in, pointing out interesting research problems that have been and are still addressed by the research, example tasks that have been addressed and some challenges on the way.","280":null,"281":null,"282":"In this authoritative and eye-opening book, Max Tegmark describes and illuminates the recent, path-breaking advances in Artificial Intelligence and how it is poised to overtake human intelligence. How will AI affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technology--and there's nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor who's helped mainstream research on how to keep AI beneficial.\r\n\r\nHow can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give today's kids? How can we make future AI systems more robust, so that they do what we want without crashing, ...","283":null,"284":"The general purpose of this research is to investigate the possibilities offered for the use of Artificial Intelligence theory and methods in advanced game environments. The real-time strategy (RTS) game genre is investigated in detail, and an architecture and solutions to some common issues are presented. An RTS AI controlled opponent named \"KAI\" is implemented for the \"TA Spring\" game engine in order to advance the state of the art in usin AI techniques in games and to gain some insight into the strengths and weaknesses of AI Controlled Player (AI CP) architectures.A goal was to create an AI with behavior that gave the impression of intelligence to the human player, by taking on certain aspects of the style in which human players play the game. Another goal for the benefit of the TA Spring development community was to create an AI which played with sufficient skill to provide experienced players with resistance, without using obvious means of cheating such as getting free resources or military assets.Several common techniques were used, among others Rule-based decision making, path planning and path replanning, influence maps, and a variant of the A* search algorithm was used for searches of various kinds. The AI also has an approach to micromanagement of units that are fighting in combination with influence maps. The AI CP program was repeatedly tested against human players and other AI CP programs in various settings throughout development. The availability of testing by the community but the sometimes sketchy feedback lead to the production of consistent behavior for tester and developer alike in order to progress. One obstacle that was met was that the rule-based approach to combat behavior resulted in high complexity.The architecture of the RTS AI CP is designed to emerge a strategy from separate agents that were situation aware. Both the actions of the enemy and the properties of the environment are taken into account. The overall approach is to strengthen the AI CP through better economic and military decisions. Micromanagement and frequent updates for moving units is an important part of improving military decisions in this architecture.This thesis goes into the topics of RTS strategies, tactics, economic decisions and military decisions and how they may be made by AI in an informed way. Direct attempts at calculation and prediction rather than having the AI learn from experience resulted in behavior that was superior to most AI CPs and many human players without a learning period. However, having support for all of the game types for TA Spring resulted in extra development time.","285":null,"286":"This article is intended to present essential facts and one of the most modern ways to improve the efficiency of education among youth. Today the productive use of innovative technologies is important in the educational process. Therefore, nowadays research papers are being conducted to develop different projects to create a system of distance education for students using innovative technologies by scientists. In the 21st century, the development of science and information and communication technologies has led to a significant acceleration of the education system. Also, this paper provides basic information about usage and development of distance education with the help of artificial intelligence that is crucial for learning new educational technologies","287":"The job market has expanded exponentially in the past few years. With many recruiters and candidates, it\r\nis not an easy task to match a perfect candidate with a perfect job. The recruiter targets candidates with\r\nthe required skill sets mentioned in the job descriptions, while candidates target their dream jobs. The\r\nsearch frictions and skills mismatch are persistent problems. In this paper, we build a model that would\r\nmatch companies with candidates with the right skills and workers with the right company. We have\r\nfurther developed an algorithm to investigate people\u2019s hiring history for better results.","288":null,"289":"This document presumptuously purports to explain how to do research. We give heuristics that may be useful in pickup up specific skills needed for research (reading, writing, programming) and for understanding and enjoying the process itself (methodology, topic and advisor selection, and emotional factors)., MIT Artificial Intelligence Laboratory","290":"<i>Computers and Thought: A Practical Introduction to Artificial Intelligence<\/i> is one of the few artificial intelligence textbooks that describes cognitive models in detail. However, one of the book's shortcomings is that it uses POP-11, an obscure Lisp-like programming language, to create the example programs. To read the example programs, you will first have to slog through a tutorial on POP-11 and, because you will probably not have a POP-11 compiler, you will not be able to try out the examples. The bulk of the book is dedicated to describing standard artificial intelligence topics, such as search and knowledge representation. <I>Computers and Thought <\/I>provides a unified, self-contained introduction to artificial intelligence for readers with little or no computing background. It presents an original extended AI programming project - the Automated Tourist Guide exercise throughout the main chapters of the text to illustrate the material covered and show how AI actually works.<br \/> <br \/> Most chapters illustrate a particular AI topic, with sections on the background to the topic, methods, applications, and the limitations of previous proposals. In addition, there are end of chapter summaries and graded exercises, suggested readings, a glossary, and an appendix on programming.<br \/> <br \/> <I>Computers and Thought <\/I>details the theory and issues involved in AI and covers computer simulation of human activities, such as problem solving and natural language understanding, and computer vision. Its investigation of AI is usefully extended to models of cognition, the nature of mind and intelligence, and the social implications of AI and cognitive science.<br \/> <br \/> The computer language is POP-11, an easy to learn language that can be used interactively, like LISP, and that has an appearance similar to PASCAL. It is not necessary to run the illustrative POP-11 programs on a computer, since a feature of the language is the ease with which it can be understood from the printed page.<br \/> <br \/> Mike Sharples, David Hogg, Chris Hutchison, Steve Torrance, and David Young have all been faculty members at The School of Cognitive and Computing Sciences, Sussex University, Brighton, England <I>Computers and Thought <\/I> is included in the series Explorations in Cognitive Science, edited by Margaret A Boden. A Bradford Book","291":null,"292":null,"293":null,"294":null,"295":null,"296":null,"297":"The work presented in this thesis aims at\n                 investigating the potential of a proposed methodology\n                 to create a cognitive control architecture for a\n                 humanoid robot. This architecture comprises three\n                 hierarchical layers: the reactive layer, the model\n                 building layer, and the reasoning layer. The\n                 architecture is built on techniques from the field of\n                 evolutionary computation, and more specifically\n                 evolutionary algorithms. Based on very simple models of\n                 organic evolution, these algorithms can be applied to\n                 various problems such as combinatorial optimisation\n                 problems or learning tasks. The field of artificial\n                 intelligence is discussed from a robotics viewpoint.\n                 The roles of different paradigms in AI research are\n                 considered, and so are the principles of embodiment and\n                 situatedness, which are fundamental in the behaviour\n                 based robotics approach. Several evolutionary\n                 experiments performed on real, physical humanoid robot\n                 platforms are presented. These are presented mainly to\n                 motivate the use of simulated evolution for control\n                 programming of robots. In addition, these experiments\n                 constitute a subset of the necessary building blocks of\n                 the proposed cognitive humanoid robot architecture,\n                 outlined in this thesis. The experiments include sound\n                 localisation, two instances of machine vision, hand-eye\n                 coordination, coordination of actuator motions in a\n                 robot foot joint, and two instances regarding learning\n                 and adaptivity.","298":"This study proposes an application of two techniques\n                 of artificial intelligence (AI) for rainfall-runoff\n                 modelling: the artificial neural networks (ANN) and the\n                 evolutionary computation (EC). Two different ANN\n                 techniques, the feed forward back propagation (FFBP)\n                 and generalised regression neural network (GRNN)\n                 methods are compared with one EC method, Gene\n                 Expression Programming (GEP) which is a new\n                 evolutionary algorithm that evolves computer programs.\n                 The daily hydrometeorological data of three rainfall\n                 stations and one streamflow station for Juniata River\n                 Basin in Pennsylvania state of USA are taken into\n                 consideration in the model development. Statistical\n                 parameters such as average, standard deviation,\n                 coefficient of variation, skewness, minimum and maximum\n                 values, as well as criteria such as mean square error\n                 (MSE) and determination coefficient (R2) are used to\n                 measure the performance of the models. The results\n                 indicate that the proposed genetic programming (GP)\n                 formulation performs quite well compared to results\n                 obtained by ANNs and is quite practical for use. It is\n                 concluded from the results that GEP can be proposed as\n                 an alternative to ANN models.","299":null,"300":null,"301":"The proliferation of AI in many aspects of human life\u2014from personal leisure, to collaborative professional work, to global policy decisions\u2014poses a sharp question about how to prepare people for an interconnected, fast-changing world which is increasingly becoming saturated with technological devices and agentic machines. What kinds of capabilities do people need in a world infused with AI? How can we conceptualise these capabilities? How can we help learners develop them? How can we empirically study and assess their development? With this paper, we open the discussion by adopting a dialogical knowledge-making approach. Our team of 11 co-authors participated in an orchestrated written discussion. Engaging in a semi-independent and semi-joint written polylogue, we assembled a pool of ideas of what these capabilities are and how learners could be helped to develop them. Simultaneously, we discussed conceptual and methodological ideas that would enable us to test and refine our hypothetical views. In synthesising these ideas, we propose that there is a need to move beyond AI-centred views of capabilities and consider the ecology of technology, cognition, social interaction, and values.","302":null,"303":null,"304":null,"305":null,"306":null,"307":null,"308":null,"309":null,"310":null,"311":null,"312":null,"313":null,"314":null,"315":null,"316":null,"317":null,"318":null,"319":null,"320":null,"321":"The Coronavirus has quickly spread to more than 200 nations, infecting several million people globally in a matter of weeks. Artificial intelligence (AI) tools have been shown to be beneficial in testing, diagnosing, and efficiently limiting viral propagation. However, several flaws or limitations have been discovered in existing AI approaches. Patients are diagnosed after the onset of symptoms, making it difficult to determine the appropriate patient management. Doctors' involvement is required, which may result in viral infection via direct contact with patients. The present project attempts to create AI approaches to address such issues. This work suggests AI capable of estimating the timing of viral infection in patients and the degree of medical treatment required prior to the development of COVID-19 symptoms. Many vital biological and human functions are measured by the proposed device, including the state of the brain and neurotransmitters, mental\/mood conditions, tension level of face muscles, hands, and body temperature, rate of pulse and pressure, oxygen level in the blood, rate of breathing and difficulty, degree of redness of eye(s), and general body imbalances. This work makes use of both hardware and software designs. To explore, test, and evaluate the integration\/work of each subsystem component and the overall system architecture, an experimental research is used. The findings demonstrated that the suggested approach might be utilised to differentiate between healthy and infected persons prior to the manifestation of COVID-19 symptoms.","322":null,"323":"https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/50525\/tracing-of-an-object-in-video-through-mean-shift-protocol\/sahila-fareed","324":"This paper introduces a new approach for the forecasting of mean\n\thourly global solar radiation received by a horizontal surface. In\n\taddition to the traditional linear methods, several artificial-intelligencebased\n\ttechniques are studied. These include linear, feed-forward, recurrent\n\tElman and Radial Basis neural networks alongside the adaptive neuro-fuzzy\n\tinference scheme. The problem is examined initially for the univariate\n\tcase, and is extended to include additional meteorological parameters\n\tin the process of estimating the optimum model. The results indicate\n\tthat the developed artificial intelligence models predict the solar\n\tradiation time series more effectively compared to the conventional\n\tprocedures based on the clearness index. The forecasting ability\n\tof some models can be further enhanced with the use of additional\n\tmeteorological parameters.","325":"The development of thinking machines is an adventure as bold and ambitious as any that humans have attempted. And the truth is that Artificial Intelligence is already an indispensable part of our daily lives. Without it, Google wouldn't have answers and your smartphone would just be a phone. But how will AI change society by 2050? Will it destroy jobs? Or even pose an existential threat? Android Dreams is a lively exploration of how AI will transform our societies, economies and selves. From robot criminals to cyber healthcare, and a sky full of empty planes, Toby Walsh's predictions about AI are guaranteed to surprise you.","326":"Various systems and methods are provided that identify prior art patent references for a subject patent application. For example, the system preprocesses a corpus of patent references to identify keywords that are present in each of the patent references, n-grams present in the corpus, and a weighting associated with the identified n-grams. To identify prior art patent references, the system requests a user to provide a patent application. The system extracts n-grams found in the provided patent application and orders the n-grams based on the assigned n-gram weights. The system compares the top Y-rated n-grams with the identified keywords and retrieves patent references that include a keyword that matches one of the top Y-rated n-grams. The system re-ranks the retrieved patent references using, for example, artificial intelligence. The top Z-ranked retrieved patent references are transmitted to a user device for display in a user interface.","327":null,"328":null,"329":"The Introduction begins by providing examples of the fields in which AI is used, along with the varying impact that this has on society. It focuses on the challenges that AI poses when it comes to setting and applying law, particularly in relation to legal rules that seek to preserve the opportunities associated with AI while avoiding or at least minimising the potential risks. The law must aim to ensure good digital governance, both with respect to the development of algorithmic systems generally but also with respect to the use of AI specifically. Particularly formidable are the challenges associated with regulating the use of learning algorithms, such as in the case of machine learning. A great difficulty in this regard is ensuring transparency, accountability, responsibility, and the ability to make revisions, as well as preventing hidden discrimination. The Chapter explores the types of rules and regulations that are available. At the same time, it emphasises that it is not enough to trust that companies that use AI will adhere to ethical principles. Rather, supplementary legal rules are indispensable, including in the areas examined in the Chapter, which are mainly characterised by company self-regulation. The Chapter concludes by stressing the need for transnational agreements and institutions.","330":null,"331":"Several governments recently adopted strategies to facilitate the development of artificial intelligence (AI), one of the reasons being the pursuit of sustainability \u2013 a claim with little scientific grounding so far. In this thesis, I strived to outline the scope of AI\u2019s potentials by showing i) the limits of its theoretical and practical usefulness and ii) the political barriers to using it for sustainability.\r\nThrough the method of immanent critique, I demonstrated that AI shares most of its theoretical limitations with statistical methods in general and concluded that it is best suited for the narrowly defined area of closed, formal systems with low decision stakes. To assess AI\u2019s practical potentials, I conducted a systematic literature review searching for AI use cases that contribute to the indicators of the German sustainability strategy. The resulting use cases mostly described how AI increased efficiencies or generated empirical knowledge, yet they only marginally addressed the major obstacles to sustainability identified by the government.\r\nSince technological possibilities do not linearly translate to real-world adoption, I turned to the realm of politics to unveil dominant discourses that may shape the goals and areas of AI\u2019s use in society. As an exemplary space of AI\u2019s discursive mediation, I examined the German government\u2019s national AI strategy using critical discourse analysis. It resulted that the government prioritized discourses of competitiveness and technological determinism over sustainability considerations. Tensions between different interest groups were linguistically concealed, while competition and technological progress were invoked as external threats to be faced in consensus. This strategy of depoliticization legitimizes and naturalizes current power relations to the advantage of certain actors. The revealed primacy of market logics further reduced the likeliness of realizing AI\u2019s limited sustainability potentials. Yet, alternative ways to govern AI exist and allow overcoming the constructed necessity of neoliberal politics.","332":"Whether current or near-term AI systems could be conscious is a topic of\r\nscientific interest and increasing public concern. This report argues for, and\r\nexemplifies, a rigorous and empirically grounded approach to AI consciousness:\r\nassessing existing AI systems in detail, in light of our best-supported\r\nneuroscientific theories of consciousness. We survey several prominent\r\nscientific theories of consciousness, including recurrent processing theory,\r\nglobal workspace theory, higher-order theories, predictive processing, and\r\nattention schema theory. From these theories we derive \"indicator properties\"\r\nof consciousness, elucidated in computational terms that allow us to assess AI\r\nsystems for these properties. We use these indicator properties to assess\r\nseveral recent AI systems, and we discuss how future systems might implement\r\nthem. Our analysis suggests that no current AI systems are conscious, but also\r\nsuggests that there are no obvious technical barriers to building AI systems\r\nwhich satisfy these indicators.","333":"Image classification is perhaps the most important part of digital image analysis. In this paper, we compare the most widely used model CNN Convolutional Neural Network , and MLP Multilayer Perceptron . We aim to show how both models differ and how both models approach towards the final goal, which is image classification. Souvik Banerjee | Dr. A Rengarajan \"Hand-Written Digit Classification\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42444.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42444\/handwritten-digit-classification\/souvik-banerjee","334":null,"335":null,"336":null,"337":"The Journal of Artificial Intelligence Research (JAIR) was one of the first scientific journals distributed over the web. It has now completed over five years of successful publication. Electronic publishing is reshaping the way academic work is disseminated, and JAIR is leading the way toward a future where scientific articles are freely and easily accessible to all. This report describes how the journal has evolved, its 'grassroots' philosophy, and prospects for the future.","338":null,"339":null,"340":null,"341":null,"342":"Artificial intelligence (AI) techniques are becoming useful as alternate\n\tapproaches to conventional techniques or as components of integrated\n\tsystems. They have been used to solve complicated practical problems\n\tin various areas and are becoming more and more popular nowadays.\n\tAItechniques have the following features: can learn from examples;\n\tare fault tolerant in the sense that they are able to handle noisy\n\tand incomplete data; are able to deal with non-linear problems; and\n\tonce trained can perform prediction and generalization at high speed.\n\tAI-based systems are being developed and deployed worldwide in a\n\tmyriad of applications, mainly because of their symbolic reasoning,\n\tflexibility and explanation capabilities. AI have been used and applied\n\tin different sectors, such as engineering, economics, medicine, military,\n\tmarine, etc. They have also been applied for modeling, identification,\n\toptimization, prediction, forecasting, and control of complex systems.\n\tThe main objective of this paper is to present an overview of the\n\tAI-techniques for sizing photovoltaic (PV) systems: stand-alone PVs,\n\tgrid-connected PV systems, PV-wind hybrid systems, etc. Published\n\tliterature presented in this paper show the potential of AI as a\n\tdesign tool for the optimal sizing of PV systems. Additionally, the\n\tadvantage of using an AI-based sizing of PV systems is that it provides\n\tgood optimization, especially in isolated areas, where the weather\n\tdata are not always available.","343":null,"344":null,"345":"Different types of software components and data have to be combined to solve an artificial intelligence challenge. An emerging marketplace for these components will allow for their exchange and distribution. To facilitate and boost the collaboration on the marketplace a solution for finding compatible artifacts is needed. We propose a concept to define compatibility on such a marketplace and suggest appropriate scenarios on how users can interact with it to support the different types of required compatibility. We also propose an initial architecture that derives from and implements the compatibility principles and makes the scenarios feasible. We matured our concept in focus group workshops and interviews with potential marketplace users from industry and academia. The results demonstrate the applicability of the concept in a real-world scenario.","346":null,"347":null,"348":null,"349":"Simple computer assisted instruction (CAI) systems suffer from the\n\tfact that in general they do not know the subject matter they are\n\tteaching. Intelligent tutoring systems (ITS) use artificial intelligence\n\t(AI) formalisms to represent knowledge in order to improve on CAI\n\tsystems. We survey a number of systems developed, and the emerging\n\tarchitecture for ITS development.","350":null,"351":null,"352":null,"353":"Objectives: This study aims to improve early detection of cardiac surgery-associated acute kidney injury using artificial intelligence-based algorithms.\r\n\r\nMethods: Data from consecutive patients undergoing cardiac surgery between 2008 and 2018 in our institution served as the source for artificial intelligence-based modeling. Cardiac surgery-associated acute kidney injury was defined according to the Kidney Disease Improving Global Outcomes criteria. Different machine learning algorithms were trained and validated to detect cardiac surgery-associated acute kidney injury within 12 hours after surgery. Demographic characteristics, comorbidities, preoperative cardiac status, intra- and postoperative variables including creatinine and hemoglobin values were retrieved for analysis. \r\n\r\nResults: From 7507 patients analyzed, 1699 patients (22.6 %) developed cardiac surgery-associated acute kidney injury. The ultimate detection model, 'Detect-A(K)I', recognizes cardiac surgery-associated acute kidney injury within 12 hours with an area under the curve of 88.0 %, sensitivity of 78.0 %, specificity of 78.9 %, and accuracy of 82.1 %. The optimal parameter set includes serial changes of creatinine and hemoglobin, operative emergency, bleeding-associated variables, cardiac ischemic time and cardiac function-associated variables, age, diuretics and active infection, chronic obstructive lung and peripheral vascular disease.  \r\n\r\nConclusion: The 'Detect-A(K)I' model successfully detects cardiac surgery-associated acute kidney injury within 12 hours after surgery with the best discriminatory characteristics reported so far.","354":"The work in this paper is directed towards sophisticated formalisms for reasoning under probabilistic uncertainty in ontologies in the Semantic Web. Ontologies play a central role in the development of the Semantic Web, since they provide a precise definition of shared terms in web resources. They are expressed in the standardized web ontology language OWL, which consists of the three increasingly expressive sublanguages OWL Lite, OWL DL, and OWL Full. The sublanguages OWL Lite and OWL DL have a formal semantics and a reasoning support through a mapping to the expressive description logics and , respectively. In this paper, we present the expressive probabilistic description logics P- and P-, which are probabilistic extensions of these description logics. They allow for expressing rich terminological probabilistic knowledge about concepts and roles as well as assertional probabilistic knowledge about instances of concepts and roles. They are semantically based on the notion of probabilistic lexicographic entailment from probabilistic default reasoning, which naturally interprets this terminological and assertional probabilistic knowledge as knowledge about random and concrete instances, respectively. As an important additional feature, they also allow for expressing terminological default knowledge, which is semantically interpreted as in Lehmann's lexicographic entailment in default reasoning from conditional knowledge bases. Another important feature of this extension of and by probabilistic uncertainty is that it can be applied to other classical description logics as well. We then present sound and complete algorithms for the main reasoning problems in the new probabilistic description logics, which are based on reductions to reasoning in their classical counterparts, and to solving linear optimization problems. In particular, this shows the important result that reasoning in the new probabilistic description logics is decidable\/computable. Furthermore, we also analyze the computational complexity of the main reasoning problems in the new probabilistic description logics in the general as well as restricted cases.","355":"ObjectivesClinical guidelines (GLs) are assuming a major role in the medical area, in order to grant the quality of the medical assistance and to optimize medical treatments within healthcare organizations. The verification of properties of the GL (e.g., the verification of GL correctness with respect to several criteria) is a demanding task, which may be enhanced through the adoption of advanced Artificial Intelligence techniques. In this paper, we propose a general and flexible approach to address such a task.Methods and materialsOur approach to GL verification is based on the integration of a computerized GL management system with a model-checker. We propose a general methodology, and we instantiate it by loosely coupling GLARE, our system for acquiring, representing and executing GLs, with the model-checker SPIN.ResultsWe have carried out an in-depth analysis of the types of properties that can be effectively verified using our approach, and we have completed an overview of the usefulness of the verification task at the different stages of the GL life-cycle. In particular, experimentation on a GL for ischemic stroke has shown that the automatic verification of properties in the model checking approach is able to discover inconsistencies in the GL that cannot be detected in advance by hand.ConclusionOur approach thus represents a further step in the direction of general and flexible automated GL verification, which also meets usability requirements.","356":null,"357":null,"358":null,"359":null,"360":"We concentrate on the task of Fashion AI, which entails creating images that are multimodal in terms of semantics. Previous research has attempted to make use of several generators for particular classes, which limits its application to datasets that have a just a few classes available. Instead, I suggest a new Group Decrease Network GroupDNet , which takes advantage in the generator of group convolutions and gradually reduces the percentages of the groups decoders convolutions. As a result, GroupDNet has a lot of influence over converting natural images with semantic marks and can produce high quality outcomes that are feasible for containing a lot of groups. Experiments on a variety of difficult datasets show that GroupDNet outperforms other algorithms in task. Ashish Jobson | Dr. Kamlraj R \"Fashion AI\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd41256.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/41256\/fashion-ai\/ashish-jobson","361":"6G technology is expected to transform many industries, including healthcare. It will enable efficient patient monitoring and telemedicine, eliminating geographical barriers. In near future, healthcare industry will be dependent on artificial intelligence. This will improve the quality of life and revolutionize medical care, especially during epidemics and pandemics. The potential of 6G in telesurgery is also being explored. Tapan Golakiya \u00dctilizing 6G Technology for Healthcare Monitoring with Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-3 , June 2023, URL: https:\/\/www.ijtsrd.com.com\/papers\/ijtsrd57493.pdf Paper URL: https:\/\/www.ijtsrd.com.com\/computer-science\/artificial-intelligence\/57493\/utilizing-6g-technology-for-healthcare-monitoring-with-machine-learning\/tapan-golakiya","362":null,"363":"Systems for Ambient Intelligence environments involve at some stage a service composition task, as a mean of adaptability to the context changes. However, users generally find themselves involved in the composition task, by selecting or deciding what to compose and how. This paper proposes the use of Artificial Intelligent Agents for the automation of the composition task, providing transparency from the user point of view.","364":"Nowadays all people are using websites to make their tour plans and to book hotels. Instead of approaching traditional travel agents, websites provide round the clock service at no cost. This paper proposes the Tourism guide website using Artificial Intelligence concept Fuzzy Logic. The Artificial Intelligence based website is more user friendly and understands the requirements of customer and helps the customer to make apt decision for making tour plans and book hotels and eating nice foods in various places. This paper proposes Fuzzy Logic based decision making to learn the mentality of customers and provides useful personalized and customized suggestions for tourists. The proposed system provides virtual tour to help the tourist for choosing the perfect tourist destinations. The proposed online system also provides a detailed itinerary for tourist spot selected by the traveller. Dr. E. J. Thomson Fredrik | P. C. Rithika Ranjith \u00d6nline Tour Booking using Fuzzy Decision Making Method\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42568.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42568\/online-tour-booking-using-fuzzy-decision-making-method\/dr-e-j-thomson-fredrik","365":null,"366":"This paper proposes a concept for an Intelligent Adaptive Learning Environment (IALE) based on a holistic Multidimensional Instructional Design Model, applied on OLAT, an open source, Java LMS, developed at the University of Zurich, to support student and\/or groups defined as \"Learning Entities\" (LE). Based primarily on an Artificial Intelligence - Tutoring Subsystem, used to identify, monitor and adapt the student's learning path, considering the students actual knowledge, learning habits and preferred learning style. The proposed concept has the peculiarity of eliminating any didactical boundaries or rigid, implied course structures (also known as unlimited didactical freedom). Relying on \"real time\" adapted profiles, it allows content authors to apply a dynamic course design, supporting tutored, collaborative sessions and activities, as suggested by modern pedagogy. The AI tutoring facility (eTutor), coupled with the LMS, is intended to support the \"human tutor\" with valuable LE performance - \/ activity data, available from the integrated \"Behavior Recorder Controller\" (BRC), allowing to confirm or manually modify actions suggested by the eTutor. The student has the option to select the level of tutoring interventions or switch to a \"subject matter\" exercise mode if he feels, and is allowed to do so. The concept presented combines a personalized level of surveillance, learning activity- and\/or learning path adaptation suggestions to ensure the students learning motivation and learning success.","367":null,"368":null,"369":null,"370":"In the past decade, internet of things IoT has been a focus of research. It makes more intelligent to core element of modern world such as hospitals, cities, organizations, and buildings. Usually, IoT has four major components including sensing, information processing, applications and services, heterogeneous access and additional components e.g. Security and privacy. In this paper, we are presenting security perspective from the perspective of layers that comprises IoT. In this we focus on the overview of IoT security perspective. Sunilkumar Malge | Pallavi Singh \"\"Internet of Things (IoT): Security Perspective\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-4 , June 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd24010.pdf\r\n\r\n Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/24010\/internet-of-things-iot-security-perspective\/sunilkumar-malge","371":null,"372":null,"373":"Genetic algorithms are playing an increasingly important role in studies\r\n\tof complex adaptive systems, ranging from adaptive agents in economic\r\n\ttheory to the use of machine learning techniques in the design of\r\n\tcomplex devices such as aircraft turbines and integrated circuits.\r\n\tAdaptation in Natural and Artificial Systems is the book that initiated\r\n\tthis field of study, presenting the theoretical foundations and exploring\r\n\tapplications.\r\n\t\r\n\tIn its most familiar form, adaptation is a biological process, whereby\r\n\torganisms evolve by rearranging genetic material to survive in environments\r\n\tconfronting them. In this now classic work, Holland presents a mathematical\r\n\tmodel that allows for the nonlinearity of such complex interactions.\r\n\tHe demonstrates the model's universality by applying it to economics,\r\n\tphysiological psychology, game theory, and artificial intelligence\r\n\tand then outlines the way in which this approach modifies the traditional\r\n\tviews of mathematical genetics.\r\n\t\r\n\tInitially applying his concepts to simply defined artificial systems\r\n\twith limited numbers of parameters, Holland goes on to explore their\r\n\tuse in the study of a wide range of complex, naturally occuring processes,\r\n\tconcentrating on systems having multiple factors that interact in\r\n\tnonlinear ways. Along the way he accounts for major effects of coadaptation\r\n\tand coevolution: the emergence of building blocks, or schemata, that\r\n\tare recombined and passed on to succeeding generations to provide,\r\n\tinnovations and improvements.\r\n\t\r\n\tJohn H. Holland is Professor of Psychology and Professor of Electrical\r\n\tEngineering and Computer Science at the University of Michigan. He\r\n\tis also Maxwell Professor at the Santa Fe Institute and isDirector\r\n\tof the University of Michigan\/Santa Fe Institute Advanced Research\r\n\tProgram.","374":null,"375":"Named Entity Linking (nel) grounds entity mentions to their corresponding node in a Knowledge Base (kb). Recently, a number of systems have been proposed for linking entity mentions in text to Wikipedia pages. Such systems typically search for candidate entities and then disambiguate them, returning either the best candidate or nil. However, comparison has focused on disambiguation accuracy, making it difficult to determine how search impacts performance. Furthermore, important approaches from the literature have not been systematically compared on standard data sets. We reimplement three seminal nel systems and present a detailed evaluation of search strategies. Our experiments find that coreference and acronym handling lead to substantial improvement, and search strategies account for much of the variation between systems. This is an interesting finding, because these aspects of the problem have often been neglected in the literature, which has focused largely on complex candidate ranking algorithms.","376":"The convergence of blockchain technology and artificial intelligence AI is heralding a transformative era across industries. This paper explores the integration of blockchain and AI, highlighting the synergies between these two technologies and the myriad possibilities they unlock. We delve into the key areas where this convergence is making an impact, including data security, decentralized applications, autonomous systems, and more. Furthermore, we examine the challenges and considerations surrounding this integration, emphasizing the potential benefits for society, businesses, and innovation. Manish Verma \"Blockchain and AI Convergence: A New Era of Possibilities\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-5 , October 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd59864.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59864\/blockchain-and-ai-convergence-a-new-era-of-possibilities\/manish-verma","377":null,"378":null,"379":null,"380":"There is controversy as to whether explicit support for pddl-like axioms and derived predicates is needed for planners to handle real-world domains effectively. Many researchers have deplored the lack of precise semantics for such axioms, while others have argued that it might be best to compile them away. We propose an adequate semantics for pddl axioms and show that they are an essential feature by proving that it is impossible to compile them away if we restrict the growth of plans and domain descriptions to be polynomial. These results suggest that adding a reasonable implementation to handle axioms inside the planner is beneficial for the performance. Our experiments confirm this suggestion.","381":null,"382":"Although enrolled in Hampshire College, not Hogwarts\n                 Academy, Raphael Crawford-Marks has spent the past year\n                 fine-tuning his Quidditch skills. Crawford-Marks - set\n                 to graduate on May 22 - has created a computerized\n                 version of the rapid-fire game played by young witches\n                 and warlocks in J.K. Rowling's series of Harry Potter\n                 novels. But Crawford-Marks is doing far more than\n                 playing a video game: he's running an artificial\n                 intelligence experiment that involves computerized\n                 generation of teams that either proceed in competition\n                 or fall by the wayside according to their ability to\n                 adapt to the Quidditch environment.","383":null,"384":null,"385":"John Holland's Adaptation in Natural and Artificial Systems is one\r\n\tof the classics in the field of complex adaptive systems. Holland\r\n\tis known as the father of genetic algorithms and classifier systems\r\n\tand in this tome he describes the theory behind these algorithms.\r\n\tDrawing on ideas from the fields of biology and economics, he shows\r\n\thow computer programs can evolve. The book contains mathematical\r\n\tproofs that are accessible only to those with strong backgrounds\r\n\tin engineering or science. Genetic algorithms are playing an increasingly\r\n\timportant role in studies of complex adaptive systems, ranging from\r\n\tadaptive agents in economic theory to the use of machine learning\r\n\ttechniques in the design of complex devices such as aircraft turbines\r\n\tand integrated circuits. Adaptation in Natural and Artificial Systems\r\n\tis the book that initiated this field of study, presenting the theoretical\r\n\tfoundations and exploring applications. In its most familiar form,\r\n\tadaptation is a biological process, whereby organisms evolve by rearranging\r\n\tgenetic material to survive in environments confronting them. In\r\n\tthis now classic work, Holland presents a mathematical model that\r\n\tallows for the nonlinearity of such complex interactions. He demonstrates\r\n\tthe model's universality by applying it to economics, physiological\r\n\tpsychology, game theory, and artificial intelligence and then outlines\r\n\tthe way in which this approach modifies the traditional views of\r\n\tmathematical genetics. Initially applying his concepts to simply\r\n\tdefined artificial systems with limited numbers of parameters, Holland\r\n\tgoes on to explore their use in the study of a wide range of complex,\r\n\tnaturally occuring processes, concentrating on systems having multiple\r\n\tfactors that interact in nonlinear ways. Along the way he accounts\r\n\tfor major effects of coadaptation and coevolution: the emergence\r\n\tof building blocks, or schemata, that are recombined and passed on\r\n\tto succeeding generations to provide, innovations and improvements.\r\n\tJohn H. Holland is Professor of Psychology and Professor of Electrical\r\n\tEngineering and Computer Science at the University of Michigan. He\r\n\tis also Maxwell Professor at the Santa Fe Institute and is Director\r\n\tof the University of Michigan\/Santa Fe Institute Advanced Research\r\n\tProgram.","386":"All around the world, inventory managers are faced with the complexities of simultaneously striving to retain constant manufacturing operation, render quality and adequate services to customer and keep goods at optimum level as well as having proper inventory management policy in place. As company grows a number of problem also arises first, a varieties of item are needed to be maintained in the inventory making it difficult to for managers to keep track, therefore a formal record keeping will have to evolve, besides that as quantities in inventory grows more space is required to do business. Due to local and global competition any company that must survive in the competitive world must take an advantage of effective management of its inventory seriously therefore the use of intelligent approach for effective and efficient inventory management system has become indispensable. This work developed an intelligent system implemented in a web based environment to integrate multiple stores also providing an effective coordination of all of the stores, intelligently determining the different reorder points of all the disparate stores in the systems and communicating the information back to the centralized store using Java Remote Method Invocation (RMI) with a secure socket layer (SSL), implemented using My Structured Query Language (MySQL), Database Management System, Hypertext Markup Language (HTML), JavaScript, Hypertext Preprocessor (PHP) and Xampp (Apache). The work optimized the performance of inventory management integrating multiple systems and providing an efficient coordination and monitoring moving away from single store into distributed system relating real time status of supplies at the different stores. Madamidola O. A | Daramola O.A | Akintola K .G\"WEB BASED INTELLIGENT INVENTORY MANAGEMENT SYSTEM\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-1 | Issue-4 , June 2017, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd107.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/107\/web-based-intelligent-inventory-management-system\/madamidola-o-a","387":"The scientific research in the area of computational mechanisms for trust and reputation in virtual societies is a recent discipline oriented to increase the reliability and performance of electronic communities. Computer science has moved from the paradigm of isolated machines to the paradigm of networks and distributed computing. Likewise, artificial intelligence is quickly moving from the paradigm of isolated and non-situated intelligence to the paradigm of situated, social and collective intelligence. The new paradigm of the so called intelligent or autonomous agents and multi-agent systems (MAS) together with the spectacular emergence of the information society technologies (specially reflected by the popularization of electronic commerce) are responsible for the increasing interest on trust and reputation mechanisms applied to electronic societies. This review wants to offer a panoramic view on current computational trust and reputation models.","388":null,"389":null,"390":null,"391":"Most of the existing image recognitions systems are based on physical parameters of the images whereas image processing methodologies relies on extraction of color, shape and edge features. Thus Transfer Learning is an efficient approach of solving classification problem with little amount of data. There are many deep learning algorithms but most tested one is AlexNet. It is well known Convolution Neural Network AlexNet CNN for recognition of images using deep learning. So for recognition and detection of the image we have proposed Deep Learning approach in this project which can analyse thousands of images which may take a lot for a human to do. Pretrained convolutional neural network i.e. AlexNet is trained by using the features such as textures, colors and shape. The model is trained on more than 1000 images and can classify images into categories which we have defined. The trained model is tested on various standard and own recorded datasets consist of rotational, translated and shifted images. Thus when a image is passed to the system it will apply AlexNet and return the results with a image category in which the image lies with high accuracy. Thus our project tends to reduce time and cost of image recognition systems using deep learning. Dr. Sachin K. Korde | Manoj J. Munda | Yogesh B. Chintamani | Yasir L. Pirjade | Akshay V. Gurme \"Image Classification using Deep Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31653.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31653\/image-classification-using-deep-learning\/dr-sachin-k-korde","392":"This study proposes Artificial Neural Network ANN based field strength prediction models for the rural areas of Abuja, the federal capital territory of Nigeria. The ANN based models were created on bases of the Generalized Regression Neural network GRNN and the Multi Layer Perceptron Neural Network MLP NN . These networks were created, trained and tested for field strength prediction using received power data recorded at 900MHz from multiple Base Transceiver Stations BTSs distributed across the rural areas. Results indicate that the GRNN and MLP NN based models with Root Mean Squared Error RMSE values of 4.78dBm and 5.56dBm respectively, offer significant improvement over the empirical Hata Okumura counterpart, which overestimates the signal strength by an RMSE value of 20.17dBm. Deme C. Abraham \"\"Mobile Network Coverage Determination at 900MHz for Abuja Rural Areas using Artificial Neural Networks\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-2 , February 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd30228.pdf\r\n\r\nPaper Url : https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/30228\/mobile-network-coverage-determination-at-900mhz-for-abuja-rural-areas-using-artificial-neural-networks\/deme-c-abraham","393":null,"394":null,"395":null,"396":null,"397":"This paper aims to introduce, review and summarize the basic concepts of reinforcement learning. It will provide an introduction to reinforcement learning in machine learning while covering reinforcement learning workflow, types, methods and algorithms used in it. Shreya Khare | Yogeshchandra Puranik \u00c4 Review on Introduction to Reinforcement Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42498.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42498\/a-review-on-introduction-to-reinforcement-learning\/shreya-khare","398":"The primary objective of this research work is to develop an expert system for identification & classification of the cervical cells in the images of the slides of Papanicolaou smear test, which is done for screening of cervical cancer. The expert system can serve as a potential tool for mass level screening of cervical cancer by characterization and classification of Papanicolaou smear images. The Expert system presented in this work is powered by a novel hierarchical probabilistic artificial neural network that works along with the knowledgebase of novel benchmark database of digitized cervical cells. The primary purpose of employing expert systems in medicine is creation of such artificially intelligent systems which can provide assistance to a medical doctor in delivering expert diagnosis. These artificial intelligent systems support the clinical decision making by anticipating the diagnostic results once they are trained using previously acquired training data. The use of Artificial intelligence in medicine has shown substantial progress in achieving timely, reliable diagnosis and more precise treatment of many diseases.The expert system developed in this work exhibited a competence of about 92% which has been evaluated by comparing its results with the identification & classification of cervical cells by human experts. Abid Sarwar\"Cervical Smear Analyzer (CSA) Expert System for identification of cervical cells in Papanicolaou smear test\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-1 , December 2017, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd7047.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/7047\/cervical-smear-analyzer-csa-expert-system-for-identification-of-cervical-cells-in-papanicolaou-smear-test\/abid-sarwar","399":"We concentrate on the task of Fashion AI, which entails creating images that are multimodal in terms of semantics. Previous research has attempted to use several class specific generators, which limits its application to datasets with a limited number of classes. Instead, we suggest a new Group Decreasing Network GroupDNet , which takes advantage in the generator of group convolutions and gradually reduces the percentages of the groups decoders convolutions. As a result, GroupDNet has a lot of influence over converting semantic labels to natural images and can produce plausible high quality results for datasets with a lot of groups. Experiments on a variety of difficult datasets show that GroupDNet outperforms other algorithms in the SMIS mission. We also demonstrate that GroupDNet can perform a variety of interesting synthesis tasks. Ashish Jobson | Dr. Kamalraj R \"Fashion AI Literature\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42378.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42378\/fashion-ai-literature\/ashish-jobson","400":null,"401":null,"402":null,"403":null,"404":null,"405":null,"406":"The 1956 Dartmouth summer research project on artificial intelligence was initiated by this August 31, 1955 proposal, authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. The original typescript consisted of 17 pages plus a title page. Copies of the typescript are housed in the archives at Dartmouth College and Stanford University. The first 5 papers state the proposal, and the remaining pages give qualifications and interests of the four who proposed the study. In the interest of brevity, this article reproduces only the proposal itself, along with the short autobiographical statements of the proposers.","407":null,"408":null,"409":null,"410":null,"411":null,"412":null,"413":null,"414":null,"415":"Artificial intelligence increasingly forms an essential context for the distribution of power within workplaces. Using the case study of an AI\u2010enabled chatbot initially created by IBM and subsequently developed by an alt\u2010labour network in the United States and a traditional union in Australia, this article outlines several distinctive ways in which the chatbot increased union resources and capabilities. Once reconfigured to reflect an \u2018organising\u2019, rather than \u2018servicing\u2019 ethos, the chatbot became an infrastructural resource that enabled otherwise marginal workers to receive basic information in a manner that reinforced union narratives of power and worker solidarity, and workplaces to be mapped more efficiently. The chatbot did not act as a labour saving tool, but stimulated wide\u2010ranging learning by bringing implicit tensions between \u2018servicing\u2019 and \u2018organising\u2019 conceptions of knowledge, power and expertise to the surface. Chatbots thus offer distinctive potential affordances to unions in enhancing their resources and capabilities as \u2018orchestrators\u2019 of worker power.","416":null,"417":null,"418":"This paper presents an approach to exploit free text descriptions of TV\r\n   programmes as available from EPG data sets for a recommendation system\r\n   that takes the content of programmes into account(3). The paper\r\n   focusses on classifying free text descriptions in relation to natural\r\n   language user queries.","419":null,"420":null,"421":null,"422":"As partisan politics began to wane and realign itself voters are questioning who they are politically. Are\r\nthey Democratic, Republican, populist nationalist or multicultural globalists? How will they categorize\r\nthemselves Black, White, Hispanic, or some other race? After the 1928 election there were a precarious\r\nchanges to the electoral demographic that influenced the 1932 election. The 2016 election is perilous will it\r\ntoo result in an era of Democratic political domination or is the old political system crumbling and a new\r\nAmerican political order is being born? Furthermore, how will the evolution of artificial intelligence (AI)\r\nimpact the status of U. S. politic in 2020? The purpose of this research addresses these important issues\r\nthat will introduce a vigorous and informed debate going forward in a society where transparency is a\r\nrequirement mandated by an ever transforming voting body.","423":null,"424":null,"425":null,"426":null,"427":null,"428":null,"429":null,"430":null,"431":null,"432":"The main purpose of the research reported here is to show that a new\n\tand more powerful type of computer-assisted instruction (CAI), based\n\ton extensive application of artificial-intelligence (AI) techniques,\n\tis feasible, and to demonstrate some of its major capabilities. A\n\tset of computer programs was written and given the name SCHOLAR.\n\tDue to its complexity, only the conception and educational aspects\n\tof this system (including an actual on-line protocol) are presented\n\tin this paper. In what may be called conventional ad hoc-frame-oriented\n\t(AFO) CAI, the data base consists of many \"frames\" of specific pieces\n\tof text, questions, and anticipated answers entered in advance by\n\tthe teacher. By contrast, an information-structure-oriented (ISO)\n\tCAI system is based on the utilization of an information network\n\tof facts, concepts, and procedures; it can generate text, questions,\n\tand corresponding answers. Because an ISO CAI system can also utilize\n\tits information network to answer questions formulated by the student,\n\ta mixed-initiative dialogue between student and computer is possible\n\twith questions and answers from both sides.","433":null,"434":null,"435":null,"436":null,"437":null,"438":"Computational creativity is a newly emerging field within AI that focuses on the capacity of machines to both generate and evaluate novel outputs that would be considered creative. It is the philosophy, science, and engineering of computational systems which exhibit behaviors that unbiased observers would regard as creative. It is mainly concerned with building creative systems. It addresses processes that would be deemed creative if performed by a human. This paper provides an introduction to computational creativity. Matthew N. O. Sadiku | Nana K. Ampah | Sarhan M. Musa \"Computational Creativity\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-6 , October 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd28094.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/28094\/computational-creativity\/matthew-n-o-sadiku","439":null,"440":null,"441":null,"442":null,"443":null,"444":null,"445":null,"446":null,"447":"Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone.","448":null,"449":"The biometric is a study of human behavior and features. Face recognition is a technique of biometric. Various approaches are used for it. Face recognition is emerging branch of biometric for security as no faces can be defeated as a security approach. So, how we can recognize a face with the help of computers is given in this paper. The typical way that a FRS can be used for identification purposes. The effectiveness of the whole system is highly dependent on the quality and characteristics of the captured face image. The process begins with face detection and extraction from the larger image, which generally contains a background and often more complex patterns and even other faces. A survey for all these techniques is in this paper for analyzing various algorithms and methods. Sagar Deshmukh | Sanjay Rawat | Shubhangi Patil\"Face Recognition Technology\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-4 , June 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd14331.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/14331\/face-recognition-technology\/sagar-deshmukh","450":"Self driving vehicles are cars or trucks in which human drivers are never required to take control to safely operate the vehicle. They can possibly reform urban portability by giving maintainable, protected, and advantageous, clog free transportability. The issues like reliably perceiving traffic lights, signs, indistinct path markings can be overwhelmed by utilizing the innovative improvement in the fields of Deep Learning DL . Here, Faster Region Based Convolution Neural Network F RCNN is proposed for detection and recognition of Traffic Lights TL and signs by utilizing transfer learning. The input can be taken from the dataset containing various images of traffic signals and signs as per Indian Traffic Signals. The model achieves its target by distinguishing the traffic light and signs with its right class type. The proposed framework can likewise be upgraded for safe driving in spite of hazy path markings. Aswathy Madhu | Veena S Nair \"Traffic Sign Detection and Recognition for Automated Driverless Cars Based on SSD\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-5 , August 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31888.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/31888\/traffic-sign-detection-and-recognition-for-automated-driverless-cars-based-on-ssd\/aswathy-madhu","451":"\"Factual knowledge\" used by natural language processing systems can be conveniently represented in the form of semantic networks. Compared to a \"linear\" representation such as that of the Predicate Calculus however, semantic networks present special problems with respect to the use of logical connectives, quantifiers, descriptions, and certain other constructions. Systematic solutions to these problems will be proposed, in the form of extensions to a more or less conventional network notation. Predicate Calculus translations of network propositions will frequently be given for comparison, to illustrate the close kinship of the two forms of representation.","452":"There has been widespread media commentary about the potential impact of generative Artificial Intelligence (AI) such as ChatGPT on the Education field, but little examination at scale of how educators believe teaching and assessment should change as a result of generative AI. This mixed methods study examines the views of educators (n\u00feinspace=\u00feinspace318) from a diverse range of teaching levels, experience levels, discipline areas, and regions about the impact of AI on teaching and assessment, the ways that they believe teaching and assessment should change, and the key motivations for changing their practices. The majority of teachers felt that generative AI would have a major or profound impact on teaching and assessment, though a sizeable minority felt it would have a little or no impact. Teaching level, experience, discipline area, region, and gender all significantly influenced perceived impact of generative AI on teaching and assessment. Higher levels of awareness of generative AI predicted higher perceived impact, pointing to the possibility of an `ignorance effect'. Thematic analysis revealed the specific curriculum, pedagogy, and assessment changes that teachers feel are needed as a result of generative AI, which centre around learning with AI, higher-order thinking, ethical values, a focus on learning processes and face-to-face relational learning. Teachers were most motivated to change their teaching and assessment practices to increase the performance expectancy of their students and themselves. We conclude by discussing the implications of these findings in a world with increasingly prevalent AI.","453":null,"454":null,"455":"Recently, Artificial Intelligence techniques have proved useful inhelping users to handle the large amount of information on the Internet.The idea of personalized search engines, intelligent software agents,and recommender systems has been widely accepted among users who requireassistance in searching, sorting, classifying, filtering and sharingthis vast quantity of information. In this paper, we present astate-of-the-art taxonomy of intelligent recommender agents on theInternet. We have analyzed 37 different systems and their references andhave sorted them into a list of 8 basic dimensions. These dimensions arethen used to establish a taxonomy under which the systems analyzed areclassified. Finally, we conclude this paper with a cross-dimensionalanalysis with the aim of providing a starting point for researchers toconstruct their own recommender system.","456":null,"457":null,"458":"Hospital management is a hard task due to the complexity of the organization, the costly infrastructure, the specialized services offered to different patients and the need for prompt reaction to emergencies. Artificial Intelligence planning and scheduling methods can offer substantial support to the management of hospitals, and help raising the standards of service. This editorial presents an overview of the achievements reported in therapy planning and hospital management together with a general roadmap of the published research in Artificial Intelligence planning and scheduling. Finally, a discussion for the future research and development in this area concludes the presentation.","459":null,"460":null,"461":"4-G mobile communications system has utilized\r\nhigh speed data communications technology having\r\nconnectivity to all sorts of networks including 2-G and 3-G\r\nmobile networks. Authentication of mobile subscribers and\r\nnetworks are a prime criterion to check and minimize security\r\nthreats and attacks. An artificial intelligence based mutual\r\nauthentication system with four entities is proposed. A person\r\ntalking salutation or greeting words in different times are\r\nalways consisting of a very narrow range of frequencies which\r\nare varying in nature from person to person. Voice frequency\r\nof the salutation or selective words used by a subscriber like\r\nHello, Good Morning etc is taken as first entity. Second entity\r\nis chosen as frequency of flipping or clapping sound of the\r\ncalling subscriber. Then third entity is taken as face image of\r\nthe calling subscriber. Fourth entity is granted as probability\r\nof salutation or greeting word from subscriber\u2019s talking habit\r\n(set of salutation words) while initializing a call. These four\r\nentities such as probability of particular range of frequencies\r\nfor the salutation word, frequency of flipping sound, face\r\nimage matching of the subscriber, particular salutation or\r\ngreeting word at the time of starting a call are used with most\r\nfrequently, more frequently and less frequently by the calling\r\nsubscriber like uncertainty in Artificial Intelligence (AI). Now\r\ndifferent relative grades are assigned for most frequently,\r\nmore frequently and less frequently used parameters and the\r\ngrades are modified according to the assumed weightage. A\r\nFuzzy Rule (condition) by Fuzzy operation is invented. If the\r\nresults obtained from fuzzy operations are satisfied by the\r\nfuzzy rule, the subscriber (MS) and the network (Switch or\r\nServer) are mutually authenticated in 4-G mobile\r\ncommunications.","462":"In this paper we present the MDA framework (standing for Mechanics,\n\tDynamics, and Aesthetics), developed and taught as part of the Game\n\tDesign and Tuning Workshop at the Game Developers Conference, San\n\tJose 2001-2004.\n\t\n\tMDA is a formal approach to understanding games --- one which attempts\n\tto bridge the gap between game design and development, game criticism,\n\tand technical game research. We believe this methodology will clarify\n\tand strengthen the iterative processes of developers, scholars and\n\tresearchers alike, making it easier for all parties to decompose,\n\tstudy and design a broad class of game designs and game artifacts.","463":"The boundary between well structured and ill structured problems is\n\tvague, fluid and not susceptible to formalization. Any problem solving\n\tprocess will appear ill structured if the problem solver is a serial\n\tmachine that has access to a very large long-term memory of potentially\n\trelevant information, and\/or access to a very large external memory\n\tthat provides information about the actual real-world consequences\n\tof problem-solving actions. There is no reason to suppose that new\n\tand hitherto unknown concepts or techniques are needed to enable\n\tartificial intelligence systems to operate successfully in domains\n\tthat have these characteristics.","464":null,"465":"We present a general theory of topological maps whereby sensory input, topological and local metrical information are combined to define the topological maps explaining such information. Topological maps correspond to the minimal models of an axiomatic theory describing the relationships between the different sources of information explained by a map. We use a circumscriptive theory to specify the minimal models associated with this representation.\r\nThe theory here proposed is independent of the exploration strategy the agent follows when building a map. We provide an algorithm to calculate the models of the theory. This algorithm supports different exploration strategies and facilitates map disambiguation when perceptual aliasing arises.","466":"This is the new era of technology development where all the things and work is done by the machines. The goal of Information Technology is to develop a device which is able to work like a human itself. For that Artificial Intelligence, Machine Learning and Deep Learning are going to be used. Machine Learning is a subpart of the Artificial Intelligent which helps a machine to learn by itself. To apply learning processes on machines it required deep knowledge of programming, mathematics and statistics. Now it is not a big problem, as the technology is changing day by day the new concept known as No Code ML and Auto Code Generation are introduced. This helps the users to create a model without doing any kind of coding. In this new technology everyone is able to create a model and use machine learning. There are several platforms which provide this kind of facilities. The models created on those platforms give good accuracy and desire outcomes as well. Hiteshkumar Babubhai Vora | Hardik Anilbhai Mirani | Vraj Bhatt \"Traditional Machine Learning and No-Code Machine Learning with its Features and Application\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-2 , February 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38287.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/38287\/traditional-machine-learning-and-nocode-machine-learning-with-its-features-and-application\/hiteshkumar-babubhai-vora","467":null,"468":null,"469":"On November 30, 2022, the American artificial intelligence company OpenAI released the large language model ChatGPT. ChatGPT, as an AI language model, is not only capable of interacting with humans but can also write articles, develop strategies, create poetry, and even write code and check for vulnerabilities. However, along with its capabilities, there are also legal risks associated with the application of ChatGPT, making it important for us to research and consider how to properly prevent these risks. The main research focus of this project is on ChatGPTs ethical responsibilities, the relationship and order of human machine coexistence, the protection of individual safety, and the governance of ChatGPT by both the nation and society. Through our research, we aim to maximize the convenience that ChatGPT offers us and effectively mitigate its potential risks. Chen Jiaqi | Zhen Yunuo | Guo Simeng \"Legal Risks and Preventive Measures in ChatGPT Applications in China\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-6 , December 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd60106.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/60106\/legal-risks-and-preventive-measures-in-chatgpt-applications-in-china\/chen-jiaqi","470":"Artificial Intelligence (AI) can now automate the algorithm selection, feature engineering, and hyperparameter tuning steps in a machine learning workflow. Commonly known as AutoML or AutoAI, these technologies aim to relieve data scientists from the tedious manual work. However, today's AutoAI systems often present only limited to no information about the process of how they select and generate model results. Thus, users often do not understand the process, neither do they trust the outputs. In this short paper, we provide a first user evaluation by 10 data scientists of an experimental system, AutoAIViz, that aims to visualize AutoAI's model generation process. We find that the proposed system helps users to complete the data science tasks, and increases their understanding, toward the goal of increasing trust in the AutoAI system.","471":"CARAMEL is a European project that aims amongst others to improve and extend cyberthreat detection and mitigation techniques for automotive driving systems. This paper highlights the important role that advanced artificial intelligence and machine learning techniques can have in proactively addressing modern autonomous vehicle cybersecurity challenges and on mitigating associated safety risks when dealing with targetted attacks on a vehicle's camera sensors. The cybersecurity solutions developed by CARAMEL are based on powerful AI tools and algorithms to combat security risks in automated driving systems and will be hosted on embedded processors and platforms. As such, it will be possible to have a specialized anti-hacking device that addresses newly introduced technological dimensions for increased robustness and cybersecurity in addition to industry needs for high speed, low latency, functional safety, light weight, low power consumption.","472":null,"473":null,"474":null,"475":null,"476":null,"477":null,"478":null,"479":null,"480":"Apocalyptic AI, the hope that we might one day upload our minds into machines and live forever in cyberspace, is a surprisingly wide-spread and influential idea, affecting everything from the world view of online gamers to government research funding and philosophical thought. In Apocalyptic AI, Robert Geraci offers the first serious account of this \"cyber-theology\u00a8and the people who promote it, drawing on interviews with roboticists and AI researchers and even devotees of the online game Second Life. He points out that the rhetoric of Apocalyptic AI is strikingly similar to that of the apocalyptic traditions of Judaism and Christianity---in both systems the believer is trapped in a dualistic universe and expects a resolution in which he or she will be translated to a transcendent new world and live forever in a glorified new body. Geraci also shows how this worldview exerts significant influence by promoting certain types of research in robotics and artificial intelligence, and has also had an impact on philosophers of mind, theologians, and even legal scholars.","481":null,"482":"Video Games are boring when they are too easy and frustrating when\n\tthey are too hard. While most singleplayer games allow players to\n\tadjust basic difficulty (easy, medium, hard, insane), their overall\n\tlevel of challenge is often static in the face of individual player\n\tinput. This lack of flexibility can lead to mismatches between player\n\tability and overall game difficulty.\n\t\n\tIn this paper, we explore the computational and design requirements\n\tfor a dynamic difficulty adjustment system. We present a probabilistic\n\tmethod (drawn predominantly from Inventory Theory) for representing\n\tand reasoning about uncertainty in games. We describe the implementation\n\tof these techniques, and discuss how the resulting system can be\n\tapplied to create flexible interactive experiences that adjust on\n\tthe fly.","483":"This is a non-expert overview of Intelligent Tutoring Systems (ITSs),\n\ta way in which Artificial Intelligence (AI) techniques are being\n\tapplied to education. It introduces ITSs and the motivation for them.\n\tIt looks at its history: its evolution from Computer-Assisted Instruction\n\t(CAI). After looking at the structure of a lsquotypicalrsquo ITS,\n\tthe paper further examines and discusses some other architectures.\n\tSeveral classic ITSs are reviewed, mainly due to their historical\n\tsignificance or because they best demonstrate some of the principles\n\tof intelligent tutoring. A reasonably representative list of ITSs\n\tis also provided in order to provide a better appreciation of this\n\tvibrant field as well as reveal the scope of existing tutors. The\n\tpaper concludes, perhaps more appropriately, with some of the author's\n\tviewpoints on a couple of controversial issues in the intelligent\n\ttutoring domain.","484":"This project is a method of recognition in real time that traces the human mood itself and to map out the human behaviour traits with their physiological features. Recognition of emotion is the phase of human emotion identification. In recognising the emotions of others, people vary widely in their precision. Usage of this technology is to help people with emotion recognition is a relatively nascent research area. In general, if several modalities are used in connection, the technology performs better. Most work has been performed to date to automate the identification of video facial expressions, audio spoken expressions, written text expressions, and physiology as measured by wearables. A real time recognition framework was built in this project that traces the very mood of the individual using some of the libraries such as Keras, OpenCV, Tensor flow, SciPy, The Python Alternative To Matlab, etc. HOF and SVM were used to tackle the problem of recognition. At each point, optical flows calculate the rotation between two frames relative to an observer. For image recognition, the Deep Convolutional Neural Networks were used. It was concluded that the application of the proposed strategy was accurate and effective. Ashish Jobson | Dr. A. Rengarajan \"Emotion Detector\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-1 , December 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38245.pdf Paper URL : https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/38245\/emotion-detector\/ashish-jobson","485":"Along with the development of the Internet, the emergence and widespread adoption of the social media concept have changed the way news is formed and published. News has become faster, less costly and easily accessible with social media. This change has come along with some disadvantages as well. In particular, beguiling content, such as fake news made by social media users, is becoming increasingly dangerous. The fake news problem, despite being introduced for the first time very recently, has become an important research topic due to the high content of social media. Writing fake comments and news on social media is easy for users. The main challenge is to determine the difference between real and fake news. In this paper, a two-step method for identifying fake news on social media has been proposed, focusing on fake news. In the first step of the method, a number of pre-processing is applied to the data set to convert un-structured data sets into the structured data set. The texts in the data set containing the news are represented by vectors using the obtained TF weighting method and Document-Term Matrix. In the second step, twenty-three supervised artificial intelligence algorithms have been implemented in the data set transformed into the structured format with the text mining methods. In this work, an experimental evaluation of the twenty-three intelligent classification methods has been performed within existing public data sets and these classification models have been compared depending on four evaluation metrics.","486":"X-ray free-electron lasers (XFELs) as the world\u2019s brightest light sources provide ultrashort X-ray pulses\nwith a duration typically in the order of femtoseconds. Recently, they have approached and entered\nthe attosecond regime, which holds new promises for single-molecule imaging and studying nonlinear\nand ultrafast phenomena such as localized electron dynamics. The technological evolution of XFELs\ntoward well-controllable light sources for precise metrology of ultrafast processes has been, however,\nhampered by the diagnostic capabilities for characterizing X-ray pulses at the attosecond frontier. In\nthis regard, the spectroscopic technique of photoelectron angular streaking has successfully proven\nhow to non-destructively retrieve the exact time\u2013energy structure of XFEL pulses on a single-shot\nbasis. By using artificial intelligence techniques, in particular convolutional neural networks, we here\nshow how this technique can be leveraged from its proof-of-principle stage toward routine diagnostics\neven at high-repetition-rate XFELs, thus enhancing and refining their scientific accessibility in all\nrelated disciplines.","487":null,"488":null,"489":null,"490":null,"491":null,"492":null,"493":null,"494":null,"495":null,"496":null,"497":null,"498":null,"499":null,"500":null,"501":null,"502":null,"503":null,"504":null,"505":null,"506":null,"507":"Face is a important part through which we can identify who we are and how people identify us. Face is a person's most valuable and unique physical characteristics through which we can identify someone very easily. While humans have the innate ability to distinguish the different faces for millions of years for adding the new technology computers are just now catching up. A face recognition system is a computer application that is capable of identifying or verifying the person from a digital image or a video frame from video source. One of the way is to do this is by compare with the selected facial features and also a face database. Humans are having good tendency to recognizing faces. Rakeshkumar H Yadav | Brajgopal Agarwal | Sheeba James\"Face Recognition System\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-4 , June 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd14453.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/14453\/face-recognition-system\/rakeshkumar-h-yadav","508":"Monte Carlo Tree Search (MCTS) is a family of directed search algorithms that has gained widespread attention in recent years. Despite the vast amount of research into MCTS, the effect of modifications on the algorithm, as well as the manner in which it performs in various domains, is still not yet fully known. In particular, the effect of using knowledge-heavy rollouts in MCTS still remains poorly understood, with surprising results demonstrating that better-informed rollouts often result in worse-performing agents. We present experimental evidence suggesting that, under certain smoothness conditions, uniformly random simulation policies preserve the ordering over action preferences. This explains the success of MCTS despite its common use of these rollouts to evaluate states. We further analyse non-uniformly random rollout policies and describe conditions under which they offer improved performance.","509":null,"510":null,"511":null,"512":null,"513":"In this article, I discuss some recent ideas in complex systems on the topic of networks, contained in or inspired by three recent complex systems books. The general science of networks is the subject of Albert-Lazlo Barabasi's Linked A.-L. Barabasi, Linked: The New Science of Networks, Perseus, New York, 2002 and Duncan Watts' Six Degrees D. Watts, Six Degrees: The Science of a Connected Age, Gardner's Books, New York, 2003. Commonalities among complex biological networks, e.g., immune systems, social insects, and cellular metabolism, and their relation to intelligence in computational systems are explored in the proceedings of a interdisciplinary conference on \"Distributed Autonomous Systems\" L.A. Segel, I.R. Cohen (Eds.), Design Principles for the Immune System and Other Distributed Autonomous Systems, Oxford University Press, New York, 2001.The ideas discussed in the third book have led to me to propose four general principles of adaptive information processing in decentralized systems. These principles, and the relevance of \"network thinking\" for artificial intelligence (and vice versa), are the subject of the last two sections of the article.","514":"The development of Information Technology has the power to make a computer think and act like a human being. Artificial intelligence is a special feature of information technology that involves developing a machine that works and responds like a human mind. The main features of artificial intelligence take into account the sensitivity of human senses. The system is able to recognize speech and touch as features set in the system to carry out the tasks of a normal state of health without human assistance. However, the wisdom of implanting the study of intelligent agents who take the environment and achieve their goal successfully. In the computer world. Most systems are designed to achieve objectives depending on the nature of the situation but on the use of special features derived from existing natural features of humans and animals. In general, an engagement thinker is a human relative who uses learning and problem-solving techniques to understand high levels of activity in human-inspired activity, the emotional process and decision-making. Architects are technically superior to human ingenuity, past and present exploratory research conducted extensively in China and the United States and a series of developments in line with future aspirations or technologies. SRIKANTH REDDY MANDATI 2020. Artificial intelligent in China and United States.\u00a0International Journal on Integrated Education. 2, 5 (Aug. 2020), 278-288. DOI:https:\/\/doi.org\/10.31149\/ijie.v2i5.552. Pdf Url : https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/552\/527 Paper Url : https:\/\/journals.researchparks.org\/index.php\/IJIE\/article\/view\/552","515":null,"516":null,"517":"The objective of this study is to examine the potential of artificial intelligence (AI) to enhance the security posture of financial institutions against malware attacks. The study identifies the current trends of malware attacks in the banking sector, assesses the various forms of malware and their impact on financial institutions, and analyzes the relevant security features of AI. The findings suggest that financial institutions must implement robust cybersecurity measures to protect against various forms of malware attacks, including ransomware attacks, phishing attacks, mobile malware attacks, APTs, and insider threats. The study recommends that financial institutions invest in AI-based security systems to improve security features and automate security tasks. To ensure the reliability and security of AI systems, it is essential to incorporate relevant security features such as explain ability, privacy, anomaly detection, intrusion detection, and data validation. The study highlights the importance of incorporating explainable AI (XAI) to enable users to understand the reasoning behind the AI's decisions and actions, identify potential security threats and vulnerabilities in the AI system, and ensure that the system operates ethically and transparently. The study also recommends incorporating privacy-enhancing technologies (PETs) into AI systems to protect user data from unauthorized access and use. Finally, the study recommends incorporating robust security measures such as anomaly detection and intrusion detection to protect against adversarial attacks and data validation and integrity checks to protect against data poisoning attacks. Overall, this study provides insights for decision-makers in implementing effective cybersecurity strategies to protect financial institutions from malware attacks.","518":"Abstract&nbsp;&nbsp;Supervised classification is one of the tasks most frequently carried out by so-called Intelligent Systems. Thus, a large\r\nnumber of techniques have been developed based on Artificial Intelligence (Logic-based techniques, Perceptron-based techniques)and Statistics (Bayesian Networks, Instance-based techniques). The goal of supervised learning is to build a concise modelof the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign classlabels to the testing instances where the values of the predictor features are known, but the value of the class label isunknown. This paper describes various classification algorithms and the recent attempt for improving classification accuracy\u2014ensemblesof classifiers.","519":"This paper is to provide a high level understanding of Generative Adversarial Networks. This paper will be covering the working of GAN\u2019s by explaining the background idea of the framework, types of GAN\u2019s in the industry, it\u2019s advantages and disadvantages, history of how GAN\u2019s are developed and enhanced along the timeline and some applications where GAN\u2019s outperforms themselves. Atharva Chitnavis | Yogeshchandra Puranik \u00c4n Extensive Review on Generative Adversarial Networks (GAN\u2019s)\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42357.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42357\/an-extensive-review-on-generative-adversarial-networks-gan\u2019s\/atharva-chitnavis","520":null,"521":null,"522":null,"523":null,"524":null,"525":"A method for recovering the three-dimensional structure of moving rigid and jointed objects from several single camera views is presented. The method is based on the fixed axis assumption: all movement consists of translations and rotations about an axis that is fixed in direction for short periods of time. This assumption makes it possible to recover the structure of any group of two or more rigidly connected points. The structure of jointed objects is recovered by analyzing them as collections of rigid parts, and then unifying the structures proposed for the parts. The method presented here has been tested on several sets of data, including movies used to demonstrate human perception of structure from motion.","526":"Artificial Intelligence (AI) covers a broad spectrum of computational\r\nproblems and use cases. Many of those implicate profound and sometimes\r\nintricate questions of how humans interact or should interact with AIs.\r\nMoreover, many users or future users do have abstract ideas of what AI is,\r\nsignificantly depending on the specific embodiment of AI applications.\r\nHuman-centered-design approaches would suggest evaluating the impact of\r\ndifferent embodiments on human perception of and interaction with AI. An\r\napproach that is difficult to realize due to the sheer complexity of\r\napplication fields and embodiments in reality. However, here XR opens new\r\npossibilities to research human-AI interactions. The article's contribution is\r\ntwofold: First, it provides a theoretical treatment and model of human-AI\r\ninteraction based on an XR-AI continuum as a framework for and a perspective of\r\ndifferent approaches of XR-AI combinations. It motivates XR-AI combinations as\r\na method to learn about the effects of prospective human-AI interfaces and\r\nshows why the combination of XR and AI fruitfully contributes to a valid and\r\nsystematic investigation of human-AI interactions and interfaces. Second, the\r\narticle provides two exemplary experiments investigating the aforementioned\r\napproach for two distinct AI-systems. The first experiment reveals an\r\ninteresting gender effect in human-robot interaction, while the second\r\nexperiment reveals an Eliza effect of a recommender system. Here the article\r\nintroduces two paradigmatic implementations of the proposed XR testbed for\r\nhuman-AI interactions and interfaces and shows how a valid and systematic\r\ninvestigation can be conducted. In sum, the article opens new perspectives on\r\nhow XR benefits human-centered AI design and development.","527":"Cognitive computing, a revolutionary paradigm in computing, seeks to replicate and enhance human like intelligence by amalgamating artificial intelligence, machine learning, and natural language processing. This paper provides an overview of cognitive computing, emphasizing its core principles and applications across diverse industries. Key components, including adaptability, learning, and problem solving capabilities, distinguish cognitive computing from traditional computing models. The integration of natural language processing enables more intuitive human machine interactions, contributing to applications such as virtual assistants and personalized services. The paper explores the ethical considerations inherent in cognitive computing, highlighting the importance of transparency and responsible use. With continuous evolution and ongoing research, cognitive computing is on the verge to shape the future of computing, offering new opportunities and challenges in various domains. This abstract encapsulates the transformative nature of cognitive computing and its potential impact on the technological landscape. Manish Verma \"Beyond AI: The Rise of Cognitive Computing as Future of Computing: ChatGPT Analysis\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-6 , December 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd61292.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/61292\/beyond-ai-the-rise-of-cognitive-computing-as-future-of-computing-chatgpt-analysis\/manish-verma","528":null,"529":"Popular interest in robotics has increased in recent years. Robotics technology has been implemented in a variety of fields including medicine, elderly care, rehabilitation, education, home appliances, search and rescue, car industry and more. Robotics constitutes one of the most exciting fields of technology today, presenting new applications for autonomous systems that can impact everyday life. Understanding where the field of robotics is heading is basically using our insights on the impact robots might make in the near future. Due to the incredible potential of robotic technology, application opportunities are limitless in the future. In this paper we discuss the future of robotics and robots. Matthew N. O. Sadiku | Kirtikumar K. Patel | Sarhan M. Musa \"Future of Robotics\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-4 , June 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd50259.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/50259\/future-of-robotics\/matthew-n-o-sadiku","530":"Artificial Intelligence (AI) covers a broad spectrum of computational problems and use cases. Many of those implicate profound and sometimes intricate questions of how humans interact or should interact with AIs. Moreover, many users or future users do have abstract ideas of what AI is, significantly depending on the specific embodiment of AI applications. Human-centered-design approaches would suggest evaluating the impact of different embodiments on human perception of and interaction with AI. An approach that is difficult to realize due to the sheer complexity of application fields and embodiments in reality. However, here XR opens new possibilities to research human-AI interactions. The article's contribution is twofold: First, it provides a theoretical treatment and model of human-AI interaction based on an XR-AI continuum as a framework for and a perspective of different approaches of XR-AI combinations. It motivates XR-AI combinations as a method to learn about the effects of prospective human-AI interfaces and shows why the combination of XR and AI fruitfully contributes to a valid and systematic investigation of human-AI interactions and interfaces. Second, the article provides two exemplary experiments investigating the aforementioned approach for two distinct AI-systems. The first experiment reveals an interesting gender effect in human-robot interaction, while the second experiment reveals an Eliza effect of a recommender system. Here the article introduces two paradigmatic implementations of the proposed XR testbed for human-AI interactions and interfaces and shows how a valid and systematic investigation can be conducted. In sum, the article opens new perspectives on how XR benefits human-centered AI design and development.","531":"We live in the digital age where everything is touched and connected by technology. The invisibility of digital technology has produced a distinctively new generation defined by digital media the digital natives. Digital natives are people who have grown up using technology from early childhood. They are the new citizens of the digital world. It is difficult to predict the exact impact of the technologies on digital natives. However, emerging technologies are poised to transform digital natives shortly. Matthew N. O. Sadiku | Kirtikumar K. Patel | Sarhan M. Musa \"Future of Digital Natives\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-3 , April 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49633.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/49633\/future-of-digital-natives\/matthew-n-o-sadiku","532":"Electronic mail is a major revolution taking place over traditional communication systems due to its convenient, economical, fast, and easy to use nature. A major bottleneck in electronic communications is the enormous dissemination of unwanted, harmful emails known as spam emails . A major concern is the developing of suitable filters that can adequately capture those emails and achieve high performance rate. Machine learning (ML) researchers have developed many approaches in order to tackle this problem. Within the context of machine learning, support vector machines (SVM) have made a large contribution to the development of spam email filtering. Based on SVM, different schemes have been proposed through text classification approaches (TC). A crucial problem when using SVM is the choice of kernels as they directly affect the separation of emails in the feature space. This paper presents thorough investigation of several distance-based kernels and specify spam filtering behaviors using SVM. The majority of used kernels in recent studies concern continuous data and neglect the structure of the text. In contrast to classical kernels, we propose the use of various string kernels for spam filtering. We show how effectively string kernels suit spam filtering problem. On the other hand, data preprocessing is a vital part of text classification where the objective is to generate feature vectors usable by SVM kernels. We detail a feature mapping variants in TC that yield improved performance for the standard SVM in filtering task. Furthermore, to cope for realtime scenarios we propose an online active framework for spam filtering. We present empirical results from an extensive study of online, transductive, and online active methods for classifying spam emails in real time. We show that active online method using string kernels achieves higher precision and recall rates.","533":null,"534":null,"535":null,"536":null,"537":null,"538":null,"539":null,"540":null,"541":null,"542":null,"543":null,"544":"We observe achievement gaps even in rich western countries, such as the UK, which in principle have the resources as well as the social and technical infrastructure to provide a better deal for all learners. The reasons for such gaps are complex and include the social and material poverty of some learners with their resulting other deficits, as well as failure by government to allocate sufficient resources to remedy the situation. On the supply side of the equation, a single teacher or university lecturer, even helped by a classroom assistant or tutorial assistant, cannot give each learner the kind of one-to-one attention that would really help to boost both their motivation and their attainment in ways that might mitigate the achievement gap.\r\n\r\nIn this chapter Benedict du Boulay, Alexandra Poulovassilis, Wayne Holmes, and Manolis Mavrikis argue that we now have the technologies to assist both educators and learners, most commonly in science, technology, engineering and mathematics subjects (STEM), at least some of the time. We present case studies from the fields of Artificial Intelligence in Education (AIED) and Big Data. We look at how they can be used to provide personalised support for students and demonstrate that they are not designed to replace the teacher. In addition, we also describe tools for teachers to increase their awareness and, ultimately, free up time for them to provide nuanced, individualised support even in large cohorts.","545":"Maximum Boolean satisfiability (max-SAT) is the optimization counterpart of Boolean satisfiability (SAT), in which a variable assignment is sought to satisfy the maximum number of clauses in a Boolean formula. A branch and bound algorithm based on the Davis-Putnam-Logemann-Loveland procedure (DPLL) is one of the most competitive exact algorithms for solving max-SAT. In this paper, we propose and investigate a number of strategies for max-SAT. The first strategy is a set of unit propagation or unit resolution rules for max-SAT. We summarize three existing unit propagation rules and propose a new one based on a nonlinear programming formulation of max-SAT. The second strategy is an effective lower bound based on linear programming (LP). We show that the LP lower bound can be made effective as the number of clauses increases. The third strategy consists of a binary-clause first rule and a dynamic-weighting variable ordering rule, which are motivated by a thorough analysis of two existing well-known variable orderings. Based on the analysis of these strategies, we develop an exact solver for both max-SAT and weighted max-SAT. Our experimental results on random problem instances and many instances from the max-SAT libraries show that our new solver outperforms most of the existing exact max-SAT solvers, with orders of magnitude of improvement in many cases.","546":null,"547":"Two methods from the field of artificial intelligence\n                 were implemented and employed on a medical data set, in\n                 order to perform data mining. The data set consisted of\n                 cases from patients who suffered recurring miscarriage,\n                 and the aim was to investigate whether the implemented\n                 methods were able to identify previously unknown\n                 factors associated with recurrent miscarriage. The\n                 first approach used a specific type of artificial\n                 neural network - Kohonen's self-organizing map for\n                 performing clustering within data sets. By using new\n                 cluster detection methods and the visualisation\n                 possibilities of the employed programming language\n                 Java, and its graphical user interface components\n                 Swing, it allows interactively the visualisation of\n                 relationships within a data set. The second, relatively\n                 unique approach, infers rules from a data set by using\n                 the paradigm of genetic programming. The rules consist\n                 of an IF-part (antecedent) and a THEN-part\n                 (consequent). The system has to be supplied with the\n                 consequent and works out antecedents, which describe\n                 the sub data set indicated by the consequent within the\n                 supplied data set. The antecedents produced take the\n                 form of a tree where Boolean operations AND, OR and NOT\n                 represent nodes, and Boolean expressions represent the\n                 leaves. Boolean expressions can be built from all types\n                 of data including free-text and real numbers. This\n                 system was also implemented with Java and offers in\n                 addition the possibility of knowledge extraction from\n                 clusters built by the self-organizing map approach.","548":"With the rapid development of digital technologies, the analysis and processing of data has become an important problem. In particular, classification, clustering and processing of complex and multi structured data required the development of new algorithms. In this process, Deep Learning solutions for solving Big Data problems are emerging. Deep Learning can be described as an advanced variant of artificial neural networks. Deep Learning algorithms are commonly used in healthcare, facial and voice recognition, defense, security and autonomous vehicles. Image processing is one of the most common applications of Deep Learning. Deep Learning software is commonly used to capture and process images by removing the errors. Image processing methods are used in many fields such as medicine, radiology, military industry, face recognition, security systems, transportation, astronomy and photography. In this study, current Deep Learning algorithms are investigated and their relationship with commonly used software in the field of image processing is determined. Ahmet \u00c3\u2013zcan | Mahmut \u00c3\u0153nver | Atilla Erg\u00c3\u00bczen \"Deep Learning Applications and Image Processing\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-2 , February 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49142.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/49142\/deep-learning-applications-and-image-processing\/ahmet-\u00c3\u00b6zcan","549":"Bayesian belief networks provide a natural, efficient method for representing probabilistic dependencies among a set of variables. For these reasons, numerous researchers are exploring the use of belief networks as a knowledge representation in artificial intelligence. Algorithms have been developed previously for efficient probabilistic inference using special classes of belief networks. More general classes of belief networks, however, have eluded efforts to develop efficient inference algorithms. We show that probabilistic inference using belief networks is NP-hard. Therefore, it seems unlikely that an exact algorithm can be developed to perform probabilistic inference efficiently over all classes of belief networks. This result suggests that research should be directed away from the search for a general, efficient probabilistic inference algorithm, and toward the design of efficient special-case, average-case, and approximation algorithms.","550":null,"551":null,"552":null,"553":"It is generally recognized that the possibility of detecting contradictions and identifying their sources is an important feature of an intelligent system. Systems that are able to detect contradictions, identify their causes, or readjust their knowledge bases to remove the contradiction, called Belief Revision Systems, Truth Maintenance Systems, or Reason Maintenance Systems, have been studied by several researchers in Artificial Intelligence (AI). In this paper, we present a logic suitable for supporting belief revision systems, discuss the properties that a belief revision system based on this logic will exhibit, and present a particular implementation of our model of a belief revision system. The system we present differs from most of the systems developed so far in three respects: First, it is based on a logic that was developed to support belief revision systems. Second, it uses the rules of inference of the logic to automatically compute the dependencies among propositions rather than having to force the user to do this, as in many existing systems. Third, it was the first belief revision system whose implementation relies on the manipulation of sets of assumptions, not justifications.","554":null,"555":null,"556":"Teachers and educational institutions seek new questions with different difficulty levels for setting up tests for their students. Also, students long for distinct and new questions to practice for their tests as redundant questions are found everywhere. However, setting up new questions every time is a tedious task for teachers. To overcome this conundrum, we have concocted an artificially intelligent system which generates questions and answers for the mathematical topic \u00e2\u20ac\u201cQuadratic equations. The system uses i Randomization technique for generating unique questions each time and ii First order logic and Automated deduction to produce solution for the generated question. The goal was achieved and the system works efficiently. It is robust, reliable and helpful for teachers, students and other organizations for retrieving Quadratic equations questions, hassle free. Rahul Bhatia | Vishakha Gautam | Yash Kumar | Ankush Garg \"\"Dynamic Question Answer Generator: An Enhanced Approach to Question Generation\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-4 , June 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd23730.pdf\r\n\r\n Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23730\/dynamic-question-answer-generator-an-enhanced-approach-to-question-generation\/rahul-bhatia","557":null,"558":null,"559":"The most well known reasons for building fires were from warming, electrical appropriation, and lighting frameworks. Chimneys utilized during the special seasons and colder months can make risks also. An alarm framework incorporates numerous segments and highlights to help keep you secured. It spares lives by notice building inhabitants of crises so they can escape risk. In case of a fire, they give discovery and warning without you busy. They can likewise consequently dispatch the local group of fire fighters to your area. At the point when an alarm is actuated, it supports your security and wellbeing during a hazardous occasion. A fire location framework utilizes a smoke alarm to distinguish a fire before it really begins. The point of the framework planned is to caution the removed land owner with proficiency and rapidly by causing short message SMS by means of GSM network. Sreejith S P | Kuldeep Baban Vayadande \u00c4dvanced Fire Monitoring System\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-1 , December 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd37978.pdf Paper URL : https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/37978\/advanced-fire-monitoring-system\/sreejith-s-p","560":"Description logics (DLs) are a family of state-of-the-art knowledge representation languages, and their expressive power has been carefully crafted to provide useful knowledge modeling primitives while allowing for practically effective decision procedures for the basic reasoning problems. Recent experience with DLs, however, has shown that their expressivity is often insufficient to accurately describe structured objects--objects whose parts are interconnected in arbitrary, rather than tree-like ways. DL knowledge bases describing structured objects are therefore usually underconstrained, which precludes the entailment of certain consequences and causes performance problems during reasoning.\r\nTo address this problem, we propose an extension of DL languages with description graphs--a knowledge modeling construct that can accurately describe objects with parts connected in arbitrary ways. Furthermore, to enable modeling the conditional aspects of structured objects, we also extend DLs with rules. We present an in-depth study of the computational properties of such a formalism. In particular, we first identify the sources of undecidability of the general, unrestricted formalism. Based on that analysis, we then investigate several restrictions of the general formalism that make reasoning decidable. We present practical evidence that such a logic can be used to model nontrivial structured objects. Finally, we present a practical decision procedure for our formalism, as well as tight complexity bounds.","561":null,"562":null,"563":"The Easy-to-Read (E2R) Methodology was created to improve the daily life of people with cognitive disabilities, who have difficulties in reading comprehension. The main goal of the E2R Methodology is to present clear and easily understood documents. This methodology includes a set of guidelines and recommendations that affect the writing of texts, the supporting images, the design and layout of documents, and the final editing format. Such guidelines are used in the manual processes of (a) adapting existing documents and (b) producing new materials. The process of adapting existing documents is cyclic and implies three activities: analysis, transformation, and validation. All these activities are human resource consuming, due to the need of involving people with cognitive disabilities as well as E2R experts. In order to alleviate such processes, we are currently investigating the development of methods, based on Artificial Intelligence (AI) techniques, to perform the analysis and transformation of documents in a (semi)-automatic fashion. In this paper we present our AI-based method for assessing a particular document with respect to the E2R guidelines as well as an initial implementation of such a method; our research on the transformation of documents is out of the scope of this paper. We carried out a comparative evaluation of the results obtained by our initial implementation against the results of the document analysis performed by people with cognitive disabilities.","564":null,"565":"Driver assistance technologies that relieve the drivers task, as well as intelligent autonomous vehicles, rely on traffic sign recognition. Normally the classification of traffic signs is a critical challenge for self driving cars. For the classification of traffic sign images, a Deep Network known as LeNet will be used in this study. There are forty three different categories of images in the dataset. There are two aspects to this structure Traffic sign identification and Traffic sign classification. ADASs are designed to perform a variety of tasks, including communications, detection of road markings, recognition of road signs, and detection of pedestrians. There are two aspects to this structure Traffic sign identification and Traffic sign classification. In the methodologies for detecting and recognizing traffic signals various techniques, such as colour segmentation and the RGB to HSI model area unit, were applied for traffic sign detection and recognition. Different elements contribute to recognition of HOG. Arpit Seth | Vijayakumar A \u00c4 Traffic Sign Classifier Model using Sage Maker\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42411.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42411\/a-traffic-sign-classifier-model-using-sage-maker\/arpit-seth","566":"With complexity of artificial intelligence systems increasing continuously in past years, studies to explain these complex systems have grown in popularity. While much work has focused on explaining artificial intelligence systems in popular domains such as classification and regression, explanations in the area of anomaly detection have only recently received increasing attention from researchers. In particular, explaining singular model decisions of a complex anomaly detector by highlighting which inputs were responsible for a decision, commonly referred to as local post-hoc feature relevance, has lately been studied by several authors. In this paper, we systematically structure these works based on their access to training data and the anomaly detection model, and provide a detailed overview of their operation in the anomaly detection domain. We demonstrate their performance and highlight their limitations in multiple experimental showcases, discussing current challenges and opportunities for future work in feature relevance XAI for anomaly detection.","567":"The evolution of Internet of Things has been tremendous in today's generation. A Promising and challenging mission is the use of autonomous robot systems to automate tasks in the field of maintenance. Major concern of evolution in robotics involves reducing human burden. This project work addresses the critical environment where Human presence is mandatory due to its nature of demanding accuracy like bomb diffusion, Chemical and radiation containment etc. A promising and challenging mission is the use of autonomous robot system to automate task in the field of maintenance. Each device is uniquely identifiable by the controlling software which is the core concept of Internet of Things Due to the remote operation procedure of a ROBOT .It lags in human precision and adaptability. This project proposes a novel idea of imitating the human hand movement with a virtually controlled robotic arm, which can be utilized in human endurance situations. A video and audio interface plug-in will be developed to recognize audio commands and also video gestures. Every method is experimentally validated and discussed. K Aishwarya | J Priyadharshini | G Rajeswari\u00c4rduino Controlled Robotic Arm\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd11512.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/11512\/arduino-controlled-robotic-arm\/k-aishwarya","568":null,"569":"Nowadays Stress has been a quite common ailment in people. We believe that when technology is used to build understanding, it can help humanity in creative and effective ways. That idea lives at the core of our paper in an easily accessible app to help users. The paper elaborates plan to develop a virtual assistant a.k.a. chatbot that would act as a therapist to the masses. We propose to use Machine Learning and NLP together with web front end technologies. As per availability of data, Experiments show that the proposed methods achieve high accuracy in patient action understanding, error identification and task recommendation. The proposed virtual PT system has the potential of enabling on demand virtual care and significantly reducing cost for both patients and health care providers. Tanmay Pachpande | Dewang Solanki | Venkat. P. Patil \"Virtual Therapist for Psychological Healthcare\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-2 , February 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38614.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/38614\/virtual-therapist-for-psychological-healthcare\/tanmay-pachpande","570":null,"571":"Face recognition across age progression is remains one of the areas most challenging tasks now a days, as the aging process affects both the shape and texture of a face. One possible solution is to apply a probabilistic model to represent a face simultaneously with its identity variable, which is stable through time, and its aging variable, which changes with time. This paper proposes a deep learning and set based approach to the face recognition subject to aging. The images for each subject taken at various times are treated as a single set, which is then compared to the sets of images belonging to other subjects. Facial features are extracted using a convolutional neural network characteristic of the deep learning. Our experimental results show that set based recognition performs better than the singleton based approach for the both face identification and face verification. We also find that by using set based recognition, it is easier to recognize older subjects from younger ones rather than younger subjects from older ones. Prathama V | Thippeswamy G \"\u00c4ge Invariant Face Recognition\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-4 , June 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd23572.pdf\r\n\r\nPaper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23572\/age-invariant-face-recognition\/prathama-v","572":null,"573":null,"574":null,"575":null,"576":null,"577":null,"578":null,"579":null,"580":null,"581":null,"582":null,"583":null,"584":null,"585":null,"586":null,"587":null,"588":null,"589":null,"590":null,"591":null,"592":null,"593":null,"594":null,"595":null,"596":null,"597":null,"598":null,"599":null,"600":null,"601":null,"602":null,"603":null,"604":null,"605":null,"606":null,"607":null,"608":null,"609":null,"610":null,"611":null,"612":null,"613":null,"614":null,"615":null,"616":null,"617":null,"618":null,"619":null,"620":"The problems of heuristic programming---of making computers solve\n\treally difficult problems---are divided into five main areas: Search,\n\tPattern-Recognition, Learning, Planning, and Induction. Wherever\n\tappropriate, the discussion is supported by extensive citation of\n\tthe literature and by descriptions of a few of the most successful\n\theuristic (problem-solving) programs constructed to date.\n\t\n\tThe adjective \"heuristic,\" as used here and widely in the literature,\n\tmeans related to improving problem-solving performance; as a noun\n\tit is also used in regard to any method or trick used to improve\n\tthe efficiency of a problem-solving system. A \"heuristic program,\"\n\tto be considered successful, must work well on a variety of problems,\n\tand may often be excused if it fails on some. We often find it worthwhile\n\tto introduce a heuristic method, which happens to cause occasional\n\tfailures, if there is an over-all improvement in performance. But\n\timperfect methods are not necessarily heuristic, nor vice versa.\n\tHence \"heuristic\" should not be regarded as opposite to \"foolproof\";\n\tthis has caused some confusion in the literature.","621":null,"622":null,"623":null,"624":null,"625":null,"626":null,"627":null,"628":null,"629":null,"630":null,"631":null,"632":null,"633":null,"634":null,"635":null,"636":"Fuzzy logic is a method to formalize the human capacity of imprecise reasoning. Fuzzy logic react how people think. It tries to make our sense of words and our decision making. As a result  it is making to human intelligent systems. There are three basic steps for fuzzy logic system i.e. Fuzzification  Rule evaluation and Defuzzification. In this paper I discuss a real life problem in which different results of same survey were presented. The survey data represent vague values in the form of fuzzy sets. To solve this problem we applied defuzzification methods to get the best result from surveys. I represent the most commonly used defuzzification methods and made comparison to choose the best one. Wai Wai Tun \"Comparison of Defuzzification Methods from a Real World Problem\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd)  ISSN: 2456-6470  Volume-3 | Issue-5   August 2019  URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd27909.pdfPaper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/27909\/comparison-of-defuzzification-methods-from-a-real-world-problem\/wai-wai-tun","637":"The astonishing growth of the Internet is the first sign that every aspect of our economy and society are likely to change. Yet for people to realize the vast promise of networked computing, Internet applications must become dramatically more powerful and easier to use. Artificial Intelligence (AI) technology holds the key to these futuristic applications with the promise of advanced features, adaptive functionality and intuitive interfaces. We group Internet applications into four categories: 1) user modeling, 2) discovery and analysis of remote information sources, 3) information integration, and 4) web-site management. The seven papers in this special issue represent some of the latest and most exciting research in three of the four categories. This introduction attempts to place the special-issue papers in context, but we caution readers that the field is too young and moving too quickly for a comprehensive survey article.","638":null,"639":null,"640":null,"641":null,"642":null,"643":null,"644":null,"645":null,"646":null,"647":null,"648":null,"649":null,"650":null,"651":null,"652":null,"653":null,"654":null,"655":null,"656":null,"657":null,"658":null,"659":null,"660":null,"661":null,"662":null,"663":null,"664":null,"665":null,"666":null,"667":null,"668":null,"669":null,"670":null,"671":null,"672":null,"673":null,"674":null,"675":null,"676":null,"677":null,"678":null,"679":null,"680":null,"681":null,"682":null,"683":null,"684":null,"685":null,"686":null,"687":null,"688":null,"689":null,"690":null,"691":null,"692":null,"693":null,"694":null,"695":null,"696":null,"697":null,"698":null,"699":null,"700":null,"701":null,"702":null,"703":null,"704":null,"705":null,"706":null,"707":null,"708":null,"709":"The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper, we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.","710":null,"711":null,"712":null,"713":null,"714":null,"715":null,"716":null,"717":null,"718":null,"719":null,"720":null,"721":null,"722":null,"723":null,"724":null,"725":"Various types of social media such as blogs, discussion forums and peer-to-peer networks present a wealth of information that can be very helpful. Given vast amount of data, one of the challenge has been to automatically identify the topic of the background chatter. Such emerging topics can be identified by the appearance of multiple posts on a unique subject matter, which is distinct from previous online discourse. We address the problem of identifying topics through the use of machine learning. I propose a topic detection method based on supervised machine learning model, where sentences are labelled, tokenized and the vectorised sentence is trained on densely connected neural network. Compared to conventional gradient descent optimization algorithm, Adam optimizer trains the data much faster and efficiently. Finally the model is tested on an Android App with live data from Google News. Mr. Ajmal Rasi | Dr. Rajasimha A Makram | Ms. Shilpa Das\"Topic Detection using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-4 , June 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd14272.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/14272\/topic-detection-using-machine-learning\/mr-ajmal-rasi","726":"Learning methods based on dynamic programming (DP) are receiving increasing\r\n\tattention in artificial intelligence. Researchers have argued that\r\n\tDP provides the appropriate basis for compiling planning results\r\n\tinto reactive strategies for real-time control, as well as for learning\r\n\tsuch strategies when the system being controlled is incompletely\r\n\tknown. We introduce an algorithm based on DP, which we call Real-Time\r\n\tDP (RTDP), by which an embedded system can improve its performance\r\n\twith experience. RTDP generalizes Korf's Learning-Real-Time-A* algorithm\r\n\tto problems involving uncertainty. We invoke results from the theory\r\n\tof asynchronous DP to prove that RTDP achieves optimal behavior in\r\n\tseveral different classes of problems. We also use the theory of\r\n\tasynchronous DP to illuminate aspects of other DP-based reinforcement\r\n\tlearning methods such as Watkins' Q-Learning algorithm. A secondary\r\n\taim of this article is to provide a bridge between AI research on\r\n\treal-time planning and learning and relevant concepts and algorithms\r\n\tfrom control theory.","727":"Agent-based computing represents an exciting new synthesis both for\n\tArtificial Intelligence (AI) and, more generally, Computer Science.\n\tIt has the potential to significantly improve the theory and the\n\tpractice of modeling, designing, and implementing computer systems.\n\tYet, to date, there has been little systematic analysis of what makes\n\tthe agent-based approach such an appealing and powerful computational\n\tmodel. Moreover, even less effort has been devoted to discussing\n\tthe inherent disadvantages that stem from adopting an agent-oriented\n\tview. Here both sets of issues are explored. The standpoint of this\n\tanalysis is the role of agent-based software in solving complex,\n\treal-world problems. In particular, it will be argued that the development\n\tof robust and scalable software systems requires autonomous agents\n\tthat can complete their objectives while situated in a dynamic and\n\tuncertain environment, that can engage in rich, high-level social\n\tinteractions, and that can operate within flexible organisational\n\tstructures.","728":"Bongard problems present an outstanding challenge to artificial intelligence. They consist of visual pattern understanding problems on which the task of the pattern perceiver is to find an abstract aspect of distinction between two classes of figures. This paper examines the philosophical question of whether objects in Bongard problems can be ascribed an a priori, metaphysical, existence\u2014the ontological question of whether objects, and their boundaries, come pre-defined, independently of any understanding or context. This is an essential issue, because it determines whether a priori symbolic representations can be of use for solving Bongard problems. The resulting conclusion of this analysis is that in the case of Bongard problems there can be no units ascribed an a priori existence\u2014and thus the objects dealt with in any specific problem must be found by solution methods (rather than given to them). This view ultimately leads to the emerging alternatives to the philosophical doctrine of metaphysical realism.","729":null,"730":"In this paper, we present an approach to commonsense causal explanation\n\tof stories that can be used for automatically determining the liable\n\tparty in legal case descriptions. The approach is based on LRICore,\n\ta core ontology for law that takes a commonsense perspective. Aside\n\tfrom our thesis that in the legal domain many terms still have a\n\tstrong commonsense flavour, the descriptions of events in legal cases,\n\tas e.g. presented at judicial trials, are cast in commonsense terms\n\tas well. We present design principles for representing commonsense\n\tcausation, and describe a process-based approach to automatic identification\n\tof causal relations in stories, which are described in terms of the\n\tcore ontology. The resulting causal explanation forms a necessary\n\tcondition for determining the liability and responsibility of agents\n\tthat play a role in the case. We describe the basic architecture\n\tand working of DIRECT, the demonstrator we are constructing to test\n\tthe validity of our process oriented view on commonsense causation.\n\tThis view holds that causal relations are in fact abstractions constructed\n\ton the basis of our commonsense understanding of physical and mental\n\tprocesses.","731":null,"732":"An estimation procedure to efficiently find approximate values of internal parameters in ultrasonic transducers intended for broadband operation would be a valuable tool to discover internal construction data. This information is necessary in the modelling and simulation of acoustic and electrical behaviour related to ultrasonic systems containing commercial transducers. There is not a general solution for this generic problem of parameter estimation in the case of broadband piezoelectric probes. In this paper, this general problem is briefly analysed for broadband conditions. The viability of application in this field of an artificial intelligence technique supported on the modelling of the transducer internal components is studied. A genetic algorithm (GA) procedure is presented and applied to the estimation of different parameters, related to two transducers which are working as pulsed transmitters. The efficiency of this GA technique is studied, considering the influence of the number and variation range of the estimated parameters. Estimation results are experimentally ratified.","733":"Statistical Relational AI---the science and engineering of making intelligent machines acting in noisy worlds composed of objects and relations among the objects---is currently motivating a lot of new AI research and has tremendous theoretical and practical implications. Theoretically, combining logic and probability in a unified representation and building general-purpose reasoning tools for it has been the dream of AI, dating back to the late 1980s. Practically, successful statistical relational AI tools enable new applications in several large, complex real-world domains including those involving big data, natural text, social networks, the web, medicine and robotics, among others. Such domains are often characterized by rich relational structure and large amounts of uncertainty. Logic helps to faithfully model the former while probability helps to effectively manage the latter. Our intention here is to give a brief (and necessarily incomplete) overview and invitation to the emerging field of Statistical Relational AI from the perspective of acting optimally and learning to act.","734":"This paper argues that the AI ethics has generally neglected the issues related to the science communication of AI. In particular, the article focuses on visual communication about AI and, more specifically, on the use of certain stock images in science communication about AI --- in particular, those characterized by an excessive use of blue color and recurrent subjects, such as androgyne faces, half-flesh and half-circuit brains, and variations on Michelangelo's The Creation of Adam. In the first section, the author refers to a ``referentialist'' ethics of science communication for an ethical assessment of these images. From this perspective, these images are unethical. While the ethics of science communication generally promotes virtues like modesty and humility, similar images are arrogant and overconfident. In the second section, the author uses French philosopher Jacques Ranci\u00e8re's concepts of ``distribution of the sensible,'' ``disagreement,'' and ``pensive image.'' Ranci\u00e8re's thought paves the way to a deeper critique of these images of AI. The problem with similar images is not their lack of reference to the ``things themselves.'' It rather lies in the way they stifle any possible forms of disagreement about AI. However, the author argues that stock images and other popular images of AI are not a problem per se, and they can also be a resource. This depends on the real possibility for these images to support forms of pensiveness. In the conclusion, the question is asked whether the kind of ethics or politics of AI images proposed in this article can be applied to AI ethics tout court.","735":null,"736":null,"737":"A general framework for the problem of coordination of multiple competing\n\tgoals in dynamic environments for physical agents is presented. This\n\tapproach to goal coordination is a novel tool to incorporate a deep\n\tcoordination ability to pure reactive agents. The framework presented\n\tis based on the notion of multi-objective optimisation. In this article\n\twe propose a kind of 'aggregating functions' formulation with the\n\tparticularity that the aggregation is weighted by means of a dynamic\n\tweighting unitary vector theta, which is dependent from the system\n\tdynamic state allowing the agent to dynamically coordinate the priorities\n\tof its single goals. This dynamic weighting unitary vector is represented\n\tas a (n - 1) set of angles. The dynamic coordination must be established\n\tby means of a mapping between the state of the agent's environment\n\tS to the set of angles ph_i(S) by means of any sort of machine-learning\n\ttool. In this work, we investigate the use of Reinforcement Learning\n\tas a first approach to learn that mapping.","738":null,"739":null,"740":"Intelligent control system combines the mathematical theory of control and artificial intelligence that is AI technology. In this paper, we will discuss in detail how the intelligent control system works and why it is used for. Also, we will cover its crucial importance in the field of technology along with its pros and cons. Artificial Intelligence is the need of the digital world to ease out work and copy the human brain. Intelligent control system take support of artificial intelligent to work for and why it is so important in the technological world. Its together with artificial intelligence called schemer which is to support such a paradigm. Schemer is worked to solve the difficult problem and work like a problem-solving agent which react, interact, and dynamically change the condition of difficult task situation to carry out problem-solving activities. Kavitha. (2019). The importance of Intelligent Control Systems.\u00a0International Journal of Human Computing Studies,\u00a01(1), 11-13. https:\/\/doi.org\/10.31149\/ijhcs.v1i1.521 Pdf Url : https:\/\/journals.researchparks.org\/index.php\/IJHCS\/article\/view\/521\/498 Paper Url : https:\/\/journals.researchparks.org\/index.php\/IJHCS\/article\/view\/521","741":null,"742":null,"743":null,"744":"Sentiment Analysis of the Reviews is important to understand the positive or negative effect of some process using their reviews after the experience. In the study the sentiment analysis of the reviews of drugs given by the patients after the usage using the boosting algorithms in machine learning. The Dataset used, provides patient reviews on some specific drugs along with the conditions the patient is suffering from and a 10 star patient rating reflecting the patient satisfaction. Exploratory Data Analysis is carried out to get more insight and engineer features. Preprocessing is done to get the data ready. The sentiment of the review is given according to the rating of the drugs. To classify the reviews as positive or negative three Classification models are trained LightGBM, XGBoost, and CatBoost and the feature importance is plotted. The result shows that LGBM is the best performing Boosting algorithm with an accuracy of 88.89 . Sumit Mishra \"Drug Review Sentiment Analysis using Boosting Algorithms\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42429.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42429\/drug-review-sentiment-analysis-using-boosting-algorithms\/sumit-mishra","745":"In every college there will be a huge number of students who uses the facility of library to access or to refer different varieties of books to gain knowledge about subjects. Every Library maintains a register to maintain the data about members who entered library. Some colleges still use the traditional approaches of maintaining register, which is difficult to maintain, to check and to analyze data. So, we propose a new system in which students are passed through camera at entry point of the library where the students facial patterns are recognized and create the entry record into the database on clicking the button. K. Ravikanth Mishra | D. Brahmeswara Rao | A. Dinesh Chowdary\"Student Library Attendance using Face Recognition\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd11281.pdf  http:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/11281\/student-library-attendance-using-face-recognition\/k-ravikanth-mishra","746":"Our goal is to go deeper into the many writings on Behavior-Based\r\n\tArtificial Intelligence Meyer et al., From Animals to Animats, NET\r\n\tPress, 1992 and to understand the interest-rather than the mechanisms-of\r\n\tlearning. Our intention is to study the complexity of the behavior\r\n\tof living beings from a theoretical point of view. To do so, we introduce\r\n\tformal environments that model the survival issue. Then we prove\r\n\tin this formal context that, many times, the extra cost imposed by\r\n\tthe conservation of information, even if it is relevant, is greater\r\n\tthan the benefit of knowing it. Consequently, in order to survive\r\n\tin our abstract worlds, one must manage his knowledge in a way that\r\n\tfits the evolution of the environment. Further-more, physiological\r\n\tobservations corroborate these purely theoretical results. Thus,\r\n\twe use these results to design a parallel system in which each module\r\n\tmanages its knowledge in a specific way. This enables us to obtain\r\n\ta virtual creature whose behavior evokes that of a biological hen.\r\n\t(C) 2001 Elsevier Science B.V. All rights reserved.","747":"For many years, researchers have tried to discover how humans solve problems. This research has answered many questions, but still many of them remain unanswered. However, knowledge gained in this field has greatly enhanced our understanding and has enabled us to design human-like intelligent systems. In the 1920s the Gestalt psychologists introduced a new field to cognitive science. They discovered that when presented with certain problems we use insight to reach a solution. In the 1950s Newell and Simon then brought the field of problem solving into the information age. They experimented with the idea of problem solving as a search for a solution in a state space. This technique is today utilized in the field of computing and Artificial Intelligence. This paper reviews techniques and looks at how we use previous experience gained by solving problems to solve new similar problems, making analogies between them. Experts' performance during problem solving is compared to that of novices.","748":null,"749":null,"750":"This study is divided into two interlinked parts: the first one exposes what data science is, the benefits produced, and the education that a data scientist should have. Therefore, a relation of data science and AI with Spanish language. Questions are raised about how the training of data scientist in Spain is, and the analysis of diagrams, which shows the percentage of Spanish and English academic articles. Moreover, there are many contributions of members and representatives of Latin American Languages Academies. They comment on the lack of AI glossary written in Spanish language. The study of art provides results that indicate the absence of involvement of Spanish with AI and all the subareas, which consequently adversely affect to the education of future professionals.","751":"This book will help you solve real-world problems with contemporary machine learning, artificial intelligence, and cloud computing tools. \nGift demystifies all the concepts and tools you need to get results---even if you don\u2019t have a strong background in math or data science.\nHe illuminates powerful off-the-shelf cloud offerings from Amazon, Google, and Microsoft, and demonstrates proven techniques using the Python data science ecosystem.\nHis workflows and examples help you streamline and simplify every step, from deployment to production, and build exceptionally scalable solutions.\nAs you learn how machine language (ML) solutions work, you\u2019ll gain a more intuitive understanding of what you can achieve with them and how to maximize their value.\nBuilding on these fundamentals, you\u2019ll walk step-by-step through building cloud-based AI\/ML applications to address realistic issues in sports marketing, project management, product pricing, real estate, and beyond.","752":null,"753":"AI is all the rage these days, and with the attention comes the responsibility -- our responsibility, as AI researchers and practitioners -- to communicate clearly about our field. What areas are progressing, and at what pace? What are the successes and the challenges? Will AI improve our lives? For example, will it increase productivity, open up new business opportunities, lengthen life expectancy and improve its quality? Or is AI a threat to society? For example, will it eliminate jobs, accentuate biases, or lead to unfair concentration of knowledge and wealth? Many people are chiming in on these questions. Opinions diverge, which is to be expected in a fast-moving area with more unknowns than knowns. It is an important conversation to have. And precisely because of the inherent uncertainty, it's important that the conversation be anchored in fact. It's hard enough for AI practitioners to keep track of everything that's going on in our exploding field and make sense of it, and it's all the more daunting to outsiders. The goal of the AI Index is to provide precisely this factual basis to the conversation, in an open, not-for-profit fashion.","754":"This is an opinion piece about the relationship between the fields of human-computer interaction (HCI), and artificial intelligence (AI). The ultimate goal of both fields is to make user interfaces more effective and easier to use for people. But historically, they have disagreed about whether intelligence or direct manipulation is the better route to achieving this. There is an unjustified perception in HCI that AI is unreliable. There is an unjustified perception in AI that interfaces are merely cosmetic. This disagreement is counterproductive. This article argues that AI's goals of intelligent interfaces would benefit enormously by the user-centered design and testing principles of HCI. It argues that HCI's stated goals of meeting the needs of users and interacting in natural ways, would be best served by application of AI. Peace.","755":"The aim of the Guidelines is to promote Trustworthy AI. Trustworthy AI has three components, which should be met throughout the system's entire life cycle: (1) it should be lawful, complying with all applicable laws and regulations (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI. Ideally, all three components work in harmony and overlap in their operation. If, in practice, tensions arise between these components, society should endeavour to align them.","756":"AI applications have been deployed and used for industrial, government, and consumer purposes for many years. The experiences have been documented in IAAI conference proceedings since 1989. Over the years, the breadth of applications has expanded many times over and AI systems have become more commonplace. Indeed, AI has recently become a focal point in the industrial and consumer consciousness. This article focuses on changes in the world of computing over the last three decades that made building AI applications more feasible. We then examine lessons learned during this time and distill these lessons into succinct advice for future application builders.","757":"Artificial Intelligence (AI) has spread across industries (e.g., business, science, art, education) to enhance user experience, improve work efficiency, and create many future job opportunities. However, public understanding of AI technologies and how to define AI literacy is under-explored. This vision poses upcoming challenges for our next generation to learn about AI. On this note, an exploratory review was conducted to conceptualize the newly emerging concept \u201cAI literacy\u201d, in search for a sound theoretical foundation to define, teach and evaluate AI literacy. Grounded in literature on 30 existing peer-reviewed articles, this review proposed four aspects (i.e., know and understand, use and apply, evaluate and create, and ethical issues) for fostering AI literacy based on the adaptation of classic literacies. This study sheds light on the consolidated definition, teaching, and ethical concerns on AI literacy, establishing the groundwork for future research such as competency development and assessment criteria on AI literacy.","758":"CiteSeerX is a digital library search engine providing access to more than five million scholarly documents with nearly a million users and millions of hits per day. We present key AI technologies used in the following components: document classification and de-duplication, document and citation clustering, automatic metadata extraction and indexing, and author disambiguation. These AI technologies have been developed by CiteSeerX group members over the past 5--6 years. We show the usage status, payoff, development challenges, main design concepts, and deployment and maintenance requirements. We also present AI technologies implemented in table and algorithm search, which are special search modes in CiteSeerX. While it is challenging to rebuild a system like CiteSeerX from scratch, many of these AI technologies are transferable to other digital libraries and\/or search engines.","759":null,"760":"Although AI and HCI explore computing and intelligent behavior and the fields have seen some cross-over, until recently there was not very much. This article outlines a history of the fields that identifies some of the forces that kept the fields at arm's length. AI was generally marked by a very ambitious, long-term vision requiring expensive systems, although the term was rarely envisioned as being as long as it proved to be, whereas HCI focused more on innovation and improvement of widely-used hardware within a short time-scale. These differences led to different priorities, methods, and assessment approaches. A consequence was competition for resources, with HCI flourishing in AI winters and moving more slowly when AI was in favor. The situation today is much more promising, in part because of platform convergence: AI can be exploited on widely-used systems.","761":null,"762":",,Explainable AI`` ist kein neues Gebiet. Vielmehr ist das Problem der Erkl\u00e4rbarkeit so alt wie die AI selbst, ja vielmehr das Resultat ihrer selbst. W\u00e4hrend regelbasierte L\u00f6sungen der fr\u00fchen AI nachvollziehbare ,,Glass-Box``-Ans\u00e4tze darstellten, lag deren Schw\u00e4che im Umgang mit Unsicherheiten der realen Welt. Durch die Einf\u00fchrung probabilistischer Modellierung und statistischer Lernmethoden wurden die Anwendungen zunehmend erfolgreicher -- aber immer komplexer und opak. Beispielsweise werden W\u00f6rter nat\u00fcrlicher Sprache auf hochdimensionale Vektoren abgebildet und dadurch f\u00fcr Menschen nicht mehr verstehbar. In Zukunft werden kontextadaptive Verfahren notwendig werden, die eine Verkn\u00fcpfung zwischen statistischen Lernmethoden und gro\u00dfen Wissensrepr\u00e4sentationen (Ontologien) herstellen und Nachvollziehbarkeit, Verst\u00e4ndlichkeit und Erkl\u00e4rbarkeit erlauben -- dem Ziel von ,,explainable AI``.","763":"In the second edition of this bestseller, the author continues to demystify the techniques associated with the field of artificial intelligence. It covers a wide variety of techniques currently defined as ?AI? and shows how they can be useful in practical, everyday applications. AI Application Programming covers both the theory and the practical applications to teach developers how to apply AI techniques in their own designs. Each chapter covers both the theory of the algorithm or the technique under discussion followed by a practical application of the technique with a detailed discussion of the source code.","764":null,"765":"AI systems in the context of employment are intrusive and have negative impacts on workers. The proposed Regulation fails to address the specificity of AI uses in employment, including platform work. An ad hoc directive on AI in employment is therefore necessary.\r\n    As a consequence of focusing the regulatory approach on high-risk applications, the majority of use cases are considered low-risk, not subject to any evaluation and de facto authorised.\r\n    High-risk uses are permitted subject to compliance with specific requirements and an ex ante conformity assessment based on internal control checks. This approach stacks the deck in favour of tech providers when the priority of this Regulation should have been to protect EU citizens and workers\u2019 rights.","766":null,"767":null,"768":null,"769":"As AI systems demonstrate increasingly strong predictive performance, their\r\nadoption has grown in numerous domains. However, in high-stakes domains such as\r\ncriminal justice and healthcare, full automation is often not desirable due to\r\nsafety, ethical, and legal concerns, yet fully manual approaches can be\r\ninaccurate and time consuming. As a result, there is growing interest in the\r\nresearch community to augment human decision making with AI assistance. Besides\r\ndeveloping AI technologies for this purpose, the emerging field of human-AI\r\ndecision making must embrace empirical approaches to form a foundational\r\nunderstanding of how humans interact and work with AI to make decisions. To\r\ninvite and help structure research efforts towards a science of understanding\r\nand improving human-AI decision making, we survey recent literature of\r\nempirical human-subject studies on this topic. We summarize the study design\r\nchoices made in over 100 papers in three important aspects: (1) decision tasks,\r\n(2) AI models and AI assistance elements, and (3) evaluation metrics. For each\r\naspect, we summarize current trends, discuss gaps in current practices of the\r\nfield, and make a list of recommendations for future research. Our survey\r\nhighlights the need to develop common frameworks to account for the design and\r\nresearch spaces of human-AI decision making, so that researchers can make\r\nrigorous choices in study design, and the research community can build on each\r\nother's work and produce generalizable scientific knowledge. We also hope this\r\nsurvey will serve as a bridge for HCI and AI communities to work together to\r\nmutually shape the empirical science and computational technologies for\r\nhuman-AI decision making.","770":"Artificial Intelligence AI has become an integral part of our post COVID world, influencing various aspects of our lives, from healthcare to remote work and education. While AI offers numerous advantages, it also poses significant risks, including ethical dilemmas, bias, privacy concerns, and potential job displacement. This abstract explores the evolving landscape of AI safety and regulations in the wake of the COVID 19 pandemic. AI safety encompasses efforts to ensure that AI systems are developed and deployed responsibly, preventing unintended consequences and safeguarding individuals and society at large. In parallel, AI regulations aim to establish a framework that guides the ethical and accountable use of AI technologies. These regulations address data privacy, bias mitigation, transparency, and accountability, among other critical aspects. The advantages of AI safety and regulation are evident in their capacity to protect public health, privacy, and fairness. In healthcare, they ensure the accuracy of diagnostic AI systems and safeguard patient data. In remote work and education, they promote equitable access to AI enhanced services. Additionally, AI safety and regulation play a crucial role in supply chain resilience, mental health support, and the development of digital health records and vaccine passports. However, several limitations and challenges need to be acknowledged. Rapid technological advancements often outpace regulatory frameworks, making it challenging to maintain relevance. Global variations in regulations can create complexities for international cooperation. Overregulation can stifle innovation, while a lack of enforcement can render regulations toothless. The future trends in AI safety and regulation will be shaped by the lessons learned from the COVID 19 pandemic. We anticipate global collaboration and standardization efforts, the proliferation of ethical AI frameworks, and sector specific regulations. Transparent AI, accountability laws, and adaptive regulations will play a significant role in shaping the responsible development and deployment of AI technologies. In conclusion, AI safety and regulation are essential components of a post COVID world that seeks to harness the benefits of AI while mitigating its potential risks. The responsible development and use of AI technologies are crucial in ensuring a secure, equitable, and ethical digital future. Manish Verma \u00c4I Safety and Regulations: Navigating the Post-COVID Era: Aims, Opportunities, and Challenges: A ChatGPT Analysis\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-6 , December 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd60087.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/60087\/ai-safety-and-regulations-navigating-the-postcovid-era-aims-opportunities-and-challenges-a-chatgpt-analysis\/manish-verma","771":"Despite a growing body of research about the design and use of conversational agents, existing work has almost exclusively focused on interactions between an agent and a human. Less is known about how an agent is perceived and used during human-human conversation. We compared conversations between dyads using AI-assisted and standard messaging apps and elicited qualitative feedback from users of the AI-assisted messaging app through interviews. We find discrepancies between the AI assistant's suggestions and the conversational content, which is also reflected in participant interviews. Our results are used to suggest some areas for improvement and future work in AI-assisted communication.","772":"From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.","773":"A conceptual and algebraically framework that did not exist up until then in its morphology, was developed. What is more, it is pioneer on its implementation in the area of Artificial Intelligence (AI) and it was started up in laboratory, on its structural aspects, as a fully operational model. At qualitative level, its greatest contribution to AI is applying the conversion or transduction of parameters obtained by ternary logic  (multi-valued systems) and associating them with an image. This image will be analysed by means of a residual artificial network ResNet34,  to warn us of an intrusion. The field of application of this framework includes everything from smartwatches, tablets, and PC\u2019s to the home automation based on the KNX standard.","774":null,"775":"The steady decline in business profitability across multiple industries threatens to erode future investment, innovation and shareholder value. Fortunately, a new factor of production---artificial intelligence (AI)---is emerging that can help kick-start profitability. AI consists of multiple technologies that can be combined in different ways to sense, comprehend, act and learn. Accenture research shows that AI has the potential to boost rates of profitability by an average of 38 percent by 2035 and lead to an economic boost of US$14 trillion across 16 industries in 12 economies by 2035. But this will only happen if organizations adopt a people-first mindset and take bold and responsible steps to apply AI technologies to their business. Our research has identified eight cross-industry strategies to help seize the AI opportunity.","776":null,"777":"Three of the world\u2019s most accomplished and deep thinkers come together to explore Artificial Intelligence (AI) and the way it is transforming human society\u2014and what this technology means for us all.\r\nAn AI learned to win chess by making moves human grand masters had never conceived. Another AI discovered a new antibiotic by analyzing molecular properties human scientists did not understand. Now, AI-powered jets are defeating experienced human pilots in simulated dogfights. AI is coming online in searching, streaming, medicine, education, and many other fields and, in so doing, transforming how humans are experiencing reality.\r\n\r\nIn The Age of AI, three leading thinkers have come together to consider how AI will change our relationships with knowledge, politics, and the societies in which we live. The Age of AI is an essential roadmap to our present and our future, an era unlike any that has come before.","778":null,"779":"Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce's abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.","780":null,"781":"Artificial intelligence (AI) is presented as a solution to the greatest challenges of our time, from global pandemics and chronic diseases to cybersecurity threats and the climate crisis. But AI also contributes to the climate crisis by running on technology that depletes scarce resources and by relying on data centres that demand excessive energy use.\r\n\r\nIs AI Good for the Planet? brings the climate crisis to the centre of debates around AI, exposing its environmental costs and forcing us to reconsider our understanding of the technology. It reveals why we should no longer ignore the environmental problems generated by AI. Embracing a green agenda for AI that puts the climate crisis at centre stage is our urgent priority.\r\n\r\nEngaging and passionately written, this book is essential reading for scholars and students of AI, environmental studies, politics, and media studies and for anyone interested in the connections between technology and the environment.","782":null,"783":"This Guest Editors' Introduction identifies an opportunity for the cross-fertilization between power systems and energy markets researchers and new developments of AI. The articles selected for this special issue provide the state-of-the-art information about research being conducted using AI in power systems and energy markets.","784":"The use of the Artificial Intelligence (AI) in education is progressively evident by the organising of learning and assessment methods. However, to ensure its absolute implementation in education, there is still a long way to go. This paper assesses those particularly students difficulties which pose a challenge to educational institutions. To this day, it is a fact which has not been solved. We are talking about basic necessities such as cognitive abilities for special education needs children, failure at primary school or lack of motivation in secondary education. This study aims reflecting on the need for progress in instruction and learning and the solutions that AI offers us. Genre, tense and number educational reforms are not enought. \u201cEducation is an ornament in prosperity and a shelter in adversity\u201d, according to the prestigious scientist, philosopher and polymath, Aristotle.","785":"Recent developments in Artificial Intelligence (AI) have\r\ngenerated great expectations for the future impact of AI\r\nin education and learning (AIED). Often these expectations\r\nhave been based on misunderstanding current technical\r\npossibilities, lack of knowledge about state-of-the-art AI in\r\neducation, and exceedingly narrow views on the functions\r\nof education in society. In this article, we provide a review\r\nof existing AI systems in education and their pedagogic and\r\neducational assumptions. We develop a typology of AIED\r\nsystems and describe different ways of using AI in education and learning, show how these are grounded in different interpretations of what AI and education is or could\r\nbe, and discuss some potential roadblocks on the AIED\r\nhighway.","786":null,"787":"The invention of ChatGPT and generative AI technologies presents educators with significant challenges, as concerns arise regarding students potentially exploiting these tools unethically, misrepresenting their work, or gaining academic merits without active participation in the learning process. To effectively navigate this shift, it is crucial to embrace AI as a contemporary educational trend and establish pedagogical principles for properly utilizing emerging technologies like ChatGPT to promote self-regulation. Rather than suppressing AI-driven tools, educators should foster collaborations among stakeholders, including educators, instructional designers, AI researchers, and developers. This paper proposes three key pedagogical principles for integrating AI chatbots in classrooms, informed by Zimmerman\u2019s Self-Regulated Learning (SRL) framework and Judgment of Learning (JOL). We argue that the current conceptualization of AI chatbots in education is inadequate, so we advocate for the incorporation of goal setting (prompting), self-assessment and feedback, and personalization as three essential educational principles. First, we propose that teaching prompting is important for developing students\u2019 SRL. Second, configuring reverse prompting in the AI chatbot\u2019s capability will help to guide students\u2019 SRL and monitoring for understanding. Third, developing a data-driven mechanism that enables an AI chatbot to provide learning analytics helps learners to reflect on learning and develop SRL strategies. By bringing in Zimmerman\u2019s SRL framework with JOL, we aim to provide educators with guidelines for implementing AI in teaching and learning contexts, with a focus on promoting students\u2019 self-regulation in higher education through AI-assisted pedagogy and instructional design.","788":"AI-powered learning technologies are increasingly being used to automate and scaffold learning activities (e.g., personalised reminders for completing tasks, automated real-time feedback for improving writing, or recommendations for when and what to study). While the prevailing view is that these technologies generally have a positive effect on student learning, their impact on students\u2019 agency and ability to self-regulate their learning is under-explored. Do students learn from the regular, detailed and personalised feedback provided by AI systems, and will they continue to exhibit similar behaviour in the absence of assistance? Or do they instead continue to rely on AI assistance without learning from it? To contribute to filling this research gap, we conducted a randomised controlled experiment that explored the impact of AI assistance on student agency in the context of peer feedback. With 1625 students across 10 courses, an experiment was conducted using peer review. During the initial four-week period, students were guided by AI features that utilised techniques such as rule-based suggestion detection, semantic similarity, and comparison with previous comments made by the reviewer to enhance their submissions if the feedback provided was deemed insufficiently detailed or general in nature. Over the following four weeks, students were divided into four different groups: control (AI) received prompts, (NR) received no prompts, (SR) received self-monitoring checklists in place of AI prompts, and (SAI) had access to both AI prompts and self-monitoring checklists. Results of the experiment suggest that students tended to rely on rather than learn from AI assistance. If AI assistance was removed, self-regulated strategies could help in filling in the gap but were not as effective as AI assistance. Results also showed that hybrid human-AI approaches that complement AI assistance with self-regulated strategies (SAI) were not more effective than AI assistance on its own. We conclude by discussing the broader benefits, challenges and implications of relying on AI assistance in relation to student agency in a world where we learn, live and work with AI.","789":"Until the mid-1980s, AI researchers assumed that an intelligent system doing high-level reasoning was necessary for the coupling of perception and action. In this traditional model, cognition mediates between perception and plans of action. Realizing that this core AI, as it was known, was illusory, Rodney A. Brooks turned the field of AI on its head by introducing the behavior-based approach to robotics.","790":"Recent developments in AI, Machine Learning and Robotics have raised concerns\r\nabout the ethical consequences of both academic and industrial AI research.\r\nLeading academics, businessmen and politicians have voiced an increasing number\r\nof questions about the consequences of AI not only over people, but also on the\r\nlarge-scale consequences on the the future of work and employment, its social\r\nconsequences and the sustainability of the planet. In this work, we analyse the\r\nuse and the occurrence of ethics-related research in leading AI, machine\r\nlearning and robotics venues. In order to do so we perform long term,\r\nhistorical corpus-based analyses on a large number of flagship conferences and\r\njournals. Our experiments identify the prominence of ethics-related terms in\r\npublished papers and presents several statistics on related topics. Finally,\r\nthis research provides quantitative evidence on the pressing ethical concerns\r\nof the AI community.","791":null,"792":null,"793":null,"794":"Research, leadership, and communication about AI futures.","795":"When people receive advice while making difficult decisions, they often make better decisions in the moment and also increase their knowledge in the process. However, such incidental learning can only occur when people cognitively engage with the information they receive and process this information thoughtfully. How do people process the information and advice they receive from AI, and do they engage with it deeply enough to enable learning? To answer these questions, we conducted three experiments in which individuals were asked to make nutritional decisions and received simulated AI recommendations and explanations. In the first experiment, we found that when people were presented with both a recommendation and an explanation before making their choice, they made better decisions than they did when they received no such help, but they did not learn. In the second experiment, participants first made their own choice, and only then saw a recommendation and an explanation from AI; this condition also resulted in improved decisions, but no learning. However, in our third experiment, participants were presented with just an AI explanation but no recommendation and had to arrive at their own decision. This condition led to both more accurate decisions and learning gains. We hypothesize that learning gains in this condition were due to deeper engagement with explanations needed to arrive at the decisions. This work provides some of the most direct evidence to date that it may not be sufficient to include explanations together with AI-generated recommendation to ensure that people engage carefully with the AI-provided information. This work also presents one technique that enables incidental learning and, by implication, can help people process AI recommendations and explanations more carefully.","796":null,"797":"The question addressed in this paper is: If we present to a user an AI system\r\nthat explains how it works, how do we know whether the explanation works and\r\nthe user has achieved a pragmatic understanding of the AI? In other words, how\r\ndo we know that an explanainable AI system (XAI) is any good? Our focus is on\r\nthe key concepts of measurement. We discuss specific methods for evaluating:\r\n(1) the goodness of explanations, (2) whether users are satisfied by\r\nexplanations, (3) how well users understand the AI systems, (4) how curiosity\r\nmotivates the search for explanations, (5) whether the user's trust and\r\nreliance on the AI are appropriate, and finally, (6) how the human-XAI work\r\nsystem performs. The recommendations we present derive from our integration of\r\nextensive research literatures and our own psychometric evaluations.","798":"Image super-resolution is one of the most popular computer vision problems\r\nwith many important applications to mobile devices. While many solutions have\r\nbeen proposed for this task, they are usually not optimized even for common\r\nsmartphone AI hardware, not to mention more constrained smart TV platforms that\r\nare often supporting INT8 inference only. To address this problem, we introduce\r\nthe first Mobile AI challenge, where the target is to develop an end-to-end\r\ndeep learning-based image super-resolution solutions that can demonstrate a\r\nreal-time performance on mobile or edge NPUs. For this, the participants were\r\nprovided with the DIV2K dataset and trained quantized models to do an efficient\r\n3X image upscaling. The runtime of all models was evaluated on the Synaptics\r\nVS680 Smart Home board with a dedicated NPU capable of accelerating quantized\r\nneural networks. The proposed solutions are fully compatible with all major\r\nmobile AI accelerators and are capable of reconstructing Full HD images under\r\n40-60 ms while achieving high fidelity results. A detailed description of all\r\nmodels developed in the challenge is provided in this paper.","799":"Recent work has proposed artificial intelligence (AI) models that can learn to decide whether to make a prediction for an instance of a task or to delegate it to a human by considering both parties\u2019 capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams\u2014compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when they are aware that an AI model delegated task instances to them. In an experimental study with 196 participants, we show that task performance and task satisfaction improve through AI delegation, regardless of whether humans are aware of the delegation. Additionally, we identify humans\u2019 increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction. Our findings provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces.","800":"While AI has been the subject of intense discussion and research in the recent past, the recent breakthroughs in generative AI mark the beginning of a new era and are raising existential questions for humanity. Decades of dedicated R&D into AI have culminated in the launch of powerful generative pre-trained transformers such as ChatGPT, Claude, DALL-E, Midjourney or Stable Diffusion. These have been made widely and freely available to anyone with an internet connection, outside of any specific regulatory framework. Unsurprisingly, their adoption has been almost instantaneous.","801":"Artificial Intelligence (AI) is increasingly affecting our lives in smaller or larger ways. In order to ensure that systems will uphold human values, design methods are needed that incorporate ethical principles and address societal concerns. In this article, I introduce the ART design principles (Accountability, Responsibility and Transparency) for the development of AI systems sensitive to human values.","802":"This Guest Editors' Introduction identifies an opportunity for the cross-fertilization between power systems and energy markets researchers and new developments of AI. The articles selected for this special issue provide the state-of-the-art information about research being conducted using AI in power systems and energy markets.","803":"From unique educational perspectives, this article reports a comprehensive review of selected empirical studies on artificial intelligence in education (AIEd) published in 1993\u20132020, as collected in the Web of Sciences database and selected AIEd-specialized journals. A total of 40 empirical studies met all selection criteria, and were fully reviewed using multiple methods, including selected bibliometrics, content analysis and categorical meta-trends analysis. This article reports the current state of AIEd research, highlights selected AIEd technologies and applications, reviews their proven and potential benefits for education, bridges the gaps between AI technological innovations and their educational applications, and generates practical examples and inspirations for both technological experts that create AIEd technologies and educators who spearhead AI innovations in education. It also provides rich discussions on practical implications and future research directions from multiple perspectives. The advancement of AIEd calls for critical initiatives to address AI ethics and privacy concerns, and requires interdisciplinary and transdisciplinary collaborations in large-scaled, longitudinal research and development efforts.","804":"Artificial Intelligence techniques are increasingly being applied to the user interface, as evidenced by growing numbers of CHI papers which have some AI aspect, and standalone conferences on the subject, such as the Intelligent User Interfaces (IUI) Conference (and this workshop!). I argue that an important, but underappreciated component for assuring the adherence of AI interfaces to CHI principles for usable interfaces, is capturing Commonsense knowledge. Commonsense knowledge can be viewed as a collection of simple facts about people and everyday life, such as &#034;Things fall down, not up&#034;, and &#034;People eat breakfast in the morning&#034;. One reason that conventional interfaces are stupid and frustrating to use, is that they lack such knowledge. At the MIT Media Lab, we have a large body of experience in creating applications across a wide variety of domains that make use of such knowledge 6. We distill from our experience some principles for application of Commonsense knowledge to make interfaces more usable.","805":"Mobile internet, cloud computing, big data technologies, and significant breakthroughs in Artificial Intelligence (AI) have all transformed education. In recent years, there has been an emergence of more advanced AI-enabled learning systems, which are gaining traction due to their ability to deliver learning content and adapt to the individual needs of students. Yet, even though these contemporary learning systems are useful educational platforms that meet students\u2019 needs, there is still a low number of implemented systems designed to address the concerns and problems faced by many students. Based on this perspective, a systematic mapping of the literature on AI-enabled adaptive learning systems was performed in this work. A total of 147 studies published between 2014 and 2020 were analysed. The major findings and contributions of this paper include the identification of the types of AI-enabled learning interventions used, a visualisation of the co-occurrences of authors associated with major research themes in AI-enabled learning systems and a review of common analytical methods and related techniques utilised in such learning systems. This mapping can serve as a guide for future studies on how to better design AI-enabled learning systems to solve specific learning problems and improve users\u2019 learning experiences.","806":"Old questions being answered with both AI and HCI.","807":"The production and promotion of ?AI? technology involves dehumanization on many fronts. I explore these processes of dehumanization and the role that cognitive science can play by bringing a richer picture of human cognition to the discourse.","808":"Unintended consequences of deployed AI systems fueled the call for more interpretability in AI systems. Often explainable AI (XAI) systems provide users with simplifying local explanations for individual predictions but leave it up to them to construct a global understanding of the model behavior. In this work, we examine if non-technical users of XAI fall for an illusion of explanatory depth when interpreting additive local explanations. We applied a mixed methods approach consisting of a moderated study with 40 participants and an unmoderated study with 107 crowd workers using a spreadsheet-like explanation interface based on the SHAP framework. We observed what non-technical users do to form their mental models of global AI model behavior from local explanations and how their perception of understanding decreases when it is examined.","809":"Perceptions about AI influence the attribution of characteristics and the interaction with AI. To find out how workers imagine an AI they would like to work with and what characteristics they attribute to it, we asked 174 working individuals to draw an AI they would like to work with, to report five adjectives they associate with their drawing and to evaluate the drawn and three other, typical AI representations (e.g. robot, smartphone) either presented as male or female. Participants mainly drew humanoid or robotic AIs. The adjectives that describe AI mainly referred to the inner characteristics, capabilities, shape, or relationship types. Regarding the evaluation, we identified four dimensions (warmth, competence, animacy, size) that can be reproduced for male and female AIs and different AI representations. This work addresses diverse conceptions of AI in the workplace and shows that human-centered AI development is necessary to address the huge design space.","810":null,"811":null,"812":null,"813":null,"814":"In recent years, there has been a growing public discourse regarding the influence AI will have on the future of work. Simultaneously, considerable critical attention has been given to the implications of AI on gender equality. Far from making precise predictions about the future, this discourse demonstrates that new technologies are instances for renegotiating the relation of gender and work. This paper examines how gender is addressed in news media discourse on AI and the future of work, focusing on Germany. We approach this question from a perspective of feminist technology studies and discourse analysis, exploring a corpus of 178 articles from 2015 to 2021 from German newspapers and newsmagazines. The findings indicate that critical AI and gender knowledge circulates in public discourse in the form of specific discursive frames, thematizing algorithmic bias, automatization and enhancement, and gender stereotypes. As a result, we show that, first, the discourse takes up feminist and scholarly discourse on gender and discusses AI in a way that is informed by social constructivism and standpoint theories. Second, gender appears as a\u2014to some extent intersectional\u2014diversity category which is critical to AI, while at the same time omitting important perspectives. Third, it can be shown that there is a renegotiating of the ideal worker norm taking place, and finally, we argue that the gendered frame of the powerful men developer responsible for AI\u2019s risk is a concept to be challenged.","815":"Artificial intelligence (AI) is finally bringing a multitude of capabilities to machines that were long thought to belong exclusively to the human realm, such as processing natural language or visual information. In this report, we explain how and where AI could affect the German industrial sector by exploring several questions: Which subindustries are most strongly affected by the automation potential of AI? What are the most promising use cases? What are pragmatic recommendations for managers of industrial players planning to harness the power of AI? Highly developed economies like Germany, with a high GDP per capita and challenges such as a quickly aging population, will increasingly need to rely on automation based on AI to achieve GDP targets. About one-third of Germany\u2019s GDP aspiration for 2030 depends on productivity gains. Automation fueled by AI is one of the most significant sources of productivity. By becoming one of the earliest adopters of AI, Germany could even exceed its 2030 GDP target by 4 percent. However, if the country adopts AI more slowly---and productivity is not increased by any other means---it could lag behind its 2030 GDP target by up to one-third. AI is expected to lift performance across all industries and especially in those with a high share of predictable tasks such as Germany's industrial sector. AI-enabled work could raise productivity in Germany by 0.8 to 1.4 percent annually. We selected eight use cases covering three essential business areas (products and services, manufacturing operations, and business processes) to highlight AI\u2019s great potential in the industrial sector. Use cases include topics such as autonomous vehicles, predictive maintenance, collaborative robotics, and supply-chain management. Five pragmatic recommendations help with getting started on the journey toward a fully AI-enabled organization.  Artificial intelligence (AI) is finally bringing a multitude of capabilities to machines which were long thought to belong exclusively to the human realm: processing natural language or visual information, recognizing patterns, and decision making. While AI undoubtedly holds great economic potential for the whole world, in this report we explain how and where AI will likely affect the German industrial sector by exploring several questions: Which subindustries are most strongly affected by the automation potential of AI? What are the most promising use cases? What are pragmatic recommendations for managers of industrial players planning to harness the power of AI? We describe several use cases in which we highlight the impact of AI and aim to quantify it. These use cases were carefully selected based on their economic potential and their ability to demonstrate the benefits of AI in practice. We do not claim that AI \u2013 despite its enormous potential \u2013 is the silver bullet for every business problem. We realize that AI is very often the enabler for performance improvements whose actual realization requires changing business processes. It is a rapidly evolving field. Thus, the present report needs to be understood as a peek into the future based on the current state of the art. With these caveats we are confident that this report will provide managers in the German industrial sector with valuable guidance on how they can benefit from AI.","816":"The present paper surveys neural approaches to conversational AI that have\r\nbeen developed in the last few years. We group conversational systems into\r\nthree categories: (1) question answering agents, (2) task-oriented dialogue\r\nagents, and (3) chatbots. For each category, we present a review of\r\nstate-of-the-art neural approaches, draw the connection between them and\r\ntraditional approaches, and discuss the progress that has been made and\r\nchallenges still being faced, using specific systems and models as case\r\nstudies.","817":"In practice it is important to provide assurances of performance for a developed system. This can only be done if its quality can be guaranteed. Validity is one fundamental requirement to assure a system&#039;s quality. Considering the problems of Knowledge Acquisition as the first AI--Bottleneck the difficulties arising during the validation process can be considered as a second one in the commercialisation of expert systems (XPS). A complete system validation would imply the examination of all imaginable situations (cases). Such an exhaustive check would be a complete and real, but unfortunately unfeasible proof of a system&#039;s quality. Even the use of a quasi--exhaustive set of test cases (QUEST ) is impracticable due the huge cardinality of QUEST . The focus of this paper will be set on the objective of a criteria--driven reduction of a given set of test cases being a quasi--exhaustive description of the XPS&#039;s domain (2, 3). This reduction has to be carried out properly and without ri...","818":"Recent trends in AI verification and Explainable AI have raised the question\r\nof whether AI planning techniques can be verified. In this paper, we present a\r\nnovel resource logic, the Proof Carrying Plans (PCP) logic that can be used to\r\nverify plans produced by AI planners. The PCP logic takes inspiration from\r\nexisting resource logics (such as Linear logic and Separation logic) as well as\r\nHoare logic when it comes to modelling states and resource-aware plan\r\nexecution. It also capitalises on the Curry-Howard approach to logics, in its\r\ntreatment of plans as functions and plan pre- and post-conditions as types.\r\nThis paper presents two main results. From the theoretical perspective, we show\r\nthat the PCP logic is sound relative to the standard possible world semantics\r\nused in AI planning. From the practical perspective, we present a complete Agda\r\nformalisation of the PCP logic and of its soundness proof. Moreover, we\r\nshowcase the Curry-Howard, or functional, value of this implementation by\r\nsupplementing it with the library that parses AI plans into Agda's proofs\r\nautomatically. We provide evaluation of this library and the resulting Agda\r\nfunctions.","819":"Are you an AI researcher at an academic institution? Are you anxious you are\r\nnot coping with the current pace of AI advancements? Do you feel you have no\r\n(or very limited) access to the computational and human resources required for\r\nan AI research breakthrough? You are not alone; we feel the same way. A growing\r\nnumber of AI academics can no longer find the means and resources to compete at\r\na global scale. This is a somewhat recent phenomenon, but an accelerating one,\r\nwith private actors investing enormous compute resources into cutting edge AI\r\nresearch. Here, we discuss what you can do to stay competitive while remaining\r\nan academic. We also briefly discuss what universities and the private sector\r\ncould do improve the situation, if they are so inclined. This is not an\r\nexhaustive list of strategies, and you may not agree with all of them, but it\r\nserves to start a discussion.","820":"When creating algorithms or systems that are supposed to be used by people, we should be able to adopt a binocular view of users' interaction with intelligent systems: a view that regards the design of interaction and the design of intelligent algorithms as interrelated parts of a single design problem. This special issue offers a coherent set of articles on two levels of generality that illustrate the binocular view and help readers to adopt it.","821":null,"822":"Evaluation should be a mechanism of progress both within and\nacross AI research projects. For the individual, evaluation can\ntell us how and why our methods and programs work and, so, tell\nus how our research should proceed. For the community, evaluation\nexpedites the understanding of available methods and, so, their\nintegration into further research. In this article, we present a\nfive-stage model of AI research and describe guidelines for\nevaluation that are appropriate for each stage. These guidelines,\nin the form of evaluation criteria and techniques, suggest how to\nperform evaluation. We conclude with a set of recommendations\nthat suggest how to encourage the evaluation of AI research.","823":"This paper examines \u2018open\u2019 AI in the context of recent attention to open and open source AI systems. We find that the terms \u2018open\u2019 and \u2018open source\u2019 are used in confusing and diverse ways, often constituting more aspiration or marketing than technical descriptor, and frequently blending concepts from both open source software and open science. This complicates an already complex landscape, in which there is currently no agreed on definition of \u2018open\u2019 in the context of AI, and as such the term is being applied to widely divergent offerings with little reference to a stable descriptor.","824":"Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.","825":null,"826":"Answering questions correctly from standardized eighth-grade science tests is itself a test of machine intelligence. Determining whether a system truly displays artificial intelligence is difficult and complex, and well-known assessments like the Turing Test are not suited to the task. The Allen Institute for Artificial Intelligence suggests that answering science exam questions successfully is a better measure of machine intelligence and designed a global competition to engage the research community in this approach. The outcome of the Allen AI Science Challenge highlights the current limitations of AI research in language understanding, reasoning, and commonsense knowledge; the highest scores are still limited to the capabilities of information-retrieval methods.","827":null,"828":null,"829":null,"830":null,"831":"Modernization of railways has forever been an issue focused on the development of the fundamental infrastructure of a nation. Since the railways represent one of the most effective modes of transport offered to the people, It is important to keep a check on the security issues that are arising in today\u2019s world. According to the need there must be an up gradation in systems we use. One such up gradation is that the role of Artificial Intelligence and e ticketing that is achieved with the assistance of face recognition technology. This technology has been extensively employed as a biometric method and hence can be used for passenger verification. Leman Kirme | Vivek Jha | Punit Chauhan | Soumya Ranjan Mohanty | Rahul Ghode \"Smart Verification of Passenger using AI\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-1 , December 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38002.pdf Paper URL : https:\/\/www.ijtsrd.com\/engineering\/information-technology\/38002\/smart-verification-of-passenger-using-ai\/leman-kirme","832":"Interface personalization aims to streamline the process of working in a feature-rich application by providing the user with an adapted interface tailored specifically to his\/her needs. The MICA (Mixed-Initiative Customization Assistance) system explores a middle ground between two opposing approaches to personalization: (1) an adaptable approach, where personalization is fully user controlled and (2) and adaptive approach, where personalization is fully system controlled. We overview MICA's strategy for providing user-adaptive recommendations to help users decide how to personalize their interfaces. In doing so, we focus primarily on how MICA handles threats to usability that are often found in adaptive interfaces including obtrusiveness and lack of understandability and control. We also describe how we evaluated MICA and highlight results from these evaluations.","833":"Recent developments in Generative Artificial Intelligence (GenAI) have created a paradigm shift in multiple areas of society, and the use of these technologies is likely to become a defining feature of education in coming decades. GenAI offers transformative pedagogical opportunities, while simultaneously posing ethical and academic challenges. Against this backdrop, we outline a practical, simple, and sufficiently comprehensive tool to allow for the integration of GenAI tools into educational assessment: the AI Assessment Scale (AIAS). The AIAS empowers educators to select the appropriate level of GenAI usage in assessments based on the learning outcomes they seek to address. The AIAS offers greater clarity and transparency for students and educators, provides a fair and equitable policy tool for institutions to work with, and offers a nuanced approach which embraces the opportunities of GenAI while recognising that there are instances where such tools may not be pedagogically appropriate or necessary. By adopting a practical, flexible approach that can be implemented quickly, the AIAS can form a much-needed starting point to address the current uncertainty and anxiety regarding GenAI in education. As a secondary objective, we engage with the current literature and advocate for a refocused discourse on GenAI tools in education, one which foregrounds how technologies can help support and enhance teaching and learning, which contrasts with the current focus on GenAI as a facilitator of academic misconduct.","834":"AI-assisted Design Thinking shows great potential for supporting collaborative creative work. To foster creative thinking processes within teams with individualized suggestions, AI has to rely on data provided by the teams. As a prerequisite, team members need to weigh their disclosure preferences against the potential benefits of AI when disclosing information. To shed light on these decisions, we identify relevant information such as emotional states or discussion arguments that design thinking teams could provide to AI to enjoy the benefits of its support. Using the privacy calculus as theoretical lens, we draft a research design to analyze user preferences for disclosing different information relevant to the service bundles that AI provides for respective information. We make explorative contributions to the body of knowledge in terms of AI use and its corresponding information disclosure. The findings are relevant for practice as they guide the design of AI that fosters information disclosure.","835":"Today, there is cut throat competition in Network Security and is major issue in Computer world also several security parameters are based on hard mathematical problems are available to tackle this problem. So many researchers trying to solve this problem from last decades. Using hard AI problems for security is up-and-coming as an exciting new concept so we have to show keen interest in this domain. Hence, in this paper, we are introducing better security parameters based on hard AI problems, explicitly, a novel family of graphical password systems built on top of Captcha technology, which we are proposing Captcha and Graphical Passwords (CaRP). CaRP is both a Captcha and a graphical password system. CaRP sort some security problems, such as online guessing attacks, relay attacks and shoulder-surfing attacks. Especially, a CaRP password can be establish only probabilistically by automatic online guessing attacks even if the password is in the search position. CaRP also offers a novel approach to address the well-known image hotspot problem in popular graphical password systems, such as PassPoints that often leads to less password choices. CaRP is not a universal solution but it offers reasonable security and usability and it may use few practical applications for getting better online security such as banking, railway reservation etc.","836":"Accuracy is an important concern for suppliers of artificial intelligence\r\n(AI) services, but considerations beyond accuracy, such as safety (which\r\nincludes fairness and explainability), security, and provenance, are also\r\ncritical elements to engender consumers' trust in a service. Many industries\r\nuse transparent, standardized, but often not legally required documents called\r\nsupplier's declarations of conformity (SDoCs) to describe the lineage of a\r\nproduct along with the safety and performance testing it has undergone. SDoCs\r\nmay be considered multi-dimensional fact sheets that capture and quantify\r\nvarious aspects of the product and its development to make it worthy of\r\nconsumers' trust. Inspired by this practice, we propose FactSheets to help\r\nincrease trust in AI services. We envision such documents to contain purpose,\r\nperformance, safety, security, and provenance information to be completed by AI\r\nservice providers for examination by consumers. We suggest a comprehensive set\r\nof declaration items tailored to AI and provide examples for two fictitious AI\r\nservices in the appendix of the paper.","837":"We review the evidence that artificial intelligence (AI) is having a large effect on the economy. Across a variety of statistics\u2014including robotics shipments, AI start-ups, and patent counts\u2014there is evidence of a large increase in AI-related activity. We also review recent research in this area that suggests that AI and robotics have the potential to increase productivity growth but may have mixed effects on labor, particularly in the short run. In particular, some occupations and industries may do well while others experience labor market upheaval. We then consider current and potential policies around AI that may help to boost productivity growth while also mitigating any labor market downsides, including evaluating the pros and cons of an AI specific regulator, expanded antitrust enforcement, and alternative strategies for dealing with the labor market impacts of AI, including universal basic income and guaranteed employment.","838":"Artificial intelligence based application for their treatment. Thus, telehealth will rapidly and radically transform in person care to remote consultation of patients. Because of this, it developed a Multilingual Conversational Bot based on Natural Language Processing NLP to provide free primary healthcare education, information, advice to chronic patients. The study introduces a novel computer application acting as a personal virtual doctor that has been opportunely designed and extensively trained to interact with patients like human beings. This application is related to a server less architecture and it aggregates the services of a doctor by providing preventive measures, home remedies, interactive counseling sessions, healthcare tips, and symptoms covering the most prevalent diseases in rural India. The paper proposes a conversational bot for delivering telehealth in India to increase the patients access to healthcare knowledge and leverage of artificial intelligence to bridge the gap of demand and supply of human healthcare providers. This AI application has resulted in reducing the barriers for access to healthcare facilities and intelligent consultations remotely to allow time to time care and quality treatment, thereby effectively assisting the society. Pradyumna Saini | Mohd Tajammul \"Conversational AI Powered Chatbot Using Lex and AWS\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-3 , April 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49722.pdf Paper URL: https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/49722\/conversational-ai-powered-chatbot-using-lex-and-aws\/pradyumna-saini","839":"The sudden popularity and availability of generative AI tools, such as ChatGPT that can write compelling essays on any topic, code in various programming languages, and ace standardized tests across domains, raises questions about the sustainability of traditional assessment practices. To seize this opportunity for innovation in assessment practice, we conducted a survey to understand both the educators' and students' perspectives on the issue. We measure and compare attitudes of both stakeholders across various assessment scenarios, building on an established framework for examining the quality of online assessments along six dimensions. Responses from 389 students and 36 educators across two universities indicate moderate usage of generative AI, consensus for which types of assessments are most impacted, and concerns about academic integrity. Educators prefer adapted assessments that assume AI will be used and encourage critical thinking, but students' reaction is mixed, in part due to concerns about a loss of creativity. The findings show the importance of engaging educators and students in assessment reform efforts to focus on the process of learning over its outputs, higher-order thinking, and authentic applications.","840":"A common criteria for Explainable AI (XAI) is to support users in establishing appropriate trust in the AI \u2013 rejecting advice when it is incorrect, and accepting advice when it is correct. Previous findings suggest that explanations can cause an over-reliance on AI (overly accepting advice). Explanations that evoke appropriate trust are even more challenging for decision-making tasks that are difficult for humans and AI. For this reason, we study decision-making by non-experts in the high-uncertainty domain of stock trading. We compare the effectiveness of three different explanation styles (influenced by inductive, abductive, and deductive reasoning) and the role of AI confidence in terms of a) the users\u2019 reliance on the XAI interface elements (charts with indicators, AI prediction, explanation), b) the correctness of the decision (task performance), and c) the agreement with the AI\u2019s prediction. In contrast to previous work, we look at interactions between different aspects of decision-making, including AI correctness, and the combined effects of AI confidence and explanations styles. Our results show that specific explanation styles (abductive and deductive) improve the user\u2019s task performance in the case of high AI confidence compared to inductive explanations. In other words, these styles of explanations were able to invoke correct decisions (for both positive and negative decisions) when the system was certain. In such a condition, the agreement between the user\u2019s decision and the AI prediction confirms this finding, highlighting a significant agreement increase when the AI is correct. This suggests that both explanation styles are suitable for evoking appropriate trust in a confident AI.","841":"A core challenge for both physics and artificial intellicence (AI) is\r\nsymbolic regression: finding a symbolic expression that matches data from an\r\nunknown function. Although this problem is likely to be NP-hard in principle,\r\nfunctions of practical interest often exhibit symmetries, separability,\r\ncompositionality and other simplifying properties. In this spirit, we develop a\r\nrecursive multidimensional symbolic regression algorithm that combines neural\r\nnetwork fitting with a suite of physics-inspired techniques. We apply it to 100\r\nequations from the Feynman Lectures on Physics, and it discovers all of them,\r\nwhile previous publicly available software cracks only 71; for a more difficult\r\ntest set, we improve the state of the art success rate from 15% to 90%.","842":"AbstractThe educational impact of Generative AI (GenAI) technologies, such as ChatGPT, has received significant attention. We use the TPACK framework to discuss the types of knowledge teachers require to effectively use GenAI tools. We highlight the qualities of GenAI that make it like other digital technologies (they are protean, opaque, and unstable) as well as qualities that make it revolutionary (namely, they are generative and social). We describe how these traits affect specific knowledge domains (TK, TPK, TCK, XK, and TPACK) and explore implications for educators. Finally, we argue for a more expansive description of Contextual Knowledge (XK), going beyond the immediate context to include considerations of how GenAI will change individuals, society and, through that, the broader educational context.","843":"We present AI Poincar\u00e9, a machine learning algorithm for auto-discovering\r\nconserved quantities using trajectory data from unknown dynamical systems. We\r\ntest it on five Hamiltonian systems, including the gravitational 3-body\r\nproblem, and find that it discovers not only all exactly conserved quantities,\r\nbut also periodic orbits, phase transitions and breakdown timescales for\r\napproximate conservation laws.","844":"Shakey the Robot, conceived fifty years ago, was a seminal contribution to AI. Shakey perceived its world, planned how to achieve a goal, and acted to carry out that plan. This was revolutionary. At the Twenty-Ninth AAAI Conference on Artificial Intelligence, attendees gathered to celebrate Shakey, and to gain insights into how the AI revolution moves ahead. The celebration included a panel, chaired by Benjamin Kuipers and featuring AI pioneers Ed Feigenbaum, Peter Hart, and Nils Nilsson. This article includes written versions of the contributions of those panelists.","845":"State-of-the-art AI models largely lack an understanding of the cause-effect relationship that governs human understanding of the real world. Consequently, these models do not generalize to unseen data, often produce unfair results, and are difficult to interpret. This has led to efforts to improve the trustworthiness aspects of AI models. Recently, causal modeling and inference methods have emerged as powerful tools. This review aims to provide the reader with an overview of causal methods that have been developed to improve the trustworthiness of AI models. We hope that our contribution will motivate future research on causality-based solutions for trustworthy AI.","846":"This workshop is the fourth in the series and continued to build upon the work carried out at the previous iterations of the International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, which were held at ICSE in 2012, 2013 and 2014. RAISE 2015 brought together researchers and practitioners from the artificial intelligence (AI) and software engineering (SE) disciplines to build on the interdis- ciplinary synergies that exist and to stimulate further interaction across these disciplines. Mutually beneficial characteristics have appeared in the past few decades and are still evolving due to new challenges and technological advances. Hence, the question that motivates and drives the RAISE Workshop series is: &#x0022;Are SE and AI researchers ignoring important insights from AI and SE?&#x0022;. To pursue this question, RAISE'15 explored not only the application of AI techniques to SE problems but also the application of SE techniques to AI problems. RAISE not only strengthens the AI- and-SE community but also continues to develop a roadmap of strategic research directions for AI and SE.","847":null,"848":"Starcraft II (SC2) is widely considered as the most challenging Real Time\r\nStrategy (RTS) game. The underlying challenges include a large observation\r\nspace, a huge (continuous and infinite) action space, partial observations,\r\nsimultaneous move for all players, and long horizon delayed rewards for local\r\ndecisions. To push the frontier of AI research, Deepmind and Blizzard jointly\r\ndeveloped the StarCraft II Learning Environment (SC2LE) as a testbench of\r\ncomplex decision making systems. SC2LE provides a few mini games such as\r\nMoveToBeacon, CollectMineralShards, and DefeatRoaches, where some AI agents\r\nhave achieved the performance level of human professional players. However, for\r\nfull games, the current AI agents are still far from achieving human\r\nprofessional level performance. To bridge this gap, we present two full game AI\r\nagents in this paper - the AI agent TStarBot1 is based on deep reinforcement\r\nlearning over a flat action structure, and the AI agent TStarBot2 is based on\r\nhard-coded rules over a hierarchical action structure. Both TStarBot1 and\r\nTStarBot2 are able to defeat the built-in AI agents from level 1 to level 10 in\r\na full game (1v1 Zerg-vs-Zerg game on the AbyssalReef map), noting that level\r\n8, level 9, and level 10 are cheating agents with unfair advantages such as\r\nfull vision on the whole map and resource harvest boosting. To the best of our\r\nknowledge, this is the first public work to investigate AI agents that can\r\ndefeat the built-in AI in the StarCraft II full game.","849":null,"850":"For decades AI researchers have built agents that are capable of carrying out tasks that require human-level or human-like intelligence. During this time, questions of how these programs compared in kind to humans have surfaced and led to beneficial interdisciplinary discussions, but conceptual progress has been slower than technological progress. Within the past decade, the term agency has taken on new import as intelligent agents have become a noticeable part of our everyday lives. Research on autonomous vehicles and personal assistants has expanded into private industry with new and increasingly capable products surfacing as a matter of routine. This wider use of AI technologies has raised questions about legal and moral agency at the highest levels of government (National Science and Technology Council 2016) and drawn the interest of other academic disciplines and the general public. Within this context, the notion of an intelligent agent in AI is too coarse and in need of refinement. We suggest that the space of AI agents can be subdivided into classes, where each class is defined by an associated degree of control.","851":"Fears of artificial intelligence (ai) have haunted humanity since the very beginning of the computer age. Hitherto these fears focused on machines using physical means to kill, enslave or replace people. But over the past couple of years new ai tools have emerged that threaten the survival of human civilisation from an unexpected direction. ai has gained some remarkable abilities to manipulate and generate language, whether with words, sounds or images. ai has thereby hacked the operating system of our civilisation.","852":null,"853":"In this paper, we review tutoring approaches of computer-supported systems for learning programming. From the survey we have learned three lessons. First, various AI-supported tutoring approaches have been developed and most existing systems use a feedback-based tutoring approach for supporting students. Second, the AI techniques deployed to support feedback-based tutoring approaches are able to identify the student's intention, i.e. the solution strategy implemented in the student solution. Third, most reviewed tutoring approaches only support individual learning. In order to fill this research gap, we propose an approach to pair learning which supports two students who solve a programming problem face-to-face.","854":"Manually indexing documents for subject-based access is a labour-intensive process that can be automated using AI technology. Algorithms for text classification must be trained and tested with examples of indexed documents, which can be obtained from existing bibliographic databases and digital collections.The National Library of Finland has created Annif, an open source toolkit for automated subject indexing and classification. Annif is multilingual, independent of the indexing vocabulary, and modular. It integrates many text classification algorithms, including Maui, fastText, Omikuji, and a neural network model based on TensorFlow. Best results can often be obtained by combining several algorithms. Many document corpora have been used for training and evaluating Annif. Finding the algorithms and configurations that give the best quality is an ongoing effort.In May 2020, we launched Finto AI, a service for automated subject indexing based on Annif. It provides a simple Web form for obtaining subject suggestions for text. The functionality is also available as a REST API. Many document repositories and the cataloguing system for electronic publications at the National Library of Finland are using it to integrate semi-automated subject indexing into their metadata workflows. In the future, we are going to extend Annif with more algorithms and new functionality, and to integrate Finto AI with other metadata management workflows.","855":"In recent years, digital object management practices to support findability,\r\naccessibility, interoperability, and reusability (FAIR) have begun to be\r\nadopted across a number of data-intensive scientific disciplines. These digital\r\nobjects include datasets, AI models, software, notebooks, workflows,\r\ndocumentation, etc. With the collective dataset at the Large Hadron Collider\r\nscheduled to reach the zettabyte scale by the end of 2032, the experimental\r\nparticle physics community is looking at unprecedented data management\r\nchallenges. It is expected that these grand challenges may be addressed by\r\ncreating end-to-end AI frameworks that combine FAIR and AI-ready datasets,\r\nadvances in AI, modern computing environments, and scientific data\r\ninfrastructure. In this work, the FAIR4HEP collaboration explores the\r\ninterpretation of FAIR principles in the context of data and AI models for\r\nexperimental high energy physics research. We investigate metrics to quantify\r\nthe FAIRness of experimental datasets and AI models, and provide open source\r\nnotebooks to guide new users on the use of FAIR principles in practice.","856":"Superpowers, states and companies around the world are all pushing hard to win the AI race. Artificial intelligence (AI) is of strategic importance for the EU, with the European Commission recently stating that \u2018artificial intelligence with a purpose can make Europe a world leader\u2019. For this to happen, though, the EU needs to put in place the right ethical and legal framework. This Foresight Brief argues that such a framework must be solidly founded on regulation \u2013 which can be achieved by updating existing legislation \u2013 and that it must pay specific attention to the protection of workers. Workers are in a subordinate position in relation to their employers, and in the EU\u2019s eagerness to win the AI race, their rights may be overlooked. This is why a protective and enforceable legal framework must be developed, with the participation of social partners.","857":"The educational applications of AI are a combination of what Pasteur's Quadrant describes as use-inspired basic and pure applied research. This article gives an overview of the classical and emerging architectures for AI in education. Early researchers focused on creating personalized teaching systems based on solitary learners, whereas recent work takes account of other people and the learning context. Various Grand Challenges illustrate the issues still facing AI in education.","858":"Nowadays most of the surveillance cameras in ATM doesn\u2019t record with detail for analysis of incidents. Due to this most of the ATM cases gets unsolved. In this paper a system to improve ATM security is proposed. The proposed system deals with the development of a application using Open CV, YOLO and AI for automation of video surveillance in ATM machines and detect any type of potential criminal activities that might be arising. Prem Krishna | Saheel Ahamed | Roshan Kartik \u00c4n AI Based ATM Intelligent Security System using Open CV and YOLO\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd41232.pdf Paper URL: https:\/\/www.ijtsrd.comengineering\/computer-engineering\/41232\/an-ai-based-atm-intelligent-security-system-using-open-cv-and-yolo\/prem-krishna","859":null,"860":"There is both much optimisim and pessimism around artificial intelligence (AI) today. The optimists are investing millions of dollars, and even in some cases billions of dollars into AI. The pessimists, on the other hand, predict that AI will end many things: jobs, warfare, and even the human race. Both the optimists and the pessimists often appeal to the idea of a technological singularity, a point in time where machine intelligence starts to run away, and a new, more intelligent 'species' starts to inhabit the earth. If the optimists are right, this will be a moment that fundamentally changes our economy and our society. If the pessimists are right, this will be a moment that also fundamentally changes our economy and our society. It is therefore very worthwhile spending some time deciding if either of them might be right.","861":"Highly automated production systems are conceived to efficiently handle evolving production requirements. This concerns any level of the system from the configuration and control to the management of production. The proposed work deals with the production scheduling level. The authors present an AI-based online scheduling controller for Reconfigurable Manufacturing Systems (RMSs) whose main advantage is its capacity of dynamically interpreting and adapting any production anomaly or system misbehavior by regenerating on-line a new schedule. The performance of the controller has been assessed by running a set of closed-loop experiments based on a real-world industrial case study. Results demonstrate that the capability of automatically synthesizing plans together with recovery actions severely contribute to ensure a high and continuous production rate.","862":"Bostrom's Superintelligence (SI) is a wide-ranging essay (2016) that has raised important questions about the future of intelligent machines and the possible malign developments they may undergo. But, and perhaps surprisingly, it is not about technical developments in artificial intelligence (AI) nor a philosophical analysis of the concept of SI. There is little of either of these in it, which is largely an extended and stimulating essay on economics, decision theory and other forms of social science, all held together by the unsubstantiated hypothesis of superintelligence that belongs more to science fiction than AI. AI may well in some future produce undesirable social effects -- the Internet itself could already be such a development -- but there is as yet no reason to think they could be on the massive and end-of-civilization scale Bostrom so confidently predicts.","863":null,"864":null,"865":null,"866":null,"867":null,"868":"Nowadays, the mobile computing paradigm and the widespread diffusion of mobile devices are quickly changing and replacing many common assumptions about software architectures and interaction\/communication models. The environment, in particular, or more generally, the so-called user context is claiming a central role in everyday's use of cellular phones, PDAs, etc. This is due to the huge amount of data suggested by the surrounding environment that can be helpful in many common tasks. For instance, the current context can help a search engine to refine the set of results in a useful way, providing the user with a more suitable and exploitable information. Moreover, we can take full advantage of this new data source by ''pushing'' active contents towards mobile devices, empowering the latter with new features (e.g., applications) that can allow the user to fruitfully interact with the current context. Following this vision, mobile devices become dynamic self-adapting tools, according to the user needs and the possibilities offered by the environment. The present work proposes MoBe: an approach for providing a basic infrastructure for pervasive context-aware applications on mobile devices, in which AI techniques (namely a principled combination of rule-based systems, Bayesian networks and ontologies) are applied to context inference. The aim is to devise a general inferential framework to make easier the development of context-aware applications by integrating the information coming from physical and logical sensors (e.g., position, agenda) and reasoning about this information in order to infer new and more abstract contexts.","869":null,"870":null,"871":null,"872":"This editorial introduction summarizes the seven guest-edited contributions to AI Magazine that explore opportunities and challenges arising from transferring and adapting semantic web technologies to the big data quest.","873":"Labeling data is an important step in the supervised machine learning lifecycle. It is a laborious human activity comprised of repeated decision making: the human labeler decides which of several potential labels to apply to each example. Prior work has shown that providing AI assistance can improve the accuracy of binary decision tasks. However, the role of AI assistance in more complex data-labeling scenarios with a larger set of labels has not yet been explored. We designed an AI labeling assistant that uses a semi-supervised learning algorithm to predict the most probable labels for each example. We leverage these predictions to provide assistance in two ways: (i) providing a label recommendation and (ii) reducing the labeler\u2019s decision space by focusing their attention on only the most probable labels. We conducted a user study (n=54) to evaluate an AI-assisted interface for data labeling in this context. Our results highlight that the AI assistance improves both labeler accuracy and speed, especially when the labeler finds the correct label in the reduced label space. We discuss findings related to the presentation of AI assistance and design implications for intelligent labeling interfaces.","874":"This book presents sequential decision theory from a\n                  novel algorithmic information theory perspective. While the former\n                  theory is suited for active agents in known environments, the\n                  latter is suited for passive prediction of unknown environments.\n                  The book introduces these two well-known but very different ideas\n                  and removes the limitations by unifying them to one parameter-free\n                  theory of an optimal reinforcement learning agent interacting with\n                  an arbitrary unknown world. Most if not all AI problems can easily\n                  be formulated within this theory, which reduces the conceptual\n                  problems to pure computational ones. Considered problem classes\n                  include sequence prediction, strategic games, function\n                  minimization, reinforcement and supervised learning. Formal\n                  definitions of intelligence order relations, the horizon problem\n                  and relations to other approaches to AI are discussed. One\n                  intention of this book is to excite a broader AI audience about\n                  abstract algorithmic information theory concepts, and conversely\n                  to inform theorists about exciting applications to AI.","875":"Image denoising is one of the most critical problems in mobile photo\r\nprocessing. While many solutions have been proposed for this task, they are\r\nusually working with synthetic data and are too computationally expensive to\r\nrun on mobile devices. To address this problem, we introduce the first Mobile\r\nAI challenge, where the target is to develop an end-to-end deep learning-based\r\nimage denoising solution that can demonstrate high efficiency on smartphone\r\nGPUs. For this, the participants were provided with a novel large-scale dataset\r\nconsisting of noisy-clean image pairs captured in the wild. The runtime of all\r\nmodels was evaluated on the Samsung Exynos 2100 chipset with a powerful Mali\r\nGPU capable of accelerating floating-point and quantized neural networks. The\r\nproposed solutions are fully compatible with any mobile GPU and are capable of\r\nprocessing 480p resolution images under 40-80 ms while achieving high fidelity\r\nresults. A detailed description of all models developed in the challenge is\r\nprovided in this paper.","876":"Artificial intelligence (AI) has strong logical reasoning abilities and the ability to learn on its own, and it can mimic the human brain's thought process. Machine learning and other AI technologies have the potential to greatly enhance the existing method of anticancer medicine development. However, AI currently has several limits. This study investigates the evolution of artificial intelligence technologies in anti-cancer therapeutic research, such as deep learning and machine learning. At the same time, we are optimistic about AI's future.","877":null,"878":"A brief command and a response is the most common form of engagement with this consumer voice-enabled AI device. But in this fleeting moment of interaction, a vast matrix of capacities is invoked: interlaced chains of resource extraction, human labor and algorithmic processing across networks of mining, logistics, distribution, prediction and optimization. The scale of this system is almost beyond human imagining. How can we begin to see it, to grasp its immensity and complexity as a connected form? We start with an outline: an exploded view of a planetary system across three stages of birth, life and death, accompanied by an essay in 21 parts. Together, this becomes an anatomical map of a single AI system.","879":"main objective of this paper is split in two parts. Firstly, it is needed to verify if there exists a cultural bias for data science in Spanish language. Secondly, we have to check how privacy data is controlled in apps, which use contact tracing techniques, and also electronic devices such as smartwatches. To carry out the first part of the study, we have investigated about datasets in English and Spanish language and its technical structure. As well as, it has been designed a database, used for demonstrating the cultural bias that exists between English and Spanish language. For the second phase, it has been explored themes such as smartwatches for minors, adding a comparative table of privacy about important consumption brands. Following the assessment, it is observed that there exists a cultural bias between Spanish and English language. In fact, the results shows that a 70% of analysed datasets are written in English. The lack of inversion in technological education is one of the reasons why Spanish speakers countries lack of an appropriate technological education. On 14 May 2020, the newspaper The Economist writes an article about the involvement of countries on technological innovation. These countries do not include Spain because Spanish inversion does not cover more than 1,25% out of the PIB total. Europe has been driven by achieving the objective of reaching, at least, 3% in 2020. So, Spain is lower than the European media. On contrary, there are other countries such as South Korea, Denmark, and Sweden, which the barrier of 3% is reached and even 4% . Consequently, it has negative effects for technological education for Spain. One proof of this is the results of the PISA index (Program for International Evaluation of Student), where Spanish obtain fewer qualifications in technology, science, and mathematics than the average of OCDE countries (Organization for economic Cooperation Development). Apart from Spain, Chile, M\u00e9xico, and Columbia are also at the bottom of the list . Moreover, this paper shows data that warn us about the importance of safeguard our security towards the technological breakthrough. Centralizing decisions in collaborative and international organizations by applying: efficient, ethics and deontological strategies could be a possible solution. \u00a0","880":null,"881":null,"882":null,"883":null,"884":null,"885":null,"886":null,"887":null,"888":null,"889":null,"890":null,"891":null,"892":null,"893":null,"894":null,"895":null,"896":null,"897":null,"898":null,"899":"Valid measurement of AI literacy is important for the selection of personnel, identification of shortages in skill and knowledge, and evaluation of AI literacy interventions. A questionnaire is missing that is deeply grounded in the existing literature on AI literacy, is modularly applicable depending on the goals, and includes further psychological competencies in addition to the typical facets of AIL. This paper presents the development and validation of a questionnaire considering the desiderata described above. We derived items to represent different facets of AI literacy and psychological competencies, such as problem-solving, learning, and emotion regulation in regard to AI. We collected data from 300 German-speaking adults to confirm the factorial structure. The result is the Meta AI Literacy Scale (MAILS) for AI literacy with the facets Use & apply AI, Understand AI, Detect AI, and AI Ethics and the ability to Create AI as a separate construct, and AI Self-efficacy in learning and problem-solving and AI Self-management (i.e., AI persuasion literacy and emotion regulation). This study contributes to the research on AI literacy by providing a measurement instrument relying on profound competency models. Psy\u00ad chological competencies are included particularly important in the context of pervasive change through AI systems.","900":"A key research question at the Large Hadron Collider (LHC) is the test of\nmodels of new physics. Testing if a particular parameter set of such a model is\nexcluded by LHC data is a challenge: It requires the time consuming generation\nof scattering events, the simulation of the detector response, the event\nreconstruction, cross section calculations and analysis code to test against\nseveral hundred signal regions defined by the ATLAS and CMS experiment. In the\nBSM-AI project we attack this challenge with a new approach. Machine learning\ntools are thought to predict within a fraction of a millisecond if a model is\nexcluded or not directly from the model parameters. A first example is SUSY-AI,\ntrained on the phenomenological supersymmetric standard model (pMSSM). About\n300,000 pMSSM model sets - each tested with 200 signal regions by ATLAS - have\nbeen used to train and validate SUSY-AI. The code is currently able to\nreproduce the ATLAS exclusion regions in 19 dimensions with an accuracy of at\nleast 93 percent. It has been validated further within the constrained MSSM and\na minimal natural supersymmetric model, again showing high accuracy. SUSY-AI\nand its future BSM derivatives will help to solve the problem of recasting LHC\nresults for any model of new physics.\n\n\nSUSY-AI can be downloaded at <a href=\"http:\/\/susyai.hepforge.org\/.\">this http URL<\/a> An on-line\ninterface to the program for quick testing purposes can be found at\n<a href=\"http:\/\/www.susy-ai.org\/.\">this http URL<\/a>","901":null,"902":null,"903":"This special issue issue of AI Magazine presents six articles on some of the most interesting question answering systems in development today. Included are articles on Project, the Semantic Research, Watson, True Knowledge, and TextRunner (University of Washington's clever use of statistical NL techniques to answer questions across the open web).","904":"For over twenty-five years Ford Motor Company has been utilizing an AI-based system to manage process planning for vehicle assembly at its assembly plants around the world. The scope of the AI system, known originally as the Direct Labor Management System and now as the Global Study Process Allocation System (GSPAS), has increased over the years to include additional functionality on Ergonomics and Powertrain Assembly (Engines and Transmission plants). The knowledge about Ford's manufacturing processes is contained in an ontology originally developed using the KL-ONE representation language and methodology. To preserve the viability of the GSPAS ontology and to make it easily usable for other applications within Ford, we needed to re-engineer and convert the KL-ONE ontology into a semantic web OWL\/RDF format. In this article, we will discuss the process by which we re-engineered the existing GSPAS KL-ONE ontology and deployed semantic web technology in our application.","905":"Recently, eXplainable AI (XAI) research has focused on the use of counterfactual explanations to address interpretability, algorithmic recourse, and bias in AI system decision-making. The proponents of these algorithms claim they meet users\u2019 requirements for counterfactual explanations. For instance, many claim that the output of their algorithms work as explanations because they prioritise \"plausible\", \u00e4ctionable\" or \"causally important\" features in their generated counterfactuals. However, very few of these claims have been tested in controlled psychological studies, and we know very little about which aspects of counterfactual explanations help users to understand AI system decisions. Furthermore, we do not know whether counterfactual explanations are an advance on more traditional causal explanations that have a much longer history in AI (in explaining expert systems and decision trees). Accordingly, we carried out two user studies to (i) test a fundamental distinction in feature-types, between categorical and continuous features, and (ii) compare the relative effectiveness of counterfactual and causal explanations. The studies used a simulated, automated decision-making app that determined safe driving limits after drinking alcohol, based on predicted blood alcohol content, and user responses were measured objectively (users\u2019 predictive accuracy) and subjectively (users\u2019 satisfaction and trust judgments). Study 1 (N=127) showed that users understand explanations referring to categorical features more readily than those referring to continuous features. It also discovered a dissociation between objective and subjective measures: counterfactual explanations elicited higher accuracy of predictions than no-explanation control descriptions but no higher accuracy than causal explanations, yet counterfactual explanations elicited greater satisfaction and trust judgments than causal explanations. Study 2 (N=211) found that users were more accurate for categorically-transformed features compared to continuous ones, and also replicated the results of Study 1. The findings delineate important boundary conditions for current and future counterfactual explanation methods in XAI.","906":null,"907":"Background: In medical imaging, prior studies have demonstrated disparate AI\r\nperformance by race, yet there is no known correlation for race on medical\r\nimaging that would be obvious to the human expert interpreting the images.\r\n  Methods: Using private and public datasets we evaluate: A) performance\r\nquantification of deep learning models to detect race from medical images,\r\nincluding the ability of these models to generalize to external environments\r\nand across multiple imaging modalities, B) assessment of possible confounding\r\nanatomic and phenotype population features, such as disease distribution and\r\nbody habitus as predictors of race, and C) investigation into the underlying\r\nmechanism by which AI models can recognize race.\r\n  Findings: Standard deep learning models can be trained to predict race from\r\nmedical images with high performance across multiple imaging modalities. Our\r\nfindings hold under external validation conditions, as well as when models are\r\noptimized to perform clinically motivated tasks. We demonstrate this detection\r\nis not due to trivial proxies or imaging-related surrogate covariates for race,\r\nsuch as underlying disease distribution. Finally, we show that performance\r\npersists over all anatomical regions and frequency spectrum of the images\r\nsuggesting that mitigation efforts will be challenging and demand further\r\nstudy.\r\n  Interpretation: We emphasize that model ability to predict self-reported race\r\nis itself not the issue of importance. However, our findings that AI can\r\ntrivially predict self-reported race -- even from corrupted, cropped, and\r\nnoised medical images -- in a setting where clinical experts cannot, creates an\r\nenormous risk for all model deployments in medical imaging: if an AI model\r\nsecretly used its knowledge of self-reported race to misclassify all Black\r\npatients, radiologists would not be able to tell using the same data the model\r\nhas access to.","908":"A synthesis of methods from cybernetics and AI yields a concept of intelligence for autonomous mobile systems that integrates closed-loop visual perception and goal-oriented action cycles using spatiotemporal models. In a layered architecture, systems dynamics methods with differential models prevail on the lower, data-intensive levels, but on higher levels, AI-type methods are used. Knowledge about the world is geared to classes of objects and subjects. Subjects are defined as objects with additional capabilities of sensing, data processing, decision making, and control application. Specialist processes for visual detection and efficient tracking of class members have been developed. On the upper levels, individual instantiations of these class members are analyzed jointly in the task context, yielding the situation for decision making. As an application, vertebrate-type vision for tasks in vehicle guidance in naturally perturbed environments was investigated with a distributed PC system. Experimental results with the test vehicle VAMORS are discussed.","909":"Creation and exchange of knowledge depends on collaboration. Recent work has suggested that the emergence of collaboration frequently relies on geographic proximity. However, being co-located tends to be associated with other dimensions of proximity, such as social ties or a shared organizational environment. To account for such factors, multiple dimensions of proximity have been proposed, including cognitive, institutional, organizational, social and geographical proximity. Since they strongly interrelate, disentangling these dimensions and their respective impact on collaboration is challenging. To address this issue, we propose various methods for measuring different dimensions of proximity. We then present an approach to compare and rank them with respect to the extent to which they indicate co-publications and co-inventions. We adapt the HypTrails approach, which was originally developed to explain human navigation, to co-author and co-inventor graphs. We evaluate this approach on a subset of the German research community, specifically academic authors and inventors active in research on artificial intelligence (AI). We find that social proximity and cognitive proximity are more important for the emergence of collaboration than geographic proximity.","910":"In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be \u2018ethical\u2019, there is debate about both what constitutes \u2018ethical AI\u2019 and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.","911":"The late 1990s saw a huge upswing in the demand for supply chain management software. The rapid commercial adoption of these tools brought into sharp relief the complexity of automating supply chain planning and optimization. This article summarizes four years of real-world experience in combining AI and OR techniques, balancing representational power against planning complexity, and understanding the practical implications of NP-completeness.","912":"This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy\u2014improve people\u2019s understanding of the AI model, help people recognize the model uncertainty, and support people\u2019s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.","913":"In the winter, 2004 issue of AI Magazine, we reported Vulcan Inc.'s first step toward creating a question-answering system called Digital Aristotle. The goal of that first step was to assess the state of the art in applied Knowledge Representation and Reasoning (KRR) by asking AI experts to represent 70 pages from the advanced placement (AP) chemistry syllabus and to deliver knowledge-based systems capable of answering questions from that syllabus. This paper reports the next step toward realizing a Digital Aristotle: we present the design and evaluation results for a system called AURA, which enables domain experts in physics, chemistry, and biology to author a knowledge base and that then allows a different set of users to ask novel questions against that knowledge base. These results represent a substantial advance over what we reported in 2004, both in the breadth of covered subjects and in the provision of sophisticated technologies in knowledge representation and reasoning, natural language processing, and question answering to domain experts and novice users.","914":"Data science can be messy, with data scientists often starting with poor-quality or inconsistent data that must be cleaned up before it can be utilized to an organization's best advantage. Here, the author discusses artificial intelligence---in particular machine learning-technologies and how they can help or hinder organizations in curating the vast amounts of data such organizations have access to.","915":null,"916":"The aim of the project is to explore and demonstrate\n                 the potential of common AI techniques in computer\n                 games. We will be concentrating on some or all of the\n                 following:\n\n                 * Logic-based planning\n\n                 * Neural networks\n\n                 * Genetic programming\n\n                 * Machine learning\n\n                 We will be using game engines from IO Interactive as a\n                 framework for implementation, in order to demonstrate\n                 these techniques.\n\n                 The primary objective is to achieve a higher level of\n                 artificial intelligence in computer games by the usage\n                 of logic-based planning. This requires development of a\n                 multi agent system, for simulating human-like\n                 behaviour, within a computer game.\n\n                 The additionally mentioned techniques are regarded as\n                 secondary techniques, which are to be used in\n                 conjunction with planning, in order to facilitate more\n                 specific behavior like learning or adaptation.\n                 Combining one or more of the secondary techniques with\n                 the primary technique is a secondary objective. The\n                 extension of usage of secondary techniques will be\n                 decided at a later stage.\n\n                 Loosely formulated, the project objective is to bridge\n                 the gap between the AI planning field and the\n                 commercial computer game industry. Alternatively, to\n                 assess the distance between the AI field, and the\n                 emerging design patterns used in the gaming field. The\n                 project can be seen as an advanced application of multi\n                 agent theory, building on previous experiences from\n                 multi agent system projects.","917":"Abstract&nbsp;&nbsp;This paper investigates the prospects of Rodney Brooks' proposal for AI without representation. It turns out that the supposedly characteristic features of  \u201d new AI\u201d (embodiment, situatedness, absence of reasoning, and absence of representation) are all present in conventional systems:  \u201d New AI\u201d is just like old AI. Brooks proposal boils down to the architectural rejection of central control in intelligent agents\u2014Which, however, turns out to be crucial. Some of more recent cognitive science suggests that we might do well to dispose of the image of intelligent agents as central representation processors. If this paradigm shift is achieved, Brooks' proposal for cognition without representation appears promising for full-blown intelligent agents\u2014Though not for conscious agents.","918":"Artificial intelligence technologies are being deployed to improve the customer service experience.","919":null,"920":null,"921":null,"922":null,"923":null,"924":null,"925":null,"926":null,"927":null,"928":null,"929":"IT support is under growing pressure to ensure efficient, flexible, and scalable use of digital technologies (Kumbakara, 2008). As a result, technical support staff is affected by monotonous work and work overload (Schmidt et al., 2022). Our research aims to augment the precarious workplace of support agents with artificial intelligence (AI). To incorporate an employee-centered perspective a priori and ensure positive impacts, we propose a framework for combining work design theory (e.g. Demerouti et al., 2001) and design science theory (Peffers et al., 2007, Niehaves & Ortbach 2016). The advances in AI promise to leverage large potential in optimizing and enhancing service processes and workplaces (Huang & Rust, 2018, de Keyser, 2019). Introducing AI into service processes, imply effects on work characteristics (Larivi\u00e8re et al., 2017). By combining human and artificial intelligence we propose hybrid intelligence (Dellermann et al., 2019) as a suitable solution for mitigating the persistent issues of support workers and the possible negative impacts of AI. To a great extent, IS research emphasizes the implied impacts of AI use in workplaces (Verma & Singh 2022, Wang et al., 2020). As such, work design models are widely used to empirically evaluate the impacts of AI design (Sturm & Peters, 2020), but are rarely utilized to substantiate the design of AI-augmented work systems. Only Poser et al. (2022) and Zschech et al. (2021) recently applied such models. The goal of this paper is to overcome the lack of work design in design science research (DSR) for AI-based systems and to steer the development into desired socio-technological configurations. The here presented work is expected to answer : How can work design theory inform the design of AI-augmented workplaces? RQ1 How should a hybrid intelligence system be designed to augment IT support agents\u2019 workplaces by incorporating work design theory? \r\nRQ2 To systematically design the integration of AI, we make use of the DSR paradigm (Peffers et al., 2007). We first interview support agents and utilize the organizing move theory of Pentland (1992) and the technical support work theory of Das (2003) to ensure relevance. Based on a review of the IS literature on work design theories, we then derive a preliminary theoretical framework (Paul & Benito, 2018) RQ1. The framework represents a kernel theory for the development of meta design requirements. Contributing to the second research question, we design and subsequently evaluate the augmentation based on work-related outcomes RQ2.","930":null,"931":null,"932":null,"933":null,"934":null,"935":null,"936":null,"937":"The emergence of machine learning (ML), the application of the more general field of artificial intelligence (AI), to automate statistical inference to detect patterns in data, has opened up entire new domains of complex data analysis. This is especially applicable in high-dimensionality datasets, where patterns and relationships are often not immediately apparent to investigators. ML approaches are increasingly promoted in medical research as a driver for a new generation of rapidly gained, data-derived, scientific insights, as listed in the recent Topol review of technology in the NHS.1 \r\n\r\nNew innovations in implementation and portability of AI in healthcare mean clinicians are already involved in developing ML applications, or incorporating \u2018off the shelf\u2019 existing applications into clinical workflows. Investigators must work with frontline teams to understand some of the inherent limitations, biases and exclusions of these applications when critically appraising their utility in the clinical domain.\r\n\r\nML algorithms develop rules and hypotheses based on data. Large numbers of features, such as patient observations, keywords and syntheses of combined parameters, are combined in models to optimise against a chosen outcome. Investigators aim to build a model based on existing data, that will perform equally as well on future data.\r\n\r\nBias exists in human society as an unreasonable assumption or prejudice towards a person or belief. In contrast, bias in ML exists as \u2018the error that is introduced by approximating a real life problem, which may be extremely complicated, by a much simpler model\u2019.2 An ML model may therefore be mathematically accurate (unbiased) to a biassed dataset, and vice versa.\r\n\r\nThe Academy of Royal Medical Colleges has recently produced guidance on using ML in healthcare, and bias is one of its key concerns.3 \r\n\r\nInvestigators must be cognisant of whether they seek to faithfully replicate the societal biases that exist within their \u2026","938":null,"939":null,"940":null,"941":null,"942":null,"943":null,"944":null,"945":"X5Learn (available at https:\/\/x5learn.org ) is a human-centered AI-powered platform for supporting access to free online educational resources. X5Learn provides users with a number of educational tools for interacting with open educational videos, and a set of tools adapted to suit the pedagogical preferences of users. It is intended to support both teachers and students, alike. For teachers, it provides a powerful platform to reuse, revise, remix, and redistribute open courseware produced by others. These can be videos, pdfs, exercises and other online material. For students, it provides a scaffolded and informative interface to select content to watch, read, make notes and write reviews, as well as a powerful personalised recommendation system that can optimise learning paths and adjust to the user\u2019s learning preferences. What makes X5Learn stand out from other educational platforms, is how it combines human-centered design with AI algorithms and software tools with the goal of making it intuitive and easy to use, as well as making the AI transparent to the user. We present the core search tool of X5Learn, intended to support exploring open educational materials.","946":"NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI\r\napplications through re-usability, abstraction, and composition. NeMo is built\r\naround neural modules, conceptual blocks of neural networks that take typed\r\ninputs and produce typed outputs. Such modules typically represent data layers,\r\nencoders, decoders, language models, loss functions, or methods of combining\r\nactivations. NeMo makes it easy to combine and re-use these building blocks\r\nwhile providing a level of semantic correctness checking via its neural type\r\nsystem. The toolkit comes with extendable collections of pre-built modules for\r\nautomatic speech recognition and natural language processing. Furthermore, NeMo\r\nprovides built-in support for distributed training and mixed precision on\r\nlatest NVIDIA GPUs. NeMo is open-source https:\/\/github.com\/NVIDIA\/NeMo","947":"The field of Human-Computer Interaction (HCI) offers designers and developers of interactive systems a large repertoire of methods for ensuring that their systems will be both usable and useful. This article offers a brief introduction to these methods, focusing on the ways in which they sometimes need to be adapted and extended to take into account the characteristic properties of systems that include some sort of AI. The discussion is organized around three types of activity: understanding users needs, interaction design, and evaluation.","948":"Automatic code graders, also called Programming Online Judges (OJ), can support students and instructors in introduction to programming courses (CS1). Using OJs in CS1, instructors select problems to compose assignment lists, whereas students submit their code solutions and receive instantaneous feedback. Whilst this process reduces the instructors' workload in evaluating students' code, selecting problems to compose assignments is arduous. Recently, recommender systems have been proposed by the literature to support OJ users. Nonetheless, there is a lack of recommenders fitted for CS1 courses and the ones found in the literature have not been assessed by the target users in a real educational scenario. It is worth noting that hybrid human\/AI systems are claimed to potentially surpass isolated human or AI. In this study, we adapted and evaluated a state-of-the-art hybrid human\/AI recommender to support CS1 instructors in selecting problems to compose variations of CS1 assignments. We compared data-driven measures (e.g., time students take to solve problems, number of logical lines of code, and hit rate) extracted from student logs whilst solving programming problems from assignments created by instructors versus when solving assignments in collaboration with an adaptation of cutting-edge hybrid\/AI method. As a result, employing a data analysis comparing experimental and control conditions using multi-level regressions, we observed that the recommender provided problems compatible with human-selected in all data-driven measures tested.","949":"Institute of Technology , , Artificial Intelligence Laboratory AI Memo 384 December 1976 Logo Memo 31 AI Based : Directions for","950":"The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its contributing fields. One conception of AI in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of AI, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems; and, the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual AI systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, AI in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.","951":"Today, machine learning underlies a range of applications we use every day, from product recommendations to voice recognition-as well as some we don't yet use everyday, including driverless cars. It is the basis of the new approach in computing where we do not write programs but collect data; the idea is to learn the algorithms for the tasks automatically from data. As computing devices grow more ubiquitous, a larger part of our lives and work is recorded digitally, and as Big Data has gotten bigger, the theory of machine learning -- the foundation of efforts to process that data into knowledge -- has also advanced. In this book, machine learning expert Ethem Alpaydin offers a concise overview of the subject for the general reader, describing its evolution, explaining important learning algorithms, and presenting example applications. Alpaydin offers an account of how digital technology advanced from number-crunching mainframes to mobile devices, putting today's machine learning boom in context. He describes the basics of machine learning and some applications; the use of machine learning algorithms for pattern recognition; artificial neural networks inspired by the human brain; algorithms that learn associations between instances, with such applications as customer segmentation and learning recommendations; and reinforcement learning, when an autonomous agent learns act so as to maximize reward and minimize penalty. Alpaydin then considers some future directions for machine learning and the new field of data science, and discusses the ethical and legal implications for data privacy and security.","952":"One long-term goal of machine learning research is to produce methods that\r\nare applicable to highly complex tasks, such as perception (vision, audition), rea-\r\nsoning, intelligent control, and other arti\ufb01cially intelligent behaviors. We argue\r\nthat in order to progress toward this goal, the Machine Learning community must\r\nendeavor to discover algorithms that can learn highly complex functions, with min-\r\nimal need for prior knowledge, and with minimal human intervention. We present\r\nmathematical and empirical evidence suggesting that many popular approaches\r\nto non-parametric learning, particularly kernel methods, are fundamentally lim-\r\nited in their ability to learn complex high-dimensional functions. Our analysis\r\nfocuses on two problems. First, kernel machines are shallow architectures, in\r\nwhich one large layer of simple template matchers is followed by a single layer\r\nof trainable coef\ufb01cients. We argue that shallow architectures can be very inef\ufb01-\r\ncient in terms of required number of computational elements and examples. Sec-\r\nond, we analyze a limitation of kernel machines with a local kernel, linked to the\r\ncurse of dimensionality, that applies to supervised, unsupervised (manifold learn-\r\ning) and semi-supervised kernel machines. Using empirical results on invariant\r\nimage recognition tasks, kernel methods are compared with deep architectures, in\r\nwhich lower-level features or concepts are progressively combined into more ab-\r\nstract and higher-level representations. We argue that deep architectures have the\r\npotential to generalize in non-local ways, i.e., beyond immediate neighbors, and\r\nthat this is crucial in order to make progress on the kind of complex tasks required\r\nfor arti\ufb01cial intelligence.","953":"Sharing data across disparate sources requires solving many problems of semantic integration, such as matching ontologies or schemas, detecting duplicate tuples, reconciling inconsistent data values, modeling complex relations between concepts in different sources, and reasoning with semantic mappings. This issue of AI Magazine includes papers that discuss various methods on establishing mappings between ontology elements or data fragments. The collection includes papers that discuss semantic-integration issues in such contexts as data integration and web services. The issue also includes a brief survey of semantic-integration research in the database community.","954":"With the advent of artificial intelligence (AI), individuals are increasingly teaming up with AI-based systems to enhance their creative collaborative performance. When working with AI-based systems, several aspects of team dynamics need to be considered, which raises the question how humans\u2019 approach and perceive their new teammates. In an experimental setting, we investigate the influence of social presence in a group ideation process with an AI-based teammate and examine its effects on the motivation to contribute. Our results show a multi-mediation model in which social presence indirectly influences whether human team members are motivated to contribute to a team with AI-based teammates, which is mediated by willingness to depend and team-oriented commitment.","955":"Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.","956":null,"957":"Computing must become much more human centered--for example, by presenting personalized information to users and by respecting personal preferences when controlling multiple devices or invoking various services. Appropriate representation of the information's semantics and of the functionality of devices and services will be critical to such personalized computing. Symbolic artificial intelligence techniques provide the method of choice for the required semantic-representation and reasoning capabilities.","958":null,"959":null,"960":null,"961":null,"962":null,"963":"Designing useful human-AI interaction for clinical workflows remains challenging despite the impressive performance of recent AI models. One specific difficulty is a lack of successful examples demonstrating how to achieve safe and efficient workflows while mitigating AI imperfections. In this paper, we present an interactive AI-powered visual search tool that supports pathologists in cancer assessments. Our evaluation with six pathologists demonstrates that it can 1) reduce time needed with maintained quality, 2) build user trust progressively, and 3) learn and improve from use. We describe our iterative design process, model development, and key features. Through interviews, design choices are related to the overall user experience. Implications for future human-AI interaction design are discussed with respect to trust, explanations, learning from use, and collaboration strategies.","964":null,"965":null,"966":null,"967":null,"968":null,"969":null,"970":null,"971":null,"972":null,"973":null,"974":null,"975":null,"976":null,"977":null,"978":null,"979":null,"980":"The inclusion of AI in an interactive system brings many potential benefits, but there are also potential side effects for the usability of the systems that ought to be taken into account from the very start. This article offers a systematic method for analyzing such usability side effects, the goal being to provide a solid basis for decisions about how to avoid or mitigate them. The analysis schema is applied in turn to nine classes of side effect. Many ideas that have been discussed in earlier literature are synthesized within this framework, which also brings into focus some concepts and issues that have received little attention so far.","981":null,"982":null,"983":null,"984":null,"985":null,"986":"Advances in artificial intelligence and machine learning have led to a steep rise in the adoption of AI to augment or support human decision-making across domains. There has been an increasing body of work addressing the benefits of model interpretability and explanations to help end-users or other stakeholders decipher the inner workings of the so-called \u201dblack box AI systems\u201d. Yet, little is currently understood about the role of modalities through which explanations can be communicated (e.g., text, visualizations, or audio) to inform, augment, and shape human decision-making. In our work, we address this research gap through the lens of a credibility assessment system. Considering the deluge of information available through various channels, people constantly make decisions while considering the perceived credibility of the information they consume. However, with an increasing information overload, assessing the credibility of the information we encounter is a non-trivial task. To help users in this task, automated credibility assessment systems have been devised as decision support systems in various contexts (e.g., assessing the credibility of news or social media posts). However, for these systems to be effective in supporting users, they need to be trusted and understood. Explanations have been shown to play an essential role in informing users\u2019 reliance on decision support systems. In this paper, we investigate the influence of explanation modalities on an AI-assisted credibility assessment task. We use a between-subjects experiment (N = 375), spanning six different explanation modalities, to evaluate the role of explanation modality on the accuracy of AI-assisted decision outcomes, the perceived system trust among users, and system usability. Our results indicate that explanations play a significant role in shaping users\u2019 reliance on the decision support system and, thereby, the accuracy of decisions made. We found that users performed with higher accuracy while assessing the credibility of statements in the presence of explanations. We also found that users had a significantly harder time agreeing on statement credibility without explanations. With explanations present, text and audio explanations were more effective than graphic explanations. Additionally, we found that combining graphical with text and\/or audio explanations were significantly effective. Such combinations of modalities led to a higher user performance than using graphical explanations alone.","987":"Reviews the book \"Experience AI: A Practitioner's Guide to Integrating Appreciative Inquiry With Experiential Learning,\" by M. W. Ricketts and J. E. Willis.; Reviews the book \"Experience AI: A Practitioner's Guide to Integrating Appreciative Inquiry With Experiential Learning,\" by M. W. Ricketts and J. E. Willis.","988":"The idea of temporal abstraction, i.e. learning, planning and representing the world at multiple time scales, has been a constant thread in AI research, spanning sub-fields from classical planning and search to control and reinforcement learning. For example, programming a robot typically involves making decisions over a set of controllers, rather than working at the level of motor torques. While temporal abstraction is a very natural concept, learning such abstractions with no human input has proved quite daunting. In this paper, we present a general architecture, called option-critic, which allows learning temporal abstractions automatically, end-to-end, simply from the agent's experience. This approach allows continual learning and provides interesting qualitative and quantitative results in several tasks.","989":null,"990":null,"991":"This special issue of AI Magazine on dialog with robots brings together a collection of articles on situated dialog. The contributing authors have been working in interrelated fields of human-robot interaction, dialog systems, virtual agents, and other related areas and address core concepts in spoken dialog with embodied robots or agents. Several of the contributors participated in the AAAI Fall Symposium on Dialog with Robots, held in November 2010, and several articles in this issue are extensions of work presented there. Others include invited contributions. The articles in this collection address diverse aspects of dialog with robots, but are unified in addressing opportunities with spoken language interaction, physical embodiment, and enriched representations of context.","992":null,"993":"In this article, I will argue in favor of both the ethical and epistemological utility of explanations in artificial intelligence (AI)-based medical technology. I will build on the notion of \u201cexplicability\u201d due to Floridi, which considers both the intelligibility and accountability of AI systems to be important for truly delivering AI-powered services that strengthen autonomy, beneficence, and fairness. I maintain that explicable algorithms do, in fact, strengthen these ethical principles in medicine, e.g., in terms of direct patient\u2013physician contact, as well as on a longer-term epistemological level by facilitating scientific progress that is informed through practice. With this article, I will therefore attempt to counter arguments against demands for explicable AI in medicine that are based on a notion of \u201cwhatever heals is right.\u201d I will elucidate my elaboration on the positive aspects of explicable AI in medicine as well as by pointing out risks of non-explicable AI.","994":null,"995":null,"996":null,"997":null,"998":"Recent years have seen important advances in the quality of state-of-the-art\r\nmodels, but this has come at the expense of models becoming less interpretable.\r\nThis survey presents an overview of the current state of Explainable AI (XAI),\r\nconsidered within the domain of Natural Language Processing (NLP). We discuss\r\nthe main categorization of explanations, as well as the various ways\r\nexplanations can be arrived at and visualized. We detail the operations and\r\nexplainability techniques currently available for generating explanations for\r\nNLP model predictions, to serve as a resource for model developers in the\r\ncommunity. Finally, we point out the current gaps and encourage directions for\r\nfuture work in this important research area.","999":"The mythological tale of Sagar Manthan, or the churning of the ocean, holds deeper parallels with modern deep sea mining endeavours. This paper explores how the narrative of Sagar Manthan can be metaphorically applied to the context of deep sea mining in todays world. By drawing comparisons between the extraction of Amrita nectar of immortality and valuable minerals from the ocean floor, we examine the applications of deep sea mining technologies and the potential rewards it offers in meeting the global demand for critical minerals. Deep sea mining has emerged as a potential solution to meet the ever increasing global demand for valuable metals and minerals. With depleting terrestrial reserves and growing demand for these resources, exploiting the vast mineral wealth lying on the ocean floor has become an attractive proposition. However, the practice of deep sea mining raises significant environmental, economic, and technological challenges. This paper provides an in depth analysis of the concept of deep sea mining, exploring its potential benefits and drawbacks, as well as the current state of technology and its impact on the marine environment. Manish Verma \"Deep Sea Mining: Environment, Economic and Hindu Technological Perspectives with AI Chatbots Analysis\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-4, August 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd59715.pdf Paper Url:https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59715\/deep-sea-mining-environment-economic-and-hindu-technological-perspectives-with-ai-chatbots-analysis\/manish-verma","1000":null,"1001":null,"1002":null,"1003":null,"1004":null,"1005":"The Companion cognitive architecture is aimed at reaching human-level AI by creating software social organisms, systems that interact with people using natural modalities, working and learning over extended periods of time as collaborators rather than tools. Our two central hypotheses about how to achieve this are (1) analogical reasoning and learning are central to cognition, and (2) qualitative representations provide a level of description that facilitates reasoning, learning, and communication. This paper discusses the evidence we have gathered supporting these hypotheses from our experiments with the Companion architecture. Although we are far from our ultimate goals, these experiments provide strong breadth for the utility of analogy and QR across a range of tasks. We also discuss three lessons learned and highlight three important open problems for cognitive systems research more broadly.","1006":null,"1007":null,"1008":null,"1009":null,"1010":null,"1011":null,"1012":null,"1013":null,"1014":null,"1015":null,"1016":null,"1017":null,"1018":null,"1019":null,"1020":null,"1021":null,"1022":null,"1023":null,"1024":null,"1025":null,"1026":null,"1027":null,"1028":null,"1029":null,"1030":null,"1031":null,"1032":null,"1033":null,"1034":null,"1035":null,"1036":null,"1037":null,"1038":null,"1039":null,"1040":"Morality is a fundamentally human trait which permeates all levels of human society, from basic etiquette and normative expectations of social groups, to formalized legal principles upheld by societies. Hence, future interactive AI systems, in particular, cognitive systems on robots deployed in human settings, will have to meet human normative expectations, for otherwise these system risk causing harm. While the interest in 'machine ethics' has increased rapidly in recent years, there are only very few current efforts in the cognitive systems community to investigate moral and ethical reasoning. And there is currently no cognitive architecture that has even rudimentary moral or ethical competence, i.e., the ability to judge situations based on moral principles such as norms and values and make morally and ethically sound decisions. We hence argue for the urgent need to instill moral and ethical competence in all cognitive system intended to be employed in human social contexts.","1041":"The purpose of this article is to begin the process of engaging the international research community in developing what can be called a standard model of the mind, where the mind we have in mind here is human-like. The notion of a standard model has its roots in physics, where over more than a half-century the international community has developed and tested a standard model that combines much of what is known about particles. This model is assumed to be internally consistent, yet still have major gaps. Its function is to serve as a cumulative reference point for the field while also driving efforts to both extend and break it.","1042":"A computer has beaten a human professional for the first time at Go \u2014 an ancient board game that has long been viewed as one of the greatest challenges for artificial intelligence (AI).","1043":null,"1044":null,"1045":null,"1046":null,"1047":null,"1048":"Although knowledge representation is one of the central and in some ways most familiar concepts in AI, the most fundamental question about it--What is it?--has rarely been answered directly. Numerous papers have lobbied for one or another variety of representation, other papers have argued for various properties a representation should have, while still others have focused on properties that are important to the notion of representation in general.\r\n\r\nIn this paper we go back to basics to address the question directly. We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and at times conflicting demands on the properties a representation should have.\r\n\r\nWe argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field.","1049":null,"1050":null,"1051":null,"1052":null,"1053":null,"1054":null,"1055":null,"1056":null,"1057":null,"1058":null,"1059":null,"1060":null,"1061":null,"1062":null,"1063":null,"1064":null,"1065":null,"1066":null,"1067":null,"1068":"Although knowledge representation is one of the central and, in some ways, most familiar concepts in AI, the most fundamental question about it -- What is it? -- has rarely been answered directly. Numerous papers have lobbied for one or another variety of representation, other papers have argued for various properties a representation should have, and still others have focused on properties that are important to the notion of representation in general. In this article, we go back to basics to address the question directly. We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and, at times, conflicting demands on the properties a representation should have. We argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field.","1069":"Voice assistants\u2019 (VAs) increasingly nuanced and natural communication via artificial intelligence (AI) opens up new opportunities for the experience of users, providing task assistance and automation possibilities, and also offer an easy interface to digital services and ecosystems. However, VAs and according ecosystems face various problems, such as low adoption and satisfaction rates as well as other negative reactions from users. Companies, therefore, need to consider what contributes to user satisfaction of VAs and related conversational AI ecosystems. Key for conversational AI ecosystems is the consideration of trust due to their agentic and pervasive nature. Nonetheless, due to the complexity of conversational AI ecosystems and different trust sources involved, we argue that we need a more detailed understanding about trust. Thus, we propose a configurational view on conversational AI ecosystems that allows us to disentangle the complex and interrelated factors that contribute to trust in VAs. We examine with a configurational approach and a survey study, how different trust sources contribute to the outcomes of conversational AI ecosystems, i.e., in our case user satisfaction. The results of our study show four distinct patterns of trust source configurations. Vice versa, we show how trust sources contribute to the absence of the outcome, i.e., user satisfaction. The derived implications provide a configurative theoretical understanding for the role of trust sources for user satisfaction that provides practitioners useful guidance for more trustworthy conversational AI ecosystems.","1070":"We have conducted a study in a large telecommunication company in Turkey to employ a software measurement program and to predict pre-release defects. We have previously built such predictors using AI techniques. This project is a transfer of our research experience into a real life setting to solve a specific problem for the company: to improve code quality by predicting pre-release defects and efficiently allocating testing resources. Our results in this project have many practical implications that managers have started benefiting: code analysis, bug tracking, effective use of version management system and defect prediction. Using version history information, developers can find around 88% of the defects with 28% false alarms, compared to same detection rate with 50% false alarms without using historical data. In this paper we also shared in detail our experience in terms of the project steps (i.e. challenges and opportunities).","1071":null,"1072":null,"1073":"Every Organization will have their own standards and work formats, to establish their standards among employees and other users, the organization will create policies. These policies need to be maintained properly and need to be retrieved whenever needed. The main difficulties in the process of creating policies are, getting approval from approvers authorities and keeping track of approval status, and extracting keywords from the policies for easier search and future retrieval. The measures to overcome these difficulties are discussed in this paper. The main aim of this paper is to explore the ways to create a modern workspace based policy management with multiple level authoring and approval with both publishing and consumption views, in order to overcome the inefficient and complex methods of maintaining and managing documents across the organization. This system has multi level approval workflow, AI based records management and use of Microsoft Office 365 Graph APIs to take care of regulatory functions based on certain business criteria. Poornima S | Muthukumarasamy S \"Modern Workspace-Based Policy Management with Automated Keyword Extraction and AI Based Records Management using Azure Cognitive Services\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31310.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/other\/31310\/modern-workspacebased-policy-management-with-automated-keyword-extraction-and-ai-based-records-management-using-azure-cognitive-services\/poornima-s","1074":null,"1075":null,"1076":null,"1077":null,"1078":null,"1079":null,"1080":null,"1081":"Genetic programming (GP) is an automatic programming\n                 technique that has recently been applied to a wide\n                 range of problems including blocks-world planning. This\n                 paper describes a series of illustrative experiments in\n                 which GP techniques are applied to traditional\n                 blocks-world planning problems. We discuss genetic\n                 planning in the context of traditional AI planning\n                 systems, and comment on the costs and benefits to be\n                 expected from further work.","1082":null,"1083":null,"1084":null,"1085":null,"1086":null,"1087":null,"1088":null,"1089":null,"1090":null,"1091":null,"1092":null,"1093":null,"1094":null,"1095":null,"1096":null,"1097":null,"1098":null,"1099":null,"1100":"How will AI evolve and what major innovations are on the horizon?\nWhat will its impact be on the job market, economy, and society?\nWhat is the path toward human-level machine intelligence?\nWhat should we be concerned about as artificial intelligence advances?\nArchitects of Intelligence contains a series of in-depth, one-to-one interviews where New York Times bestselling author, Martin Ford, uncovers the truth behind these questions from some of the brightest minds in the Artificial Intelligence community.\nMartin has wide-ranging conversations with twenty-three of the world's foremost researchers and entrepreneurs working in AI and robotics: Demis Hassabis (DeepMind), Ray Kurzweil (Google), Geoffrey Hinton (Univ. of Toronto and Google), Rodney Brooks (Rethink Robotics), Yann LeCun (Facebook) , Fei-Fei Li (Stanford and Google), Yoshua Bengio (Univ. of Montreal), Andrew Ng (AI Fund), Daphne Koller (Stanford), Stuart Russell (UC Berkeley), Nick Bostrom (Univ. of Oxford), Barbara Grosz (Harvard), David Ferrucci (Elemental Cognition), James Manyika (McKinsey), Judea Pearl (UCLA), Josh Tenenbaum (MIT), Rana el Kaliouby (Affectiva), Daniela Rus (MIT), Jeff Dean (Google), Cynthia Breazeal (MIT), Oren Etzioni (Allen Institute for AI), Gary Marcus (NYU), and Bryan Johnson (Kernel).","1101":null,"1102":"The Novamente AI Engine is a novel software architecture that, unlike most \r\ncontemporary AI projects, is specifically oriented towards artificial general intelligence (AGI), rather than being restricted by design to one particular domain, or narrow range of cognitive functions.\r\nNovamente integrates aspects of prior AI projects and approaches, including symbolic, neural-network, evolutionary programming and reinforcement learning.  \r\nHowever, its overall architecture is unique, drawing on system-theoretic ideas regarding complex mental dynamics and associated emergent patterns.  Thus Novamente addresses the problem of \"creating a whole mind\" in a novel way through this integrative \r\nmechanism...","1103":"In the last decade, machine learning and artificial intelligence applications\r\nhave received a significant boost in performance and attention in both academic\r\nresearch and industry. The success behind most of the recent state-of-the-art\r\nmethods can be attributed to the latest developments in deep learning. When\r\napplied to various scientific domains that are concerned with the processing of\r\nnon-tabular data, for example, image or text, deep learning has been shown to\r\noutperform not only conventional machine learning but also highly specialized\r\ntools developed by domain experts. This review aims to summarize AI-based\r\nresearch for GPCR bioactive ligand discovery with a particular focus on the\r\nmost recent achievements and research trends. To make this article accessible\r\nto a broad audience of computational scientists, we provide instructive\r\nexplanations of the underlying methodology, including overviews of the most\r\ncommonly used deep learning architectures and feature representations of\r\nmolecular data. We highlight the latest AI-based research that has led to the\r\nsuccessful discovery of GPCR bioactive ligands. However, an equal focus of this\r\nreview is on the discussion of machine learning-based technology that has been\r\napplied to ligand discovery in general and has the potential to pave the way\r\nfor successful GPCR bioactive ligand discovery in the future. This review\r\nconcludes with a brief outlook highlighting the recent research trends in deep\r\nlearning, such as active learning and semi-supervised learning, which have\r\ngreat potential for advancing bioactive ligand discovery.","1104":"There is a saying\u2014attributed to Norman Adams\u2014that \u00d6bjects are a poor man's closures.\" In this article we discuss what closures are and how objects and closures are related, show code samples to make these\nabstract ideas concrete, and implement a Scheme Object System which solves the problems we uncover along the way.","1105":"Agent-based systems technology has generated lots of excitement in recent years because of its promise as a new paradigm for conceptualizing, designing, and implementing software systems. This promise is particularly attractive for creating software that operates in environments that are distributed and open, such as the internet. Currently, the great majority of agent-based systems consist of a single agent. However, as the technology matures and addresses increasingly complex applications, the need for systems that consist of multiple agents that communicate in a peer-to-peer fashion is becoming apparent. Central to the design and effective operation of such multiagent systems (MASs) are a core set of issues and research questions that have been studied over the years by the distributed AI community. In this article, I present some of the critical notions in MASs and the research work that has addressed them. I organize these notions around the concept of problem-solving coherence, which I believe is one of the most critical overall characteristics that an MAS should exhibit.","1106":null,"1107":null,"1108":null,"1109":null,"1110":null,"1111":null,"1112":"Innovation requires organizations to tap into the knowledge and creativity of teams. However, teams are confronted with massive amounts of data and information, necessitating a broad set of knowledge, methodologies, and approaches to solve problems and innovate. Consequently, team composition has become a critical challenge. Recent advances in artificial intelligence (AI) may assist in addressing this challenge. As AI is permeating both business and private sectors, organizational teams may be augmented with AI team members. However, given the nascent nature of this phenomenon, little is known about the specific roles and requirements for such AI teammates. Within an interview study we discover common challenges in teams and identify recurring capability gaps of participants and behaviors that negatively impact the team's collective performance. Based on our findings, we propose requirements for AI-based teammates to address these gaps and support beneficial collaboration between humans and AI in teams.","1113":"In this paper we describe how we are exploiting AItechnologies to infuse workflow systems with adaptivecapabilities. This work is part of an ongoing appliedresearch programme between AIAI and a number ofindustrial and academic partners. We begin by presentingthe requirements of adaptive workflow within a taxonomyconsisting of the layers of domain, process, agents,organisation, and infrastructure. We then show how eachlevel can be substantially addressed with AI technologies.Specifically, infrastructure adaptation is addressed withmulti-agent toolkits, agent adaptation through knowledgebasedcapability matching, organisational adaptationthrough authority based capability matching, processadaptation through AI planning and executionarchitectures, and domain adaptation through rationalecapture. We conclude by identifying important challengesfor further work as being the improvement of rationalecapture and the support for the evolution of the processmodels that underlie ex...","1114":"Recent advances in the performance of machine learning algorithms have led to the adoption of AI models in decision making contexts across various domains such as healthcare, finance, and education. Different research communities have attempted to optimize and evaluate human-AI team performance through empirical studies by increasing transparency of AI systems, or providing explanations to aid human understanding of such systems. However, the variety in decision making tasks considered and their operationalization in prior empirical work, has led to an opacity around how findings from one task or domain carry forward to another. The lack of a standardized means of considering task attributes prevents straightforward comparisons across decision tasks, thereby limiting the generalizability of findings. We argue that the lens of \u2018task complexity\u2019 can be used to tackle this problem of under-specification and facilitate comparison across empirical research in this area. To retrospectively explore how different HCI communities have considered the influence of task complexity in designing experiments in the realm of human-AI decision making, we survey literature and provide an overview of empirical studies on this topic. We found a serious dearth in the consideration of task complexity across various studies in this realm of research. Inspired by Robert Wood\u2019s seminal work on the construct, we operationalized task complexity with respect to three dimensions (component, coordinative, and dynamic) and quantified the complexity of decision tasks in existing work accordingly. We then summarized current trends and proposed research directions for the future. Our study highlights the need to account for task complexity as an important design choice. This is a first step to help the scientific community in drawing meaningful comparisons across empirical studies in human-AI decision making and to provide opportunities to generalize findings across diverse domains and experimental settings.","1115":null,"1116":null,"1117":null,"1118":null,"1119":"This paper presents an integrated system for emotion detection using facial detection and recognition. we have taken into account the fact that emotions are most widely represented with eyes and mouth expressions. In this research effort, we implement a general convolutional neural network CNN building framework for designing real time CNNs. We validate our models by creating a real time vision system that accomplishes the tasks of face detection, emotion classification, and generating the content according to the emotion or mood of the person simultaneously in one blended step using our proposed CNN architecture. Our proposed model consisted of modules such as image processing, Feature extraction, feature classification, and recommendation process. The images used in the experiment are pre processed with various image processing methods like canny edge detection, histogram equalization, fit ellipse, and FER dataset is mediated for conducting the experiments. With a trained profile that can be updated flexibly, a user can detect his her behavior on a real time basis. It utilizes the state of the art of face detection and recognition algorithms. Sanket Godbole | Jaivardhan Shelke \u00c4I Therapist \u2013 Emotion Detection using Facial Detection and Recognition & Showing Content According to Emotions\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33267.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/33267\/ai-therapist-\u2013-emotion-detection-using-facial-detection-and-recognition-and-showing-content-according-to-emotions\/sanket-godbole","1120":null,"1121":null,"1122":null,"1123":null,"1124":"This article reports the findings of AI4People, an Atomium---EISMD initiative\u00a0designed to lay the foundations for a ``Good AI Society''. We introduce the core opportunities and risks of AI for society; present a synthesis of five ethical principles that should undergird its development and adoption; and offer 20 concrete recommendations---to assess, to develop, to incentivise, and to support good AI---which in some cases may be undertaken directly by national or supranational policy makers, while in others may be led by other stakeholders. If adopted, these recommendations would serve as a firm foundation for the establishment of a Good AI Society.","1125":null,"1126":null,"1127":null,"1128":null,"1129":null,"1130":null,"1131":null,"1132":null,"1133":"Customer support service employees are facing an increased workload, while artificial intelligence (AI) appears to possess the potential to change the way we work. With the advent of modern types of generative AI, new opportunities to augment frontline service employees have emerged. However, little is known about how to integrate generative AI in customer support service organizations and purposefully change service employee work routines. Following multi-method qualitative research, we performed a literature review, conducted workshops, and interviewed IT support agents, managers, and AI experts. Thereby, we examine AI augmentation for frontline service employees in the context of IT support to carve out where and how GenAI can be leveraged to develop more efficient and higher-quality customer support. Our resulting framework reveals that especially adapting solutions and retaining knowledge is subject to a high degree of AI augmentation.","1134":null,"1135":null,"1136":null,"1137":null,"1138":null,"1139":null,"1140":null,"1141":null,"1142":null,"1143":null,"1144":null,"1145":null,"1146":null,"1147":null,"1148":null,"1149":null,"1150":null,"1151":null,"1152":null,"1153":null,"1154":null,"1155":null,"1156":null,"1157":null,"1158":null,"1159":null,"1160":null,"1161":null,"1162":null,"1163":null,"1164":null,"1165":null,"1166":null,"1167":null,"1168":null,"1169":null,"1170":null,"1171":null,"1172":null,"1173":null,"1174":null,"1175":null,"1176":null,"1177":null,"1178":null,"1179":null,"1180":null,"1181":null,"1182":null,"1183":null,"1184":null,"1185":null,"1186":null,"1187":null,"1188":null,"1189":null,"1190":null,"1191":null,"1192":null,"1193":null,"1194":null,"1195":null,"1196":null,"1197":null,"1198":null,"1199":null,"1200":null,"1201":null,"1202":null,"1203":null,"1204":null,"1205":null,"1206":null,"1207":null,"1208":null,"1209":null,"1210":null,"1211":null,"1212":null,"1213":null,"1214":null,"1215":null,"1216":null,"1217":null,"1218":null,"1219":null,"1220":null,"1221":null,"1222":null,"1223":null,"1224":null,"1225":null,"1226":null,"1227":null,"1228":null,"1229":null,"1230":null,"1231":null,"1232":null,"1233":null,"1234":null,"1235":null,"1236":null,"1237":null,"1238":null,"1239":null,"1240":null,"1241":null,"1242":null,"1243":null,"1244":null,"1245":null,"1246":null,"1247":null,"1248":null,"1249":null,"1250":null,"1251":null,"1252":null,"1253":null,"1254":null,"1255":null,"1256":null,"1257":null,"1258":null,"1259":null,"1260":null,"1261":null,"1262":null,"1263":null,"1264":null,"1265":null,"1266":null,"1267":null,"1268":null,"1269":null,"1270":null,"1271":null,"1272":null,"1273":null,"1274":null,"1275":null,"1276":null,"1277":null,"1278":null,"1279":null,"1280":null,"1281":null,"1282":null,"1283":null,"1284":null,"1285":null,"1286":null,"1287":null,"1288":null,"1289":null,"1290":null,"1291":null,"1292":null,"1293":null,"1294":null,"1295":null,"1296":null,"1297":null,"1298":null,"1299":null,"1300":null,"1301":null,"1302":null,"1303":null,"1304":null,"1305":null,"1306":null,"1307":null,"1308":null,"1309":null,"1310":null,"1311":null,"1312":null,"1313":null,"1314":null,"1315":null,"1316":null,"1317":null,"1318":null,"1319":null,"1320":null,"1321":null,"1322":null,"1323":null,"1324":null,"1325":null,"1326":null,"1327":null,"1328":null,"1329":null,"1330":null,"1331":null,"1332":null,"1333":null,"1334":null,"1335":null,"1336":null,"1337":null,"1338":null,"1339":null,"1340":null,"1341":null,"1342":null,"1343":null,"1344":null,"1345":null,"1346":null,"1347":null,"1348":null,"1349":null,"1350":null,"1351":null,"1352":null,"1353":null,"1354":null,"1355":null,"1356":null,"1357":null,"1358":null,"1359":null,"1360":"During the last two years there has been a plethora of large generative\r\nmodels such as ChatGPT or Stable Diffusion that have been published.\r\nConcretely, these models are able to perform tasks such as being a general\r\nquestion and answering system or automatically creating artistic images that\r\nare revolutionizing several sectors. Consequently, the implications that these\r\ngenerative models have in the industry and society are enormous, as several job\r\npositions may be transformed. For example, Generative AI is capable of\r\ntransforming effectively and creatively texts to images, like the DALLE-2\r\nmodel; text to 3D images, like the Dreamfusion model; images to text, like the\r\nFlamingo model; texts to video, like the Phenaki model; texts to audio, like\r\nthe AudioLM model; texts to other texts, like ChatGPT; texts to code, like the\r\nCodex model; texts to scientific texts, like the Galactica model or even create\r\nalgorithms like AlphaTensor. This work consists on an attempt to describe in a\r\nconcise way the main models are sectors that are affected by generative AI and\r\nto provide a taxonomy of the main generative models published recently.","1361":null,"1362":null,"1363":null,"1364":null,"1365":null,"1366":null,"1367":null,"1368":null,"1369":null,"1370":null,"1371":null,"1372":null,"1373":null,"1374":"AI based data is supposed to be the subset of human general intelligence patterns, trends, opinions, or biases with an impact on the socio digital imprints of human activity. AI based data is supposed to mimic the digital version of karmas in the avatars in web 3.0 and it is supposed to be the mining of the aspects of big data of the associated concepts in daily routine. AI based data can significantly improve the content and applicability of context with sustainable objectives being modified with ease of linguistic convergence. The digital library concepts of 24 hours x 7 days of continuous voluntary activity of data sharing and retrieval with digital search with various activities is being significantly improved by the introduction of AI based digital chatbots with data veracity. Manish Verma \"Novel Study on AI-Based Chatbot (ChatGPT) Impacts on the Traditional Library Management\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-1 , February 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd52767.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/52767\/novel-study-on-aibased-chatbot-chatgpt-impacts-on-the-traditional-library-management\/manish-verma","1375":null,"1376":null,"1377":null,"1378":null,"1379":null,"1380":null,"1381":null,"1382":null,"1383":null,"1384":null,"1385":null,"1386":null,"1387":null,"1388":null,"1389":null,"1390":null,"1391":null,"1392":null,"1393":null,"1394":null,"1395":null,"1396":null,"1397":null,"1398":null,"1399":null,"1400":null,"1401":null,"1402":null,"1403":null,"1404":null,"1405":null,"1406":null,"1407":null,"1408":null,"1409":null,"1410":null,"1411":null,"1412":null,"1413":null,"1414":null,"1415":null,"1416":null,"1417":null,"1418":null,"1419":null,"1420":null,"1421":null,"1422":null,"1423":null,"1424":null,"1425":null,"1426":null,"1427":null,"1428":null,"1429":null,"1430":null,"1431":null,"1432":null,"1433":null,"1434":null,"1435":null,"1436":null,"1437":null,"1438":null,"1439":null,"1440":null,"1441":null,"1442":null,"1443":null,"1444":null,"1445":null,"1446":null,"1447":null,"1448":null,"1449":null,"1450":null,"1451":null,"1452":null,"1453":null,"1454":null,"1455":null,"1456":null,"1457":null,"1458":null,"1459":null,"1460":null,"1461":null,"1462":null,"1463":null,"1464":null,"1465":null,"1466":null,"1467":null,"1468":null,"1469":null,"1470":null,"1471":null,"1472":null,"1473":null,"1474":null,"1475":null,"1476":null,"1477":null,"1478":null,"1479":null,"1480":null,"1481":null,"1482":null,"1483":null,"1484":null,"1485":null,"1486":null,"1487":null,"1488":null,"1489":null,"1490":null,"1491":null,"1492":null,"1493":null,"1494":null,"1495":null,"1496":null,"1497":null,"1498":null,"1499":null,"1500":null,"1501":null,"1502":null,"1503":null,"1504":null,"1505":null,"1506":null,"1507":null,"1508":null,"1509":null,"1510":null,"1511":null,"1512":null,"1513":null,"1514":null,"1515":null,"1516":null,"1517":null,"1518":null,"1519":null,"1520":null,"1521":null,"1522":null,"1523":null,"1524":null,"1525":null,"1526":null,"1527":null,"1528":null,"1529":null,"1530":null,"1531":null,"1532":null,"1533":null,"1534":null,"1535":null,"1536":null,"1537":null,"1538":null,"1539":null,"1540":null,"1541":null,"1542":null,"1543":null,"1544":null,"1545":null,"1546":null,"1547":null,"1548":null,"1549":null,"1550":null,"1551":null,"1552":null,"1553":null,"1554":null,"1555":null,"1556":null,"1557":null,"1558":null,"1559":null,"1560":null,"1561":null,"1562":null,"1563":null,"1564":null,"1565":null,"1566":null,"1567":null,"1568":null,"1569":null,"1570":null,"1571":null,"1572":null,"1573":null,"1574":null,"1575":null,"1576":null,"1577":null,"1578":null,"1579":null,"1580":null,"1581":null,"1582":null,"1583":null,"1584":null,"1585":null,"1586":null,"1587":null,"1588":null,"1589":null,"1590":null,"1591":null,"1592":null,"1593":null,"1594":null,"1595":null,"1596":null,"1597":null,"1598":null,"1599":null,"1600":null,"1601":null,"1602":null,"1603":null,"1604":null,"1605":null,"1606":null,"1607":null,"1608":null,"1609":null,"1610":null,"1611":null,"1612":null,"1613":null,"1614":null,"1615":null,"1616":null,"1617":null,"1618":null,"1619":null,"1620":null,"1621":null,"1622":null,"1623":null,"1624":null,"1625":null,"1626":null,"1627":null,"1628":null,"1629":null,"1630":null,"1631":null,"1632":null,"1633":null,"1634":null,"1635":null,"1636":null,"1637":null,"1638":null,"1639":null,"1640":null,"1641":null,"1642":null,"1643":null,"1644":null,"1645":null,"1646":null,"1647":null,"1648":null,"1649":null,"1650":null,"1651":null,"1652":null,"1653":null,"1654":null,"1655":null,"1656":"Ein wichtiges Merkmal von Systemen mit k\u00fcnstlicher Intelligenz ist die F\u00e4higkeit, selbst\u00e4ndig zu lernen. Anders als bei klassischer Software, die Probleme und Fragen auf Basis von vorher festgelegten Regeln abarbeitet, k\u00f6nnen selbstlernende Machine Learning Algorithmen die besten Regeln f\u00fcr die L\u00f6su","1657":"It's well known that 80\\% of the effort of a data scientist is spent on data preparation. Semantic integration is arguably the best way to spend this effort more efficiently and to reuse it between tasks, projects and organizations. Knowledge Graphs (KG) and Linked Open Data (LOD) have become very popular recently. They are used by Google, Amazon, Bing, Samsung, Springer Nature, Microsoft Academic, AirBnb\u2026 and any large enterprise that would like to have a holistic (360 degree) view of its business. The Semantic Web (web 3.0) is a way to build a Giant Global Graph, just like the normal web is a Global Web of Documents. IEEE already talks about Big Data Semantics. We review the topic of KGs and their applicability to Machine Learning.","1658":"DescriptionContentsResourcesCoursesAbout the Authors\r\nThe fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book's web site.","1659":"Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package----PMTK (probabilistic modeling toolkit)---that is freely available online.","1660":"This work offers a grounding in machine learning concepts combined with practical advice on applying machine learning tools and techniques in real-world data mining situations.","1661":"The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. Subjects include supervised learning; Bayesian decision theory; parametric, semi-parametric, and nonparametric methods; multivariate analysis; hidden Markov models; reinforcement learning; kernel machines; graphical models; Bayesian estimation; and statistical testing. Machine learning is rapidly becoming a skill that computer science students must master before graduation. The third edition of Introduction to Machine Learning reflects this shift, with added support for beginners, including selected solutions for exercises and additional example data sets (with code available online). Other substantial changes include discussions of outlier detection; ranking algorithms for perceptrons and support vector machines; matrix decomposition and spectral methods; distance estimation; new kernel algorithms; deep learning in multilayered perceptrons; and the nonparametric approach to Bayesian methods.","1662":"Machine learning has shown great potential applications in material science. It is widely used in material design, corrosion detection, material screening, new material discovery, and other fields of materials science. The majority of ML approaches in materials science is based on artificial neural networks ANNs . The use of ML and related techniques for materials design, development, and characterization has matured to a main stream field. This paper focuses on the applications of machine learning strategies for material characterization. Matthew N. O. Sadiku | Guddi K. Suman | Sarhan M. Musa \"Machine Learning in Material Characterization\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-6 , October 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd46392.pdf Paper URL : https:\/\/www.ijtsrd.com\/engineering\/electrical-engineering\/46392\/machine-learning-in-material-characterization\/matthew-n-o-sadiku","1663":null,"1664":"This book provides a general introduction to machine learning. It covers fundamental modern topics in machine learning while providing the theoretical basis and conceptual tools needed for the discussion and justification of algorithms. It also describes several key aspects of the application of these algorithms. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. This book is unique in its focus on the analysis and theory of algorithms. The first four chapters lay the theoretical foundation for what follows; subsequent chapters are mostly self-contained. Topics covered include the Probably Approximately Correct (PAC) learning framework; generalization bounds based on Rademacher complexity and VC-dimension; Support Vector Machines (SVMs); kernel methods; boosting; on-line learning; multi-class classification; ranking; regression; algorithmic stability; dimensionality reduction; learning automata and languages; and reinforcement learning. This second edition offers three new chapters, on model selection, maximum entropy models, and conditional entropy models. New material in the appendixes includes a major section on Fenchel duality, expanded coverage of concentration inequalities, and an entirely new entry on information theory.","1665":"<p class=\u00e4bstract\">\r\n\r\nDerivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply \u00e2\u0080\u009cautodiff\u00e2\u0080\u009d, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names \u00e2\u0080\u009cdynamic computational graphs\u00e2\u0080\u009d and \u00e2\u0080\u009cdifferentiable programming\u00e2\u0080\u009d. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms \u00e2\u0080\u009cautodiff\u00e2\u0080\u009d, \u00e2\u0080\u009cautomatic differentiation\u00e2\u0080\u009d, and \u00e2\u0080\u009csymbolic differentiation\u00e2\u0080\u009d as these are encountered more and more in machine learning settings.\r\n\r\n<\/p>\r\n<font color=\"gray\"><p>abs<\/font><a id=\"pdf\" target=\"_blank\" href=\"\/papers\/volume18\/17-468\/17-468.pdf\">pdf<\/a><a id=\"bib\" href=\"\/papers\/v18\/17-468.bib\">bib<\/a>\r\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\r\n\r\n\r\n\r\n                      \r\n    <table width=\"100%\"> <tr>\r\n    <td\r\n    align=\"right\">","1666":"Ein wichtiges Merkmal von Systemen mit k\u00fcnstlicher Intelligenz ist die F\u00e4higkeit, selbst\u00e4ndig zu lernen. Anders als bei klassischer Software, die Probleme und Fragen auf Basis von vorher festgelegten Regeln abarbeitet, k\u00f6nnen selbstlernende Machine Learning Algorithmen die besten Regeln f\u00fcr die L\u00f6su","1667":"Specifically designed for non-mathematicians, this useful guide presents a breakdown of each variant of machine learning, with examples and working code. You'll learn the various algorithms, data preparation techniques, trees, and networks, and get acquainted with the tools that help you get more from your data. Learn the languages of machine learning: Weka, Mahout, Spark, and R. Make the right data storage and cleaning decisions, tailored to your desired output. Understand decision trees, Bayesian networks, artificial neural networks, and association rule learning. Implement support vector machines knowing the relevant advantages and limitations. Apply Big Data processing techniques with Hadoop, Mahout, and MapReduce. Use Spring XD to capture streaming data and learn in real time. Access the tools you need to plan your project and acquire and process data.","1668":"Methods based on machine learning have recently made substantial inroads in\r\nmany corners of cosmology. Through this process, new computational tools, new\r\nperspectives on data collection, model development, analysis, and discovery, as\r\nwell as new communities and educational pathways have emerged. Despite rapid\r\nprogress, substantial potential at the intersection of cosmology and machine\r\nlearning remains untapped. In this white paper, we summarize current and\r\nongoing developments relating to the application of machine learning within\r\ncosmology and provide a set of recommendations aimed at maximizing the\r\nscientific impact of these burgeoning tools over the coming decade through both\r\ntechnical development as well as the fostering of emerging communities.","1669":"The demand for artificial intelligence has grown significantly over the last\r\ndecade and this growth has been fueled by advances in machine learning\r\ntechniques and the ability to leverage hardware acceleration. However, in order\r\nto increase the quality of predictions and render machine learning solutions\r\nfeasible for more complex applications, a substantial amount of training data\r\nis required. Although small machine learning models can be trained with modest\r\namounts of data, the input for training larger models such as neural networks\r\ngrows exponentially with the number of parameters. Since the demand for\r\nprocessing training data has outpaced the increase in computation power of\r\ncomputing machinery, there is a need for distributing the machine learning\r\nworkload across multiple machines, and turning the centralized into a\r\ndistributed system. These distributed systems present new challenges, first and\r\nforemost the efficient parallelization of the training process and the creation\r\nof a coherent model. This article provides an extensive overview of the current\r\nstate-of-the-art in the field by outlining the challenges and opportunities of\r\ndistributed machine learning over conventional (centralized) machine learning,\r\ndiscussing the techniques used for distributed machine learning, and providing\r\nan overview of the systems that are available.","1670":"To assist you with the entire farming operation, we use cutting edge machine learning and deep learning technologies. Make educated decisions about your areas demographics, the factors that influence your crop, and how to keep them safe for a super awesome good yield. With the rise of big data technology and high performance computing, machine learning has opened up new possibilities for data intensive research in the multi disciplinary agri technologies domain. a Plant disease forecast, b fertilizer recommendation, and c crop recommendation The papers presented have been filtered and classified to show how machine learning technology can support agriculture. Farm management systems are evolving into real time artificial intelligence powered programmes that provide rich suggestions and insights for farmer decision support and action through applying machine learning to sensor data. Ms. A. Benazir Begum | Ajith Manoj | Nithya E | Anamika S S | Sneshna \"Cultivation of Crops using Machine Learning and Deep Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-3 , April 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd39891.pdf Paper URL: https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/39891\/cultivation-of-crops-using-machine-learning-and-deep-learning\/ms-a-benazir-begum","1671":null,"1672":"Learning according to the structural risk minimization principle can be naturally expressed as an Ivanov regularization problem. Vapnik himself pointed out this connection, when deriving an actual learning algorithm from this principle, like the well-known support vector machine, but quickly suggested to resort to a Tikhonov regularization schema, instead. This was, at that time, the best choice because the corresponding optimization problem is easier to solve and in any case, under certain hypothesis, the solutions obtained by the two approaches coincide. On the other hand, recent advances in learning theory clearly show that the Ivanov regularization scheme allows a more effective control of the learning hypothesis space and, therefore, of the generalization ability of the selected hypothesis. We prove in this paper the equivalence between the Ivanov and Tikhonov approaches and, for the sake of completeness, their connection to Morozov regularization, which has been show to be useful when effective estimation of the noise in the data is available. We also show that this equivalence is valid under milder conditions on the loss function with respect to Vapnik's original proposal. These results allows us to derive several methods for performing SRM learning according to an Ivanov or Morozov regularization scheme, but using Tikhonov-based solvers, which have been thoroughly studied in the last decades and for which very efficient implementations have been proposed.","1673":"Wir zeigen an Beispieldaten, wie man ein Machine-Learning-Projekt in Python umsetzt. So kann etwa ein Programm sehr genau die Unterart einer Pflanze bestimmen.","1674":null,"1675":null,"1676":null,"1677":"Machine learning is the field that focuses on how computers learn from data. Today, machine learning is playing an integral role in the medical industry. This is due to its ability to process huge datasets beyond the scope of human capability, and then convert the data analyzed into clinical insights that aid physicians in providing care. Machine learning is a powerful, relatively easy to implement tool with numerous possibilities to enhance medical practice. The applications of machine learning in medicine are advancing medicine into a new realm. Therefore, educating the next generation of medical professionals with machine learning is essential. This paper provides a brief introduction to applying machine learning in medicine. Matthew N. O Sadiku | Sarhan M. Musa | Adedamola Omotoso \"Machine Learning in Medicine: A Primer\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-2 , February 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd20255.pdf","1678":"Recent technical advances has made machine learning (ML) a promising\r\ncomponent to include in end user facing systems. However, user experience (UX)\r\npractitioners face challenges in relating ML to existing user-centered design\r\nprocesses and how to navigate the possibilities and constraints of this design\r\nspace. Drawing on our own experience, we characterize designing within this\r\nspace as navigating trade-offs between data gathering, model development and\r\ndesigning valuable interactions for a given model performance. We suggest that\r\nthe theoretical description of how machine learning performance scales with\r\ntraining data can guide designers in these trade-offs as well as having\r\nimplications for prototyping. We exemplify the learning curve's usage by\r\narguing that a useful pattern is to design an initial system in a bootstrap\r\nphase that aims to exploit the training effect of data collected at increasing\r\norders of magnitude.","1679":"Phishing is a social engineering Technique which they main aim is to target the user Information like user id, password, credit card information and so on. Which result a financial loss to the user. Detecting Phishing is the one of the challenge problem that relay to human vulnerabilities. This paper proposed the Detecting Phishing Web Sites using different Machine Learning Approaches. In this to evaluate different classification models to predict malicious and benign websites by using Machine Learning Algorithms. Experiments are performed on data set consisting malicious and benign, In This paper the results shows the proposed Algorithms has high detection accuracy. Nakkala Srinivas Mudiraj \"\"Detecting Phishing using Machine Learning\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-4 , June 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd23755.pdf\r\n\r\n Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/computer-security\/23755\/detecting-phishing-using-machine-learning\/nakkala-srinivas-mudiraj","1680":"We consider the problem of learning to perform information extraction in domains where linguistic processing is problematic, such as Usenet posts, email, and finger plan files. In place of syntactic and semantic information, other sources of information can be used, such as term frequency, typography, formatting, and mark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rote learner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relational rule learner. Experiments on 14 information extraction problems defined over four diverse document collections demonstrate the effectiveness of these approaches. Finally, we describe a multistrategy approach which combines these learners and yields performance competitive with or better than the best of them. This technique is modular and flexible, and could find application in other machine learning problems.","1681":"An array of large observational programs using ground-based and space-borne\r\ntelescopes is planned in the next decade. The forthcoming wide-field sky\r\nsurveys are expected to deliver a sheer volume of data exceeding an exabyte.\r\nProcessing the large amount of multiplex astronomical data is technically\r\nchallenging, and fully automated technologies based on machine learning and\r\nartificial intelligence are urgently needed. Maximizing scientific returns from\r\nthe big data requires community-wide efforts. We summarize recent progress in\r\nmachine learning applications in observational cosmology. We also address\r\ncrucial issues in high-performance computing that are needed for the data\r\nprocessing and statistical analysis.","1682":"Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package----PMTK (probabilistic modeling toolkit)---that is freely available online.","1683":"Network has brought convenience to the earth by permitting versatile transformation of information, however it conjointly exposes a high range of vulnerabilities. A Network Intrusion Detection System helps network directors and system to view network security violation in their organizations. Characteristic unknown and new attacks are one of the leading challenges in Intrusion Detection System researches. Deep learning that a subfield of machine learning cares with algorithms that are supported the structure and performance of brain known as artificial neural networks. The improvement in such learning algorithms would increase the probability of IDS and the detection rate of unknown attacks. Throughout, we have a tendency to suggest a deep learning approach to implement increased IDS and associate degree economical. Priya N | Ishita Popli \"Comparative Study on Machine Learning Algorithms for Network Intrusion Detection System\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-1 , December 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38175.pdf Paper URL : https:\/\/www.ijtsrd.com\/computer-science\/computer-network\/38175\/comparative-study-on-machine-learning-algorithms-for-network-intrusion-detection-system\/priya-n","1684":"Indian politics suffered from a great set back due to fake news. Fake news is intentionally written to mislead the audience to believe the false propaganda, which makes it difficult to detect based on news content. The fake news has hindered the mindset of the common people. Due to this widespread of the fake news online it is the need of the hour to check the authenticity of the news. The spread of fake news has the potential for extremely negative impact on society. The proposed approach is to use machine learning to detect fake news. Using vectorisation of the news title and then analysing the tokens of words with our dataset. The dataset we are using is a predefined curated list of news with their property of being a fake news or not. Our goal is to develop a model that classifies a given article as either true or fake. General Terms Fake News, Self Learning, Pattern Matching, Response Generation, Artificial Intelligence, Natural Language Processing, Context Free Grammar, Term Frequency Inverse Document Frequency, Stochastic Gradient Decent, Word2Vec. Nikhil Sharma \"Fake News Detection using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-4 , June 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd31148.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/other\/31148\/fake-news-detection-using-machine-learning\/nikhil-sharma","1685":"Music genre classification has been a toughest task in the area of music information retrieval MIR . Classification of genre can be important to clarify some genuine fascinating issues, such as, making songs references, discovering related songs, finding societies who will like that particular song. The inspiration behind the research is to find the appropriate machine learning algorithm that predict the genres of music utilizing k nearest neighbor k NN and Support Vector Machine SVM . GTZAN dataset is the frequently used dataset for the classification music genre. The Mel Frequency cepstral coefficients MFCC is utilized to extricate features for the dataset. From results we found that k NN classifier gave more exact results compared to support vector machine classifier. If the training data is bigger than number of features, k NN gives better outcomes than SVM. SVM can only identify limited set of patterns. KNN classifier is more powerful for the classification of music genre. Seethal V | Dr. A. Vijayakumar \"Music Genre Classification using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd41263.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/data-processing\/41263\/music-genre-classification-using-machine-learning\/seethal-v","1686":"Machine learning encompasses a broad range of algorithms and modeling tools\r\nused for a vast array of data processing tasks, which has entered most\r\nscientific disciplines in recent years. We review in a selective way the recent\r\nresearch on the interface between machine learning and physical sciences.This\r\nincludes conceptual developments in machine learning (ML) motivated by physical\r\ninsights, applications of machine learning techniques to several domains in\r\nphysics, and cross-fertilization between the two fields. After giving basic\r\nnotion of machine learning methods and principles, we describe examples of how\r\nstatistical physics is used to understand methods in ML. We then move to\r\ndescribe applications of ML methods in particle physics and cosmology, quantum\r\nmany body physics, quantum computing, and chemical and material physics. We\r\nalso highlight research and development into novel computing architectures\r\naimed at accelerating ML. In each of the sections we describe recent successes\r\nas well as domain-specific methodology and challenges.","1687":"Blood and its elements have a vital position in human life and are the best indicator for deciding many pathological states. Specifically, white blood cells are of great significance for diagnosing hematological disorders. In this analysis, 350 microscopic blood smudge images have experimented with six machine learning algorithms for the sort of white blood cells, and their renditions have resembled. Thirty-five distinct geometric and statistical (consistency) features have been pulled from blood pictures for practicum and test parameters of machine learning algorithms. According to the outcomes, the Multinomial Logistic Regression (MLR) algorithm accomplished better than the other techniques, with an average of 95% test victory. The MLR can be utilized for the automatic classification of white blood cells. It can be used mainly as a source for diagnosing diseases for hematologists and internal medicine experts.","1688":"Over the past five years, modern machine learning has been quietly\r\nrevolutionizing particle physics. Old methodology is being outdated and\r\nentirely new ways of thinking about data are becoming commonplace. This article\r\nwill review some aspects of the natural synergy between modern machine learning\r\nand particle physics, focusing on applications at the Large Hadron Collider. A\r\nsampling of examples is given, from signal\/background discrimination tasks\r\nusing supervised learning to direct data-driven approaches. Some comments on\r\npersistent challenges and possible future directions for the field are included\r\nat the end.","1689":null,"1690":"We present seven myths commonly believed to be true in machine learning\r\nresearch, circa Feb 2019. This is an archival copy of the blog post at\r\nhttps:\/\/crazyoscarchang.github.io\/2019\/02\/16\/seven-myths-in-machine-learning-research\/\r\n  Myth 1: TensorFlow is a Tensor manipulation library\r\n  Myth 2: Image datasets are representative of real images found in the wild\r\n  Myth 3: Machine Learning researchers do not use the test set for validation\r\n  Myth 4: Every datapoint is used in training a neural network\r\n  Myth 5: We need (batch) normalization to train very deep residual networks\r\n  Myth 6: Attention $>$ Convolution\r\n  Myth 7: Saliency maps are robust ways to interpret neural networks","1691":"In this work we offer a framework for reasoning about a wide class of\r\nexisting objectives in machine learning. We develop a formal correspondence\r\nbetween this work and thermodynamics and discuss its implications.","1692":"The problems of privacy and security is becoming a major challenge when it comes to the distributed systems, federated machine learning system especially when data are been transmitted or learned on a network , this necessitated the reasons for this research work which is all about wireless federated machine learning process using a Raspberry Pi. The Raspberry Pi 4 is a single hardware board with built in Linux operating system. We used data set of names from nine (9) different languages and then develop a training model using recurrent neural network to train this names compare to the names in the existing language like French, Scottish to predict if the names are from any of this language, this is done wirelessly with the Wi-Fi network in a federated machine learning environment for experimental setup with PySft\u2019s that is installed in the python environment. The system was able to predict that name from which the language it originate from, the methodology that is implore in the research work is the Rapid Application Development (RAD). The benefits of this system are to ensure privacy, reduces the computing power, ensure real time learning and most importantly it is cost effective","1693":"As the development of machine learning and deep learning, more and more people or organizations use multiple algorithms to analyse large collections of data to produce meaningful results that help to predict behaviour. And this kind of technology is increasingly used in medical field to predict some severe illness in their early stage, for example, cervical cancer. Cervical Cancer is one of the main reasons of deaths in countries having a low capita income. It is the second most common cancer in India in women accounting for 22.86 of all cancer cases in women. It becomes quite complicated while examining a patient on the basis of result obtained from various doctor\u2019s preferred test to determine if the patient is positive with the cancer. There were 96,922 new cases of cervical cancer diagnosed in India in 2018. Around the globe, around a quarter of million people die owing to cervical cancer. Screening and different deterministic tests confuse the available Computed Aided Diagnosis CAD to treat the patient correctly for the cancer. Machine learning and Deep learning algorithms are used in this project and determine if the patient has cancer based on the analyses of the risk factors available in the dataset. Predicting the presence of cervical cancer can help the diagnosis process to start at an early stage and comparing various models will help in finding out the best prediction model for predicting the presence of cervical cancer effectively. Kayalvizhi. K. R | N Kanimozhi \"Prediction of Cervical Cancer using Machine Learning and Deep Learning Algorithms\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33378.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/33378\/prediction-of-cervical-cancer-using-machine-learning-and-deep-learning-algorithms\/kayalvizhi-k-r","1694":"Sentiment analysis is used in opinion mining. It helps businesses understand the customers\u2019 reviews with a particular product by analyzing their emotional from the product reviews they post, the online recommendations they make, their survey responses and other forms of social media text. Businesses can get feedback on how happy or sad the customer is, and use this insight to gain a competitive edge. In this article, we explore how to conduct sentiment analysis on a piece of text using some machine learning techniques. Python happens to be one of the best programming language, when it comes to machine learning as it is easy to learn, is open source, and is effective in catering to machine learning requirements like processing big datasets and performing mathematical computations. Natural Language ToolKit NLTK is one of the popular packages in Python that can use for in sentiment analysis. Mohit Chaudhari \"Sentimental Emotion Analysis using Python and Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd41198.pdf Paper URL: https:\/\/www.ijtsrd.comengineering\/computer-engineering\/41198\/sentimental-emotion-analysis-using-python-and-machine-learning\/mohit-chaudhari","1695":"Machine learning approaches to modeling of epidemiologic data are becoming increasingly more prevalent in the literature. These methods have the potential to improve our understanding of health and opportunities for intervention, far beyond our past capabilities. This article provides a walkthrough for creating supervised machine learning models with current examples from the literature. From identifying an appropriate sample and selecting features through training, testing, and assessing performance, the end-to-end approach to machine learning can be a daunting task. We take the reader through each step in the process and discuss novel concepts in the area of machine learning, including identifying treatment effects and explaining the output from machine learning models.","1696":null,"1697":"Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in\r\nmachine learning. Automatic differentiation (AD), also called algorithmic\r\ndifferentiation or simply \u00e4utodiff\", is a family of techniques similar to but\r\nmore general than backpropagation for efficiently and accurately evaluating\r\nderivatives of numeric functions expressed as computer programs. AD is a small\r\nbut established field with applications in areas including computational fluid\r\ndynamics, atmospheric sciences, and engineering design optimization. Until very\r\nrecently, the fields of machine learning and AD have largely been unaware of\r\neach other and, in some cases, have independently discovered each other's\r\nresults. Despite its relevance, general-purpose AD has been missing from the\r\nmachine learning toolbox, a situation slowly changing with its ongoing adoption\r\nunder the names \"dynamic computational graphs\" and \"differentiable\r\nprogramming\". We survey the intersection of AD and machine learning, cover\r\napplications where AD has direct relevance, and address the main implementation\r\ntechniques. By precisely defining the main differentiation techniques and their\r\ninterrelationships, we aim to bring clarity to the usage of the terms\r\n\u00e4utodiff\", \u00e4utomatic differentiation\", and \"symbolic differentiation\" as\r\nthese are encountered more and more in machine learning settings.","1698":"Various computational models are used around the world to predict the number of infected individuals and the death rate of the COVID 19 outbreak 3 . Machine learning based models are important to take proper actions. Due to the ample of uncertainty and crucial data, the aerodynamic models have been challenged regarding higher accuracy for long term prediction of this disease 1 . By researching the COVID19 problem, it is observed that lockdown and isolation are important techniques for preventing the spread of COVID 19 2 . In India, public health and the economical condition are impacted by COVID 19, our goal is to visualize the spread of this disease 5 . Machine Learning Algorithms are used in various applications for detecting adverse risk factors. Three ML algorithms we are using that is Logistic Regression LR , Support Vector Machine SVM , and Random Forest Classifier RFC . These machine learning models are predicting the total number of recovered patients as per the date of each state in India 8 . Sarfraj Alam | Vipul Kumar | Sweta Singh | Sweta Joshi | Madhu Kirola \"Covid-19 Prediction in India using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Special Issue | International Conference on Advances in Engineering, Science and Technology - 2021 , May 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd42458.pdf Paper URL : https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/42458\/covid19-prediction-in-india-using-machine-learning\/sarfraj-alam","1699":"Machine learning and knowledge acquisition from experts have distinct capabilities that appear to complement one another. We report a study that demonstrates the integration of these approaches can both improve the accuracy of the developed knowledge base and reduce development time. In addition, we found that users expected the expert systems created through the integrated approach to have higher accuracy than those created without machine learning and rated the integrated approach less difficult to use. They also provided favorable evaluations of both the specific integrated software, system called The Knowledge Factory, and of the general value of machine learning for knowledge acquisition.","1700":"In this modern age, social media platforms have become indispensable tools for communication and information sharing. However, this unprecedented connectivity has also given rise to a concerning proliferation of hate speech and offensive content. This research article presents a comprehensive study on the development and evaluation of machine learning ML models for the automatic detection of hate speech on Twitter. We leverage a diverse dataset collected from Twitter, encompassing a wide range of hate speech categories, including hate speech targeting race, gender, religion, and more. To address the multifaceted nature of hate speech, we employ a hybrid approach that combines traditional natural language processing NLP techniques with state of the art machine learning algorithms. Our methodology involves extensive preprocessing of the text data, including tokenization, stemming, and feature extraction. We then experiment with various machine learning algorithms, including Na\u00efve Bayes NB , K nearest Neighbor KNN , Random Forest RF , and Support Vector Machines SVM . The models are trained and fine tuned on a labeled dataset and evaluated using robust metrics to assess their performance. Subrata Saha | Md. Motinur Rahman | Md. Mahbub Alam \"Machine Learning Approach to Classify Twitter Hate Speech\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-5 , October 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd59873.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/59873\/machine-learning-approach-to-classify-twitter-hate-speech\/subrata-saha","1701":"Technologys scope has evolved into one of the most powerful tools for human development in a variety of fields.AI and machine learning have become one of the most powerful tools for completing tasks quickly and accurately without the need for human intervention. This project demonstrates how deep machine learning can be used to create a caption or a sentence for a given picture. This can be used for visually impaired persons, as well as automobiles for self identification, and for various applications to verify quickly and easily. The Convolutional Neural Network CNN is used to describe the alphabet, and the Long Short Term Memory LSTM is used to organize the right meaningful sentences in this model. The flicker 8k and flicker 30k datasets were used to train this. Sreejith S P | Vijayakumar A \"Image Captioning Generator using Deep Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42344.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/artificial-intelligence\/42344\/image-captioning-generator-using-deep-machine-learning\/sreejith-s-p","1702":"Lecture notes on optimization for machine learning, derived from a course at\r\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\r\nSimons Foundation, Berkeley.","1703":"In this work, the problem of Quality of Experience (QoE) monitoring of web browsing is addressed. In particular, the inference of common Web QoE metrics like RUMSI with machine learning is investigated. Based on a large dataset collected with WebPageTest on three different devices, a unique feature set is used to approximate Web QoE metrics with regression and classification approaches. This work highlights work in progress.","1704":"Accurately estimating evaporation is necessary for calculating and scheduling irrigation water requirements. Current literature points to the use of individual machine learning models for better estimation of evaporation. However, such methods have not been used in the Indian framework. Moreover, given the diversity of climate, it is necessary to develop an ensemble technique incorporating a significant number of machine learning algorithms to have a better estimation of weekly evaporation. The purpose of this paper is to develop an ensemble technique that makes the machine learning models that have a better estimation of weekly evaporation. The results showed that the Bagging Random Forest model has a much better performance in estimating weekly evaporation compared to other fitted ensemble models. R. S. Parmar | G. B. Chaudhari | S. H. Bhojani \"Estimating Evaporation using Machine Learning Based Ensemble Technique\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-4, August 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd59847.pdf Paper Url:https:\/\/www.ijtsrd.com\/engineering\/agricultural-engineering\/59847\/estimating-evaporation-using-machine-learning-based-ensemble-technique\/r-s-parmar","1705":"Building effective neural networks requires many design choices. These\r\ninclude the network topology, optimization procedure, regularization, stability\r\nmethods, and choice of pre-trained parameters. This design is time consuming\r\nand requires expert input. Automatic Machine Learning aims automate this\r\nprocess using hyperparameter optimization. However, automatic model building\r\nframeworks optimize performance on each task independently, whereas human\r\nexperts leverage prior knowledge when designing a new network. We propose\r\nTransfer Automatic Machine Learning, a method to accelerate network design\r\nusing knowledge of prior tasks. For this, we build upon reinforcement learning\r\narchitecture design methods to support parallel training on multiple tasks and\r\ntransfer the search strategy to new tasks. Tested on NLP and Image\r\nclassification tasks, Transfer Automatic Machine Learning reduces convergence\r\ntime over single-task methods by almost an order of magnitude on 13 out of 14\r\ntasks. It achieves better test set accuracy on 10 out of 13 tasks NLP tasks and\r\nimproves performance on CIFAR-10 image recognition from 95.3% to 97.1%.","1706":null,"1707":"Titanic disaster occurred 100 years ago on April 15, 1912, killing about 1500 passengers and crew members. The fateful incidents still compel the researchers and analysts to understand what could have led to the survival of some passengers and demise of the others. With the use of machine learning methods and a dataset consisting of 891 rows in the train set and 418 rows in the test set, we attempt to determine the correlation between factors such as age, sex, passenger class, fare etc. to the chance of survival of the passengers. These factors may or may not have impacted the survival rates of the passengers. In this research paper, we use various machine learning algorithms namely Logistic Regression, Na\u00c3\u00afve Bayes, Decision Tree, Random Forest to predict the survival of passengers. In particular, we attempt to compare these algorithms. Dr. Prabha Shreeraj Nair\u00c4nalyzing Titanic Disaster using Machine Learning Algorithms\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-1 , December 2017, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd7003.pdf  http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/7003\/analyzing-titanic-disaster-using-machine-learning-algorithms\/dr-prabha-shreeraj-nair","1708":null,"1709":"Talent management involves a lot of managerial decisions to allocate right people with the right skills employed at appropriate location and time. Authors report machine learning solution for Human Resource HR attrition analysis and forecast. The data for this investigation is retrieved from Kaggle, a Data Science and Machine Learning platform 1 . Present study exhibits performance estimation of various classification algorithms and compares the classification accuracy. The performance of the model is evaluated in terms of Error Matrix and Pseudo R Square estimate of error rate. Performance accuracy revealed that Random Forest model can be effectively used for classification. This analysis concludes that employee attrition depends more on employees\u00e2\u20ac\u2122 satisfaction level as compared to other attributes. Dr. R. S. Kamath | Dr. S. S. Jamsandekar | Dr. P. G. Naik \"Machine Learning Approach for Employee Attrition Analysis\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Special Issue | Fostering Innovation, Integration and Inclusion Through Interdisciplinary Practices in Management , March 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd23065.pdfPaper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/23065\/machine-learning-approach-for-employee-attrition-analysis\/dr-r-s-kamath","1710":"Graphical causal inference as pioneered by Judea Pearl arose from research on\r\nartificial intelligence (AI), and for a long time had little connection to the\r\nfield of machine learning.\r\n  This article discusses where links have been and should be established,\r\nintroducing key concepts along the way. It argues that the hard open problems\r\nof machine learning and AI are intrinsically related to causality, and explains\r\nhow the field is beginning to understand them.","1711":"Users of Amazons online shopping service are allowed to leave feedback for the items they buy. Amazon makes no effort to monitor or limit the scope of these reviews. Although the amount of reviews for various items varies, the reviews provide easily accessible and abundant data for a variety of applications. This paper aims to apply and expand existing natural language processing and sentiment analysis research to data obtained from Amazon. The number of stars given to a product by a user is used as training data for supervised machine learning. Since more people are dependent on online products these days, the value of a review is increasing. Before making a purchase, a buyer must read thousands of reviews to fully comprehend a product. In this day and age of machine learning, however, sorting through thousands of comments and learning from them would be much easier if a model was used to polarize and learn from them. We used supervised learning to polarize a massive Amazon dataset and achieve satisfactory accuracy. Ravi Kumar Singh | Dr. Kamalraj Ramalingam \u00c4mazon Product Review Sentiment Analysis with Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42372.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/data-processing\/42372\/amazon-product-review-sentiment-analysis-with-machine-learning\/ravi-kumar-singh","1712":"Historically, among other payment forms, gold has been used to fund trading purchases around the globe. Several states have retained and increasing their gold deposits, and have been known as democratic and prosperous nations. Actually, precious metals such as gold are kept by central banks in all countries to guarantee external debt servicing, and even to manage inflation. Furthermore it also reflects the countrys financial strength. In addition to government departments, numerous international companies and individuals have participated in gold reserves. In addition to the commoditys demand and supply on the market, the performance of the worlds leading economies also greatly influences gold rates. This rise in gold value coupled with volatility and falling prices from other markets such as capital markets and real estate markets has attracted more and more investors to gold as an attractive investment. Although there is still strong uncertainty of the late gold market, and transactions in gold are getting more dangerous. Theres a fear that those high prices will be sustainable and that the prices will reverse. Although there are a number of studies that analyze the correlation between the gold price and certain economic variables. Machine learning was often applied to predicting financial variables, but usually focused on predicting stocks rather than commodities. In this study, we proposed the development of forecasting model for predicting future gold price using Linear Regression LR . Dr. Abhay Kumar Agarwal | Swati Kumari \"Gold Price Prediction using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-5 , August 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33143.pdf Paper Url :https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/33143\/gold-price-prediction-using-machine-learning\/dr-abhay-kumar-agarwal","1713":"It is quite interesting to recognize the human emotions in the field of machine learning. Using a person's facial expression one can know his emotions or what the person wants to express. But at the same time it's not easy to recognize one's emotion easily its quite challenging at times. Facial expression consist of various human emotions such as sad happy  excited angry frustrated and surprise. Few years back Natural language processing was used to detect the sentiment from the text and then it took a step forward towards emotion detection. Sentiments can be positive negative or neutral where as emotions are more refined categories. There are many techniques used to recognize emotions. This paper provides a review of research work carried out and published in the field of human emotion recognition and various techniques used for human emotions recognition. Prof. Mrs. Dhanamma Jagli | Ms. Pooja Shetty \"Human Emotion Recognition using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd) ISSN: 2456-6470 Volume-3 | Issue-5  August 2019 URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd25217.pdfPaper URL: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/25217\/human-emotion-recognition-using-machine-learning\/prof-mrs-dhanamma-jagli","1714":"Nowadays, Heart disease has become dangerous to a human being, it effects very badly to human body. If anyone is suffering from heart disease, then it leads to blood clotting. Heart disease prediction is very difficult task to predict in the field of medical science. Affiliation has predicted that 12 million people fail horrendously every year as a result of heart disease. In this paper, we propose a k Nearest Neighbors Algorithm KNN way to deal with improve the exactness of heart determination. We show that k Nearest Neighbors Algorithm KNN have better accuracy than random forest algorithm for viewing heart disease. The k Nearest Neighbors Algorithm give more precise and exact outcome . We have taken 13 attributes in the dataset and a target attribute, by applying machine learning we achieved 84 accuracy in the heart disease detection. Ravi Kumar Singh | Dr. A Rengarajan \"Heart Disease Prediction using Machine Learning Algorithm\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-2 , February 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38358.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/other\/38358\/heart-disease-prediction-using-machine-learning-algorithm\/ravi-kumar-singh","1715":"The problem of Fake news has evolved much faster in the recent years. Social media has dramatically changed its reach and impact as a whole. On one hand, it\u2019s low cost, and easy accessibility with rapid share of information draws more attention of people to read news from it. On the other hand, it enables wide spread of Fake news, which are nothing but false information to mislead people. As a result, automating Fake news detection has become crucial in order to maintain robust online and social media. Artificial Intelligence and Machine learning are the recent technologies to recognize and eliminate the Fake news with the help of Algorithms. In this work, Machine learning methods are employed to detect the credibility of news based on the text content and responses given by users. A comparison is made to show that the latter is more reliable and effective in terms of determining all kinds of news. The method applied in this work is highest posterior probability of tokens in the response of two classes. It uses frequency based features to train the Algorithms including supervised learning algorithms and classification algorithm technique. The work also highlights a wide range of features established recently in this area that gives a clearer picture for the automation of this problem. An experiment was conducted in the work to match the lists of Fake related words in the text of responses, to find out whether the response based detection is a good measure to determine the credibility or not. Pujitha E | Dr. B S Shylaja \"Detection of Fake News using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33345.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/computer-security\/33345\/detection-of-fake-news-using-machine-learning\/pujitha-e","1716":"Stock market prediction is a typical task to forecast the upcoming stock values. It is very difficult to forecast because of unbalanced nature of stocks. In this work, an attempt is made for prediction of stock market trend. This research aims to combine multiple existing techniques into a much more robust prediction model which can handle various scenarios in which investment can be beneficial. By combing both techniques, this prediction model can provide more accurate and flexible recommendations. However instead of using those traditional methods, we approached the problems using machine learning techniques. We tried to revolutionize the way people address data processing problems in stock market by predicting the behavior of the stocks. In fact, if we can predict how the stock will behave in the short term future we can queue up our transactions earlier and be faster than everyone else. In theory, this allows us to maximize our profit without having the need to be physically located close to the data sources. We examined three main models. Firstly we used a complete prediction using a moving average. Secondly we used a LSTM model and finally a model called ARIMA model. The only motive is to increase the accuracy of predictive the stock market price. Each of those models was applied on real stock market data and checked whether it could return profit. Subham Kumar Gupta | Dr. Bhuvana J | Dr. M N Nachappa \"Stock Market Prediction using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-3 , April 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49868.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/other\/49868\/stock-market-prediction-using-machine-learning\/subham-kumar-gupta","1717":"Broadcasting forensic is the practice of using scientific methods and techniques to analyse and authenticate Multimedia content. Over the past decade, consumer grade imaging sensors have become increasingly prevalent, generating vast quantities of images and videos that are used for various public and private communication purposes. Such applications include publicity, advocacy, disinformation, and deception, among others. This paper aims to develop tools that can extract knowledge from these visuals and comprehend their provenance. However, many images and videos undergo modification and manipulation before public release, which can misrepresent the facts and deceive viewers. To address this issue, we propose a set of forensics and counter forensic techniques that can help establish the authenticity and integrity of Multimedia content. Additionally, we suggest ways to modify the content intentionally to mislead potential adversaries. Our proposed tools are evaluated using publicly available datasets and independently organized challenges. Our results show that the forensics and counter forensic techniques can accurately identify manipulated content and can help restore the original image or video. Furthermore, in this paper demonstrate that the modified content can successfully deceive potential adversaries while remaining undetected by state of the art forensic methods. Amit Kapoor | Prof. Vinod Mahor \"Broadcasting Forensics Using Machine Learning Approaches\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-3 , June 2023, URL: https:\/\/www.ijtsrd.com.com\/papers\/ijtsrd57545.pdf Paper URL: https:\/\/www.ijtsrd.com.com\/engineering\/computer-engineering\/57545\/broadcasting-forensics-using-machine-learning-approaches\/amit-kapoor","1718":"As machine learning is increasingly deployed in high-stakes contexts\r\naffecting people's livelihoods, there have been growing calls to open the black\r\nbox and to make machine learning algorithms more explainable. Providing useful\r\nexplanations requires careful consideration of the needs of stakeholders,\r\nincluding end-users, regulators, and domain experts. Despite this need, little\r\nwork has been done to facilitate inter-stakeholder conversation around\r\nexplainable machine learning. To help address this gap, we conducted a\r\nclosed-door, day-long workshop between academics, industry experts, legal\r\nscholars, and policymakers to develop a shared language around explainability\r\nand to understand the current shortcomings of and potential solutions for\r\ndeploying explainable machine learning in service of transparency goals. We\r\nalso asked participants to share case studies in deploying explainable machine\r\nlearning at scale. In this paper, we provide a short summary of various case\r\nstudies of explainable machine learning, lessons from those studies, and\r\ndiscuss open challenges.","1719":null,"1720":"Modern three dimensional 3 D medical imaging offers the potential and promise for major advances in science and medicine as higher fidelity images are produced. Due to advances in computer aided diagnosis and continuous progress in the field of computerized medical image visualization, there is need to develop one of the most important fields within scientific imaging. From the early basis report on cancer patients it has been seen that a greater number of people die of lung cancer than from other cancers such as colon, breast and prostate cancers combined. Lung cancer are related to smoking or secondhand smoke , or less often to exposure to radon or other environmental factors that\u2019s why this can be prevented. But still it is not yet clear if these cancers can be prevented or not. In this research work, approach of segmentation, feature extraction and Convolution Neural Network CNN will be applied for locating, characterizing cancer portion. Harpreet Singh | Er. Ravneet Kaur | \"Lung Cancer Detection using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33659.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/computer-architecture\/33659\/lung-cancer-detection-using-machine-learning\/harpreet-singh","1721":"In the Philippines, the province of Nueva Ecija produces fifty-four percent of its annual onion production. However, the level of onion growth production was reduced; since the outbreak of 2016, armyworms destroyed thousands of hectares of farms resulting in a loss of billions of pesos, which lead to the decline of the onion harvest. In this study, we develop machine learning models to forecast an outbreak of armyworms to help evade or reduce the damage caused by an armyworm outbreak. Climatic data; particularly Maximum temperature, Minimum Temperature, Ultraviolet Index, Humidity, Cloudiness, Wind Speed, Sun Hours, Rainfall, and Pressure from the Philippine Atmospheric, Geophysical and Astronomical Services Administration (PAGASA) and armyworm outbreak occurrences data from the Provincial Agriculture Office (PAO) of Nueva Ecija was used as the dataset for this study Using Tree-based machine learning models Decision Tree and Random Forest. Binary classifiers were developed and evaluated to forecast the occurrence or non-occurrence of the armyworm outbreak and the use of feature importance to distinguish the most critical climatic features that significantly contribute to forecasting an armyworm outbreak in the province of Nueva Ecija. These tree-based models produced satisfactory results, with the Random Forest model exhibiting a better forecasting capability than the Decision Tree model.","1722":"Despite incredible recent advances in machine learning, building machine\r\nlearning applications remains prohibitively time-consuming and expensive for\r\nall but the best-trained, best-funded engineering organizations. This expense\r\ncomes not from a need for new and improved statistical models but instead from\r\na lack of systems and tools for supporting end-to-end machine learning\r\napplication development, from data preparation and labeling to\r\nproductionization and monitoring. In this document, we outline opportunities\r\nfor infrastructure supporting usable, end-to-end machine learning applications\r\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\r\nStanford.","1723":"Climate change is one of the greatest challenges facing humanity, and we, as\r\nmachine learning experts, may wonder how we can help. Here we describe how\r\nmachine learning can be a powerful tool in reducing greenhouse gas emissions\r\nand helping society adapt to a changing climate. From smart grids to disaster\r\nmanagement, we identify high impact problems where existing gaps can be filled\r\nby machine learning, in collaboration with other fields. Our recommendations\r\nencompass exciting research questions as well as promising business\r\nopportunities. We call on the machine learning community to join the global\r\neffort against climate change.","1724":"Summary\r\nMachine learning has been heavily researched and widely used in many disciplines. However, achieving high accuracy requires a large amount of data that is sometimes difficult, expensive, or impractical to obtain. Integrating human knowledge into machine learning can significantly reduce data requirement, increase reliability and robustness of machine learning, and build explainable machine learning systems. This allows leveraging the vast amount of human knowledge and capability of machine learning to achieve functions and performance not available before and will facilitate the interaction between human beings and machine learning systems, making machine learning decisions understandable to humans. This paper gives an overview of the knowledge and its representations that can be integrated into machine learning and the methodology. We cover the fundamentals, current status, and recent progress of the methods, with a focus on popular and new topics. The perspectives on future directions are also discussed.","1725":"Machine learning (ML) continues to permeate all layers of academia, industry\r\nand society. Despite its successes, mental frameworks to capture and represent\r\nmachine learning workflows in a consistent and coherent manner are lacking. For\r\ninstance, the de facto process modeling standard, Business Process Model and\r\nNotation (BPMN), managed by the Object Management Group, is widely accepted and\r\napplied. However, it is short of specific support to represent machine learning\r\nworkflows. Further, the number of heterogeneous tools for deployment of machine\r\nlearning solutions can easily overwhelm practitioners. Research is needed to\r\nalign the process from modeling to deploying ML workflows.\r\n  We analyze requirements for standard based conceptual modeling for machine\r\nlearning workflows and their serverless deployment. Confronting the\r\nshortcomings with respect to consistent and coherent modeling of ML workflows\r\nin a technology independent and interoperable manner, we extend BPMN's\r\nMeta-Object Facility (MOF) metamodel and the corresponding notation and\r\nintroduce BPMN4sML (BPMN for serverless machine learning). Our extension\r\nBPMN4sML follows the same outline referenced by the Object Management Group\r\n(OMG) for BPMN. We further address the heterogeneity in deployment by proposing\r\na conceptual mapping to convert BPMN4sML models to corresponding deployment\r\nmodels using TOSCA.\r\n  BPMN4sML allows technology-independent and interoperable modeling of machine\r\nlearning workflows of various granularity and complexity across the entire\r\nmachine learning lifecycle. It aids in arriving at a shared and standardized\r\nlanguage to communicate ML solutions. Moreover, it takes the first steps toward\r\nenabling conversion of ML workflow model diagrams to corresponding deployment\r\nmodels for serverless deployment via TOSCA.","1726":"Heart is one of the most important part of the body. It helps to purify and circulate blood to all parts of the body. Most number of deaths in the world are due to Heart Diseases. Some symptoms like chest pain, faster heartbeat, discomfort in breathing are recorded. This data is analysed on regular basis. In this review, an overview of the heart disease and its current procedures is firstly introduced. Furthermore, an in-depth analysis of the most relevant machine learning techniques available on the literature for heart disease prediction is briefly elaborated. The discussed machine learning algorithms are Decision Tree, SVM, ANN, Naive Bayes, Random Forest, KNN. The algorithms are compared on the basis of features. We are working on the algorithm with best accuracy. This will help the doctors to assist the heart problem easily.","1727":"Tapping into the \"folk knowledge\" needed to advance machine learning applications.","1728":"This text aims to present and explain quantum machine learning algorithms to\r\na data scientist in an accessible and consistent way. The algorithms and\r\nequations presented are not written in rigorous mathematical fashion, instead,\r\nthe pressure is put on examples and step by step explanation of difficult\r\ntopics. This contribution gives an overview of selected quantum machine\r\nlearning algorithms, however there is also a method of scores extraction for\r\nquantum PCA algorithm proposed as well as a new cost function in feed-forward\r\nquantum neural networks is introduced. The text is divided into four parts: the\r\nfirst part explains the basic quantum theory, then quantum computation and\r\nquantum computer architecture are explained in section two. The third part\r\npresents quantum algorithms which will be used as subroutines in quantum\r\nmachine learning algorithms. Finally, the fourth section describes quantum\r\nmachine learning algorithms with the use of knowledge accumulated in previous\r\nparts.","1729":"As we know and living in the era of digital world, Credit card fraud is increasing rapidly by transactions of unauthorized or any fraudulent use of someone else information of credit card to purchase and obtain benefits of financial. The victims of credit card fraud may have severe repercussions. Financial losses, harm to credit scores, and the trouble of dealing with unauthorized transactions can all arise from it. Secure your card information, keep a close eye on your account activity, and alert your card issuer right away to any odd transactions if you want to prevent credit card theft. To help combat fraud, many financial institutions additionally provide extra security features like two factor authentication and fraud detection systems. To resolve these problem we developed a system of Credit Card Fraud detection by hybrid techniques of machine learning which combines supervised and unsupervised methods to improve the system of fraud detection. In this paper we are using machine learning algorithms like K Nearest Neighbor, Logistic Regression and XGBoost model and we had made a comparison of accuracy score with other different models by using the data of European Cardholders 2013, by that data we had make comparison and decided that which model is best for defining the fraud system of credit card. Tripti Gautam | Ghanshyam Sahu | Lalit Kumar P. Bhiaya \"Credit Card Fraud Detection Using Hybrid Machine Learning Algorithm\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-7 | Issue-6 , December 2023, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd60102.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/other\/60102\/credit-card-fraud-detection-using-hybrid-machine-learning-algorithm\/tripti-gautam","1730":null,"1731":"This paper describes two separate learning flows for improving the efficiency of simulation-based design analysis. Machine learning concepts and methods are explained in the context of realizing the two learning flows. Experimental results are presented to demonstrate their feasibility. Generality of the proposed learning flows is illustrated using the kernel-based learning concept.","1732":"Dermatological Diseases are one of the biggest medical issues in 21st century due to its highly complex and expensive diagnosis with difficulties and subjectivity of human interpretation. In cases of fatal diseases like Melanoma diagnosis in early stages play a vital role in determining the probability of getting cured. We believe that the application of automated methods will help in early diagnosis especially with the set of images with variety of diagnosis. Hence, in this article we present a completely automated system of dermatological disease recognition through lesion images, a machine intervention in contrast to conventional medical personnel based detection. Our model is designed into three phases compromising of data collection and augmentation, designing model and finally prediction. We have used multiple AI algorithms like Convolutional Neural Network and Support Vector Machine and amalgamated it with image processing tools to form a better structure, leading to higher accuracy of 85 . Vijayalakshmi M M \"\"Melanoma Skin Cancer Detection using Image Processing and Machine Learning\"\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-4 , June 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd23936.pdf\r\n\r\n Paper URL: https:\/\/www.ijtsrd.com\/engineering\/other\/23936\/melanoma-skin-cancer-detection-using-image-processing-and-machine-learning\/vijayalakshmi-m-m","1733":"Astronomy is experiencing a rapid growth in data size and complexity. This\r\nchange fosters the development of data-driven science as a useful companion to\r\nthe common model-driven data analysis paradigm, where astronomers develop\r\nautomatic tools to mine datasets and extract novel information from them. In\r\nrecent years, machine learning algorithms have become increasingly popular\r\namong astronomers, and are now used for a wide variety of tasks. In light of\r\nthese developments, and the promise and challenges associated with them, the\r\nIAC Winter School 2018 focused on big data in Astronomy, with a particular\r\nemphasis on machine learning and deep learning techniques. This document\r\nsummarizes the topics of supervised and unsupervised learning algorithms\r\npresented during the school, and provides practical information on the\r\napplication of such tools to astronomical datasets. In this document I cover\r\nbasic topics in supervised machine learning, including selection and\r\npreprocessing of the input dataset, evaluation methods, and three popular\r\nsupervised learning algorithms, Support Vector Machines, Random Forests, and\r\nshallow Artificial Neural Networks. My main focus is on unsupervised machine\r\nlearning algorithms, that are used to perform cluster analysis, dimensionality\r\nreduction, visualization, and outlier detection. Unsupervised learning\r\nalgorithms are of particular importance to scientific research, since they can\r\nbe used to extract new knowledge from existing datasets, and can facilitate new\r\ndiscoveries.","1734":"Majority of Indian framers depend on rainfall for agriculture. Thus, in an agricultural country like India, rainfall prediction becomes very important. Rainfall causes natural disasters like flood and drought, which are encountered by people across the globe every year. Rainfall prediction over drought regions has a great importance for countries like India whose economy is largely dependent on agriculture. A sufficient data length can play an important role in a proper estimation drought, leading to a better appraisal for drought risk reduction. Due to dynamic nature of atmosphere statistical techniques fail to provide good accuracy for rainfall prediction. So, we are going to use Machine Learning algorithms like Multiple Linear Regression, Random Forest Regressor and AdaBoost Regressor, where different models are going to be trained using training data set and tested using testing data set. The dataset which we have collected has the rainfall data from 1901 2015, where across the various drought affected states. Nonlinearity of rainfall data makes Machine Learning algorithms a better technique. Comparison of different approaches and algorithms will increase an accuracy rate of predicting rainfall over drought regions. We are going to use Python to code for algorithms. Intention of this project is to say, which algorithm can be used to predict rainfall, in order to increase the countries socioeconomic status. Mylapalle Yeshwanth | Palla Ratna Sai Kumar | Dr. G. Mathivanan M.E., Ph.D \"Comparative Study of Machine Learning Algorithms for Rainfall Prediction\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-3 , April 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd22961.pdf","1735":null,"1736":"Noisy data, non-convex objectives, model misspecification, and numerical\r\ninstability can all cause undesired behaviors in machine learning systems. As a\r\nresult, detecting actual implementation errors can be extremely difficult. We\r\ndemonstrate a methodology in which developers use an interactive proof\r\nassistant to both implement their system and to state a formal theorem defining\r\nwhat it means for their system to be correct. The process of proving this\r\ntheorem interactively in the proof assistant exposes all implementation errors\r\nsince any error in the program would cause the proof to fail. As a case study,\r\nwe implement a new system, Certigrad, for optimizing over stochastic\r\ncomputation graphs, and we generate a formal (i.e. machine-checkable) proof\r\nthat the gradients sampled by the system are unbiased estimates of the true\r\nmathematical gradients. We train a variational autoencoder using Certigrad and\r\nfind the performance comparable to training the same model in TensorFlow.","1737":"Many recent machine learning models rely on fine-grained dynamic control flow\r\nfor training and inference. In particular, models based on recurrent neural\r\nnetworks and on reinforcement learning depend on recurrence relations,\r\ndata-dependent conditional execution, and other features that call for dynamic\r\ncontrol flow. These applications benefit from the ability to make rapid\r\ncontrol-flow decisions across a set of computing devices in a distributed\r\nsystem. For performance, scalability, and expressiveness, a machine learning\r\nsystem must support dynamic control flow in distributed and heterogeneous\r\nenvironments.\r\n  This paper presents a programming model for distributed machine learning that\r\nsupports dynamic control flow. We describe the design of the programming model,\r\nand its implementation in TensorFlow, a distributed machine learning system.\r\nOur approach extends the use of dataflow graphs to represent machine learning\r\nmodels, offering several distinctive features. First, the branches of\r\nconditionals and bodies of loops can be partitioned across many machines to run\r\non a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs.\r\nSecond, programs written in our model support automatic differentiation and\r\ndistributed gradient computations, which are necessary for training machine\r\nlearning models that use control flow. Third, our choice of non-strict\r\nsemantics enables multiple loop iterations to execute in parallel across\r\nmachines, and to overlap compute and I\/O operations.\r\n  We have done our work in the context of TensorFlow, and it has been used\r\nextensively in research and production. We evaluate it using several real-world\r\napplications, and demonstrate its performance and scalability.","1738":"Collectively, machine learning (ML) researchers are engaged in the creation\r\nand dissemination of knowledge about data-driven algorithms. In a given paper,\r\nresearchers might aspire to any subset of the following goals, among others: to\r\ntheoretically characterize what is learnable, to obtain understanding through\r\nempirically rigorous experiments, or to build a working system that has high\r\npredictive accuracy. While determining which knowledge warrants inquiry may be\r\nsubjective, once the topic is fixed, papers are most valuable to the community\r\nwhen they act in service of the reader, creating foundational knowledge and\r\ncommunicating as clearly as possible.\r\n  Recent progress in machine learning comes despite frequent departures from\r\nthese ideals. In this paper, we focus on the following four patterns that\r\nappear to us to be trending in ML scholarship: (i) failure to distinguish\r\nbetween explanation and speculation; (ii) failure to identify the sources of\r\nempirical gains, e.g., emphasizing unnecessary modifications to neural\r\narchitectures when gains actually stem from hyper-parameter tuning; (iii)\r\nmathiness: the use of mathematics that obfuscates or impresses rather than\r\nclarifies, e.g., by confusing technical and non-technical concepts; and (iv)\r\nmisuse of language, e.g., by choosing terms of art with colloquial connotations\r\nor by overloading established technical terms.\r\n  While the causes behind these patterns are uncertain, possibilities include\r\nthe rapid expansion of the community, the consequent thinness of the reviewer\r\npool, and the often-misaligned incentives between scholarship and short-term\r\nmeasures of success (e.g., bibliometrics, attention, and entrepreneurial\r\nopportunity). While each pattern offers a corresponding remedy (don't do it),\r\nwe also discuss some speculative suggestions for how the community might combat\r\nthese trends.","1739":"Optical character recognition OCR deals with the process of identification of alphabets and various scripts. Optical character recognition OCR is one of the trending topics of all time. Optical character recognition OCR is used for pattern detection and artificial intelligence. Machine learning is widely used in the field of OCR to provide good accuracy in the result. In Python, Pytesseract is an optical character recognition OCR tool for python. The paper starts with an introduction and brief background and history of Optical character recognition OCR systems. Then the various techniques of OCR systems such as optical scanning, location segmentation, pre processing, feature extraction and post processing. The different applications of OCR systems are highlighted next followed by the current uses of the OCR systems. The future of the Optical character recognition OCR systems with machine learning environment is presented. Mr. Rishabh Dubey \"Machine Learning in the Field of Optical Character Recognition (OCR)\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-5 , August 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33233.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/other\/33233\/machine-learning-in-the-field-of-optical-character-recognition-ocr\/mr-rishabh-dubey","1740":"Though being seemingly disparate and with relatively new intersection, high\r\nenergy nuclear physics and machine learning have already begun to merge and\r\nyield interesting results during the last few years. It's worthy to raise the\r\nprofile of utilizing this novel mindset from machine learning in high energy\r\nnuclear physics, to help more interested readers see the breadth of activities\r\naround this intersection. The aim of this mini-review is to introduce to the\r\ncommunity the current status and report an overview of applying machine\r\nlearning for high energy nuclear physics, to present from different aspects and\r\nexamples how scientific questions involved in high energy nuclear physics can\r\nbe tackled using machine learning.","1741":"In the era of modern technologies emerging at rapid pace there is no reason why a crucial event in education sector such as attendance should be done in the old boring traditional way. Attendance monitoring system will save a lot of time and energy for the both parties teaching staff as well as the students. Attendance will be monitored by the face recognition algorithm by recognizing only the face of the students from the rest of the objects and then marking the students as present. The system will be pre feed with the images of all the students enrolled in the class and with the help of this pre feed data the algorithm will detect the students who are present and match the features with the already saved images of the students in the database. Benazir Begum A | Sreeyuktha R | Haritha M P | Vishnuprasad \"Face Recognition Based Attendance System using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-3 , April 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd39856.pdf Paper URL: https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/39856\/face-recognition-based-attendance-system-using-machine-learning\/benazir-begum-a","1742":"The Linking Open Data (LOD) project is an ongoing effort to construct a global data space, i.e. the Web of Data. One important part of this project is to establish owl:sameAs links among structured data sources. Such links indicate equivalent instances that refer to the same real-world object. The problem of discovering owl:sameAs links between pairwise data sources is called instance matching. Most of the existing approaches addressing this problem rely on the quality of prior schema matching, which is not always good enough in the LOD scenario. In this paper, we propose a schema-independent instance-pair similarity metric based on several general descriptive features. We transform the instance matching problem to the binary classification problem and solve it by machine learning algorithms. Furthermore, we employ some transfer learning methods to utilize the existing owl:sameAs links in LOD to reduce the demand for labeled data. We carry out experiments on some datasets of OAEI2010. The results show that our method performs well on real-world LOD data and outperforms the participants of OAEI2010.","1743":"To detect audio manipulation in a pre recorded evidence videos by developing a synchronization verification algorithm to match the lip movements along with its audio pitch values. Audio video recognition has been considered as a key for speech recognition tasks when the audio is sullied, as well as visual recognition method used for speaker authentication in multispeaker scenarios. The primary aim of this paper is to point out the correspondence between the audio and video streams. Acquired audio feature sequences are processed with a Gaussian model. 1.This proposed method achieves parallel processing by effectively examining multiple videos at a time.In this paper, we train the machine by convolutional neural network (CNN) and deep neural network (DNN).CNN architecture maps both the modalities into a depiction space to evaluate the correspondence of audio '\u201cvisual streams using the learned multimodal features. DNN is used as a discriminative model between the two modalities in order to concurrently distinguish between the correlated and uncorrelated components. The proposed architecture will deploy both spatial and temporal information jointly to effectively discover the correlation between temporal information for different modalities. We train a system by capturing the motion picture. This method achieves relative enhancement over 20% on the equal error rate and 7% on the average precision in comparison to the state of the art method. G. Abinaya | K. Sridevi Nattar | Dr. Rajini Girinath\u00c4nticipation of Forged Video Evidence using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd11357.pdf  http:\/\/www.ijtsrd.com\/computer-science\/other\/11357\/anticipation-of-forged-video-evidence-using-machine-learning\/g-abinaya","1744":"The automated keyword extraction task is to define a collection of representative terms for the text. Extracting keywords defines a small collection of terms, key phrases and keywords that define the document\u2019s context. Keyword search allows large document collections to be searched effectively. To allocate suitable key phrases to new documents, text categorization techniques can be applied. A predefined collection of key phrases from which all key phrases for new documents are selected is given in the training documents. The training data for each key phrase describes a collection of documents associated with it. Standard machine learning techniques are used for each key phrase to construct a classifier from the training materials, using those relevant to it as positive examples and the rest as negative examples. Provided a new text, it is processed by the classifier of each key phrase. Preeti Sondhi | Aakib Jabbar \"Survey on Key Phrase Extraction using Machine Learning Approaches\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-3 , April 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd39890.pdf Paper URL: https:\/\/www.ijtsrd.com\/other-scientific-research-area\/other\/39890\/survey-on-key-phrase-extraction-using-machine-learning-approaches\/preeti-sondhi","1745":null,"1746":"Machine Learning (ML) is one of the most exciting and dynamic areas of modern\r\nresearch and application. The purpose of this review is to provide an\r\nintroduction to the core concepts and tools of machine learning in a manner\r\neasily understood and intuitive to physicists. The review begins by covering\r\nfundamental concepts in ML and modern statistics such as the bias-variance\r\ntradeoff, overfitting, regularization, generalization, and gradient descent\r\nbefore moving on to more advanced topics in both supervised and unsupervised\r\nlearning. Topics covered in the review include ensemble models, deep learning\r\nand neural networks, clustering and data visualization, energy-based models\r\n(including MaxEnt models and Restricted Boltzmann Machines), and variational\r\nmethods. Throughout, we emphasize the many natural connections between ML and\r\nstatistical physics. A notable aspect of the review is the use of Python\r\nJupyter notebooks to introduce modern ML\/statistical packages to readers using\r\nphysics-inspired datasets (the Ising Model and Monte-Carlo simulations of\r\nsupersymmetric decays of proton-proton collisions). We conclude with an\r\nextended outlook discussing possible uses of machine learning for furthering\r\nour understanding of the physical world as well as open problems in ML where\r\nphysicists may be able to contribute. (Notebooks are available at\r\nhttps:\/\/physics.bu.edu\/~pankajm\/MLnotebooks.html )","1747":"MXNet is a multi-language machine learning (ML) library to ease the\r\ndevelopment of ML algorithms, especially for deep neural networks. Embedded in\r\nthe host language, it blends declarative symbolic expression with imperative\r\ntensor computation. It offers auto differentiation to derive gradients. MXNet\r\nis computation and memory efficient and runs on various heterogeneous systems,\r\nranging from mobile devices to distributed GPU clusters.\r\n  This paper describes both the API design and the system implementation of\r\nMXNet, and explains how embedding of both symbolic expression and tensor\r\noperation is handled in a unified fashion. Our preliminary experiments reveal\r\npromising results on large scale deep neural network applications using\r\nmultiple GPU machines.","1748":"In recent years, Social network use is increasingly build up. The various statistics are split widely through social media Such as Facebook, Twitter. Data about the person and what they communicate through the status updates are important for research in human personality. This paper intends to scrutinize the forecasting of personality traits of Facebook users bases on machine learning and part of the Big ve model this experiment uses my personality data set of Facebook users are used for linguistic factors respective to personality correlation. We used the Data Prepossessing concept of data mining after that feature Extraction. Next, we will work on feature selection. The Personality Prediction system built in the XGboosting classi cation model. Poonam L Patil | Dr. S. R. Jadhao \u00dcser Personality Prediction on Facebook Social Media using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33414.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/data-miining\/33414\/user-personality-prediction-on-facebook-social-media-using-machine-learning\/poonam-l-patil","1749":"This paper provides a review and commentary on the past, present, and future\r\nof numerical optimization algorithms in the context of machine learning\r\napplications. Through case studies on text classification and the training of\r\ndeep neural networks, we discuss how optimization problems arise in machine\r\nlearning and what makes them challenging. A major theme of our study is that\r\nlarge-scale machine learning represents a distinctive setting in which the\r\nstochastic gradient (SG) method has traditionally played a central role while\r\nconventional gradient-based nonlinear optimization techniques typically falter.\r\nBased on this viewpoint, we present a comprehensive theory of a\r\nstraightforward, yet versatile SG algorithm, discuss its practical behavior,\r\nand highlight opportunities for designing algorithms with improved performance.\r\nThis leads to a discussion about the next generation of optimization methods\r\nfor large-scale machine learning, including an investigation of two main\r\nstreams of research on techniques that diminish noise in the stochastic\r\ndirections and methods that make use of second-order derivative approximations.","1750":"Tensors are multidimensional arrays of numerical values and therefore\r\ngeneralize matrices to multiple dimensions. While tensors first emerged in the\r\npsychometrics community in the $20^th$ century, they have since then\r\nspread to numerous other disciplines, including machine learning. Tensors and\r\ntheir decompositions are especially beneficial in unsupervised learning\r\nsettings, but are gaining popularity in other sub-disciplines like temporal and\r\nmulti-relational data analysis, too.\r\n  The scope of this paper is to give a broad overview of tensors, their\r\ndecompositions, and how they are used in machine learning. As part of this, we\r\nare going to introduce basic tensor concepts, discuss why tensors can be\r\nconsidered more rigid than matrices with respect to the uniqueness of their\r\ndecomposition, explain the most important factorization algorithms and their\r\nproperties, provide concrete examples of tensor decomposition applications in\r\nmachine learning, conduct a case study on tensor-based estimation of mixture\r\nmodels, talk about the current state of research, and provide references to\r\navailable software libraries.","1751":null,"1752":null,"1753":"This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.","1754":"Due to the high level of growth in each number of transactions done using credit card has led to high rise in fraudulent activities. Fraud is one of the major issues related to credit card business, since each individual do more of offline or online purchase of product via internet there is need to developed a secured approach of detecting if the credit card been used is a fraudulent transaction or not. Pattern involves in the fraud detection has to be re analyze to change from reactive approach to a proactive approach. In this paper, our objectives are to detect at least 95 of fraudulent activities using machine learning to deployed anomaly detection system such as logistic regression, k nearest neighbor and support vector machine algorithm. Ajayi Kemi Patience | Dr. Lakshmi J. V. N \u00c4 Study on Credit Card Fraud Detection using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-3 , April 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd30688.pdf","1755":"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.","1756":"As machine learning systems become ubiquitous, there has been a surge of\r\ninterest in interpretable machine learning: systems that provide explanation\r\nfor their outputs. These explanations are often used to qualitatively assess\r\nother criteria such as safety or non-discrimination. However, despite the\r\ninterest in interpretability, there is very little consensus on what\r\ninterpretable machine learning is and how it should be measured. In this\r\nposition paper, we first define interpretability and describe when\r\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\r\nfor rigorous evaluation and expose open questions towards a more rigorous\r\nscience of interpretable machine learning.","1757":"How useful can machine learning be in a quantum laboratory? Here we raise the\r\nquestion of the potential of intelligent machines in the context of scientific\r\nresearch. A major motivation for the present work is the unknown reachability\r\nof various entanglement classes in quantum experiments. We investigate this\r\nquestion by using the projective simulation model, a physics-oriented approach\r\nto artificial intelligence. In our approach, the projective simulation system\r\nis challenged to design complex photonic quantum experiments that produce\r\nhigh-dimensional entangled multiphoton states, which are of high interest in\r\nmodern quantum experiments. The artificial intelligence system learns to create\r\na variety of entangled states, and improves the efficiency of their\r\nrealization. In the process, the system autonomously (re)discovers experimental\r\ntechniques which are only now becoming standard in modern quantum optical\r\nexperiments - a trait which was not explicitly demanded from the system but\r\nemerged through the process of learning. Such features highlight the\r\npossibility that machines could have a significantly more creative role in\r\nfuture research.","1758":"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.","1759":null,"1760":"The use of compression algorithms in machine learning tasks such as clustering and classification has appeared in a variety of fields, sometimes with the promise of reducing problems of explicit feature selection. The theoretical justification for such methods has been founded on an upper bound on Kolmogorov complexity and an idealized information space. An alternate view shows compression algorithms implicitly map strings into implicit feature space vectors, and compressionbased similarity measures compute similarity within these feature spaces. Thus, compression-based methods are not a \"parameter free\" magic bullet for feature selection and data representation, but are instead concrete similarity measures within defined feature spaces, and are therefore akin to explicit feature vector models used in standard machine learning algorithms. To underscore this point, we find theoretical and empirical connections between traditional machine learning vector models and compression, encouraging cross-fertilization in future work.","1761":"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.","1762":"Ontology Matching (OM) plays an important role in many domains such as\r\nbioinformatics and the Semantic Web, and its research is becoming increasingly\r\npopular, especially with the application of machine learning (ML) techniques.\r\nAlthough the Ontology Alignment Evaluation Initiative (OAEI) represents an\r\nimpressive effort for the systematic evaluation of OM systems, it still suffers\r\nfrom several limitations including limited evaluation of subsumption mappings,\r\nsuboptimal reference mappings, and limited support for the evaluation of\r\nML-based systems. To tackle these limitations, we introduce five new biomedical\r\nOM tasks involving ontologies extracted from Mondo and UMLS. Each task includes\r\nboth equivalence and subsumption matching; the quality of reference mappings is\r\nensured by human curation, ontology pruning, etc.; and a comprehensive\r\nevaluation framework is proposed to measure OM performance from various\r\nperspectives for both ML-based and non-ML-based OM systems. We report\r\nevaluation results for OM systems of different types to demonstrate the usage\r\nof these resources, all of which are publicly available","1763":"The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, recognize faces or spoken speech, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. <i>Introduction to Machine Learning<\/i> is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. It discusses many methods based in different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining, in order to present a unified treatment of machine learning problems and solutions. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The book can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods.<br \/> <br \/> After an introduction that defines machine learning and gives examples of machine learning applications, the book covers supervised learning, Bayesian decision theory, parametric methods, multivariate methods, dimensionality reduction, clustering, nonparametric methods, decision trees, linear discrimination, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, combining multiple learners, and reinforcement learning.","1764":"In this work, we have reviewed the issue of spam mail which is a big problem in the area of Internet. The growing size of uncalled mass e mail or spam has produced the requirement of a dependable anti spam filter. Now a days the Machine learning ML proedures are being employed to spontaneously filter the spam e mail in an effective manner. In this work, we have reviewed some of the prevalent ML approaches such as Rough sets, Bayesian classification, SVMs, k NN, ANNs and Artificial immune system and of their use fullness in the issue of spam Email taxonomy. We have provided the depictions of the procedures and the divergence of their enactment on the basis of the quantity of Spam Assassin. Anu | Ms. Preeti \u00c4 Deep Analysis on Prevailing Spam Mail Filteration Machine Learning Approaches\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33261.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/data-processing\/33261\/a-deep-analysis-on-prevailing-spam-mail-filteration-machine-learning-approaches\/anu","1765":"We present a proof of concept that machine learning techniques can be used to\r\npredict the properties of CNOHF energetic molecules from their molecular\r\nstructures. We focus on a small but diverse dataset consisting of 109 molecular\r\nstructures spread across ten compound classes. Up until now, candidate\r\nmolecules for energetic materials have been screened using predictions from\r\nexpensive quantum simulations and thermochemical codes. We present a\r\ncomprehensive comparison of machine learning models and several molecular\r\nfeaturization methods - sum over bonds, custom descriptors, Coulomb matrices,\r\nbag of bonds, and fingerprints. The best featurization was sum over bonds (bond\r\ncounting), and the best model was kernel ridge regression. Despite having a\r\nsmall data set, we obtain acceptable errors and Pearson correlations for the\r\nprediction of detonation pressure, detonation velocity, explosive energy, heat\r\nof formation, density, and other properties out of sample. By including another\r\ndataset with 309 additional molecules in our training we show how the error can\r\nbe pushed lower, although the convergence with number of molecules is slow. Our\r\nwork paves the way for future applications of machine learning in this domain,\r\nincluding automated lead generation and interpreting machine learning models to\r\nobtain novel chemical insights.","1766":"Large scale industrialization and the increase in the number of factories and industries across major cities in the world have been contributing to the decreasing air quality. This is since a rapid increase in the population across the world has prompted the majority of the companies across the globe to adopt mass production activities to keep up with the increasing demand. This is evident in the fact that most of the big cities have an increasing number of cases of respiratory illnesses and asthmatic symptoms in the populous. Therefore, there is an urgent need to address these issues to provide a better environment and reduce such incidences. The Internet of Things or IoT platform is a quite a promising platform for this approach which has been getting increasingly affordable and approachable. Therefore, in the approach stipulated in this research, the IoT platform has been utilized in addition to the Machine Learning paradigms to achieve accurate air quality predictions. The proposed methodology utilizes K nearest neighbors and Linear Regression, along with the Hidden Markov Model for effective Pollution level estimation. Suraj Kapse | Akshay Kurumkar | Vighnesh Manthapurwar | Prof. Rajesh Tak \"Enabling Air Pollution Prediction through IoT and Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-3 , April 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd30739.pdf","1767":"Mushroom hunting is gaining popularity as a leisure activity for the last couple of years. Modern studies suggest that some mushrooms can be useful to treat anemia, improve body immunity, fight diabetes and a few are even effective to treat cancer. But not all the mushrooms prove to be beneficial. Some mushrooms are poisonous as well and consumption of these may result in severe illnesses in humans and can even cause death. This study aims to examine the data and build different supervised machine learning models that will detect if the mushroom is edible or poisonous. Principal Component Analysis PCA algorithm is used to select the best features from the dataset. Different classifiers like Logistic Regression, Decision Tree, K Nearest Neighbor KNN , Support Vector Machine SVM , Na\u00c3\u00afve Bayes and Random Forest are applied on the dataset of UCI to classify the mushrooms as edible or poisonous. The performance of the algorithms is compared using Receiver Operating Characteristic ROC Curve. Kanchi Tank \u00c4 Comparative Study on Mushroom Classification using Supervised Machine Learning Algorithms\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-5 , August 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd42441.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/embedded-system\/42441\/a-comparative-study-on-mushroom-classification-using-supervised-machine-learning-algorithms\/kanchi-tank","1768":"Breakthroughs in machine learning are rapidly changing science and society,\r\nyet our fundamental understanding of this technology has lagged far behind.\r\nIndeed, one of the central tenets of the field, the bias-variance trade-off,\r\nappears to be at odds with the observed behavior of methods used in the modern\r\nmachine learning practice. The bias-variance trade-off implies that a model\r\nshould balance under-fitting and over-fitting: rich enough to express\r\nunderlying structure in data, simple enough to avoid fitting spurious patterns.\r\nHowever, in the modern practice, very rich models such as neural networks are\r\ntrained to exactly fit (i.e., interpolate) the data. Classically, such models\r\nwould be considered over-fit, and yet they often obtain high accuracy on test\r\ndata. This apparent contradiction has raised questions about the mathematical\r\nfoundations of machine learning and their relevance to practitioners.\r\n  In this paper, we reconcile the classical understanding and the modern\r\npractice within a unified performance curve. This \"double descent\" curve\r\nsubsumes the textbook U-shaped bias-variance trade-off curve by showing how\r\nincreasing model capacity beyond the point of interpolation results in improved\r\nperformance. We provide evidence for the existence and ubiquity of double\r\ndescent for a wide spectrum of models and datasets, and we posit a mechanism\r\nfor its emergence. This connection between the performance and the structure of\r\nmachine learning models delineates the limits of classical analyses, and has\r\nimplications for both the theory and practice of machine learning.","1769":null,"1770":"As the bioinformatics field grows, it must keep pace not only with new data\r\nbut with new algorithms. Here we contribute a thorough analysis of 13\r\nstate-of-the-art, commonly used machine learning algorithms on a set of 165\r\npublicly available classification problems in order to provide data-driven\r\nalgorithm recommendations to current researchers. We present a number of\r\nstatistical and visual comparisons of algorithm performance and quantify the\r\neffect of model selection and algorithm tuning for each algorithm and dataset.\r\nThe analysis culminates in the recommendation of five algorithms with\r\nhyperparameters that maximize classifier performance across the tested\r\nproblems, as well as general guidelines for applying machine learning to\r\nsupervised classification problems.","1771":null,"1772":"Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.","1773":"Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce Kardam, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against 1\/3 Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than f\/n, where f is the number of Byzantine failures tolerated and n the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.","1774":null,"1775":"Asynchronous distributed machine learning solutions have proven very\r\neffective so far, but always assuming perfectly functioning workers. In\r\npractice, some of the workers can however exhibit Byzantine behavior, caused by\r\nhardware failures, software bugs, corrupt data, or even malicious attacks. We\r\nintroduce Kardam, the first distributed asynchronous stochastic gradient\r\ndescent (SGD) algorithm that copes with Byzantine workers. Kardam consists of\r\ntwo complementary components: a filtering and a dampening component. The first\r\nis scalar-based and ensures resilience against $13$ Byzantine workers.\r\nEssentially, this filter leverages the Lipschitzness of cost functions and acts\r\nas a self-stabilizer against Byzantine workers that would attempt to corrupt\r\nthe progress of SGD. The dampening component bounds the convergence rate by\r\nadjusting to stale information through a generic gradient weighting scheme. We\r\nprove that Kardam guarantees almost sure convergence in the presence of\r\nasynchrony and Byzantine behavior, and we derive its convergence rate. We\r\nevaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead\r\nwith respect to non Byzantine-resilient solutions. We empirically show that\r\nKardam does not introduce additional noise to the learning procedure but does\r\ninduce a slowdown (the cost of Byzantine resilience) that we both theoretically\r\nand empirically show to be less than $f\/n$, where $f$ is the number of\r\nByzantine failures tolerated and $n$ the total number of workers.\r\nInterestingly, we also empirically observe that the dampening component is\r\ninteresting in its own right for it enables to build an SGD algorithm that\r\noutperforms alternative staleness-aware asynchronous competitors in\r\nenvironments with honest workers.","1776":"Black box machine learning models are currently being used for high stakes\r\ndecision-making throughout society, causing problems throughout healthcare,\r\ncriminal justice, and in other domains. People have hoped that creating methods\r\nfor explaining these black box models will alleviate some of these problems,\r\nbut trying to explain black box models, rather than creating models\r\nthat are interpretable in the first place, is likely to perpetuate bad\r\npractices and can potentially cause catastrophic harm to society. There is a\r\nway forward -- it is to design models that are inherently interpretable. This\r\nmanuscript clarifies the chasm between explaining black boxes and using\r\ninherently interpretable models, outlines several key reasons why explainable\r\nblack boxes should be avoided in high-stakes decisions, identifies challenges\r\nto interpretable machine learning, and provides several example applications\r\nwhere interpretable models could potentially replace black box models in\r\ncriminal justice, healthcare, and computer vision.","1777":null,"1778":"Despite its great success, machine learning can have its limits when dealing\r\nwith insufficient training data. A potential solution is the additional\r\nintegration of prior knowledge into the training process which leads to the\r\nnotion of informed machine learning. In this paper, we present a structured\r\noverview of various approaches in this field. We provide a definition and\r\npropose a concept for informed machine learning which illustrates its building\r\nblocks and distinguishes it from conventional machine learning. We introduce a\r\ntaxonomy that serves as a classification framework for informed machine\r\nlearning approaches. It considers the source of knowledge, its representation,\r\nand its integration into the machine learning pipeline. Based on this taxonomy,\r\nwe survey related research and describe how different knowledge representations\r\nsuch as algebraic equations, logic rules, or simulation results can be used in\r\nlearning systems. This evaluation of numerous papers on the basis of our\r\ntaxonomy uncovers key methods in the field of informed machine learning.","1779":"Despite its great success, machine learning can have its limits when dealing\r\nwith insufficient training data. A potential solution is the additional\r\nintegration of prior knowledge into the training process which leads to the\r\nnotion of informed machine learning. In this paper, we present a structured\r\noverview of various approaches in this field. We provide a definition and\r\npropose a concept for informed machine learning which illustrates its building\r\nblocks and distinguishes it from conventional machine learning. We introduce a\r\ntaxonomy that serves as a classification framework for informed machine\r\nlearning approaches. It considers the source of knowledge, its representation,\r\nand its integration into the machine learning pipeline. Based on this taxonomy,\r\nwe survey related research and describe how different knowledge representations\r\nsuch as algebraic equations, logic rules, or simulation results can be used in\r\nlearning systems. This evaluation of numerous papers on the basis of our\r\ntaxonomy uncovers key methods in the field of informed machine learning.","1780":"Quantum machine learning has the potential to computationally outperform\r\nclassical machine learning, but it is not yet clear whether it will actually be\r\nvaluable for practical problems. While some artificial scenarios have shown\r\nthat certain quantum machine learning techniques may be advantageous compared\r\nto their classical counterpart, it is unlikely that quantum machine learning\r\nwill outclass traditional methods on popular classical datasets such as MNIST.\r\nIn contrast, dealing with quantum data, such as quantum states or circuits, may\r\nbe the task where we can benefit from quantum methods. Therefore, it is\r\nimportant to develop practically meaningful quantum datasets for which we\r\nexpect quantum methods to be superior. In this paper, we propose a machine\r\nlearning task that is likely to soon arise in the real world: clustering and\r\nclassification of quantum circuits. We provide a dataset of quantum circuits\r\noptimized by the variational quantum eigensolver. We utilized six common types\r\nof Hamiltonians in condensed matter physics, with a range of 4 to 16 qubits,\r\nand applied ten different ans\u00e4tze with varying depths (ranging from 3 to\r\n32) to generate a quantum circuit dataset of six distinct classes, each\r\ncontaining 300 samples. We show that this dataset can be easily learned using\r\nquantum methods. In particular, we demonstrate a successful classification of\r\nour dataset using real 4-qubit devices available through IBMQ. By providing a\r\nsetting and an elementary dataset where quantum machine learning is expected to\r\nbe beneficial, we hope to encourage and ease the advancement of the field.","1781":"One of the main drivers behind the rapid recent advances in machine learning\r\nhas been the availability of efficient system support. This comes both through\r\nhardware acceleration, but also in the form of efficient software frameworks\r\nand programming models. Despite significant progress, scaling compute-intensive\r\nmachine learning workloads to a large number of compute nodes is still a\r\nchallenging task, with significant latency and bandwidth demands. In this\r\npaper, we address this challenge, by proposing SPARCML, a general, scalable\r\ncommunication layer for machine learning applications. SPARCML is built on the\r\nobservation that many distributed machine learning algorithms either have\r\nnaturally sparse communication patters, or have updates which can be sparsified\r\nin a structured way for improved performance, without any convergence or\r\naccuracy loss. To exploit this insight, we design and implement a set of\r\ncommunication efficient protocols for sparse input data, in conjunction with\r\nefficient machine learning algorithms which can leverage these primitives. Our\r\ncommunication protocols generalize standard collective operations, by allowing\r\nprocesses to contribute sparse input data vectors, of heterogeneous sizes. We\r\ncall these operations sparse-input collectives, and present efficient practical\r\nalgorithms with strong theoretical bounds on their running time and\r\ncommunication cost. Our generic communication layer is enriched with additional\r\nfeatures, such support for non-blocking (asynchronous) operations, and support\r\nfor low-precision data representations. We validate our algorithmic results\r\nexperimentally on a range of large-scale machine learning applications and\r\ntarget architectures, showing that we can leverage sparsity for order-\r\nof-magnitude runtime savings, compared to state-of-the art methods and\r\nframeworks.","1782":"TensorNetwork is an open source library for implementing tensor network\r\nalgorithms. Tensor networks are sparse data structures originally designed for\r\nsimulating quantum many-body physics, but are currently also applied in a\r\nnumber of other research areas, including machine learning. We demonstrate the\r\nuse of the API with applications both physics and machine learning, with\r\ndetails appearing in companion papers.","1783":"In this work, we introduce the concept of bandlimiting into the theory of\r\nmachine learning because all physical processes are bandlimited by nature,\r\nincluding real-world machine learning tasks. After the bandlimiting constraint\r\nis taken into account, our theoretical analysis has shown that all practical\r\nmachine learning tasks are asymptotically solvable in a perfect sense.\r\nFurthermore, the key towards this solvability almost solely relies on two\r\nfactors: i) a sufficiently large amount of training samples beyond a threshold\r\ndetermined by a difficulty measurement of the underlying task; ii) a\r\nsufficiently complex model that is properly bandlimited. Moreover, for unimodal\r\ndata distributions, we have derived a new error bound for perfect learning,\r\nwhich can quantify the difficulty of learning. This case-specific bound is much\r\ntighter than the uniform bounds in conventional learning theory.","1784":null,"1785":"The correct use of model evaluation, model selection, and algorithm selection\r\ntechniques is vital in academic machine learning research as well as in many\r\nindustrial settings. This article reviews different techniques that can be used\r\nfor each of these three subtasks and discusses the main advantages and\r\ndisadvantages of each technique with references to theoretical and empirical\r\nstudies. Further, recommendations are given to encourage best yet feasible\r\npractices in research and applications of machine learning. Common methods such\r\nas the holdout method for model evaluation and selection are covered, which are\r\nnot recommended when working with small datasets. Different flavors of the\r\nbootstrap technique are introduced for estimating the uncertainty of\r\nperformance estimates, as an alternative to confidence intervals via normal\r\napproximation if bootstrapping is computationally feasible. Common\r\ncross-validation techniques such as leave-one-out cross-validation and k-fold\r\ncross-validation are reviewed, the bias-variance trade-off for choosing k is\r\ndiscussed, and practical tips for the optimal choice of k are given based on\r\nempirical evidence. Different statistical tests for algorithm comparisons are\r\npresented, and strategies for dealing with multiple comparisons such as omnibus\r\ntests and multiple-comparison corrections are discussed. Finally, alternative\r\nmethods for algorithm selection, such as the combined F-test 5x2\r\ncross-validation and nested cross-validation, are recommended for comparing\r\nmachine learning algorithms when datasets are small.","1786":null,"1787":"Researchers and practitioners from different disciplines have highlighted the ethical and legal challenges posed by the use of machine learned models and data-driven systems, and the potential for such systems to discriminate against certain population groups, due to biases in algorithmic decision-making systems. This tutorial aims to present an overview of algorithmic bias \/ discrimination issues observed over the last few years and the lessons learned, key regulations and laws, and evolution of techniques for achieving fairness in machine learning systems. We will motivate the need for adopting a \"fairness-first\" approach (as opposed to viewing algorithmic bias \/ fairness considerations as an afterthought), when developing machine learning based models and systems for different consumer and enterprise applications. Then, we will focus on the application of fairness-aware machine learning techniques in practice, by presenting case studies from different technology companies. Based on our experiences in industry, we will identify open problems and research challenges for the data mining \/ machine learning community.","1788":"This is a collection of (mostly) pen-and-paper exercises in machine learning.\r\nThe exercises are on the following topics: linear algebra, optimisation,\r\ndirected graphical models, undirected graphical models, expressive power of\r\ngraphical models, factor graphs and message passing, inference for hidden\r\nMarkov models, model-based learning (including ICA and unnormalised models),\r\nsampling and Monte-Carlo integration, and variational inference.","1789":"Falls have become common nowadays among the elderly. It has been noted by the World Health Organization WHO that approximately one out of 3 elderly people aged above 65 living alone tend to fall and the rate can increase in the coming years. Many ideas have been proposed and worked out including using of inertial sensors, accelerometer and gyro meter. This paper proposes a method where the video from the camera is processed and the features are extracted. The features are extracted using HOG and statistical approach. The database contains fall and daily activities and Support Vector Machine SVM is used for classification which gives an accuracy of 100 . Hephzibah Thomas | Thyla B \"Intelligent Fall Detection Using Statistical Features and Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-1 , December 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd19024.pdf","1790":"Advanced machine learning methods might help to identify dementia risk from neuroimaging, but their accuracy to date is unclear.We systematically reviewed the literature, 2006 to late 2016, for machine learning studies differentiating healthy aging from dementia of various types, assessing study quality, and comparing accuracy at different disease boundaries.Of 111 relevant studies, most assessed Alzheimer's disease versus healthy controls, using AD Neuroimaging Initiative data, support vector machines, and only T1-weighted sequences. Accuracy was highest for differentiating Alzheimer's disease from healthy controls and poor for differentiating healthy controls versus mild cognitive impairment versus Alzheimer's disease or mild cognitive impairment converters versus nonconverters. Accuracy increased using combined data types, but not by data source, sample size, or machine learning method.Machine learning does not differentiate clinically relevant disease categories yet. More diverse data sets, combinations of different types of data, and close clinical integration of machine learning would help to advance the field.","1791":"We present morphological classifications obtained using machine learning for\r\nobjects in SDSS DR6 that have been classified by Galaxy Zoo into three classes,\r\nnamely early types, spirals and point sources\/artifacts. An artificial neural\r\nnetwork is trained on a subset of objects classified by the human eye and we\r\ntest whether the machine learning algorithm can reproduce the human\r\nclassifications for the rest of the sample. We find that the success of the\r\nneural network in matching the human classifications depends crucially on the\r\nset of input parameters chosen for the machine-learning algorithm. The colours\r\nand parameters associated with profile-fitting are reasonable in separating the\r\nobjects into three classes. However, these results are considerably improved\r\nwhen adding adaptive shape parameters as well as concentration and texture. The\r\nadaptive moments, concentration and texture parameters alone cannot distinguish\r\nbetween early type galaxies and the point sources\/artifacts. Using a set of\r\ntwelve parameters, the neural network is able to reproduce the human\r\nclassifications to better than 90% for all three morphological classes. We find\r\nthat using a training set that is incomplete in magnitude does not degrade our\r\nresults given our particular choice of the input parameters to the network. We\r\nconclude that it is promising to use machine- learning algorithms to perform\r\nmorphological classification for the next generation of wide-field imaging\r\nsurveys and that the Galaxy Zoo catalogue provides an invaluable training set\r\nfor such purposes.","1792":"Interpretability in machine learning (ML) is crucial for high stakes\r\ndecisions and troubleshooting. In this work, we provide fundamental principles\r\nfor interpretable ML, and dispel common misunderstandings that dilute the\r\nimportance of this crucial topic. We also identify 10 technical challenge areas\r\nin interpretable machine learning and provide history and background on each\r\nproblem. Some of these problems are classically important, and some are recent\r\nproblems that have arisen in the last few years. These problems are: (1)\r\nOptimizing sparse logical models such as decision trees; (2) Optimization of\r\nscoring systems; (3) Placing constraints into generalized additive models to\r\nencourage sparsity and better interpretability; (4) Modern case-based\r\nreasoning, including neural networks and matching for causal inference; (5)\r\nComplete supervised disentanglement of neural networks; (6) Complete or even\r\npartial unsupervised disentanglement of neural networks; (7) Dimensionality\r\nreduction for data visualization; (8) Machine learning models that can\r\nincorporate physics and other generative or causal constraints; (9)\r\nCharacterization of the \"Rashomon set\" of good models; and (10) Interpretable\r\nreinforcement learning. This survey is suitable as a starting point for\r\nstatisticians and computer scientists interested in working in interpretable\r\nmachine learning.","1793":"This document gives a concise outline of some of the common mistakes that\r\noccur when using machine learning techniques, and what can be done to avoid\r\nthem. It is intended primarily as a guide for research students, and focuses on\r\nissues that are of particular concern within academic research, such as the\r\nneed to do rigorous comparisons and reach valid conclusions. It covers five\r\nstages of the machine learning process: what to do before model building, how\r\nto reliably build models, how to robustly evaluate models, how to compare\r\nmodels fairly, and how to report results.","1794":"An extortion endeavor to get touchy and individual data like secret key, username, and bank subtleties like credit check card subtleties by concealing as a dependable association in electronic correspondence. The phishing site will show up equivalent to the genuine site and guides the client to a page to enter individual subtleties of the client on the phony site. Through AI calculations one can improve the exactness of the expectation. The proposed strategy predicts the URL put together phishing sites based with respect to highlights and furthermore gives most extreme exactness. This technique utilizes uniform asset finder URL highlights. We distinguished highlights that phishing site URLs contain. The proposed technique utilizes those highlights for phishing discovery. The proposed framework predicts the URL based phishing sites with most extreme precision. We will discuss different AI, the calculation which can help in dynamic and forecast. We will utilize one of the calculation to improve exactness of forecast. Dr. C. Umarani | Vinay Singh Dhapola \"Detection of URL Based Phishing Websites using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd35774.pdf Paper Url: https:\/\/www.ijtsrd.com\/computer-science\/artificial-intelligence\/35774\/detection-of-url-based-phishing-websites-using-machine-learning\/dr-c-umarani","1795":null,"1796":"In this project we attempt to implement machine learning approach to predict stock prices. Machine learning is effectively implemented in forecasting stock prices. The objective is to predict the stock prices in order to make more informed and accurate investment decisions. We propose a stock price prediction system that integrates mathematical functions, machine learning, and other external factors for the purpose of achieving better stock prediction accuracy and issuing profitable trades. There are two types of stocks. You may know of intraday trading by the commonly used term day trading. Intraday traders hold securities positions from at least one day to the next and often for several days to weeks or months. LSTMs are very powerful in sequence prediction problems because they\u2019re able to store past information. This is important in our case because the previous price of a stock is crucial in predicting its future price. While predicting the actual price of a stock is an uphill climb, we can build a model that will predict whether the price will go up or down. Manuri Raju | Dr. D. Jakir Hussain \u00c4 Study on Prediction of Share Price by Using Machine Learning LSTM Model\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-6 , October 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd51976.pdf Paper URL: https:\/\/www.ijtsrd.com\/other-scientific-research-area\/other\/51976\/a-study-on-prediction-of-share-price-by-using-machine-learning-lstm-model\/manuri-raju","1797":"One crucial challenge of crowdfunding is that it is hard for fundraisers and backers to anticipate the outcome of crowdfunding campaigns. Across platforms, many crowdfunding campaigns fail to achieve their funding goal. Hence, studies focusing of the outcome of crowdfunding is also growing. In this study, we implement a supervised machine learning methodology to investigate the determinants of the level of crowdfunding with emphasis on Africa. The statistical methods used in the study produced a high prediction accuracy. Irrespective of the method used, the number of backers is identified to be the most important predictor of the level of funding. Also, the average amount pledged to the project and the duration of the project are important features that predict the level of funding. Isaac Okyere Paintsil | Zhao Xicang | Oliver Joseph Abban \"Predicting the Level of Crowdfunding Outcome in Africa: A Supervised Machine Learning Approach\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42539.pdf Paper URL: https:\/\/www.ijtsrd.comeconomics\/finance\/42539\/predicting-the-level-of-crowdfunding-outcome-in-africa-a-supervised-machine-learning-approach\/isaac-okyere-paintsil","1798":"We review the development of generative modeling techniques in machine\r\nlearning for the purpose of reconstructing real, noisy, many-qubit quantum\r\nstates. Motivated by its interpretability and utility, we discuss in detail the\r\ntheory of the restricted Boltzmann machine. We demonstrate its practical use\r\nfor state reconstruction, starting from a classical thermal distribution of\r\nIsing spins, then moving systematically through increasingly complex pure and\r\nmixed quantum states. Intended for use on experimental noisy intermediate-scale\r\nquantum (NISQ) devices, we review recent efforts in reconstruction of a cold\r\natom wavefunction. Finally, we discuss the outlook for future experimental\r\nstate reconstruction using machine learning, in the NISQ era and beyond.","1799":"The question of generalization in machine learning---how algorithms are able\r\nto learn predictors from a training sample to make accurate predictions\r\nout-of-sample---is revisited in light of the recent breakthroughs in modern\r\nmachine learning technology.\r\n  The classical approach to understanding generalization is based on\r\nbias-variance trade-offs, where model complexity is carefully calibrated so\r\nthat the fit on the training sample reflects performance out-of-sample.\r\n  However, it is now common practice to fit highly complex models like deep\r\nneural networks to data with (nearly) zero training error, and yet these\r\ninterpolating predictors are observed to have good out-of-sample accuracy even\r\nfor noisy data.\r\n  How can the classical understanding of generalization be reconciled with\r\nthese observations from modern machine learning practice?\r\n  In this paper, we bridge the two regimes by exhibiting a new \"double descent\"\r\nrisk curve that extends the traditional U-shaped bias-variance curve beyond the\r\npoint of interpolation.\r\n  Specifically, the curve shows that as soon as the model complexity is high\r\nenough to achieve interpolation on the training sample---a point that we call\r\nthe \"interpolation threshold\"---the risk of suitably chosen interpolating\r\npredictors from these models can, in fact, be decreasing as the model\r\ncomplexity increases, often below the risk achieved using non-interpolating\r\nmodels.\r\n  The double descent risk curve is demonstrated for a broad range of models,\r\nincluding neural networks and random forests, and a mechanism for producing\r\nthis behavior is posited.","1800":null,"1801":"As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.","1802":"Causal Machine Learning (CausalML) is an umbrella term for machine learning\r\nmethods that formalize the data-generation process as a structural causal model\r\n(SCM). This perspective enables us to reason about the effects of changes to\r\nthis process (interventions) and what would have happened in hindsight\r\n(counterfactuals). We categorize work in CausalML into five groups according to\r\nthe problems they address: (1) causal supervised learning, (2) causal\r\ngenerative modeling, (3) causal explanations, (4) causal fairness, and (5)\r\ncausal reinforcement learning. We systematically compare the methods in each\r\ncategory and point out open problems. Further, we review data-modality-specific\r\napplications in computer vision, natural language processing, and graph\r\nrepresentation learning. Finally, we provide an overview of causal benchmarks\r\nand a critical discussion of the state of this nascent field, including\r\nrecommendations for future work.","1803":"Programming by demonstration enables users to easily personalize their\r\n\tapplications, automating repetitive tasks simply by executing a few\r\n\texamples. We formalize programming by demonstration as a machine\r\n\tlearning problem: given the changes in the application state that\r\n\tresult from the user\ufffds demonstrated actions, learn the general program\r\n\tthat maps from one application state to the next. We present a methodology\r\n\tfor learning in this space of complex functions. First we extend\r\n\tversion spaces to learn arbitrary functions, not just concepts. Then\r\n\twe introduce the version space algebra, a method for composing simpler\r\n\tversion spaces to construct more complex spaces. Finally, we apply\r\n\tour version space algebra to the text-editing domain and describe\r\n\tan implemented system called SMARTedit that learns repetitive text-editing\r\n\tprocedures by example. We evaluate our approach by measuring the\r\n\tnumber of examples required for the system to learn a procedure that\r\n\tworks on the remainder of examples, and by an informal user study\r\n\tmeasuring the effort users spend using our system versus performing\r\n\tthe task by hand. The results show that SMARTedit is capable of generalizing\r\n\tcorrectly from as few as one or two examples, and that users generally\r\n\tsave a significant amount of effort when completing tasks with SMARTedit\ufffds\r\n\thelp.","1804":"The advancement of social media plays an important role in increasing the population of youngsters on the web. And it has become the biggest medium of expressing one\u00e2\u20ac\u2122s thoughts and emotions. Recent studies report that cyberbullying constitutes a growing problem among youngsters on the web. These kinds of attacks have a major influence on the current generation\u00e2\u20ac\u2122s personal and social life because youngsters are ready to adopt online life instead of a real one, which leads them into an imaginary world. So, we are proposing a system for early detection of cyberbullying on the web and comparing different machine learning Algorithms to obtain the optimal result. We are comparing four different algorithms which can be effectively used for the detection of cyberbullying, with the implementation of the bag of words algorithm with different n gram methods. Comparatively na\u00c3\u00afve Bayes algorithm has the highest accuracy of 79 with trigram implementation of the bag of words algorithm. Rohini K R | Sreehari T Anil | Sreejith P M | Yedumohan P M \"Comparative Study of Cyberbullying Detection using Different Machine Learning Algorithms\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-3 , April 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd30765.pdf","1805":"Machine learning is a novel and powerful technology and has been widely used\r\nin various science topics. We demonstrate a machine-learning based approach\r\nbuilt by a set of general metrics and rules inspired by physics. Taking\r\nadvantages of physical constraints, such as dimension identity, symmetry and\r\ngeneralization, we succeed to rediscover the GellMann Okubo formula using a\r\ntechnique of symbolic regression. This approach can effectively find explicit\r\nsolutions among user-defined observable, and easily extend to study on exotic\r\nhadron spectrum.","1806":"Credit and Debit cards have become the choice mode of payment online as a result of the proliferation of electronic transactions and advancement in Information and Communication Technology ICT . Because of the increased use of credit cards for payment online, the number of fraud cases associated with it has also increased scammers and fraudsters are stealing credit card information of victims online and thereby stealing their monies. There is the need therefore to stop or abate these frauds using very powerful fraud detection system that detects patterns of credit card frauds in order to prevent it from occurring. In this paper we x rayed the concept of credit card frauds and how they are carried out by fraudsters. Python 3.7.6 programming language, Jupyter Notebook 6.0.3 and Anaconda Navigator 1.9.12 were used as experimental test bed. Also, we implemented two different supervised machine learning algorithms on an imbalanced dataset such as Decision Tree and Random forest techniques. A comparative analysis of the credit card detection capabilities of these machine learning algorithms were carried out to ascertain the best detection algorithm using different performance evaluation metrics such as accuracy, precision, recall, f1 score, confusion matrix. Experimental results showed that Random Forest outperformed Decision Tree algorithm slightly in performance metrics used for performance evaluation. Obodoeze Fidelis C. | Oliver Ifeoma Catherine | Onyemachi George Olisamaka | Udeh Ifeanyi Frank Gideon | Obiokafor, Ifeyinwa Nkemdilim \"Credit Cards Frauds and Cybersecurity Threats: Machine Learning Detection Algorithms as Countermeasures\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-7 , December 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd52440.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/computer-security\/52440\/credit-cards-frauds-and-cybersecurity-threats-machine-learning-detection-algorithms-as-countermeasures\/obodoeze-fidelis-c","1807":"The problem of determining whether a given quantum state is entangled lies at\r\nthe heart in quantum information processing. Despite the many methods -- such\r\nas the positive partial transpose (PPT) criterion and the $k$-symmetric\r\nextendibility criterion -- to tackle this problem, none of them enables a\r\ngeneral, practical solution due to the problem's NP-hard complexity.\r\nExplicitly, states that are separable form a high-dimensional convex set of\r\nvastly complicated structure. In this work, we build a new\r\nseparability-entanglement classifier underpinned by machine learning\r\ntechniques. Our method outperforms the existing methods in generic cases in\r\nterms of both speed and accuracy, opening up the avenues to explore quantum\r\nentanglement via the machine learning approach.","1808":"Visual analytics (VA) systems help data analysts solve complex problems interactively, by integrating automated data analysis and mining, such as machine learning (ML) based methods, with interactive visualizations. We propose a conceptual framework that models human interactions with ML components in the VA process, and that puts the central relationship between automated algorithms and interactive visualizations into sharp focus. The framework is illustrated with several examples and we further elaborate on the interactive ML process by identifying key scenarios where ML methods are combined with human feedback through interactive visualization. We derive five open research challenges at the intersection of ML and visualization research, whose solution should lead to more effective data analysis.","1809":"Depression is a general mental health disorder that presents state of low mood, negative thoughts, mental disturbance, typically with lack of energy , difficulty in maintaining concentration, guilty, irritable, restless and cognitive difficulties such as lose interest in different new things. Clinical depression is a major risk factor in suicides and is associated with high mortality rates, therefore making it one of the leading causes of death worldwide every year. The landmark World Health organisation(WHO) Global Burden of Disease (GBD) quantified depression as the second highest leading cause of disability world-wide1. It is observed that, there is increase in tendency of clinical depression in adolescents (i.e. age between 13'\u201c20 years) has been linked to a range of serious problem, basically an increase in the number of suicide attempts and deaths. This is making public health concern. In this project we are detecting whether the person is in depression or not using tensor flow software. There various biomarkers of depression like facial expressions, speech, pupil, T-body shape, MRI, EEG, etc. Here we are processing on speech feature extracted from database by SVM technique. Again among features of speech like TEO, MFCC, pitch, etc. Here we are extracting MFCC feature of speech from database. Ms. Anjum Shaikh | Ms. Firdos Shaikh | Mr. Suhaib Ramzan | Prof. M. M. Patil\"Clinical Depression Detection Using Speech Feature With Machine Learning Approach\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-4 , June 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd14363.pdf  http:\/\/www.ijtsrd.com\/engineering\/electronics-and-communication-engineering\/14363\/clinical-depression-detection-using-speech-feature-with-machine-learning-approach\/ms-anjum-shaikh","1810":null,"1811":"The many ways in which machine and deep learning are transforming the\r\nanalysis and simulation of data in particle physics are reviewed. The main\r\nmethods based on boosted decision trees and various types of neural networks\r\nare introduced, and cutting-edge applications in the experimental and\r\ntheoretical\/phenomenological domains are highlighted. After describing the\r\nchallenges in the application of these novel analysis techniques, the review\r\nconcludes by discussing the interactions between physics and machine learning\r\nas a two-way street enriching both disciplines and helping to meet the present\r\nand future challenges of data-intensive science at the energy and intensity\r\nfrontiers.","1812":"This paper is a broad and accessible survey of the methods we have at our\r\ndisposal for Monte Carlo gradient estimation in machine learning and across the\r\nstatistical sciences: the problem of computing the gradient of an expectation\r\nof a function with respect to parameters defining the distribution that is\r\nintegrated; the problem of sensitivity analysis. In machine learning research,\r\nthis gradient problem lies at the core of many learning problems, in\r\nsupervised, unsupervised and reinforcement learning. We will generally seek to\r\nrewrite such gradients in a form that allows for Monte Carlo estimation,\r\nallowing them to be easily and efficiently used and analysed. We explore three\r\nstrategies--the pathwise, score function, and measure-valued gradient\r\nestimators--exploring their historical developments, derivation, and underlying\r\nassumptions. We describe their use in other fields, show how they are related\r\nand can be combined, and expand on their possible generalisations. Wherever\r\nMonte Carlo gradient estimators have been derived and deployed in the past,\r\nimportant advances have followed. A deeper and more widely-held understanding\r\nof this problem will lead to further advances, and it is these advances that we\r\nwish to support.","1813":"Functional MRI (fMRI) and diffusion MRI (dMRI) are non-invasive imaging\r\nmodalities that allow in-vivo analysis of a patient's brain network (known as a\r\nconnectome). Use of these technologies has enabled faster and better diagnoses\r\nand treatments of neurological disorders and a deeper understanding of the\r\nhuman brain. Recently, researchers have been exploring the application of\r\nmachine learning models to connectome data in order to predict clinical\r\noutcomes and analyze the importance of subnetworks in the brain. Connectome\r\ndata has unique properties, which present both special challenges and\r\nopportunities when used for machine learning. The purpose of this work is to\r\nreview the literature on the topic of applying machine learning models to\r\nMRI-based connectome data. This field is growing rapidly and now encompasses a\r\nlarge body of research. To summarize the research done to date, we provide a\r\ncomparative, structured summary of 77 relevant works, tabulated according to\r\ndifferent criteria, that represent the majority of the literature on this\r\ntopic. (We also published a living version of this table online at\r\nhttp:\/\/connectomelearning.cs.sfu.ca that the community can continue to\r\ncontribute to.) After giving an overview of how connectomes are constructed\r\nfrom dMRI and fMRI data, we discuss the variety of machine learning tasks that\r\nhave been explored with connectome data. We then compare the advantages and\r\ndrawbacks of different machine learning approaches that have been employed,\r\ndiscussing different feature selection and feature extraction schemes, as well\r\nas the learning models and regularization penalties themselves. Throughout this\r\ndiscussion, we focus particularly on how the methods are adapted to the unique\r\nnature of graphical connectome data. Finally, we conclude by summarizing the\r\ncurrent state of the art and by outlining what we believe are strategic\r\ndirections for future research.","1814":"With the fast improvement of neuroimaging data acquisition strategies, there has been a significant growth in learning neurological disorders among data mining and machine learning communities. Neurological disorders are the ones that impact the central nervous system (including the human brain) and also include over 600 disorders ranging from brain aneurysm to epilepsy. Every year, based on World Health Organization (WHO), neurological disorders affect much more than one billion people worldwide and count for up to seven million deaths. Hence, useful investigation of neurological disorders is actually of great value. The vast majority of datasets useful for diagnosis of neurological disorders like electroencephalogram (EEG) are actually complicated and poses challenges that are many for data mining and machine learning algorithms due to their increased dimensionality, non stationarity, and non linearity. Hence, an better feature representation is actually key to an effective suite of data mining and machine learning algorithms in the examination of neurological disorders. With this exploration, we use a well defined EEG dataset to train as well as test out models. A preprocessing stage is actually used to extend, arrange and manipulate the framework of free data sets to the needs of ours for better training and tests results. Several techniques are used by us to enhance system accuracy. This particular paper concentrates on dealing with above pointed out difficulties and appropriately analyzes different EEG signals that would in turn help us to boost the procedure of feature extraction and enhance the accuracy in classification. Along with acknowledging above issues, this particular paper proposes a framework that would be useful in determining man stress level and also as a result, differentiate a stressed or normal person\/subject. Vijaykumar Janga | Prof. E Sreenivasa Reddy\u00c4 review on Machine Learning Techniques for Neurological disorders estimation by Analyzing EEG Waves\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-1 , December 2017, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd7082.pdf  http:\/\/www.ijtsrd.com\/engineering\/information-technology\/7082\/a-review-on-machine-learning-techniques-for-neurological-disorders-estimation-by-analyzing-eeg-waves\/vijaykumar-janga","1815":"Accurate Medical diagnosis is not always possible at every medical center, especially in the Developing Countries where poor healthcare services and lack of advanced diagnostic methods and equipments affects procedures of medical diagnosis .Also, physician intuition and experience are not always sufficient to achieve high quality medical procedures results. Therefore, diagnostic errors and undesirable results are reasons for a need for Machine Learning Techniques based decision support system, which in turns reduce diagnostic errors, increasing the patient safety and save lives. This research focuses on this aspect of Medical diagnosis by learning pattern through the collected dataset of respiratory diseases such as pneumonia and Covid 19, also consist implementation and test of intelligent medical decision support system to assist physicians and radiologists can deliver great assistance by improving their decision making ability. In this Research paper, the proposed System use Neural network Resnet 50 and transfer learning technique to classify these severe diseases and performs evaluation of precision, accuracy, speci city of Decision support system. Patel Smitkumar Hareshbhai \"Role of Advanced Machine Learning Techniques and Deep Learning Approach Based Decision Support System for Accurate Diagnosis of Severe Respiratory Diseases\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-6 , October 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd47655.pdf Paper URL : https:\/\/www.ijtsrd.com\/engineering\/bio-mechanicaland-biomedical-engineering\/47655\/role-of-advanced-machine-learning-techniques-and-deep-learning-approach-based-decision-support-system-for-accurate-diagnosis-of-severe-respiratory-diseases\/patel-smitkumar-hareshbhai","1816":"This article presents WekaCoin, a peer-to-peer cryptocurrency based on a new distributed consensus protocol called Proof-of-Learning. Proof-of-learning achieves distributed consensus by ranking machine learning systems for a given task. The aim of this protocol is to alleviate the computational waste involved in hashing-based puzzles and to create a public distributed and verifiable database of state-of-the-art machine learning models and experiments.","1817":"In machine learning tasks, especially in the tasks of prediction, scientists\r\ntend to rely solely on available historical data and disregard unproven\r\ninsights, such as experts' opinions, polls, and betting odds. In this paper, we\r\npropose a general three-step framework for utilizing experts' insights in\r\nmachine learning tasks and build four concrete models for a sports game\r\nprediction case study. For the case study, we have chosen the task of\r\npredicting NCAA Men's Basketball games, which has been the focus of a group of\r\nKaggle competitions in recent years. Results highly suggest that the good\r\nperformance and high scores of the past models are a result of chance, and not\r\nbecause of a good-performing and stable model. Furthermore, our proposed models\r\ncan achieve more steady results with lower log loss average (best at 0.489)\r\ncompared to the top solutions of the 2019 competition (>0.503), and reach the\r\ntop 1%, 10% and 1% in the 2017, 2018 and 2019 leaderboards, respectively.","1818":"We introduce PrivPy, a practical privacy-preserving collaborative computation\r\nframework, especially optimized for machine learning tasks. PrivPy provides an\r\neasy-to- use and highly compatible Python programming front- end which supports\r\nhigh-level array operations and different secure computation engines to allow\r\nfor security assumptions and performance trade-offs. With PrivPy, programmers\r\ncan write modern machine learning algorithms conveniently and efficiently in\r\nPython. We also design and implement a new efficient computation engine, with\r\nwhich people can use competing cloud providers to efficiently perform general\r\narithmetics over real numbers. We demonstrate the usability and scalability of\r\nPrivPy using common machine learning models (e.g. logistic regression and\r\nconvolutional neural networks) and real- world datasets (including a\r\n5000-by-1-million matrix).","1819":"We describe TF-Replicator, a framework for distributed machine learning\r\ndesigned for DeepMind researchers and implemented as an abstraction over\r\nTensorFlow. TF-Replicator simplifies writing data-parallel and model-parallel\r\nresearch code. The same models can be effortlessly deployed to different\r\ncluster architectures (i.e. one or many machines containing CPUs, GPUs or TPU\r\naccelerators) using synchronous or asynchronous training regimes. To\r\ndemonstrate the generality and scalability of TF-Replicator, we implement and\r\nbenchmark three very different models: (1) A ResNet-50 for ImageNet\r\nclassification, (2) a SN-GAN for class-conditional ImageNet image generation,\r\nand (3) a D4PG reinforcement learning agent for continuous control. Our results\r\nshow strong scalability performance without demanding any distributed systems\r\nexpertise of the user. The TF-Replicator programming model will be open-sourced\r\nas part of TensorFlow 2.0 (see\r\nhttps:\/\/github.com\/tensorflow\/community\/pull\/25).","1820":"Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.","1821":null,"1822":"In this time, Novel Corona Virus is an important issue in the world, it also named COVID 19. This virus has been come from Wuhan, China in last December 2019. This virus has created critical circumstances in the whole world especially Bangladesh. The outbreak of COVID 19 is increasing gradually in Bangladesh. To predict and forecasting COVID 19 in Bangladesh we have used machine learning ML Linear Regression model. LR model is useful to predict the outbreak of COVID 19 in Bangladesh. It can be helped efficiently to predict some common numerical data like observation day, tested case, affected case, death case, recover cases, and forecast the number of upcoming cases for the next 30 days in Bangladesh. Our paper to study to analyze the epidemic growth of the COVID 19 in Bangladesh. We have applied the mathematical regression model to analyze the prediction and forecast for the effective threat of the COVID 19 in Bangladesh. The main objective of this paper how to predict the virus affected cases, recover cases, death cases, tested cases, and forecasting the future situation of Bangladesh. S M Abdullah Al Shuaeb | Md. Kamruzaman | Mohammad Al-Amin \"COVID-19 Outbreak Prediction and Forecasting in Bangladesh using Machine Learning Algorithm\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-1 , December 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd38068.pdf Paper URL : https:\/\/www.ijtsrd.com\/computer-science\/other\/38068\/covid19-outbreak-prediction-and-forecasting-in-bangladesh-using-machine-learning-algorithm\/s-m-abdullah-al-shuaeb","1823":"Induction of a concept description given noisy instances is difficult and is further exacerbated when the concepts may change over time. This paper presents a solution which has been guided by psychological and mathematical results. The method is based on a distributed concept description which is composed of a set of weighted, symbolic characterizations. Two learning processes incrementally modify this description. One adjusts the characterization weights and another creates new characterizations. The latter process is described in terms of a search through the space of possibilities and is shown to require linear space with respect to the number of attribute-value pairs in the description language. The method utilizes previously acquired concept definitions in subsequent learning by adding an attribute for each learned concept to instance descriptions. A program called STAGGER fully embodies this method, and this paper reports on a number of empirical analyses of its performance. Since understanding the relationships between a new learning method and existing ones can be difficult, this paper first reviews a framework for discussing machine learning systems and then describes STAGGER in that framework.","1824":null,"1825":null,"1826":"Machine learning-based modeling of physical systems has experienced increased\r\ninterest in recent years. Despite some impressive progress, there is still a\r\nlack of benchmarks for Scientific ML that are easy to use but still challenging\r\nand representative of a wide range of problems. We introduce PDEBench, a\r\nbenchmark suite of time-dependent simulation tasks based on Partial\r\nDifferential Equations (PDEs). PDEBench comprises both code and data to\r\nbenchmark the performance of novel machine learning models against both\r\nclassical numerical simulations and machine learning baselines. Our proposed\r\nset of benchmark problems contribute the following unique features: (1) A much\r\nwider range of PDEs compared to existing benchmarks, ranging from relatively\r\ncommon examples to more realistic and difficult problems; (2) much larger\r\nready-to-use datasets compared to prior work, comprising multiple simulation\r\nruns across a larger number of initial and boundary conditions and PDE\r\nparameters; (3) more extensible source codes with user-friendly APIs for data\r\ngeneration and baseline results with popular machine learning models (FNO,\r\nU-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to\r\nextend the benchmark freely for their own purposes using a standardized API and\r\nto compare the performance of new models to existing baseline methods. We also\r\npropose new evaluation metrics with the aim to provide a more holistic\r\nunderstanding of learning methods in the context of Scientific ML. With those\r\nmetrics we identify tasks which are challenging for recent ML methods and\r\npropose these tasks as future challenges for the community. The code is\r\navailable at https:\/\/github.com\/pdebench\/PDEBench.","1827":"One of the most important contributors to the expansion and progression of a nation's economy is its banking and financial industry. In particular, over the recent past, there has been a significant increase in the utilization of credit and debit cards, whereby all customers trade transactions either digitally over the internet or physically at the stores. Here, the customers, banking institutions, and financial organizations are all being put in a difficult position by fraudulent actors. Because more recent technology is now readily available, internet banking has become an important avenue for commercial transactions. Fake banking activities and fraudulent transactions are serious problem that affects both the users' sense of safety and their trust in the system. In addition, fraudulent activities result in enormous losses because of the proliferation of sophisticated frauds such as virus infections, scams, and fake websites. These frauds are all examples of advanced fraud. This study makes three contributions toward the prevention of fraudulent activity involving credit card transactions.","1828":"Machine-learning models have demonstrated great success in learning complex\r\npatterns that enable them to make predictions about unobserved data. In\r\naddition to using models for prediction, the ability to interpret what a model\r\nhas learned is receiving an increasing amount of attention. However, this\r\nincreased focus has led to considerable confusion about the notion of\r\ninterpretability. In particular, it is unclear how the wide array of proposed\r\ninterpretation methods are related, and what common concepts can be used to\r\nevaluate them.\r\n  We aim to address these concerns by defining interpretability in the context\r\nof machine learning and introducing the Predictive, Descriptive, Relevant (PDR)\r\nframework for discussing interpretations. The PDR framework provides three\r\noverarching desiderata for evaluation: predictive accuracy, descriptive\r\naccuracy and relevancy, with relevancy judged relative to a human audience.\r\nMoreover, to help manage the deluge of interpretation methods, we introduce a\r\ncategorization of existing techniques into model-based and post-hoc categories,\r\nwith sub-groups including sparsity, modularity and simulatability. To\r\ndemonstrate how practitioners can use the PDR framework to evaluate and\r\nunderstand interpretations, we provide numerous real-world examples. These\r\nexamples highlight the often under-appreciated role played by human audiences\r\nin discussions of interpretability. Finally, based on our framework, we discuss\r\nlimitations of existing methods and directions for future work. We hope that\r\nthis work will provide a common vocabulary that will make it easier for both\r\npractitioners and researchers to discuss and choose from the full range of\r\ninterpretation methods.","1829":"A well ordered traffic management system is required in all types of roads, such as off roads, highways, etc. There has been several laws and speed controlled measures are taken in all places with different perspectives. Also Speed limit may vary from road to road. So there are number of methods has been proposed using computer Vision and machine learning algorithms for object tracking. Here vehicles are recognized and detected from the videos that taken using surveillance camera. The aim is to identification of the vehicles and tracking using Haar Classifier, then determine the speed of the vehicle and Finally Detecting the License plate of the vehicle. Detecting the License plate and vehicle speed using machine learning is tough but beneficial task. For the past few years Convolution Neural Network CNN has been widely used in computer vision for vehicle detection and identification. Dlibs are used to track the multiple objects at the same time. P. Devi Mahalakshmi | Dr. M. Babu \u00c4utomated License Plate detection and Speed estimation of Vehicle Using Machine Learning - Haar Classifier Algorithm\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33395.pdf Paper Url: https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/33395\/automated-license-plate-detection-and-speed-estimation-of-vehicle-using-machine-learning--haar-classifier-algorithm\/p-devi-mahalakshmi","1830":"We showcase a variety of functions and classes that implement sampling\r\nprocedures with improved exploration of the parameter space assisted by machine\r\nlearning. Special attention is paid to setting sane defaults with the objective\r\nthat adjustments required by different problems remain minimal. This collection\r\nof routines can be employed for different types of analysis, from finding\r\nbounds on the parameter space to accumulating samples in areas of interest. In\r\nparticular, we discuss two methods assisted by incorporating different machine\r\nlearning models: regression and classification. We show that a machine learning\r\nclassifier can provide higher efficiency for exploring the parameter space.\r\nAlso, we introduce a boosting technique to improve the slow convergence at the\r\nstart of the process. The use of these routines is better explained with the\r\nhelp of a few examples that illustrate the type of results one can obtain. We\r\nalso include examples of the code used to obtain the examples as well as\r\ndescriptions of the adjustments that can be made to adapt the calculation to\r\nother problems. We finalize by showing the impact of these techniques when\r\nexploring the parameter space of the two Higgs doublet model that matches the\r\nmeasured Higgs Boson signal strength. The code used for this paper and\r\ninstructions on how to use it are available on the web.","1831":"A Drowsy Driver Detection System has been created, utilizing a non meddling machine Vision based absolutely ideas. The framework utilizes a touch monochrome surveillance camera that focuses Directly towards the drivers face and screens the drivers eyes along these lines on watch weakness. In Such a case once exhaustion is identified, an alarm is given to caution the main impetus. This Report depicts the gratitude to see the eyes, and together the gratitude to check if the eyes zone unit open or Closed. The algorithmic standard created is restrictive to any directly unconcealed papers, that was a Primary goal of the venture. The framework manages exploitation data acquired for the Binary form of the picture to go glancing out the edges of the face, that limits the domain of where the Eyes may exist. When the face region is discovered, the eyes zone unit found by registering the flat Averages at stretches the region. Taking into thought the data that eye locales at stretches the face blessing decent power changes, the eyes zone unit put by finding the various force changes at spans the face. When the eyes zone unit set, live the separations between the force changes at spans the consideration zone confirm whether or not the eyes region unit open or shut. AN outsized separation relates to Eye conclusion. On the off chance that the eyes region unit discovered shut for 5 back to back edges, the framework draws in the Conclusion that the main impetus is nodding off and gives an alarm. The framework is likewise ready to watch once the eyes can not be found, and works beneath modest lighting Conditions. here we will in general also track client live area on the off chance that any crisis shows up, at that point framework precisely send area to closest emergency clinic, police central command comparatively its individuals from the family. right now we will in general also notice client square measure alcoholic or not by abuse liquor police work sensors. Here we will say that our framework is extra affordable that current frameworks. Shruti Chandrakant Zarekar | Priyanka Dattatray Desai | Manjusha Prabhakar Randhave| Surajsingh Rajendrasingh Chauhan | Dr. Shyam Gupta \"Drowsiness and Alcohol Detection for Accident Prevention using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33587.pdf Paper Url: https:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/33587\/drowsiness-and-alcohol-detection-for-accident-prevention-using-machine-learning\/shruti-chandrakant-zarekar","1832":"In this paper we have perform the political fact checking and fake news detection using various technologies such as Python libraries , Anaconda , and algorithm such as Na\u00c3\u00afve Bayes, we present an analytical study on the language of news media. To find linguistic features of untrustworthy text, we compare the language of real news with that of satire, hoaxes, and propaganda. We are also presenting a case study based on PolitiFact.com using their factuality judgments on a 6 point scale to prove the feasibility of automatic political fact checking. Experiments show that while media fact checking remains an open research issue, stylistic indications can help determine the veracity of the text. Chandni Jain | S. Vignesh \"Era of Sociology News Rumors News Detection using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-3 , April 2019, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd23534.pdf","1833":null,"1834":null,"1835":null,"1836":"The paper focuses on predicting the Nifty 50 Index by using 8 Supervised\r\nMachine Learning Models. The techniques used for empirical study are Adaptive\r\nBoost (AdaBoost), k-Nearest Neighbors (kNN), Linear Regression (LR), Artificial\r\nNeural Network (ANN), Random Forest (RF), Stochastic Gradient Descent (SGD),\r\nSupport Vector Machine (SVM) and Decision Trees (DT). Experiments are based on\r\nhistorical data of Nifty 50 Index of Indian Stock Market from 22nd April, 1996\r\nto 16th April, 2021, which is time series data of around 25 years. During the\r\nperiod there were 6220 trading days excluding all the non trading days. The\r\nentire trading dataset was divided into 4 subsets of different size-25% of\r\nentire data, 50% of entire data, 75% of entire data and entire data. Each\r\nsubset was further divided into 2 parts-training data and testing data. After\r\napplying 3 tests- Test on Training Data, Test on Testing Data and Cross\r\nValidation Test on each subset, the prediction performance of the used models\r\nwere compared and after comparison, very interesting results were found. The\r\nevaluation results indicate that Adaptive Boost, k- Nearest Neighbors, Random\r\nForest and Decision Trees under performed with increase in the size of data\r\nset. Linear Regression and Artificial Neural Network shown almost similar\r\nprediction results among all the models but Artificial Neural Network took more\r\ntime in training and validating the model. Thereafter Support Vector Machine\r\nperformed better among rest of the models but with increase in the size of data\r\nset, Stochastic Gradient Descent performed better than Support Vector Machine.","1837":"We extend a machine learning (ML) framework presented previously to model\r\ngalaxy formation and evolution in a hierarchical universe using N-body +\r\nhydrodynamical simulations. In this work, we show that ML is a promising\r\ntechnique to study galaxy formation in the backdrop of a hydrodynamical\r\nsimulation. We use the Illustris Simulation to train and test various\r\nsophisticated machine learning algorithms. By using only essential dark matter\r\nhalo physical properties and no merger history, our model predicts the gas\r\nmass, stellar mass, black hole mass, star formation rate, $g-r$ color, and\r\nstellar metallicity fairly robustly. Our results provide a unique and powerful\r\nphenomenological framework to explore the galaxy-halo connection that is built\r\nupon a solid hydrodynamical simulation. The promising reproduction of the\r\nlisted galaxy properties demonstrably place ML as a promising and a\r\nsignificantly more computationally efficient tool to study small-scale\r\nstructure formation. We find that ML mimics a full-blown hydrodynamical\r\nsimulation surprisingly well in a computation time of mere minutes. The\r\npopulation of galaxies simulated by ML, while not numerically identical to\r\nIllustris, is statistically and physically robust and follows the same\r\nfundamental observational constraints. Machine learning offers an intriguing\r\nand promising technique to create quick mock galaxy catalogs in the future.","1838":"BACKGROUND:Virtually all currently available microRNA target site prediction algorithms require the presence of a (conserved) seed match to the 5' end of the microRNA. Recently however, it has been shown that this requirement might be too stringent, leading to a substantial number of missed target sites.RESULTS:We developed TargetSpy, a novel computational approach for predicting target sites regardless of the presence of a seed match. It is based on machine learning and automatic feature selection using a wide spectrum of features covering current biological knowledge. Our model does not rely on evolutionary conservation, which allows for the detection of species-specific interactions and makes TargetSpy suitable for analyzing unconserved genomic sequences. \r\nIn order to allow for an unbiased comparison of TargetSpy to other methods, we classified all algorithms into three groups: I) no seed match requirement, II) seed match requirement, and III) conserved seed match requirement. TargetSpy predictions for classes II and III are generated by appropriate postfiltering. On a human dataset revealing fold-change in protein production for five selected microRNAs our method shows superior performance in all classes. In Drosophila melanogaster not only our class II and III predictions are on par with other algorithms, but notably the class I (no-seed) predictions are just marginally less acurate.  We estimate that TargetSpy predicts between 26 and 112 functional target sites without a seed match per microRNA that are missed by all other currently available algorithms. CONCLUSION:Only a few algorithms can predict target sites without demanding a seed match and TargetSpy demonstrates a substantial improvement in prediction accuracy in that class. Furthermore, when conservation and the presence of a seed match are required, the performance is comparable with state-of-the-art algorithms. TargetSpy was trained on mouse and performs well in human and drosophila, suggesting that it may be applicable to a broad range of species. Moreover, we have demonstrated that the application of machine learning techniques in combination with upcoming deep sequencing data results in a powerful microRNA target site prediction tool (www.targetspy.org).","1839":"Machine learning for therapeutics is an emerging field with incredible\r\nopportunities for innovation and expansion. Despite the initial success, many\r\nkey challenges remain open. Here, we introduce Therapeutics Data Commons (TDC),\r\nthe first unifying framework to systematically access and evaluate machine\r\nlearning across the entire range of therapeutics. At its core, TDC is a\r\ncollection of curated datasets and learning tasks that can translate\r\nalgorithmic innovation into biomedical and clinical implementation. To date,\r\nTDC includes 66 machine learning-ready datasets from 22 learning tasks,\r\nspanning the discovery and development of safe and effective medicines. TDC\r\nalso provides an ecosystem of tools, libraries, leaderboards, and community\r\nresources, including data functions, strategies for systematic model\r\nevaluation, meaningful data splits, data processors, and molecule generation\r\noracles. All datasets and learning tasks are integrated and accessible via an\r\nopen-source library. We envision that TDC can facilitate algorithmic and\r\nscientific advances and accelerate development, validation, and transition into\r\nproduction and clinical implementation. TDC is a continuous, open-source\r\ninitiative, and we invite contributions from the research community. TDC is\r\npublicly available at https:\/\/tdcommons.ai.","1840":"The final goal of all industrial machine learning (ML) projects is to develop\r\nML products and rapidly bring them into production. However, it is highly\r\nchallenging to automate and operationalize ML products and thus many ML\r\nendeavors fail to deliver on their expectations. The paradigm of Machine\r\nLearning Operations (MLOps) addresses this issue. MLOps includes several\r\naspects, such as best practices, sets of concepts, and development culture.\r\nHowever, MLOps is still a vague term and its consequences for researchers and\r\nprofessionals are ambiguous. To address this gap, we conduct mixed-method\r\nresearch, including a literature review, a tool review, and expert interviews.\r\nAs a result of these investigations, we provide an aggregated overview of the\r\nnecessary principles, components, and roles, as well as the associated\r\narchitecture and workflows. Furthermore, we furnish a definition of MLOps and\r\nhighlight open challenges in the field. Finally, this work provides guidance\r\nfor ML researchers and practitioners who want to automate and operate their ML\r\nproducts with a designated set of technologies.","1841":null,"1842":"Because of increment in measure of Hindi substance on the web in past years, there are more prerequisites to perform feeling examination for Hindi Language. Conclusion Analysis (SA) is an undertaking which discovers introduction of one's feeling in a snippet of data as for an element. It manages examining feelings, sentiments, and the state of mind of a speaker or an author from a given snippet of data. Estimation Analysis includes catching of client's conduct, different preferences of a person from the content. In this research study HindiSentiWordNet (HSWN) to find the overall sentiment associated with the document  polarity of words in the review are extracted from HSWN and then final aggregated polarity is calculated which can sum as either positive, negative or neutral. Synset replacement algorithm is used to find polarity of those words which don't have polarity associated with it in HSWN. Negation and discourse relations which are mostly present in Hindi movie review are also handled to improve the performance of the system. For this genre we present three different approaches for performing sentiment classification such as- 1. Using Subjective Lexicon 2. N-Gram Method 3. Weighed N-Gram Sheetal Sharma | S K Bharti | Raj Kumar Goel\u00c4 Frame Study on Sentiment Analysis of Hindi Language Using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-4 , June 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd14397.pdf  http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/14397\/a-frame-study-on-sentiment-analysis-of-hindi-language-using-machine-learning\/sheetal-sharma","1843":"The quantity of computing application that interacts with users through gesture or body motion has been growing. Among these applications is the sign language recognizer used to help hearing impaired people. This work proposes an architecture able to recognize Brazilian sign language (LIBRAS) in an embedded platform. The system focuses on a simple feature from 'finger spelling expressions' represented by a series of hands gestural images, and uses the Extreme Learning Machine network to classify them. The proposed structure uses camera images only and does not need any gloves or sensors. The obtained results are 5 times faster and 16 times better than classical approaches.","1844":"Email has turned out to be a standout amongst the most essential types of correspondence. Lately, everybody utilizes email. Ordinary billions of messages are being passed around and many spam messages are additionally sent. Spam messages are essentially messages that are intended to advance an item or benefit and are conveyed in mass to various email addresses. Spam is a major issue for everybody from the individual home Internet client to the multi-national organization that relies on upon email correspondences to direct business. Not exclusively is it a disturbance, it can likewise show a security danger to our system. It requires a great deal of investment to sift through the spam from which are truly essential. Spam shirking is vital from a security viewpoint. The point is to locate the best strategy to decide the importance of the email is coming in with the littlest misclassification rate. R. Aswin | E. Ganesh | M. Babu\"E-Mail Security Algorithm to Filter Out Spam E-mails using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd10815.pdf  http:\/\/www.ijtsrd.com\/engineering\/computer-engineering\/10815\/e-mail-security-algorithm-to-filter-out-spam-e-mails-using-machine-learning\/r-aswin","1845":"Current machine learning systems operate, almost exclusively, in a\r\nstatistical, or model-free mode, which entails severe theoretical limits on\r\ntheir power and performance. Such systems cannot reason about interventions and\r\nretrospection and, therefore, cannot serve as the basis for strong AI. To\r\nachieve human level intelligence, learning machines need the guidance of a\r\nmodel of reality, similar to the ones used in causal inference tasks. To\r\ndemonstrate the essential role of such models, I will present a summary of\r\nseven tasks which are beyond reach of current machine learning systems and\r\nwhich have been accomplished using the tools of causal modeling.","1846":"Molecular biology and all the biomedical sciences are undergoing a true revolution as a result of the emergence and growing impact of a series of new disciplines and tools sharing the \u2019-omics\u2019 suffix in their name. These include in particular genomics, transcriptomics, proteomics and metabolomics, devoted respectively to the examination of the entire systems of genes, transcripts, proteins and metabolites present in a given cell or tissue type. The availability of these new, highly effective tools for biological exploration is dramatically changing the way one performs research in at least two respects. First, the amount of available experimental data is not a limiting factor any more; on the contrary, there is a plethora of it. Given the research question, the challenge has shifted towards identifying the relevant pieces of information and making sense out of it (a \u2019data mining\u2019 issue). Second, rather than focus on components in isolation, we can now try to understand how biological systems behave as a result of the integration and interaction between the individual components that one can now monitor simultaneously, so called \u2019systems biology\u2019. Machine learning naturally appears as one of the main drivers of progress in this context, where most of the targets of interest deal with complex structured objects: sequences, 2D and 3D structures or interaction networks. At the same time bioinformatics and systems biology have already induced significant new developments of general interest in machine learning, for example in the context of learning with structured data, graph inference, semi- supervised learning, system identification, and novel combinations of optimization and learning algorithms. This book contains the scientific contributions presented at the Third International Workshop on Machine Learning in Systems Biology (MLSB\u20192009), held in Ljubljana, Slovenia from September 5 to 6, 2009. The workshop was organized as a core event of the PASCAL2 Network of Excellence, under the IST programme of European Union. The aim of the workshop was to contribute to the cross-fertilization between the research in machine learning methods and their applications to systems biology (i.e., complex biological and medical questions) by bringing together method developers and experimentalists. The technical program of the workshop consisted of invited lectures, oral presentations and poster presentations. Invited lectures were given by Diego di Bernardo, Roman Jerala, Nick Juty, Yannis Kalaidzidis, Ross D. King, and William Stafford Noble. Twelve oral presentations were given, for which extended abstracts (papers) are included in this book: these were selected from 18 submissions, each reviewed by three members of the scientific program committee. Twenty-two poster presentations were given, for which one-page abstracts are included here. We would like to thank all the people contributing to the technical programme, the scientific program committee, the local organizers and the sponsors for making the workshop possible. Ljubljana, September 2009 Saso Dzeroski, Pierre Geurts and Juho Rousu","1847":"These days, security threats detection, generally discussed to as intrusion, has befitted actual significant and serious problem in network, information and data security. Thus, an intrusion detection system (IDS) has befitted actual important element in computer or network security. Avoidance of such intrusions wholly bases on detection ability of Intrusion Detection System (IDS) which productions necessary job in network security such it identifies different kinds of attacks in network. Moreover, the data mining has been playing an important job in the different disciplines of technologies and sciences. For computer security, data mining are presented for serving intrusion detection System (IDS) to detect intruders accurately. One of the vital techniques of data mining is characteristic, so we suggest Intrusion Detection System utilizing data mining approach: SVM (Support Vector Machine). In suggest system, the classification will be through by employing SVM and realization concerning the suggested system efficiency will be accomplish by executing a number of experiments employing KDD Cup\u201999 dataset. SVM (Support Vector Machine) is one of the best distinguished classification techniques in the data mining region. KDD Cup\u201999 data set is utilized to execute several investigates in our suggested system. The experimental results illustration that we can decrease wide time is taken to construct SVM model by accomplishment suitable data set pre-processing. False Positive Rate (FPR) is decrease and Attack detection rate of SVM is increased .applied with classification algorithm gives the accuracy highest result. Implementation Environment Intrusion detection system is implemented using Mat lab 2015 programming language, and the examinations have been implemented in the environment of Windows-7 operating system mat lab R2015a, the processor: Core i7- Duo CPU 2670, 2.5 GHz, and (8GB) RAM.","1848":"Seasonal prediction of summer rainfall is crucial to reduction of regional disasters, but currently it has a low prediction skill. We developed a dynamical and machine learning hybrid (MLD) seasonal prediction method for summer rainfall in China based on circulation fields from the Chinese Academy of Sciences (CAS) Flexible Global Ocean-Atmosphere-Land System Model finite volume version 2 (FGOALS-f2) operational dynamical prediction model. Through selecting optimum hyperparameters for three machine learning methods to obtain the best fit and least overfitting, an ensemble mean of the random forest and gradient boosting regression tree methods was shown to have the highest prediction skill measured by the anomalous correlation coefficient. The skill has an average value of 0.34 in the historical cross-validation period (1981--2010) and 0.20 in the 10-yr period (2011--2020) of independent prediction, which significantly improves the dynamical prediction skill by 400\\%. Both reducing overfitting and using the best dynamical prediction are important in applications of the MLD method and in-depth analysis of these warrants a further investigation.","1849":"African and South American (ASA) wildfires account for more than 70 % of global burned areas and have strong connection to local climate for sub-seasonal to seasonal wildfire dynamics. However, representation of the wildfire-climate relationship remains challenging, due to spatiotemporally heterogenous responses of wildfires to climate variability and human influences. Here, we developed an interpretable Machine Learning (ML) fire model (AttentionFire_v1.0) to resolve the complex spatial- heterogenous and time-lagged controls from climate on burned area and to better predict burned areas over ASA regions. Our ML fire model substantially improved predictability of burned area for both spatial and temporal dynamics compared with five commonly used machine learning models. More importantly, the model revealed strong time-lagged control from climate wetness on the burned areas. The model also predicted that under a high emission future climate scenario, the recently observed declines in burned area will reverse in South America in the near future due to climate changes. Our study provides reliable and interpretable fire model and highlights the importance of lagged wildfire-climate relationships in historical and future predictions.","1850":"Understanding why machine learning models behave the way they do empowers\nboth system designers and end-users in many ways: in model selection, feature\nengineering, in order to trust and act upon the predictions, and in more\nintuitive user interfaces. Thus, interpretability has become a vital concern in\nmachine learning, and work in the area of interpretable models has found\nrenewed interest. In some applications, such models are as accurate as\nnon-interpretable ones, and thus are preferred for their transparency. Even\nwhen they are not accurate, they may still be preferred when interpretability\nis of paramount importance. However, restricting machine learning to\ninterpretable models is often a severe limitation. In this paper we argue for\nexplaining machine learning predictions using model-agnostic approaches. By\ntreating the machine learning models as black-box functions, these approaches\nprovide crucial flexibility in the choice of models, explanations, and\nrepresentations, improving debugging, comparison, and interfaces for a variety\nof users and models. We also outline the main challenges for such methods, and\nreview a recently-introduced model-agnostic explanation approach (LIME) that\naddresses these challenges.","1851":null,"1852":null,"1853":null,"1854":"This paper reviews and extends the field of similarity-based classification, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classification problems. Specifically, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning.","1855":"In-depth modeling of the complex interplay among multiple omics data measured from cancer cell lines or patient tumors is providing new opportunities toward identification of tailored therapies for individual cancer patients. Supervised machine learning algorithms are increasingly being applied to the omics profiles as they enable integrative analyses among the high-dimensional data sets, as well as personalized predictions of therapy responses using multi-omics panels of response-predictive biomarkers identified through feature selection and cross-validation. However, technical variability and frequent missingness in input ``big data'' require the application of dedicated data preprocessing pipelines that often lead to some loss of information and compressed view of the biological signal. We describe here the state-of-the-art machine learning methods for anti-cancer drug response modeling and prediction and give our perspective on further opportunities to make better use of high-dimensional multi-omics profiles along with knowledge about cancer pathways targeted by anti-cancer compounds when predicting their phenotypic responses.","1856":"This article provides an overview of Supervised Machine Learning (SML) with a\r\nfocus on applications to banking. The SML techniques covered include Bagging\r\n(Random Forest or RF), Boosting (Gradient Boosting Machine or GBM) and Neural\r\nNetworks (NNs). We begin with an introduction to ML tasks and techniques. This\r\nis followed by a description of: i) tree-based ensemble algorithms including\r\nBagging with RF and Boosting with GBMs, ii) Feedforward NNs, iii) a discussion\r\nof hyper-parameter optimization techniques, and iv) machine learning\r\ninterpretability. The paper concludes with a comparison of the features of\r\ndifferent ML algorithms. Examples taken from credit risk modeling in banking\r\nare used throughout the paper to illustrate the techniques and interpret the\r\nresults of the algorithms.","1857":"The use of machine learning algorithms to address classification problems is\r\non the rise in many research areas. The current study is aimed at testing the\r\npotential of using such algorithms to auto-select the best solvers for\r\ntransport problems in uniform slabs. Three solvers are used in this work:\r\nRichardson, diffusion synthetic acceleration, and nonlinear diffusion\r\nacceleration. Three parameters are manipulated to create different transport\r\nproblem scenarios. Five machine learning algorithms are applied: linear\r\ndiscriminant analysis, K-nearest neighbors, support vector machine, random\r\nforest, and neural networks. We present and analyze the results of these\r\nalgorithms for the test problems, showing that random forest and K-nearest\r\nneighbors are potentially the best suited candidates for this type of\r\nclassification problem.","1858":"Uterine Cervix Cancer is one of the leading Cancer names effecting the female population worldwide 1 2. Incidence of Cervical Cancer can be reduced by 80% through a routine Pap smear test. Pap smear test requires skilled cytologists and is always prone to inaccurate and inconsistent diagnosis due to manual error. Automated systems for easy recognition and proper staging of the cancerous cells can assists the medical professionals in correct diagnosis and planning of the proper treatment modality 3. In this research 23 well-known machine learning algorithms available in MatlabR2016 are extensively analyzed for their classification potential of Pap smear cases. To Train and Test the algorithms a huge database is created containing 8091 cervical cell images pertaining to 200 clinical cases collected from three medical institutes of northern India. The raw cases of cervical cancer in form of Pap smear slides were photographed under a multi-headed digital microscope. After profiling the cells were vigilantly assigned classes by multiple cytotechnicians and histopathologists 4. Cervical cases have seven classes of diagnosis 4.Quadratic SVM performed best among the 23 algorithms applied. Abid Sarwar\u00c4nalysis of Machine Learning and Statistics Tool Box (Matlab R2016) over Novel Benchmark Cervical Cancer Database\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-1 , December 2017, URL: http:\/\/www.ijtsrd.com\/papers\/ijtsrd7048.pdf  http:\/\/www.ijtsrd.com\/computer-science\/other\/7048\/analysis-of-machine-learning-and-statistics-tool-box--matlab-r2016-over-novel-benchmark-cervical-cancer-database\/abid-sarwar","1859":"We introduce NetKet, a comprehensive open source framework for the study of\r\nmany-body quantum systems using machine learning techniques. The framework is\r\nbuilt around a general and flexible implementation of neural-network quantum\r\nstates, which are used as a variational ansatz for quantum wave functions.\r\nNetKet provides algorithms for several key tasks in quantum many-body physics\r\nand quantum technology, namely quantum state tomography, supervised learning\r\nfrom wave-function data, and ground state searches for a wide range of\r\ncustomizable lattice models. Our aim is to provide a common platform for open\r\nresearch and to stimulate the collaborative development of computational\r\nmethods at the interface of machine learning and many-body physics.","1860":"In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.","1861":"Dementia is a complex disorder characterized by poor outcomes for the patients and high costs of care. After decades of research little is known about its mechanisms. Having prognostic estimates about dementia can help researchers, patients and public entities in dealing with this disorder. Thus, health data, machine learning and microsimulation techniques could be employed in developing prognostic estimates for dementia.The goal of this paper is to present evidence on the state of the art of studies investigating and the prognosis of dementia using machine learning and microsimulation techniques.To achieve our goal we carried out a systematic literature review, in which three large databases-Pubmed, Socups and Web of Science were searched to select studies that employed machine learning or microsimulation techniques for the prognosis of dementia. A single backward snowballing was done to identify further studies. A quality checklist was also employed to assess the quality of the evidence presented by the selected studies, and low quality studies were removed. Finally, data from the final set of studies were extracted in summary tables.In total 37 papers were included. The data summary results showed that the current research is focused on the investigation of the patients with mild cognitive impairment that will evolve to Alzheimer's disease, using machine learning techniques. Microsimulation studies were concerned with cost estimation and had a populational focus. Neuroimaging was the most commonly used variable.Prediction of conversion from MCI to AD is the dominant theme in the selected studies. Most studies used ML techniques on Neuroimaging data. Only a few data sources have been recruited by most studies and the ADNI database is the one most commonly used. Only two studies have investigated the prediction of epidemiological aspects of Dementia using either ML or MS techniques. Finally, care should be taken when interpreting the reported accuracy of ML techniques, given studies' different contexts.","1862":"We discuss algorithms for learning and revising user profiles that can determine which World Wide Web sites on a given topic would be interesting to a user. We describe the use of a naive Bayesian classifier for this task, and demonstrate that it can incrementally learn profiles from user feedback on the interestingness of Web sites. Furthermore, the Bayesian classifier may easily be extended to revise user provided profiles. In an experimental evaluation we compare the Bayesian classifier to computationally more intensive alternatives, and show that it performs at least as well as these approaches throughout a range of different domains. In addition, we empirically analyze the effects of providing the classifier with background knowledge in form of user defined profiles and examine the use of lexical knowledge for feature selection. We find that both approaches can substantially increase the prediction accuracy.","1863":"A visual prank exposes an Achilles\u2019 heel of computer vision systems: Unlike humans, they can\u2019t do a double take.","1864":"The organizer of a machine learning competition faces the problem of\r\nmaintaining an accurate leaderboard that faithfully represents the quality of\r\nthe best submission of each competing team. What makes this estimation problem\r\nparticularly challenging is its sequential and adaptive nature. As participants\r\nare allowed to repeatedly evaluate their submissions on the leaderboard, they\r\nmay begin to overfit to the holdout data that supports the leaderboard. Few\r\ntheoretical results give actionable advice on how to design a reliable\r\nleaderboard. Existing approaches therefore often resort to poorly understood\r\nheuristics such as limiting the bit precision of answers and the rate of\r\nre-submission.\r\n  In this work, we introduce a notion of \"leaderboard accuracy\" tailored to the\r\nformat of a competition. We introduce a natural algorithm called \"the Ladder\"\r\nand demonstrate that it simultaneously supports strong theoretical guarantees\r\nin a fully adaptive model of estimation, withstands practical adversarial\r\nattacks, and achieves high utility on real submission files from an actual\r\ncompetition hosted by Kaggle.\r\n  Notably, we are able to sidestep a powerful recent hardness result for\r\nadaptive risk estimation that rules out algorithms such as ours under a\r\nseemingly very similar notion of accuracy. On a practical note, we provide a\r\ncompletely parameter-free variant of our algorithm that can be deployed in a\r\nreal competition with no tuning required whatsoever.","1865":"In the recent years, the problem of addressing fairness in Machine Learning\r\n(ML) and automatic decision-making has attracted a lot of attention in the\r\nscientific communities dealing with Artificial Intelligence. A plethora of\r\ndifferent definitions of fairness in ML have been proposed, that consider\r\ndifferent notions of what is a \"fair decision\" in situations impacting\r\nindividuals in the population. The precise differences, implications and\r\n\u00f6rthogonality\" between these notions have not yet been fully analyzed in the\r\nliterature. In this work, we try to make some order out of this zoo of\r\ndefinitions.","1866":"To assess the exposure of citizens to pollutants like NOx or particulate matter in urban areas, land use regression (LUR) models are a well established method. LUR models leverage information about environmental and anthropogenic factors such as cars, heating, or industry to predict air pollution in areas where no measurements have been made. However, existing approaches are often not globally applicable and require tedious hyper-parameter tuning to enable high quality predictions. In this work, we tackle these issues by introducing OpenLUR, an off-the-shelf approach for modeling air pollution that (i) works on a set of novel features solely extracted from the globally and openly available data source OpenStreetMap and (ii) is based on state-of-the-art machine learning featuring automated hyper-parameter tuning in order to minimize manual effort. We show that our proposed features are able to outperform their counterparts from local and closed sources, and illustrate how automated hyper parameter tuning can yield competitve results while alleviating the need for expert knowledge in machine learning and manual effort. Importantly, we further demonstrate the potential of the global availability of our features by applying cross-learning across different cities in order to reduce the need for a large amount of training samples. Overall, OpenLUR represents an off-the-shelf approach that facilitates easily reproducible experiments and the development of globally applicable models.","1867":"Migration crisis, climate change or tax havens: Global challenges need global\r\nsolutions. But agreeing on a joint approach is difficult without a common\r\nground for discussion. Public spheres are highly segmented because news are\r\nmainly produced and received on a national level. Gain- ing a global view on\r\ninternational debates about important issues is hindered by the enormous\r\nquantity of news and by language barriers. Media analysis usually focuses only\r\non qualitative re- search. In this position statement, we argue that it is\r\nimperative to pool methods from machine learning, journalism studies and\r\nstatistics to help bridging the segmented data of the international public\r\nsphere, using the Transatlantic Trade and Investment Partnership (TTIP) as a\r\ncase study.","1868":"This research study utilizes an open source AI\/ML framework named Annif, developed by the National Library of Finland, to explore the feasibility of automated subject indexing. The framework loads the linked open data format of LCSH and trains the model with a comprehensive training dataset comprising MARC records downloaded from different libraries all over the world. It then compares a set of selected machine learning backends of Annif, namely TF-IDF, Omikuji, and Neural Network, against a set of retrieval metrics to measure the suitability of these backends for the bibliographic data universe. The study concludes that the fusion backend in Annif named Neural Network has the potential to provide support for an automated subject indexing system.","1869":"In this paper we explore some of the opportunities and challenges for machine learning on the Semantic Web. The Semantic Web provides standardized formats for the representation of both data and ontological background knowledge. Semantic Web standards are used to describe meta data but also have great potential as a general data format for data communication and data integration. Within a broad range of possible applications machine learning will play an increasingly important role: Machine learning solutions have been developed to support the management of ontologies, for the semi-automatic annotation of unstructured data, and to integrate semantic information into web mining. Machine learning will increasingly be employed to analyze distributed data sources described in Semantic Web formats and to support approximate Semantic Web reasoning and querying. In this paper we discuss existing and future applications of machine learning on the Semantic Web with a strong focus on learning algorithms that are suitable for the relational character of the Semantic Web\u2019s data structure. We discuss some of the particular aspects of learning that we expect will be of relevance for the Semantic Web such as scalability, missing and contradicting data, and the potential to integrate ontological background knowledge. In addition we review some of the work on the learning of ontologies and on the population of ontologies, mostly in the context of textual data.","1870":"Subject indexing, i.e., the enrichment of metadata records for textual resources with descriptors from a controlled vocabulary, is one of the core activities of libraries. However, due to the proliferation of digital documents it is no longer possible to annotate every single document intellectually, which is why we need to explore the potentials of automation. At ZBW the efforts to partially or completely automate the subject indexing process have started around the year 2000 but the prototypical machine learning solutions that we developed in an applied research project over the past few years have yet to be integrated into productive operations at the library. In this short paper, we outline the challenges that we perceive and the steps that we are taking towards completing the transfer of our solutions into practice \u2013 in particular, we are in the process of specifying what a suitable architecture for that task should look like and establishing a roadmap for the next two years indicating the milestones that have to be reached in order to build and test that architecture and to subsequently ensure its availability and continuous development during running operations.","1871":null,"1872":null,"1873":null,"1874":"Social interaction will be key to enabling robots and machines in\n\tgeneral to learn new tasks from ordinary people (not experts in robotics\n\tor machine learning). Everyday people who need to teach their machines\n\tnew things will find it natural for to rely on their interpersonal\n\tinteraction skills. This thesis provides several contributions towards\n\tthe understanding of this Socially Guided Machine Learning scenario.\n\t\n\tWhile the topic of human input to machine learning algorithms has\n\tbeen explored to some extent, prior works have not gone far enough\n\tto understand what people will try to communicate when teaching a\n\tmachine and how algorithms and learning systems can be modified to\n\tbetter accommodate a human partner. Interface techniques have been\n\tbased on intuition and assumptions rather than grounded in human\n\tbehavior, and often techniques are not demonstrated or evaluated\n\twith everyday people.\n\t\n\tUsing a computer game, Sophie's Kitchen, an experiment with human\n\tsubjects provides several insights about how people approach the\n\ttask of teaching a machine. In particular, people want to direct\n\tand guide an agent's exploration process, they quickly use the behavior\n\tof the agent to infer a mental model of the learning process, and\n\tthey utilize positive and negative feedback in asymmetric ways. Using\n\ta robotic platform, Leonardo, and 200 people in follow-up studies\n\tof modified versions of the Sophie's Kitchen game, four research\n\tthemes are developed.\n\t\n\tThe use of human guidance in a machine learning exploration can be\n\tsuccessfully incorporated to improve learning performance. Novel\n\tlearning approaches demonstrate aspects of goal-oriented learning.\n\tThe transparency of the machine learner can have significant effects\n\ton the nature of the instruction received from the human teacher,\n\twhich in turn positively impacts the learning process. Utilizing\n\tasymmetric interpretations of positive and negative feedback from\n\ta human partner, can result in a more efficient and robust learning\n\texperience.","1875":"To ensure high surface quality and low costs during production milling processes, the tool wear has to be monitored constantly. However, manual inspection can be highly time consuming and erroneous. Simple automatic solutions are often not sufficient, since the tool wear behaves usually non-linear and depends on many factors. \r\n\r\nIn this work, we successfully employed machine learning methods on microscope images of the tool and force\/acceleration sensor data for indirect determination of tool wear. The data was generated in the lab using a CNC machine in a multi interpolation manufacturing process and three tool wear classes were identified by experts.","1876":null,"1877":"Among the trendy fashions for COVID 19 international pandemic prediction, easy epidemiological and statistical fashions have acquired extra interest by using authorities, and they are famous in the media. Due to a excessive stage of uncertainty and lack of quintessential data, fashionable fashions have proven low accuracy for long term prediction. This paper gives a signs and symptoms evaluation of computer studying to predict the COVID 19. Among a large vary of computing device getting to know fashions Based on the effects suggested here, and due to the surprisingly complicated nature of the COVID 19 outbreak and version in its behaviour from nation to nation, this learn about suggests desktop getting to know as an advantageous device to mannequin the outbreak. This paper affords an preliminary benchmarking to exhibit the conceivable of computer gaining knowledge of for future research. Mr. Umesh R Maurya | Prof. Sudeshna Roy \"Predict the Covid-19 using Machine Learning Model from a Symptoms of the Body\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-5 , August 2020, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd33226.pdf Paper Url :https:\/\/www.ijtsrd.com\/computer-science\/other\/33226\/predict-the-covid19-using-machine-learning-model-from-a-symptoms-of-the-body\/mr-umesh-r-maurya","1878":null,"1879":"Machine learning applications are increasingly deployed not only to serve\r\npredictions using static models, but also as tightly-integrated components of\r\nfeedback loops involving dynamic, real-time decision making. These applications\r\npose a new set of requirements, none of which are difficult to achieve in\r\nisolation, but the combination of which creates a challenge for existing\r\ndistributed execution frameworks: computation with millisecond latency at high\r\nthroughput, adaptive construction of arbitrary task graphs, and execution of\r\nheterogeneous kernels over diverse sets of resources. We assert that a new\r\ndistributed execution framework is needed for such ML applications and propose\r\na candidate approach with a proof-of-concept architecture that achieves a 63x\r\nperformance improvement over a state-of-the-art execution framework for a\r\nrepresentative application.","1880":"Learning-based pattern classifiers, including deep networks, have shown\r\nimpressive performance in several application domains, ranging from computer\r\nvision to cybersecurity. However, it has also been shown that adversarial input\r\nperturbations carefully crafted either at training or at test time can easily\r\nsubvert their predictions. The vulnerability of machine learning to such wild\r\npatterns (also referred to as adversarial examples), along with the design of\r\nsuitable countermeasures, have been investigated in the research field of\r\nadversarial machine learning. In this work, we provide a thorough overview of\r\nthe evolution of this research area over the last ten years and beyond,\r\nstarting from pioneering, earlier work on the security of non-deep learning\r\nalgorithms up to more recent work aimed to understand the security properties\r\nof deep learning algorithms, in the context of computer vision and\r\ncybersecurity tasks. We report interesting connections between these\r\napparently-different lines of work, highlighting common misconceptions related\r\nto the security evaluation of machine-learning algorithms. We review the main\r\nthreat models and attacks defined to this end, and discuss the main limitations\r\nof current work, along with the corresponding future challenges towards the\r\ndesign of more secure learning algorithms.","1881":"First-order stochastic methods are the state-of-the-art in large-scale\r\nmachine learning optimization owing to efficient per-iteration complexity.\r\nSecond-order methods, while able to provide faster convergence, have been much\r\nless explored due to the high cost of computing the second-order information.\r\nIn this paper we develop second-order stochastic methods for optimization\r\nproblems in machine learning that match the per-iteration cost of gradient\r\nbased methods, and in certain settings improve upon the overall running time\r\nover popular first-order methods. Furthermore, our algorithm has the desirable\r\nproperty of being implementable in time linear in the sparsity of the input\r\ndata.","1882":"Supervised machine learning is the search for algorithms that reason from externally supplied instances to produce general hypotheses, which then make predictions about future instances. In other words, the goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances\r\nwhere the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single article cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major\r\ntheoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.","1883":null,"1884":"Array CGH is a recently introduced technology that measures changes in the gene copy number of hundreds of genes in a single experiment. The primary goal of this study was to develop machine learning models that classify non-small Lung Cancers according to histopathology types and to compare several machine learning methods in this learning task. DNA from tumors of 37 patients (21 squamous carcinomas, and 16 adenocarcinomas) were extracted and hybridized onto a 452 BAC clone array. The following algorithms were used: KNN, Decision Tree Induction, Support Vector Machines and Feed-Forward Neural Networks. Performance was measured via leave-one-out classification accuracy. The best multi-gene model found had a leave-one-out accuracy of 89.2\\%. Decision Trees performed poorer than the other methods in this learning task and dataset. We conclude that gene copy numbers as measured by array CGH are, collectively, an excellent indicator of histological subtype. Several interesting research directions are discussed.","1885":"Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines.","1886":"The Motion Learning Toolbox is a Python library designed to facilitate the preprocessing of motion tracking data in extended reality (XR) setups. It's particularly useful for researchers and engineers wanting to use XR tracking data as input for machine learning models. Originally developed for academic research targeting the identification of XR users by their motions, this toolbox includes a variety of data encoding methods that enhance machine learning model performance.","1887":"This paper introduces a new learning paradigm, called Learning Using Statistical Invariants (LUSI), which is different from the classical one. In a classical paradigm, the learning machine constructs a classification rule that minimizes the probability of expected error; it is data-driven model of learning. In the LUSI paradigm, in order to construct the desired classification function, a learning machine computes statistical invariants that are specific for the problem, and then minimizes the expected error in a way that preserves these invariants; it is thus both data- and invariant-driven learning. From a mathematical point of view, methods of the classical paradigm employ mechanisms of strong convergence of approximations to the desired function, whereas methods of the new paradigm employ both strong and weak convergence mechanisms. This can significantly increase the rate of convergence.","1888":null,"1889":"In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of \u201corganization,\u201d \u201cperson,\u201d or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.","1890":"We have identified molecules that exhibit synthetic lethality in cells with loss of the neurofibromin 1 (NF1) tumor suppressor gene. However, recognizing tumors that have inactivation of the NF1 tumor suppressor function is challenging because the loss may occur via mechanisms that do not involve mutation of the genomic locus. Degradation of the NF1 protein, independent of NF1 mutation status, phenocopies inactivating mutations to drive tumors in human glioma cell lines. NF1 inactivation may alter the transcriptional landscape of a tumor and allow a machine learning classifier to detect which tumors will benefit from synthetic lethal molecules.We developed a strategy to predict tumors with low NF1 activity and hence tumors that may respond to treatments that target cells lacking NF1. Using RNAseq data from The Cancer Genome Atlas (TCGA), we trained an ensemble of 500 logistic regression classifiers that integrates mutation status with whole transcriptomes to predict NF1 inactivation in glioblastoma (GBM). On TCGA data, the classifier detected NF1 mutated tumors (test set area under the receiver operating characteristic curve (AUROC) mean\u2009=\u20090.77, 95% quantile\u2009=\u20090.53 - 0.95) over 50 random initializations. On RNA-Seq data transformed into the space of gene expression microarrays, this method produced a classifier with similar performance (test set AUROC mean\u2009=\u20090.77, 95% quantile\u2009=\u20090.53 - 0.96). We applied our ensemble classifier trained on the transformed TCGA data to a microarray validation set of 12 samples with matched RNA and NF1 protein-level measurements. The classifier's NF1 score was associated with NF1 protein concentration in these samples.We demonstrate that TCGA can be used to train accurate predictors of NF1 inactivation in GBM. The ensemble classifier performed well for samples with very high or very low NF1 protein concentrations but had mixed performance in samples with intermediate NF1 concentrations. Nevertheless, high-performing and validated predictors have the potential to be paired with targeted therapies and personalized medicine.","1891":"The linear matter power spectrum $P(k,z)$ connects theory with large scale\r\nstructure observations in cosmology. Its scale dependence is entirely encoded\r\nin the matter transfer function $T(k)$, which can be computed numerically by\r\nBoltzmann solvers, and can also be computed semi-analytically by using fitting\r\nfunctions such as the well-known Bardeen-Bond-Kaiser-Szalay (BBKS) and\r\nEisenstein-Hu (EH) formulae. However, both the BBKS and EH formulae have some\r\nsignificant drawbacks. On the one hand, although BBKS is a simple expression,\r\nit is only accurate up to $10\\%$, which is well above the $1\\%$ precision goal\r\nof forthcoming surveys. On the other hand, while EH is as accurate as required\r\nby upcoming experiments, it is a rather long and complicated expression. Here,\r\nwe use the Genetic Algorithms (GAs), a particular machine learning technique,\r\nto derive simple and accurate fitting formulae for the transfer function\r\n$T(k)$. When the effects of massive neutrinos are also considered, our\r\nexpression slightly improves over the EH formula, while being notably shorter\r\nin comparison.","1892":"OpenML is an online machine learning platform where researchers can easily\r\nshare data, machine learning tasks and experiments as well as organize them\r\nonline to work and collaborate more efficiently. In this paper, we present an R\r\npackage to interface with the OpenML platform and illustrate its usage in\r\ncombination with the machine learning R package mlr. We show how the OpenML\r\npackage allows R users to easily search, download and upload data sets and\r\nmachine learning tasks. Furthermore, we also show how to upload results of\r\nexperiments, share them with others and download results from other users.\r\nBeyond ensuring reproducibility of results, the OpenML platform automates much\r\nof the drudge work, speeds up research, facilitates collaboration and increases\r\nthe users' visibility online.","1893":null,"1894":null,"1895":"Machine learning comprises algorithms that can perform tasks they were not explicitly programmed to perform. Explicitly programmed algorithms perform tasks according to a predefined sequence of instructions. Conversely, machine learning algorithms are programmed to learn to perform tasks using input data. In the era of abundant data, affordable data storage, and computational capabilities, understanding machine learning algorithms is critical to better explore and answer questions that can advance surgical science.","1896":"MLtuner automatically tunes settings for training tunables (such as the\r\nlearning rate, the momentum, the mini-batch size, and the data staleness bound)\r\nthat have a significant impact on large-scale machine learning (ML)\r\nperformance. Traditionally, these tunables are set manually, which is\r\nunsurprisingly error-prone and difficult to do without extensive domain\r\nknowledge. MLtuner uses efficient snapshotting, branching, and\r\noptimization-guided online trial-and-error to find good initial settings as\r\nwell as to re-tune settings during execution. Experiments show that MLtuner can\r\nrobustly find and re-tune tunable settings for a variety of ML applications,\r\nincluding image classification (for 3 models and 2 datasets), video\r\nclassification, and matrix factorization. Compared to state-of-the-art ML\r\nauto-tuning approaches, MLtuner is more robust for large problems and over an\r\norder of magnitude faster.","1897":"Indias economy is mostly based on agricultural yield growth and linked agro industry products, as it is an agricultural country. Rainwater, which is often unpredictable in India, has a significant impact on agriculture. Agriculture growth is also influenced by a variety of soil parameters, such as nitrogen, phosphorus, and potassium, as well as crop rotation, soil moisture, and surface temperature, as well as climatic factors such as temperature and rainfall. India is quickly advancing in terms of technical advancement. As a result, technology will benefit agriculture by increasing crop productivity, resulting in higher yields for farmers. The suggested project provides a solution for storing temperature, rainfall, and soil characteristics in order to determine which crops are suited for cultivation in a given area. This paper describes a system, implemented as an android application, that employs data analytics techniques to predict the most profitable crop based on current weather and soil conditions. The suggested system will combine data from the repository and the meteorological department to make a prediction of the most suited crops based on current environmental conditions using a machine learning method called Multiple Linear Regression. This gives a farmer a wide range of crops to choose from. As a result, the project creates a system that integrates data from diverse sources, performs data analytics, and conducts predictive analysis in order to improve crop production productivity and boost farmer profit margins over time. Machine learning, crop prediction, and yield estimation are some of the terms used in this paper. Manju D C | Murugan R \"Crop Prediction System using Machine Learning\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-3 , April 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd49444.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/data-processing\/49444\/crop-prediction-system-using-machine-learning\/manju-d-c","1898":null,"1899":null,"1900":"In recent years, academics and investigative journalists have criticized\r\ncertain commercial risk assessments for their black-box nature and failure to\r\nsatisfy competing notions of fairness. Since then, the field of interpretable\r\nmachine learning has created simple yet effective algorithms, while the field\r\nof fair machine learning has proposed various mathematical definitions of\r\nfairness. However, studies from these fields are largely independent, despite\r\nthe fact that many applications of machine learning to social issues require\r\nboth fairness and interpretability. We explore the intersection by revisiting\r\nthe recidivism prediction problem using state-of-the-art tools from\r\ninterpretable machine learning, and assessing the models for performance,\r\ninterpretability, and fairness. Unlike previous works, we compare against two\r\nexisting risk assessments (COMPAS and the Arnold Public Safety Assessment) and\r\ntrain models that output probabilities rather than binary predictions. We\r\npresent multiple models that beat these risk assessments in performance, and\r\nprovide a fairness analysis of these models. Our results imply that machine\r\nlearning models should be trained separately for separate locations, and\r\nupdated over time.","1901":"Background\r\nMachine learning algorithms have demonstrated high performance in the risk stratification of patients for colorectal cancer. Despite the promise, there has not yet been a proportionate clinical impact from this technology. We provide a comprehensive overview of machine learning and big data analysis in the field of colorectal cancer risk prediction and stratification.\r\nMethods\r\nOriginal articles in the English language between January 2011-December 2021 from databases Ovid (Embase and MedLine), PubMed and Google Scholar. Subject key terms including machine learning, artificial intelligence, neural networks, colorectal cancer, and diagnosis\/risk. Primary outcome measures included area under the curve and the odds ratio.\r\nResults\r\nOf the 14 studies included in the final analysis, the number of patients varied from 17,095 to 2,550,119. All articles used for model synthesis contained a minimum of 70,000 patients. The area under the curve varied between 0.738 and 0.896. Commonly used methods included random forests, neural networks and logistic regression, but no specific machine learning method was found to be superior for colorectal cancer prediction. Comparison with STROBE guidelines showed consistent strength and clarity in reporting.\r\nConclusion\r\nThese studies demonstrate significant and effective risk stratification and prediction of colorectal cancer. Current barriers include lack of external verification, heterogeneity amongst machine learning algorithms and difficult cost\/benefit analysis. Clinical integration is fundamental for further external validation which will improve understanding and trust in such algorithms. However, it is vital that this technology is assessed using the same rigorous cost\/benefit analysis as for other medical technologies to help valuate the place of such technology in the modern treatment of colorectal cancer.","1902":null,"1903":null,"1904":"We propose a continuous normalizing flow for sampling from the\r\nhigh-dimensional probability distributions of Quantum Field Theories in\r\nPhysics. In contrast to the deep architectures used so far for this task, our\r\nproposal is based on a shallow design and incorporates the symmetries of the\r\nproblem. We test our model on the $\\phi^4$ theory, showing that it\r\nsystematically outperforms a realNVP baseline in sampling efficiency, with the\r\ndifference between the two increasing for larger lattices. On the largest\r\nlattice we consider, of size $3232$, we improve a key metric, the\r\neffective sample size, from 1% to 66% w.r.t. the realNVP baseline.","1905":"The field of pattern recognition has undergone substantial development over the years. This book reflects these developments while providing a grounding in the basic concepts of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners.","1906":null,"1907":null,"1908":"For enabling automatic deployment and management of cellular networks, the\r\nconcept of self-organizing network (SON) was introduced. SON capabilities can\r\nenhance network performance, improve service quality, and reduce operational\r\nand capital expenditure (OPEX\/CAPEX). As an important component in SON,\r\nself-healing is defined as a network paradigm where the faults of target\r\nnetworks are mitigated or recovered by automatically triggering a series of\r\nactions such as detection, diagnosis and compensation. Data-driven machine\r\nlearning has been recognized as a powerful tool to bring intelligence into\r\nnetwork and to realize self-healing. However, there are major challenges for\r\npractical applications of machine learning techniques for self-healing. In this\r\narticle, we first classify these challenges into five categories: 1) data\r\nimbalance, 2) data insufficiency, 3) cost insensitivity, 4) non-real-time\r\nresponse, and 5) multi-source data fusion. Then we provide potential technical\r\nsolutions to address these challenges. Furthermore, a case study of\r\ncost-sensitive fault detection with imbalanced data is provided to illustrate\r\nthe feasibility and effectiveness of the suggested solutions.","1909":"We implement an all-optical setup demonstrating kernel-based quantum machine\r\nlearning for two-dimensional classification problems. In this hybrid approach,\r\nkernel evaluations are outsourced to projective measurements on suitably\r\ndesigned quantum states encoding the training data, while the model training is\r\nprocessed on a classical computer. Our two-photon proposal encodes data points\r\nin a discrete, eight-dimensional feature Hilbert space. In order to maximize\r\nthe application range of the deployable kernels, we optimize feature maps\r\ntowards the resulting kernels' ability to separate points, i.e., their\r\nresolution, under the constraint of finite, fixed Hilbert space dimension.\r\nImplementing these kernels, our setup delivers viable decision boundaries for\r\nstandard nonlinear supervised classification tasks in feature space. We\r\ndemonstrate such kernel-based quantum machine learning using specialized\r\nmultiphoton quantum optical circuits. The deployed kernel exhibits\r\nexponentially better scaling in the required number of qubits than a direct\r\ngeneralization of kernels described in the literature.","1910":"Interest in machine-learning applications within medicine has been growing, but few studies have progressed to deployment in patient care. We present a framework, context and ultimately guidelines for accelerating the translation of machine-learning-based interventions in health care. To be successful, translation will require a team of engaged stakeholders and a systematic process from beginning (problem formulation) to end (widespread deployment).","1911":"As decision\u2010making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data\u2010driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness\u2010aware ML solutions have been proposed which involve fairness\u2010related interventions in the data, learning algorithms, and\/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real\u2010world datasets used for fairness\u2010aware ML. We focus on tabular data as the most common data representation for fairness\u2010aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis.","1912":"The advent of advanced machine learning systems has often been debated in terms of the very \u2018big\u2019 concepts: intentionality, consciousness, intelligence. But the technological development of the last few years has shown two things: that a human-equivalent AI is still far away, if it is ever possible; and that the philosophically most interesting changes occur in nuanced rather than overarching concepts. The example this contribution will explore is the concept of a limited type of meaning \u2013 I call it dumb meaning. For the longest time, computers were understood as machines computing only syntax, while their semantic abilities were seen as limited by the \u2018symbol grounding problem\u2019: Since computers operate with mere symbols without any indexical relation to the world, their understanding would forever be limited to the handling of empty signifiers, while their meaning is \u2018parasitically\u2019 dependent on a human interpreter. This was true for classic or symbolic AI. With subsymbolic AI and neural nets, however, an artificial semantics seems possible, even though it still is far away from any comprehensive understanding of meaning. I explore this limited semantics, which has been brought about by the immense increase of correlated data, by looking at two examples: the implicit knowledge of large language models and the indexical meaning of multimodal AI such as DALL\u00b7E 2. The semantics of each process may not be meaning proper, but as dumb meaning it is far more than mere syntax.","1913":"We present the Cosmology and Astrophysics with MachinE Learning Simulations\r\n--CAMELS-- project. CAMELS is a suite of 4,233 cosmological simulations of\r\n$(25~h^-1Mpc)^3$ volume each: 2,184 state-of-the-art\r\n(magneto-)hydrodynamic simulations run with the AREPO and GIZMO codes,\r\nemploying the same baryonic subgrid physics as the IllustrisTNG and SIMBA\r\nsimulations, and 2,049 N-body simulations. The goal of the CAMELS project is to\r\nprovide theory predictions for different observables as a function of cosmology\r\nand astrophysics, and it is the largest suite of cosmological\r\n(magneto-)hydrodynamic simulations designed to train machine learning\r\nalgorithms. CAMELS contains thousands of different cosmological and\r\nastrophysical models by way of varying $\u00d8mega_m$, $\\sigma_8$, and four\r\nparameters controlling stellar and AGN feedback, following the evolution of\r\nmore than 100 billion particles and fluid elements over a combined volume of\r\n$(400~h^-1Mpc)^3$. We describe the simulations in detail and\r\ncharacterize the large range of conditions represented in terms of the matter\r\npower spectrum, cosmic star formation rate density, galaxy stellar mass\r\nfunction, halo baryon fractions, and several galaxy scaling relations. We show\r\nthat the IllustrisTNG and SIMBA suites produce roughly similar distributions of\r\ngalaxy properties over the full parameter space but significantly different\r\nhalo baryon fractions and baryonic effects on the matter power spectrum. This\r\nemphasizes the need for marginalizing over baryonic effects to extract the\r\nmaximum amount of information from cosmological surveys. We illustrate the\r\nunique potential of CAMELS using several machine learning applications,\r\nincluding non-linear interpolation, parameter estimation, symbolic regression,\r\ndata generation with Generative Adversarial Networks (GANs), dimensionality\r\nreduction, and anomaly detection.","1914":"Relational properties, e.g., the connectivity structure of nodes in a\r\ndistributed system, have many applications in software design and analysis.\r\nHowever, such properties often have to be written manually, which can be costly\r\nand error-prone. This paper introduces the MCML approach for empirically\r\nstudying the learnability of a key class of such properties that can be\r\nexpressed in the well-known software design language Alloy. A key novelty of\r\nMCML is quantification of the performance of and semantic differences among\r\ntrained machine learning (ML) models, specifically decision trees, with respect\r\nto entire input spaces (up to a bound on the input size), and not just for\r\ngiven training and test datasets (as is the common practice). MCML reduces the\r\nquantification problems to the classic complexity theory problem of model\r\ncounting, and employs state-of-the-art approximate and exact model counters for\r\nhigh efficiency. The results show that relatively simple ML models can achieve\r\nsurprisingly high performance (accuracy and F1 score) at learning relational\r\nproperties when evaluated in the common setting of using training and test\r\ndatasets -- even when the training dataset is much smaller than the test\r\ndataset -- indicating the seeming simplicity of learning these properties.\r\nHowever, the use of MCML metrics based on model counting shows that the\r\nperformance can degrade substantially when tested against the whole (bounded)\r\ninput space, indicating the high complexity of precisely learning these\r\nproperties, and the usefulness of model counting in quantifying the true\r\naccuracy.","1915":"This paper describes FOIL, a system that learns Horn clauses from data expressed as relations. FOIL is based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order formalism. This new system has been applied successfully to several tasks taken from the machine learning literature.","1916":null,"1917":null,"1918":"Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks. The paper also discusses some open research issues.","1919":"Automated ontology learning from unstructured textual sources has been proposed in literature as a way to support the difficult and time-consuming task of knowledge modeling for semantic applications. In this paper we propose a system, based on a neural network in the encoder-decoder configuration, to translate natural language definitions into Description Logics formulae through syntactic transformation. The model has been evaluated to asses its capacity to generalize over different syntactic structures, tolerate unknown words, and improve its performance by enriching the training set with new annotated examples. The results obtained in our evaluation show how approaching the ontology learning problem as a neural machine translation task can be a valid way to tackle long term expressive ontology learning challenges such as language variability, domain independence, and high engineering costs.","1920":null,"1921":"We investigated the effects of example-based explanations for a machine learning classifier on end users' appropriate trust. We explored the effects of spatial layout and visual representation in an in-person user study with 33 participants. We measured participants' appropriate trust in the classifier, quantified the effects of different spatial layouts and visual representations, and observed changes in users' trust over time. The results show that each explanation improved users' trust in the classifier, and the combination of explanation, human, and classification algorithm yielded much better decisions than the human and classification algorithm separately. Yet these visual explanations lead to different levels of trust and may cause inappropriate trust if an explanation is difficult to understand. Visual representation and performance feedback strongly affect users' trust, and spatial layout shows a moderate effect. Our results do not support that individual differences (e.g., propensity to trust) affect users' trust in the classifier. This work advances the state-of-the-art in trust-able machine learning and informs the design and appropriate use of automated systems.","1922":null,"1923":"Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue.","1924":null,"1925":null,"1926":null,"1927":"Tapping into the \"folk knowledge\" needed to advance machine learning applications.","1928":null,"1929":null,"1930":"In many learning problems prior knowledge about pattern variations can be formalized and beneficially incorporated into the analysis system. The corresponding notion of invariance is commonly used in conceptionally different ways. We propose a more distinguishing treatment in particular in the active field of kernel methods for machine learning and pattern analysis. Additionally, the fundamental relation of invariant kernels and traditional invariant pattern analysis by means of invariant representations will be clarified. After addressing these conceptional questions, we focus on practical aspects and present two generic approaches for constructing invariant kernels. The first approach is based on a technique called invariant integration. The second approach builds on invariant distances. In principle, our approaches support general transformations in particular covering discrete and non-group or even an infinite number of pattern-transformations. Additionally, both enable a smooth interpolation between invariant and non-invariant pattern analysis, i.e. they are a covering general framework. The wide applicability and various possible benefits of invariant kernels are demonstrated in different kernel methods.","1931":"PySR is an open-source library for practical symbolic regression, a type of\r\nmachine learning which aims to discover human-interpretable symbolic models.\r\nPySR was developed to democratize and popularize symbolic regression for the\r\nsciences, and is built on a high-performance distributed back-end, a flexible\r\nsearch algorithm, and interfaces with several deep learning packages. PySR's\r\ninternal search algorithm is a multi-population evolutionary algorithm, which\r\nconsists of a unique evolve-simplify-optimize loop, designed for optimization\r\nof unknown scalar constants in newly-discovered empirical expressions. PySR's\r\nbackend is the extremely optimized Julia library SymbolicRegression.jl, which\r\ncan be used directly from Julia. It is capable of fusing user-defined operators\r\ninto SIMD kernels at runtime, performing automatic differentiation, and\r\ndistributing populations of expressions to thousands of cores across a cluster.\r\nIn describing this software, we also introduce a new benchmark,\r\n\"EmpiricalBench,\" to quantify the applicability of symbolic regression\r\nalgorithms in science. This benchmark measures recovery of historical empirical\r\nequations from original and synthetic datasets.","1932":null,"1933":null,"1934":null,"1935":"Author disambiguation in digital libraries becomes increasingly difficult as the number of publications and consequently the number of ambiguous author names keep growing. The fully automatic author disambiguation approach could not give satisfactory results due to the lack of signals in many cases. Furthermore, human judgment on the basis of automatic algorithms is also not suitable because the automatically disambiguated results are often mixed and not understandable for humans. In this paper, we propose a Labeling Oriented Author Disambiguation approach, called LOAD, to combine machine learning and human judgment together in author disambiguation. LOAD exploits a framework which consists of high precision clustering, high recall clustering, and top dissimilar clusters selection and ranking. In the framework, supervised learning algorithms are used to train the similarity functions between publications and a clustering algorithm is further applied to generate clusters. To validate the effectiveness and efficiency of the proposed LOAD approach, comprehensive experiments are conducted. Comparing to conventional author disambiguation algorithms, the LOAD yields much more accurate results to assist human labeling. Further experiments show that the LOAD approach can save labeling time dramatically.","1936":"In software engineering, the main aim is to develop projects that produce the desired results within limited schedule and budget. The most important factor affecting the budget of a project is the effort. Therefore, estimating effort is crucial because hiring people more than needed leads to a loss of income and hiring people less than needed leads to an extension of schedule. The main objective of this research is making an analysis of software effort estimation to overcome problems related to it: budget and schedule extension. To accomplish this, we propose a model that uses machine learning methods. We evaluate these models on public datasets and data gathered from software organizations in Turkey. It is found out in the experiments that the best method for a dataset may change and this proves the point that the usage of one model cannot always produce the best results.","1937":"We present an automated method for measuring media bias. Inferring which\r\nnewspaper published a given article, based only on the frequencies with which\r\nit uses different phrases, leads to a conditional probability distribution\r\nwhose analysis lets us automatically map newspapers and phrases into a bias\r\nspace. By analyzing roughly a million articles from roughly a hundred\r\nnewspapers for bias in dozens of news topics, our method maps newspapers into a\r\ntwo-dimensional bias landscape that agrees well with previous bias\r\nclassifications based on human judgement. One dimension can be interpreted as\r\ntraditional left-right bias, the other as establishment bias. This means that\r\nalthough news bias is inherently political, its measurement need not be.","1938":"There has been considerable growth and interest in industrial applications of\r\nmachine learning (ML) in recent years. ML engineers, as a consequence, are in\r\nhigh demand across the industry, yet improving the efficiency of ML engineers\r\nremains a fundamental challenge. Automated machine learning (AutoML) has\r\nemerged as a way to save time and effort on repetitive tasks in ML pipelines,\r\nsuch as data pre-processing, feature engineering, model selection,\r\nhyperparameter optimization, and prediction result analysis. In this paper, we\r\ninvestigate the current state of AutoML tools aiming to automate these tasks.\r\nWe conduct various evaluations of the tools on many datasets, in different data\r\nsegments, to examine their performance, and compare their advantages and\r\ndisadvantages on different test cases.","1939":"In 1988, Langley wrote an influential editorial in the journal Machine Learning titled \"Machine Learning as an Experimental Science,\" arguing persuasively for a greater focus on performance testing. Since that time the emphasis has become progressively stronger. Nowadays, to be accepted to one of our major conferences or journals, a paper must typically contain a large experimental section with many tables of results, concluding with a statistical test. In revisiting this paper, I claim that we have ignored most of its advice. We have focused largely on only one aspect, hypothesis testing, and a narrow version at that. This version provides us with evidence that is much more impoverished than many people realize. I argue that such tests are of limited utility either for comparing algorithms or for promoting progress in our field. As such they should not play such a prominent role in our work and publications.","1940":"A method for filtering spam blogs (splogs) based on a machine learning technique, and its evaluation results are described. Today, spam blogs (splogs) became one of major issues on theWeb. The problem of splogs is that values of blog sites are different by people. We propose a novel user-oriented splog filtering method that can adapt each user's preference for valuable blogs. We use the SVM(Support Vector Machine) for creating a personalized splog filter for each user. We had two experiments: (1) an experiment of individual splog judgement, and (2) an experiment for user oriented splog filtering. From the former experiment, we found existence of 'gray' blogs that are needed to treat by persons. From the latter experiment, we found that we can provide appropriate personalized filters by choosing the best feature set for each user. An overview of proposed method, and evaluation results are described.","1941":"Machine learning techniques offer promise to improve suicide risk prediction. In the current systematic review, we aimed to review the existing literature on the application of machine learning techniques to predict self-injurious thoughts and behaviors (SITBs).We systematically searched PsycINFO, PsycARTICLES, ERIC, CINAHL, and MEDLINE for articles published through February 2018.Thirty-five articles met criteria to be included in the review. Included articles were reviewed by outcome: suicide death, suicide attempt, suicide plan, suicidal ideation, suicide risk, and non-suicidal self-injury. We observed three general aims in the use of SITB-focused machine learning analyses: (1) improving prediction accuracy, (2) identifying important model indicators (i.e., variable selection) and indicator interactions, and (3) modeling underlying subgroups. For studies with the aim of boosting predictive accuracy, we observed greater prediction accuracy of SITBs than in previous studies using traditional statistical methods. Studies using machine learning for variable selection purposes have both replicated findings of well-known SITB risk factors and identified novel variables that may augment model performance. Finally, some of these studies have allowed for subgroup identification, which in turn has helped to inform clinical cutoffs.Limitations of the current review include relatively low paper sample size, inconsistent reporting procedures resulting in an inability to compare model accuracy across studies, and lack of model validation on external samples.We concluded that leveraging machine learning techniques to further predictive accuracy and identify novel indicators will aid in the prediction and prevention of suicide.","1942":"Even todays most advanced machine learning models are easily fooled by almost\r\nimperceptible perturbations of their inputs. Foolbox is a new Python package to\r\ngenerate such adversarial perturbations and to quantify and compare the\r\nrobustness of machine learning models. It is build around the idea that the\r\nmost comparable robustness measure is the minimum perturbation needed to craft\r\nan adversarial example. To this end, Foolbox provides reference implementations\r\nof most published adversarial attack methods alongside some new ones, all of\r\nwhich perform internal hyperparameter tuning to find the minimum adversarial\r\nperturbation. Additionally, Foolbox interfaces with most popular deep learning\r\nframeworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows\r\ndifferent adversarial criteria such as targeted misclassification and top-k\r\nmisclassification as well as different distance measures. The code is licensed\r\nunder the MIT license and is openly available at\r\nhttps:\/\/github.com\/bethgelab\/foolbox . The most up-to-date documentation can be\r\nfound at http:\/\/foolbox.readthedocs.io .","1943":null,"1944":"We present the Seismic Laboratory for Imaging and Modeling\/Monitoring (SLIM)\r\nopen-source software framework for computational geophysics and, more\r\ngenerally, inverse problems involving the wave-equation (e.g., seismic and\r\nmedical ultrasound), regularization with learned priors, and learned neural\r\nsurrogates for multiphase flow simulations. By integrating multiple layers of\r\nabstraction, our software is designed to be both readable and scalable. This\r\nallows researchers to easily formulate their problems in an abstract fashion\r\nwhile exploiting the latest developments in high-performance computing. We\r\nillustrate and demonstrate our design principles and their benefits by means of\r\nbuilding a scalable prototype for permeability inversion from time-lapse\r\ncrosswell seismic data, which aside from coupling of wave physics and\r\nmultiphase flow, involves machine learning.","1945":null,"1946":null,"1947":null,"1948":"Machine-learning research is to study and apply the computer modeling of learning processes in their multiple manifestations, which facilitate the development of intelligent system. In this paper, we have introduced aclusteringbasedmachine-learningalgorithm called clusteringalgorithm system (CAS). The CAS algorithm is tested to evaluate its performance and find fruitful results. We have been presented some heuristics to facilitate machine-learning authors to boost up their research works. The InfoBase of the Ministry of Civil Services is used to analyze the CAS algorithm. The CAS algorithm is compared with other machine-learningalgorithms like UNIMEM, COBWEB, and CLASSIT, and was found to have some strong points over them. The proposed algorithm combined advantages of two different approaches to machinelearning. The first approach is learning from Examples, CAS supports Single and Multiple Inheritance and Exceptions. CAS also avoids probability assumptions which are well understood in concept formation. The second approach is learning by Observation. CAS applies a set of operators that have proven to be effective in conceptual clustering. We have shown how CAS builds and searches through a clusters hierarchy to incorporate or characterize an object.","1949":"Recent advances in generative modeling of text have demonstrated remarkable\r\nimprovements in terms of fluency and coherency. In this work we investigate to\r\nwhich extent a machine can discriminate real from machine generated text. This\r\nis important in itself for automatic detection of computer generated stories,\r\nbut can also serve as a tool for further improving text generation. We show\r\nthat learning a dedicated scoring function to discriminate between real and\r\nfake text achieves higher precision than employing the likelihood of a\r\ngenerative model. The scoring functions generalize to other generators than\r\nthose used for training as long as these generators have comparable model\r\ncomplexity and are trained on similar datasets.","1950":"Bias in machine learning has manifested injustice in several areas, such as\r\nmedicine, hiring, and criminal justice. In response, computer scientists have\r\ndeveloped myriad definitions of fairness to correct this bias in fielded\r\nalgorithms. While some definitions are based on established legal and ethical\r\nnorms, others are largely mathematical. It is unclear whether the general\r\npublic agrees with these fairness definitions, and perhaps more importantly,\r\nwhether they understand these definitions. We take initial steps toward\r\nbridging this gap between ML researchers and the public, by addressing the\r\nquestion: does a lay audience understand a basic definition of ML fairness? We\r\ndevelop a metric to measure comprehension of three such\r\ndefinitions--demographic parity, equal opportunity, and equalized odds. We\r\nevaluate this metric using an online survey, and investigate the relationship\r\nbetween comprehension and sentiment, demographics, and the definition itself.","1951":null,"1952":"Machine learning allows computers to learn and discern patterns without actually being programmed. When Statistical techniques and machine learning are combined together they are a powerful tool for analysing various kinds of data in many computer science\/engineering areas including, image processing, speech processing, natural language processing, robot control, as well as in fundamental sciences such as biology, medicine, astronomy, physics, and materials. This book provides a general introduction to machine learning that covers a wide range of topics concisely and will help you bridge the gap between theory and practice. Part I discusses the fundamental concepts of statistics and probability that are used in describing machine learning algorithms. Part II and Part III explain the two major approaches of machine learning techniques; generative methods and discriminative methods. While Part III provides an in-depth look at advanced topics that play essential roles in making machine learning algorithms more useful in practice. The accompanying MATLAB\/Octave programs provide you with the necessary practical skills needed to accomplish a wide range of data analysis tasks.","1953":"Machine learning is often used to build predictive models by extracting patterns from large datasets. These models are used in predictive data analytics applications including price prediction, risk assessment, predicting customer behavior, and document classification. This introductory textbook offers a detailed and focused treatment of the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications. Technical and mathematical material is augmented with explanatory worked examples, and case studies illustrate the application of these models in the broader business context. After discussing the trajectory from data to insight to decision, the book describes four approaches to machine learning: information-based learning, similarity-based learning, probability-based learning, and error-based learning. Each of these approaches is introduced by a nontechnical explanation of the underlying concept, followed by mathematical models and algorithms illustrated by detailed worked examples. Finally, the book considers techniques for evaluating prediction models and offers two case studies that describe specific data analytics projects through each phase of development, from formulating the business problem to implementation of the analytics solution.","1954":"Education should not be a privilege but a common good. It should be openly\r\naccessible to everyone, with as few barriers as possible; even more so for key\r\ntechnologies such as Machine Learning (ML) and Data Science (DS). Open\r\nEducational Resources (OER) are a crucial factor for greater educational\r\nequity. In this paper, we describe the specific requirements for OER in ML and\r\nDS and argue that it is especially important for these fields to make source\r\nfiles publicly available, leading to Open Source Educational Resources (OSER).\r\nWe present our view on the collaborative development of OSER, the challenges\r\nthis poses, and first steps towards their solutions. We outline how OSER can be\r\nused for blended learning scenarios and share our experiences in university\r\neducation. Finally, we discuss additional challenges such as credit assignment\r\nor granting certificates.","1955":"Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components---a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2% increase in app installs resulting from improved data and model analysis.","1956":"As with any burgeoning technology that enjoys commercial attention, the use of data mining is surrounded by a great deal of hype. Exaggerated reports tell of secrets that can be uncovered by setting algorithms loose on oceans of data. But there is no magic in machine learning, no hidden power, no alchemy. Instead there is an identifiable body of practical techniques that can extract useful information from raw data. This book describes these techniques and shows how they work. <br><br>The book is a major revision of the first edition that appeared in 1999. While the basic core remains the same, it has been updated to reflect the changes that have taken place over five years, and now has nearly double the references. The highlights for the new edition include thirty new technique sections; an enhanced Weka machine learning workbench, which now features an interactive interface; comprehensive information on neural networks; a new section on Bayesian networks; plus much more.<br><br>+ Authors, Ian Witten and Eibe Frank, recipients of the 2005 ACM SIGKDD Service Award.<br>+ Algorithmic methods at the heart of successful data miningincluding tried and true techniques as well as leading edge methods; <br>+ Performance improvement techniques that work by transforming the input or output; <br>+ Downloadable Weka, a collection of machine learning algorithms for data mining tasks, including tools for data pre-processing, classification, regression, clustering, association rules, and visualizationin a new, interactive interface.","1957":null,"1958":"Predicting disease at an early stage becomes critical and the most difficult challenge is to predict it correctly along with the sickness. The prediction happens on the basis of the symptoms of an individual. The model presented can work like a digital doctor for the disease prediction which helps to diagnose the disease timely and can be efficient for the person to take immediate measures. The model is much more accurate in prediction of potential ailments. The work is tested with four machine learning algorithms and got the best accuracy with Random Forest.","1959":"Essentials of the scientific discovery process have remained largely\r\nunchanged for centuries: systematic human observation of natural phenomena is\r\nused to form hypotheses that, when validated through experimentation, are\r\ngeneralized into established scientific theory. Today, however, we face major\r\nchallenges because automated instrumentation and large-scale data acquisition\r\nare generating data sets of such volume and complexity as to defy human\r\nanalysis. Radically different scientific approaches are needed, with machine\r\nlearning (ML) showing great promise, not least for materials science research.\r\nHence, given recent advances in ML analysis of synthetic data representing\r\nelectronic quantum matter (EQM), the next challenge is for ML to engage\r\nequivalently with experimental data. For example, atomic-scale visualization of\r\nEQM yields arrays of complex electronic structure images, that frequently elude\r\neffective analyses. Here we report development and training of an array of\r\nartificial neural networks (ANN) designed to recognize different types of\r\nhypothesized order hidden in EQM image-arrays. These ANNs are used to analyze\r\nan experimentally-derived EQM image archive from carrier-doped cuprate Mott\r\ninsulators. Throughout these noisy and complex data, the ANNs discover the\r\nexistence of a lattice-commensurate, four-unit-cell periodic,\r\ntranslational-symmetry-breaking EQM state. Further, the ANNs find these\r\nphenomena to be unidirectional, revealing a coincident nematic EQM state.\r\nStrong-coupling theories of electronic liquid crystals are congruent with all\r\nthese observations.","1960":"Machine learning is obviously the hottest trend in the tech industry at the moment, thanks to the huge amount of data collected in many organizations. It is so powerful to make decisions and predictions, based on big data. Fraud detection, natural-language processing, self-driving cars and image recognition are a few examples of machine learning applications. Machine learning is a combination of statistics, computer science, linear algebra, and mathematical optimization methods.","1961":"The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application. The repeatability is important because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations. The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate. Three advances are described in this paper. First, we present a new heuristic for feature detection and, using machine learning, we derive a feature detector from this which can fully process live PAL video using less than 5 percent of the available processing time. By comparison, most other detectors cannot even operate at frame rate (Harris detector 115 percent, SIFT 195 percent). Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency. Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes. We show that, despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors. Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and of very high quality.","1962":"This workshop draws on feminist and other critical methodologies to construct interdisciplinary interventions in the design of machine learning systems. Theoretical concepts of \u201cfiguration\u201d,\r\n\u201csituating\/situated knowledge\u201d, \u201ccritical fabulation\/speculation,\u201d and \u201cdiffraction\u201d\r\nare explored through hands-on experimentation to imagine and design machine learning\r\nsystems in a more situated, inclusive, contextualize and accountable way. Through\r\nthis \u201ctheory turned practice\u201d approach the workshop aims to address systemic socio-cultural\r\nbiases and develop more socially responsible frameworks of design. The workshop provides\r\nspace for building a network for future research on interdisciplinary machine learning\r\nsystems design.","1963":"KI-Modelle und Machine-Learning-Anwendungen halten Einzug in die allt\u00e4gliche Praxis von Unternehmen und k\u00f6nnen mitbestimmt werden. Um die Interessen und Rechte der Besch\u00e4ftigten zu ber\u00fccksichtigen und zu sch\u00fctzen, sollten die aktuellen Regelungen in betrieblichen IT-Vereinbarungen hinterfragt und hinsichtlich ihrer Praxistauglichkeit gepr\u00fcft werden.\r\nDie Auswertung \"Machine-Learning-Anwendungen in der betrieblichen Praxis\" zeigt Handlungsm\u00f6glichkeiten anhand von Regelungspunkten aus insgesamt 29 abgeschlossenen Betriebs- und Dienstvereinbarungen. Die Ergebnisse wurden in Workshops mit Betriebs- und Personalr\u00e4ten diskutiert und relevante Regelungsaspekte zu KI-Modellen und Machine-Learning-Anwendungen abgeleitet. Sie sind Bestandteil der hier vorgestellten Handlungsempfehlungen.","1964":null,"1965":"ABSTRACT\r\nThe rise of big data has led to new demands for machine learning (ML) systems to learn complex models, with millions to billions of parameters, that promise adequate capacity to digest massive datasets and offer powerful predictive analytics (such as high-dimensional latent features, intermediate representations, and decision functions) thereupon. In order to run ML algorithms at such scales, on a distributed cluster with tens to thousands of machines, it is often the case that significant engineering efforts are required\u2014and one might fairly ask whether such engineering truly falls within the domain of ML research. Taking the view that \u201cbig\u201d ML systems can benefit greatly from ML-rooted statistical and algorithmic insights\u2014and that ML researchers should therefore not shy away from such systems design\u2014we discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions. These principles and strategies span a continuum from application, to engineering, and to theoretical research and development of big ML systems and architectures, with the goal of understanding how to make them efficient, generally applicable, and supported with convergence and scaling guarantees. They concern four key questions that traditionally receive little attention in ML research: How can an ML program be distributed over a cluster? How can ML computation be bridged with inter-machine communication? How can such communication be performed? What should be communicated between machines? By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs, and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks, we present opportunities for ML researchers and practitioners to further shape and enlarge the area that lies between ML and systems.","1966":"Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today\u2019s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.","1967":"The last decade has seen considerable growth in interest in Artificial Intelligence and Machine Learning. In the broadest sense, these fields aim to \u2018learn something useful\u2019 about the environment within which the organism operates. How gathered information is processed leads to the development of algorithms \u2013 how to process high dimensional data and deal with uncertainty. In the early stages of research in Machine Learning and related areas, similar techniques were discovered in relatively isolated research communities. Whilst not all techniques have a natural description in terms of probability theory, many do, and it is the framework of Graphical Models (a marriage between graph and probability theory) that has enabled the understanding and transference of ideas from statistical physics, statistics, machine learning and information theory. To this extent it is now reasonable to expect that machine learning researchers are familiar with the basics of statistical modelling techniques.\r\n\r\nThis book concentrates on the probabilistic aspects of information processing and machine learning. Certainly no claim is made as to the correctness or that this is the only useful approach. Indeed, one might counter that this is unnecessary since \u201cbiological organisms don\u2019t use probability theory\u201d. Whether this is the case or not, it is undeniable that the framework of graphical models and probability has helped with the explosion of new algorithms and models in the machine learning community. One should also be clear that Bayesian viewpoint is not the only way to go about describing machine learning and information processing. Bayesian and probabilistic techniques really come into their own in domains where uncertainty is a necessary consideration.","1968":null,"1969":"Good benchmarks are hard to find because they require a substantial effort to keep them representative for the constantly changing challenges of a particular field. Synthetic benchmarks are a common approach to deal with this, and methods from machine learning are natural candidates for synthetic benchmark generation. In this paper we investigate the usefulness of machine learning in the prominent CLgen benchmark generator. We re-evaluate CLgen by comparing the benchmarks generated by the model with the raw data used to train it. This re-evaluation indicates that, for the use case considered, machine learning did not yield additional benefit over a simpler method using the raw data. We investigate the reasons for this and provide further insights into the challenges the problem could pose for potential future generators.","1970":"At first blush, user modeling appears to be a prime candidate for straight forward application of standard machine learning techniques. Observations of the user's behavior can provide training examples that a machine learning system can use to form a model designed to predict future actions. However, user modeling poses a number of challenges for machine learning that have hindered its application in user modeling, including: the need for large data sets; the need for labelled data; concept drift; and computational complexity. This paper examines each of these issues and reviews approaches to resolving them.","1971":"In this work we discuss the impact of nuisance parameters on the\r\neffectiveness of machine learning in high-energy physics problems, and provide\r\na review of techniques that may reduce or remove their effect in the search for\r\noptimal selection criteria and variable transformations. Nuisance parameters\r\noften limit the usefulness of supervised learning in physical analyses due to\r\nthe degradation of model performances in real data and\/or the reduction of\r\ntheir statistical reach. The approaches discussed include nuisance-parametrized\r\nmodels, modified or adversary losses, semi-supervised learning approaches and\r\ninference-aware techniques.","1972":"We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images\r\nof 70,000 fashion products from 10 categories, with 7,000 images per category.\r\nThe training set has 60,000 images and the test set has 10,000 images.\r\nFashion-MNIST is intended to serve as a direct drop-in replacement for the\r\noriginal MNIST dataset for benchmarking machine learning algorithms, as it\r\nshares the same image size, data format and the structure of training and\r\ntesting splits. The dataset is freely available at\r\nhttps:\/\/github.com\/zalandoresearch\/fashion-mnist","1973":"While real-world decisions involve many competing objectives, algorithmic\r\ndecisions are often evaluated with a single objective function. In this paper,\r\nwe study algorithmic policies which explicitly trade off between a private\r\nobjective (such as profit) and a public objective (such as social welfare). We\r\nanalyze a natural class of policies which trace an empirical Pareto frontier\r\nbased on learned scores, and focus on how such decisions can be made in noisy\r\nor data-limited regimes. Our theoretical results characterize the optimal\r\nstrategies in this class, bound the Pareto errors due to inaccuracies in the\r\nscores, and show an equivalence between optimal strategies and a rich class of\r\nfairness-constrained profit-maximizing policies. We then present empirical\r\nresults in two different contexts -- online content recommendation and\r\nsustainable abalone fisheries -- to underscore the applicability of our\r\napproach to a wide range of practical decisions. Taken together, these results\r\nshed light on inherent trade-offs in using machine learning for decisions that\r\nimpact social welfare.","1974":null,"1975":null,"1976":null,"1977":null,"1978":"Scikit-learn is an increasingly popular machine learning li- brary. Written\r\nin Python, it is designed to be simple and efficient, accessible to\r\nnon-experts, and reusable in various contexts. In this paper, we present and\r\ndiscuss our design choices for the application programming interface (API) of\r\nthe project. In particular, we describe the simple and elegant interface shared\r\nby all learning and processing units in the library and then discuss its\r\nadvantages in terms of composition and reusability. The paper also comments on\r\nimplementation details specific to the Python ecosystem and analyzes obstacles\r\nfaced by users and developers of the library.","1979":null,"1980":"The ML-Schema, proposed by the W3C Machine Learning Schema Community Group, is a top-level ontology that provides a set of classes, properties, and restrictions for representing and interchanging information on machine learning algorithms, datasets, and experiments. It can be easily extended and specialized and it is also mapped to other more domain-specific ontologies developed in the area of machine learning and data mining. In this paper we overview existing state-of-the-art machine learning interchange formats and present the first release of ML-Schema, a canonical format resulted of more than seven years of experience among different research institutions. We argue that exposing semantics of machine learning algorithms, models, and experiments through a canonical format may pave the way to better interpretability and to realistically achieve the full interoperability of experiments regardless of platform or adopted workflow solution.","1981":"If you want to recognize speech or filter out spam emails, you will probably write a machine learning algorithm and will not try to write the whole program using a \"traditional\" software specification and implementation. There are many examples of successful machine learning solutions, but can we more broadly apply the techniques to most or all software problems, and for most or all programmers, from the novice in their first programming course to the seasoned professional?","1982":null,"1983":"The problem of identifying the phase of a given system for a certain value of\r\nthe temperature can be reformulated as a classification problem in Machine\r\nLearning. Taking as a prototype the Ising model and using the Support Vector\r\nMachine as a tool to classify Monte Carlo generated configurations, we show\r\nthat the critical region of the system can be clearly identified and the\r\nsymmetry that drives the transition can be reconstructed from the performance\r\nof the learning process. The role of the discrete symmetry of the system in\r\nobtaining this result is discussed. A finite size analysis of the learned\r\nSupport Vector Machine decision function allows us to determine the critical\r\ntemperature and critical exponents with a precision that is comparable to that\r\nof the most efficient numerical approaches relying on a known Hamiltonian\r\ndescription of the system. For the determination of the critical temperature\r\nand of the critical exponent connected with the divergence of the correlation\r\nlength, other than the availability of a range of temperatures having\r\ninformation on both phases, the method we propose does not rest on any physical\r\ninput on the system, and in particular is agnostic to its Hamiltonian, its\r\nsymmetry properties and its order parameter. Hence, our investigation provides\r\na first significant step in the direction of devising robust tools for\r\nquantitative analyses of phase transitions in cases in which an order parameter\r\nis not known.","1984":"Interactive interfaces in tandem with Machine Learning (ML) models support user understanding of model uncertainty, build confidence, improve predictive accuracy and enable users to teach application-specific concepts that are difficult for the model to learn otherwise. These systems offer empirically proven benefits due to tightly coupled feedback loops and workflow scaffolding. However, deployment with ML non-experts who cannot manage the complex, expertise-heavy process remains challenging. Through deployment with non-expert users in a common classification task, we investigate the impact of human factors of machine teaching interfaces such as user expectations, their perceptions of the learning process and user engagement with respect to teaching process and outcomes. We measure how affective and performance attributes shape the success or failure of the process. Finally, we reflect on how intelligent user interfaces can be designed to accommodate these factors for successful deployment with a broad spectrum of human adjudicators.","1985":"Evidence of a limitation in certain machine-learning algorithms.","1986":"Cardiovascular disease prediction aids practitioners in making more accurate health decisions for their patients. Early detection can aid people in making lifestyle changes and, if necessary, ensuring effective medical care. Machine learning (ML) is a plausible option for reducing and understanding heart symptoms of disease. The chi-square statistical test is performed to select specific attributes from the Cleveland heart disease (HD) dataset. Support vector machine (SVM), Gaussian Naive Bayes, logistic regression, LightGBM, XGBoost, and random forest algorithm have been employed for developing heart disease risk prediction model and obtained the accuracy as 80.32%, 78.68%, 80.32%, 77.04%, 73.77%, and 88.5%, respectively. The data visualization has been generated to illustrate the relationship between the features. According to the findings of the experiments, the random forest algorithm achieves 88.5% accuracy during validation for 303 data instances with 13 selected features of the Cleveland HD dataset.","1987":null,"1988":"Topological concepts open many new horizons for photonic devices, from integrated optics to lasers. The complexity of large scale topological devices asks for an effective solution of the inverse problem: how best to engineer the topology for a specific application? We introduce a novel machine learning approach to the topological inverse problem. We train a neural network system with the band structure of the Aubry-Andre-Harper model and then adopt the network for solving the inverse problem. Our application is able to identify the parameters of a complex topological insulator in order to obtain protected edge states at target frequencies. One challenging aspect is handling the multivalued branches of the direct problem and discarding unphysical solutions. We overcome this problem by adopting a self-consistent method to only select physically relevant solutions. We demonstrate our technique in a realistic topological laser design and by resorting to the widely available open-source TensorFlow library. Our results are general and scalable to thousands of topological components. This new inverse design technique based on machine learning potentially extends the applications of topological photonics, for example, to frequency combs, quantum sources, neuromorphic computing and metrology.","1989":"Detecting code smells and treating them with refactoring are trivial part of maintaining vast and sophisticated software. There is an urgent need for automatic system to treat code smells. Tools provide variable results, based on threshold values and subjective interpretation of smells. Machine learning is one of the best approaches that provides effective solution to this problem. Practitioners do not need expert knowledge on smell's characteristics for detection, which makes this approach accessible. In this paper, we have implemented 32 machine learning algorithms after performing feature selection through six variations of the filter method. We have used multiple correlation methodologies to discard similar features. Mutual information, fisher score, and univariate ROC--AUC feature selection techniques were used with brute force and random forest correlation strategies. Feature selection eliminates dimensionality curse and improves performance measures drastically. It is the selection of relevant feature subset based on the relation between dependent and independent variables. We have compared performance of classifiers implemented with and without performing feature selection. Results show that accuracy of machine learning models has increased up to 26.5\\%, f-measure by 70.9\\%, area under ROC curve has surged up to 26.74\\%, and average training time has reduced up to 62 s as compared to performance measures of machine learning models executed without feature selection. Mutual information feature selection strategy with random forest correlation methodology has the highest impact on performance measures among all the filter methods. Among 32 classifiers, boosted decision trees (J48) and Naive Bayes algorithms gave best performance after dimensionality reduction.","1990":"This paper presents the development of a hybrid dynamic expert system for the diagnosis of peripheral diabetes and remedies using a rule based machine learning technique. The aim was to develop a solution to the risk factors of peripheral diabetes. The methodology applied in this study is the experimental method, and the software design methodology used was the agile methodology. Data was collected from Nnamdi Azikiwe University Teaching Hospitals NAUTH and the Lagos State University Teaching Hospital LASUTH for patients between the ages of 28 87years suffering from peripheral neuropathy. Other methods used were data integration by applying uniform data access UDA technique, data processing using Infinite Impulse Response Filter IIRF , data extraction with a computerized approach, machine learning algorithm with Dynamic Feed Forward Neural Network DFNN , rule base algorithm. The modeling of the hybrid dynamic expert system and remedies was achieved using the DFNN for the detection of DPN and a rule based model for remedies and recommendations. The models were implemented with MATLAB and Java programming languages. The result when evaluated achieved a Mean Square Error MSE of 4.9392e 11 and Regression R of 0.99823. The implication of the result showed that the peripheral diabetes detection model correctly learns the peripheral diabetes attributes and was also able to correctly detect peripheral diabetes in patients. The model when compared with other sophisticated models also showed that it achieved a better regression score. The reason was due to the appropriate steps used in the data preparation such as integration and the use of IIFR filter, feature extraction, and the deep configuration of the regression model. Omeye Emmanuel C. | Ngene John N. | Dr. Anyaragbu Hope U. | Dr. Ozioko Ekene | Dr. Iloka Bethram C. | Prof. Inyiama Hycent C. \"Development of a Hybrid Dynamic Expert System for the Diagnosis of Peripheral Diabetes and Remedies using a Rule-Based Machine Learning Technique\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-6 | Issue-7 , December 2022, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd52356.pdf Paper URL: https:\/\/www.ijtsrd.com\/computer-science\/other\/52356\/development-of-a-hybrid-dynamic-expert-system-for-the-diagnosis-of-peripheral-diabetes-and-remedies-using-a-rulebased-machine-learning-technique\/omeye-emmanuel-c","1991":"We present a method for improving the efficiency and user experience of\r\nfreeform illumination design with machine learning. By utilizing orthogonal\r\npolynomials to interface with artificial neural networks, we are able to\r\ngeneralize relationships between freeform surface shapes and design parameters.\r\nThen, by training the network to generalize the relationship between high-level\r\ndesign goals and final performance, we were able to transform what is\r\ntraditionally a difficult and computationally intensive problem into a compact,\r\nuser friendly form. The potential of the proposed method is demonstrated\r\nthrough the design of uniform square patterns from off-axis positions and\r\nrectangular patterns of tuneable aspect ratios and distances from the target.","1992":"This is the Proceedings of the 2016 ICML Workshop on Human Interpretability\r\nin Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.\r\n  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,\r\nand Hanna Wallach.","1993":"AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics\/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI\/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called \u201cmulti-dimensional discrimination\u201d or \u201cmulti-dimensional fairness\u201d problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination\/fairness in the legal domain as well as how they have been transferred\/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions.","1994":"Extracting structured data from narrated medical reports is challenged by the complexity of heterogeneous structures and vocabularies and often requires significant manual effort. Traditional machine-based approaches lack the capability to take user feedbacks for improving the extraction algorithm in real time.Our goal was to provide a generic information extraction framework that can support diverse clinical reports and enables a dynamic interaction between a human and a machine that produces highly accurate results.A clinical information extraction system IDEAL-X has been built on top of online machine learning. It processes one document at a time, and user interactions are recorded as feedbacks to update the learning model in real time. The updated model is used to predict values for extraction in subsequent documents. Once prediction accuracy reaches a user-acceptable threshold, the remaining documents may be batch processed. A customizable controlled vocabulary may be used to support extraction.Three datasets were used for experiments based on report styles: 100 cardiac catheterization procedure reports, 100 coronary angiographic reports, and 100 integrated reports-each combines history and physical report, discharge summary, outpatient clinic notes, outpatient clinic letter, and inpatient discharge medication report. Data extraction was performed by 3 methods: online machine learning, controlled vocabularies, and a combination of these. The system delivers results with F1 scores greater than 95%.IDEAL-X adopts a unique online machine learning-based approach combined with controlled vocabularies to support data extraction for clinical reports. The system can quickly learn and improve, thus it is highly adaptable.","1995":null,"1996":"The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.","1997":"Interactive Machine Learning (iML) ist ein relativ neuer Ansatz basierend auf neuen Algorithmen, die mit -- teils menschlichen -- Agenten interagieren und durch diese Interaktion ihr Lernverhalten optimieren k\u00f6nnen.","1998":"This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task.","1999":"The annual number of publications at scientific venues, for example,\r\nconferences and journals, is growing quickly. Hence, even for researchers\r\nbecomes harder and harder to keep track of research topics and their progress.\r\nIn this task, researchers can be supported by automated publication analysis.\r\nYet, many such methods result in uninterpretable, purely numerical\r\nrepresentations. As an attempt to support human analysts, we present\r\ntopic space trajectories, a structure that allows for the comprehensible\r\ntracking of research topics. We demonstrate how these trajectories can be\r\ninterpreted based on eight different analysis approaches. To obtain\r\ncomprehensible results, we employ non-negative matrix factorization as well as\r\nsuitable visualization techniques. We show the applicability of our approach on\r\na publication corpus spanning 50 years of machine learning research from 32\r\npublication venues. Our novel analysis method may be employed for paper\r\nclassification, for the prediction of future research topics, and for the\r\nrecommendation of fitting conferences and journals for submitting unpublished\r\nwork.","2000":null,"2001":"As the bioinformatics field grows, it must keep pace not only with new data but with new algorithms. Here we contribute a thorough analysis of 13\r\nstate-of-the-art, commonly used machine learning algorithms on a set of 165\r\npublicly available classification problems in order to provide data-driven\r\nalgorithm recommendations to current researchers. We present a number of\r\nstatistical and visual comparisons of algorithm performance and quantify the\r\neffect of model selection and algorithm tuning for each algorithm and dataset.\r\nThe analysis culminates in the recommendation of five algorithms with\r\nhyperparameters that maximize classifier performance across the tested\r\nproblems, as well as general guidelines for applying machine learning to\r\nsupervised classification problems.","2002":"The Machine Learning is a data analysis technique that automates the development of analytic models. It is an important field of study based on the premise that machine data can be studied and that trends and judgments can be made with little human input. On the basis of fresh discoveries, machine learning algorithms may be constructed from provided data.","2003":"Machines would be more useful if they could learn to\n                 perform tasks for which they were not given precise\n                 methods. Difficulties that attend giving a machine this\n                 ability are discussed. It is proposed that the program\n                 of a stored-program computer be gradually improved by a\n                 learning procedure which tries many programs and\n                 chooses, from the intructions that may occupy a given\n                 location, the one most often associated with a\n                 successful result. An experimental test of this\n                 principle is described in detail.","2004":"The main objective of this paper is to prepare a Clinical Decision Support System (CDSS) for a multi-class classification of ElectroCardioGram (ECG) signals into certain cardiac diseases. This CDSS is based on Artificial Neural Network (ANN) as a machine learning classifier and uses time scale input features. Fourty eight (48) ECG signals were selected from MIT-BIH arrhythmia database, of one minute recording. Unfortunately, among several types of learning algorithms for the ANN classifier, finding the appropriate one demands a comparative study. So, in this study, we have evaluated the impact of two learning algorithms, which are the Levenberg-Marquardt (trainlm) and the Bayesian-Regularization (trainbr) on the proposed ANN performance. Consequently, we have achieved that trainbr reaches the most accurate result (93.8%), while trainlm generates the highest classification speed (0.582s). Subsequently, in order to assess the efficiency of this work, a second comparative study with related works, is done. Therefore, despite not being in the same working conditions, the obtained accuracy (93.8%) is considered acceptable.","2005":null,"2006":null,"2007":"Automated image processing algorithms can improve the quality, efficiency,\r\nand consistency of classifying the morphology of heterogeneous carbonate rock\r\nand can deal with a massive amount of data and images seamlessly. Geoscientists\r\nface difficulties in setting the direction of the optimum method for\r\ndetermining petrophysical properties from rock images, Micro-Computed\r\nTomography (uCT), or Magnetic Resonance Imaging (MRI). Most of the successful\r\nwork is from the homogeneous rocks focusing on 2D images with less focus on 3D\r\nand requiring numerical simulation. Currently, image analysis methods converge\r\nto three approaches: image processing, artificial intelligence, and combined\r\nimage processing with artificial intelligence. In this work, we propose two\r\nmethods to determine the porosity from 3D uCT and MRI images: an image\r\nprocessing method with Image Resolution Optimized Gaussian Algorithm (IROGA);\r\nadvanced image recognition method enabled by Machine Learning Difference of\r\nGaussian Random Forest (MLDGRF). We have built reference 3D micro models and\r\ncollected images for calibration of IROGA and MLDGRF methods. To evaluate the\r\npredictive capability of these calibrated approaches, we ran them on 3D uCT and\r\nMRI images of natural heterogeneous carbonate rock. We measured the porosity\r\nand lithology of the carbonate rock using three and two industry-standard ways,\r\nrespectively, as reference values. Notably, IROGA and MLDGRF have produced\r\nporosity results with an accuracy of 96.2% and 97.1% on the training set and\r\n91.7% and 94.4% on blind test validation, respectively, in comparison with the\r\nthree experimental measurements. We measured limestone and pyrite reference\r\nvalues using two methods, X-ray powder diffraction, and grain density\r\nmeasurements. MLDGRF has produced lithology (limestone and Pyrite) volumes with\r\n97.7% accuracy.","2008":"The goal of precipitation nowcasting is to predict the future rainfall\r\nintensity in a local region over a relatively short period of time. Very few\r\nprevious studies have examined this crucial and challenging weather forecasting\r\nproblem from the machine learning perspective. In this paper, we formulate\r\nprecipitation nowcasting as a spatiotemporal sequence forecasting problem in\r\nwhich both the input and the prediction target are spatiotemporal sequences. By\r\nextending the fully connected LSTM (FC-LSTM) to have convolutional structures\r\nin both the input-to-state and state-to-state transitions, we propose the\r\nconvolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model\r\nfor the precipitation nowcasting problem. Experiments show that our ConvLSTM\r\nnetwork captures spatiotemporal correlations better and consistently\r\noutperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for\r\nprecipitation nowcasting.","2009":"Prognostic modelling is important in clinical practice and epidemiology for patient management and research. Electronic health records (EHR) provide large quantities of data for such models, but conventional epidemiological approaches require significant researcher time to implement. Expert selection of variables, fine-tuning of variable transformations and interactions, and imputing missing values are time-consuming and could bias subsequent analysis, particularly given that missingness in EHR is both high, and may carry meaning. Using a cohort of 80,000 patients from the CALIBER programme, we compared traditional modelling and machine-learning approaches in EHR. First, we used Cox models and random survival forests with and without imputation on 27 expert-selected, preprocessed variables to predict all-cause mortality. We then used Cox models, random forests and elastic net regression on an extended dataset with 586 variables to build prognostic models and identify novel prognostic factors without prior expert input. We observed that data-driven models used on an extended dataset can outperform conventional models for prognosis, without data preprocessing or imputing missing values. An elastic net Cox regression based with 586 unimputed variables with continuous values discretised achieved a C-index of 0.801 (bootstrapped 95% CI 0.799 to 0.802), compared to 0.793 (0.791 to 0.794) for a traditional Cox model comprising 27 expert-selected variables with imputation for missing values. We also found that data-driven models allow identification of novel prognostic variables; that the absence of values for particular variables carries meaning, and can have significant implications for prognosis; and that variables often have a nonlinear association with mortality, which discretised Cox models and random forests can elucidate. This demonstrates that machine-learning approaches applied to raw EHR data can be used to build models for use in research and clinical practice, and identify novel predictive variables and their effects to inform future research.","2010":null,"2011":"Abstract.Coronary Artery Disease is the type of cardiovascular disease (CVD) that happens whenthe  blood  vessels  which  stream  the  blood  toward  the  heart,  either  become  tapered  or  blocked.   Ofthis, the heart is incapable to push sufficient blood to encounter its requirements.  This would lead toangina (chest pain).  CVDs are the leading cause of mortality worldwide.  According to WHO, in theyear 2019  17.9 million people deceased from CVD. Machine Learning is a type of artificial intelligencethat uses algorithms to help analyse large datasets more efficiently.  It can be used in medical researchto  help  process  large  amounts  of  data  quickly,  such  as  patient  records  or  medical  images.   By  usingMachine Learning techniques and methods, scientists can automate the analysis of complex and largedatasets  to  gain  deeper  insights  into  the  data.   Machine  Learning  is  a  type  of  technology  that  helpswith gathering data and understanding patterns.  Recently, researchers in the healthcare industry havebeen using Machine Learning techniques to assist with diagnosing heart-related diseases.  This meansthat the professionals involved in the diagnosis process can use Machine Learning to help them figureout what is wrong with a patient and provide appropriate treatment.  This paper evaluates differentmachine  learning  models  performances.   The  Supervised  Learning  algorithms  are  used  commonly  inMachine Learning which means that the training is done using labelled data, belonging to a particularclassification.   Such  classification  methods  like  Random  Forest,  Decision  Tree,  K-Nearest  Neighbour,XGBoost algorithm, Na \u0308\u0131ve Bayes, and Support Vector Machine will be used to assess the cardiovasculardisease by Machine Learning.","2012":"Injuries have a great impact on professional soccer, due to their large influence on team performance and the considerable costs of rehabilitation for players. Existing studies in the literature provide just a preliminary understanding of which factors mostly affect injury risk, while an evaluation of the potential of statistical models in forecasting injuries is still missing. In this paper, we propose a multidimensional approach to injury prediction in professional soccer which is based on GPS measurements and machine learning. By using GPS tracking technology, we collect data describing the training workload of players in a professional soccer club during a season. We show that our injury predictors are both accurate and interpretable by providing a set of case studies of interest to soccer practitioners. Our approach opens a novel perspective on injury prevention, providing a set of simple and practical rules for evaluating and interpreting the complex relations between injury risk and training performance in professional soccer.","2013":"While first-order optimization methods such as stochastic gradient descent\r\n(SGD) are popular in machine learning (ML), they come with well-known\r\ndeficiencies, including relatively-slow convergence, sensitivity to the\r\nsettings of hyper-parameters such as learning rate, stagnation at high training\r\nerrors, and difficulty in escaping flat regions and saddle points. These issues\r\nare particularly acute in highly non-convex settings such as those arising in\r\nneural networks. Motivated by this, there has been recent interest in\r\nsecond-order methods that aim to alleviate these shortcomings by capturing\r\ncurvature information. In this paper, we report detailed empirical evaluations\r\nof a class of Newton-type methods, namely sub-sampled variants of trust region\r\n(TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex\r\nML problems. In doing so, we demonstrate that these methods not only can be\r\ncomputationally competitive with hand-tuned SGD with momentum, obtaining\r\ncomparable or better generalization performance, but also they are highly\r\nrobust to hyper-parameter settings. Further, in contrast to SGD with momentum,\r\nwe show that the manner in which these Newton-type methods employ curvature\r\ninformation allows them to seamlessly escape flat regions and saddle points.","2014":"This article describes an automated sensor-based system to detect pedestrians in an autonomous vehicle application. Although the vehicle is equipped with a broad set of sensors, the article focuses on the processing of the information generated by a Velodyne HDL-64E LIDAR sensor. The cloud of points generated by the sensor (more than 1 million points per revolution) is processed to detect pedestrians, by selecting cubic shapes and applying machine vision and machine learning algorithms to the XY, XZ, and YZ projections of the points contained in the cube. The work relates an exhaustive analysis of the performance of three different machine learning algorithms: k-Nearest Neighbours (kNN), Na\\\u00ef\\ve Bayes classifier (NBC), and Support Vector Machine (SVM). These algorithms have been trained with 1931 samples. The final performance of the method, measured a real traffic scenery, which contained 16 pedestrians and 469 samples of non-pedestrians, shows sensitivity (81.2%), accuracy (96.2%) and specificity (96.8%).","2015":"The Centers for Disease Control and Prevention (CDC) coordinates a\r\nlabor-intensive process to measure the prevalence of autism spectrum disorder\r\n(ASD) among children in the United States. Random forests methods have shown\r\npromise in speeding up this process, but they lag behind human classification\r\naccuracy by about 5 percent. We explore whether newer document classification\r\nalgorithms can close this gap. We applied 6 supervised learning algorithms to\r\npredict whether children meet the case definition for ASD based solely on the\r\nwords in their evaluations. We compared the algorithms? performance across 10\r\nrandom train-test splits of the data, and then, we combined our top 3\r\nclassifiers to estimate the Bayes error rate in the data. Across the 10\r\ntrain-test cycles, the random forest, neural network, and support vector\r\nmachine with Naive Bayes features (NB-SVM) each achieved slightly more than\r\n86.5 percent mean accuracy. The Bayes error rate is estimated at approximately\r\n12 percent meaning that the model error for even the simplest of our\r\nalgorithms, the random forest, is below 2 percent. NB-SVM produced\r\nsignificantly more false positives than false negatives. The random forest\r\nperformed as well as newer models like the NB-SVM and the neural network.\r\nNB-SVM may not be a good candidate for use in a fully-automated surveillance\r\nworkflow due to increased false positives. More sophisticated algorithms, like\r\nhierarchical convolutional neural networks, would not perform substantially\r\nbetter due to characteristics of the data. Deep learning models performed\r\nsimilarly to traditional machine learning methods at predicting the\r\nclinician-assigned case status for CDC's autism surveillance system. While deep\r\nlearning methods had limited benefit in this task, they may have applications\r\nin other surveillance systems.","2016":null,"2017":"Machine learning methods are being explored in many areas of science, with\r\nthe aim of finding solution to problems that evade traditional scientific\r\napproaches due to their complexity. In general, an order parameter capable of\r\nidentifying two different phases of matter separated by a correspond- ing phase\r\ntransition is constructed based on symmetry arguments. This parameter measures\r\nthe degree of order as the phase transition proceeds. However, when the two\r\ndistinct phases are highly disordered it is not trivial to identify broken\r\nsymmetries with which to find an order parameter. This poses an excellent\r\nproblem to be addressed using machine learning procedures. Room tem- perature\r\nliquid water is hypothesized to be a supercritical liquid, with fluctuations of\r\ntwo different molecular orders associated to two parent liquid phases, one with\r\nhigh density and another one with low density. The validity of this hypothesis\r\nis linked to the existence of an order parameter capable of identifying the two\r\ndistinct liquid phases and their fluctuations. In this work we show how two\r\ndifferent machine learning procedures are capable of recognizing local order in\r\nliquid water. We argue that when in order to learn relevant features from this\r\ncomplexity, an initial, physically motivated preparation of the available data\r\nis as important as the quality of the data set, and that machine learning can\r\nbecome a successful analysis tool only when coupled to high level physical\r\ninformation.","2018":"The steady increase in annual car manufacturing over the past decade is reflected in 2016's record high of more than 90 million passenger vehicles. As a result, there is now a booming industry dedicated to pre-owned automobiles. Both buyers and sellers can now more easily access information on the factors that determine a used car's current market value thanks to the proliferation of internet marketplaces. Using Machine Learning Algorithms like Lasso Regression, Multiple Regression, and Regression Trees, we'll attempt to build a statistical model that can predict the price of a used car based on historical client data and a number of characteristics. Predicting the future value of a car is essential for both consumers and sellers in the auto market. The ability of machine learning algorithms to reliably estimate car pricing based on factors like make, model, mileage, year, and more has been demonstrated. In this research, we offer a model for predicting the future cost of a car using machine learning. In this research, we offer a machine learning-based method for predicting future auto prices. By using feature engineering, data normalisation, and missing value handling, among other pre-processing approaches, we examine a sizable collection of historical automobile sales data. Then, we use machine learning algorithms like linear regression, decision trees, random forests, and support vector machines to train and assess the performance of our model.","2019":null,"2020":"Many machine learning (ML) approaches are widely used to generate bioclimatic models for prediction of geographic range of organism as a function of climate. Applications such as prediction of range shift in organism, range of invasive species influenced by climate change are important parameters in understanding the impact of climate change. However, success of machine learning-based approaches depends on a number of factors. While it can be safely said that no particular ML technique can be effective in all applications and success of a technique is predominantly dependent on the application or the type of the problem, it is useful to understand their behavior to ensure informed choice of techniques. This paper presents a comprehensive review of machine learning-based bioclimatic model generation and analyses the factors influencing success of such models. Considering the wide use of statistical techniques, in our discussion we also include conventional statistical techniques used in bioclimatic modelling.","2021":null,"2022":null,"2023":null,"2024":"This paper presents a series of feminist epistemological concepts as tools for developing critical, more accountable, and contextualised approaches to machine learning systems design. Namely, we suggest that the methods of situated knowledges or situating, figurations or figuring, diffraction or diffracting, and critical fabulation or speculation can be productively actualised in the field of machine learning systems design. We also suggest that the meta-method for doing this actualisation requires not so much translation but transposition - a creative and critical adaptation to speak to machine learning contexts.","2025":"We consider the problem of classifying documents not by topic, but\n        by overall sentiment, e.g., determining whether a review is positive\n        or negative. Using movie reviews as data, we find that standard\n        machine learning techniques definitively outperform human-produced\n        baselines. However, the three machine learning methods we employed\n        (Naive Bayes, maximum entropy classification, and support vector\n        machines) do not perform as well on sentiment classification as\n        on traditional topic-based categorization. We conclude by examining\n        factors that make the sentiment classification problem more challenging.","2026":"\"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering\"--","2027":null,"2028":"We study on-line generalized linear regression with multidimensional outputs, i.e., neural networks with multiple output nodes but no hidden nodes. We allow at the final layer transfer functions such as the softmax function that need to consider the linear activations to all the output neurons. The weight vectors used to produce the linear activations are represented indirectly by maintaining separate parameter vectors. We get the weight vector by applying a particular parameterization function to the parameter vector. Updating the parameter vectors upon seeing new examples is done additively, as in the usual gradient descent update. However, by using a nonlinear parameterization function between the parameter vectors and the weight vectors, we can make the resulting update of the weight vector quite different from a true gradient descent update. To analyse such updates, we define a notion of a matching loss function and apply it both to the transfer function and to the parameterization function. The loss function that matches the transfer function is used to measure the goodness of the predictions of the algorithm. The loss function that matches the parameterization function can be used both as a measure of divergence between models in motivating the update rule of the algorithm and as a measure of progress in analyzing its relative performance compared to an arbitrary fixed model. As a result, we have a unified treatment that generalizes earlier results for the gradient descent and exponentiated gradient algorithms to multidimensional outputs, including multiclass logistic regression.","2029":"We identify multi-wavelength counterparts to 1,147 submillimeter sources from\r\nthe S2COSMOS SCUBA-2 survey of the COSMOS field by employing a recently\r\ndeveloped radio$+$machine-learning method trained on a large sample of\r\nALMA-identified submillimeter galaxies (SMGs), including 260 SMGs identified in\r\nthe AS2COSMOS pilot survey. In total, we identify 1,222\r\noptical\/near-infrared(NIR)\/radio counterparts to the 897 S2COSMOS submillimeter\r\nsources with S$_850$>1.6mJy, yielding an overall identification rate of\r\n($78\\pm9$)%. We find that ($22\\pm5$)% of S2COSMOS sources have multiple\r\nidentified counterparts. We estimate that roughly 27% of these multiple\r\ncounterparts within the same SCUBA-2 error circles very likely arise from\r\nphysically associated galaxies rather than line-of-sight projections by chance.\r\nThe photometric redshift of our radio$+$machine-learning identified SMGs ranges\r\nfrom z=0.2 to 5.7 and peaks at $z=2.3\\pm0.1$. The AGN fraction of our sample is\r\n($19\\pm4$)%, which is consistent with that of ALMA SMGs in the literature.\r\nComparing with radio\/NIR-detected field galaxy population in the COSMOS field,\r\nour radio+machine-learning identified counterparts of SMGs have the highest\r\nstar-formation rates and stellar masses. These characteristics suggest that our\r\nidentified counterparts of S2COSMOS sources are a representative sample of SMGs\r\nat z<3. We employ our machine-learning technique to the whole COSMOS field and\r\nidentified 6,877 potential SMGs, most of which are expected to have\r\nsubmillimeter emission fainter than the confusion limit of our S2COSMOS surveys\r\n(S$_850$<1.5mJy). We study the clustering properties of SMGs based on this\r\nstatistically large sample, finding that they reside in high-mass dark matter\r\nhalos ($(1.2\\pm0.3)\\times10^13\\,h^-1\\,M_\u00f8dot$), which suggests that\r\nSMGs may be the progenitors of massive ellipticals we see in the local\r\nUniverse.","2030":null,"2031":"KI-Modelle und Machine-Learning-Anwendungen hielten Einzug in die allt\u00e4gliche Praxis von Unternehmen und k\u00f6nnten mitbestimmt werden. Um die Interessen und Rechte der Besch\u00e4ftigten zu beru\u0308cksichtigen und zu schu\u0308tzen, sollten die aktuellen Regelungen in betrieblichen IT-Vereinbarungen hinterfragt und hinsichtlich ihrer Praxistauglichkeit gepru\u0308ft werden. Die Auswertung \"Machine-Learning-Anwendungen in der betrieblichen Praxis\" zeige Handlungsm\u00f6glichkeiten anhand von Regelungspunkten aus insgesamt 29 abgeschlossenen Betriebs- und Dienstvereinbarungen auf. Die Ergebnisse wurden in Workshops mit Betriebs- und Personalr\u00e4ten diskutiert und relevante Regelungsaspekte zu KI-Modellen und Machine-Learning-Anwendungen abgeleitet. Sie sind Bestandteil der hier vorgestellten Handlungsempfehlungen.","2032":"Cardiovascular diseases are considered as the most life-threatening syndromes with the highest mortality rate globally. Over a period of time, they have become very common and are now overstretching the healthcare systems of countries. The major factors of cardiovascular diseases are high blood pressure, family history, stress, age, gender, cholesterol, Body Mass Index (BMI), and unhealthy lifestyle. Based on these factors, researchers have proposed various approaches for early diagnosis. However, the accuracy of proposed techniques and approaches needs certain improvements due to the inherent criticality and life threatening risks of cardiovascular diseases. In this article, a MaLCaDD (Machine Learning based Cardiovascular Disease Diagnosis) framework is proposed for the effective prediction of cardiovascular diseases with high precision. Particularly, the framework first deals with the missing values (via mean replacement technique) and data imbalance (via Synthetic Minority Over-sampling Technique - SMOTE). Subsequently, Feature Importance technique is utilized for feature selection. Finally, an ensemble of Logistic Regression and K-Nearest Neighbor (KNN) classifiers is proposed for prediction with higher accuracy. The validation of framework is performed through three benchmark datasets (i.e. Framingham, Heart Disease and Cleveland) and the accuracies of 99.1%, 98.0% and 95.5 % are achieved respectively. Finally, the comparative analysis proves that MaLCaDD predictions are more accurate (with reduced set of features) as compared to the existing state-of-the-art approaches. Therefore, MaLCaDD is highly reliable and can be applied in real environment for the early diagnosis of cardiovascular diseases.","2033":"This thesis examines the use of machine learning techniques in various tasks of natural language processing, mainly for the task of information extraction from texts. The objectives are the improvement of adaptability of information extraction systems to new thematic domains (or even languages), and the improvement of their performance using as fewer resources (either linguistic or human) as possible. This thesis has examined two main axes: a) the research and assessment of existing algorithms of machine learning mainly in the stages of linguistic pre-processing (such as part of speech tagging) and named-entity recognition, and b) the creation of a new machine learning algorithm and its assessment on synthetic data, as well as in real world data from the task of relation extraction between named entities. This new algorithm belongs to the category of inductive grammar learning, and can infer context free grammars from positive examples only.","2034":null,"2035":"13C metabolic flux analysis (13C-MFA) has been widely used to measure in vivo enzyme reaction rates (i.e., metabolic flux) in microorganisms. Mining the relationship between environmental and genetic factors and metabolic fluxes hidden in existing fluxomic data will lead to predictive models that can significantly accelerate flux quantification. In this paper, we present a web-based platform MFlux (http:\/\/mflux.org) that predicts the bacterial central metabolism via machine learning, leveraging data from approximately 100 13C-MFA papers on heterotrophic bacterial metabolisms. Three machine learning methods, namely Support Vector Machine (SVM), k-Nearest Neighbors (k-NN), and Decision Tree, were employed to study the sophisticated relationship between influential factors and metabolic fluxes. We performed a grid search of the best parameter set for each algorithm and verified their performance through 10-fold cross validations. SVM yields the highest accuracy among all three algorithms. Further, we employed quadratic programming to adjust flux profiles to satisfy stoichiometric constraints. Multiple case studies have shown that MFlux can reasonably predict fluxomes as a function of bacterial species, substrate types, growth rate, oxygen conditions, and cultivation methods. Due to the interest of studying model organism under particular carbon sources, bias of fluxome in the dataset may limit the applicability of machine learning models. This problem can be resolved after more papers on 13C-MFA are published for non-model species.","2036":"The traditional means of extracting information from the Web are keyword-based search and browsing. The Semantic Web adds structured information (i.e., semantic annotations and references) supporting both activities. One of the most interesting recent developments is Linked Open Data (LOD), where information is presented in the form of facts -- often originating from published domain-specific databases -- that can be accessed both by a human and a machine via specific query endpoints. In this article, we argue that machine learning provides a new way to query web data, in particular LOD, by analyzing and exploiting statistical regularities. We discuss challenges when applying machine learning to the Web and discuss the particular learning approaches we have been pursuing in THESEUS. We discuss a number of applications where the Web is queried via machine learning and describe several extensions to our approaches.","2037":"Ontology Matching (OM) playsan important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation results for OM systems of different types to demonstrate the usage of these resources, all of which are publicly available as part of the new Bio-ML track at OAEI 2022.","2038":"The fight against social exclusion is at the heart of the Europe 2020 strategy: 120 million people are at risk of suffering this condition in the EU. Risk prediction models are widely used in insurance companies and health services. However, the use of these models to allow an early detection of social exclusion by social workers is not a common practice. This paper describes a data analysis of over 16K cases with over 60 predictors from the Spanish region of Castilla y Le\u00f3n. The use of machine learning paradigms such as logistic regression and random forest makes possible a high precision in predicting chronic social exclusion. The paper is complemented with a responsive web available online that allows social workers to calculate the risk of a social exclusion case to become chronic through a smartphone.","2039":"The topic of this monograph falls within the, so-called, biologically motivated computing paradigm, in which biology provides the source of models and inspiration towards the development of computational intelligence and machine learning systems. Specifically, artificial immune systems are presented as a valid metaphor towards the creation of abstract and high level representations of biological components or functions that lay the foundations for an alternative machine learning paradigm. Therefore, focus is given on addressing the primary problems of Pattern Recognition by developing Artificial Immune System-based machine learning algorithms for the problems of Clustering, Classification and One-Class Classification. Pattern Classification, in particular, is studied within the context of the Class Imbalance Problem. The main source of inspiration stems from the fact that the Adaptive Immune System constitutes one of the most sophisticated biological systems that is exceptionally evolved in order to continuously address an extremely unbalanced pattern classification problem, namely, the self \/ non-self discrimination process. The experimental results presented in this monograph involve a wide range of degenerate binary classification problems where the minority class of interest is to be recognized against the vast volume of the majority class of negative patterns. In this context, Artificial Immune Systems are utilized for the development of personalized software as the core mechanism behind the implementation of Recommender Systems.","2040":"Cancer analysis and prediction is the utmost important research field for\r\nwell-being of humankind. The Cancer data are analyzed and predicted using\r\nmachine learning algorithms. Most of the researcher claims the accuracy of the\r\npredicted results within 99%. However, we show that machine learning algorithms\r\ncan easily predict with an accuracy of 100% on Wisconsin Diagnostic Breast\r\nCancer dataset. We show that the method of gaining accuracy is an unethical\r\napproach that we can easily mislead the algorithms. In this paper, we exploit\r\nthe weakness of Machine Learning algorithms. We perform extensive experiments\r\nfor the correctness of our results to exploit the weakness of machine learning\r\nalgorithms. The methods are rigorously evaluated to validate our claim. In\r\naddition, this paper focuses on correctness of accuracy. This paper report\r\nthree key outcomes of the experiments, namely, correctness of accuracies,\r\nsignificance of minimum accuracy, and correctness of machine learning\r\nalgorithms.","2041":"Tapping into the 'folk knowledge' needed to advance machine learning applications. Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. Machine learning is widely used in computer science and other fields. However, developing successful machine learning applications requires a substantial amount of 'black art' that is difficult to find in textbooks. This article summarizes 12 key lessons that machine learning researchers and practitioners have learned. These include pitfalls to avoid, important issues to focus on, and answers to common questions.","2042":"This paper describes an application of established machine learning principles to student modelling. Unlike previous machine learning based approaches to student modelling, the new approach is based on attribute-value machine learning. In contrast to many previous approaches it is not necessary for the lesson author to identify all forms of error that may be detected. Rather, the lesson author need only identify the relevant attributes both of the tasks to be performed by the student and of the student's actions. The values of these attributes are automatically processed by the student modeler to produce the student model.","2043":null,"2044":"In a previous paper we presented the results of applying machine learning to\r\nclassify whether an HI 21-cm absorption spectrum arises in a source intervening\r\nthe sight-line to a more distant radio source or within the host of the radio\r\nsource itself. This is usually determined from an optical spectrum giving the\r\nsource redshift. However, not only will this be impractical for the large\r\nnumber of sources expected to be detected with the Square Kilometre Array, but\r\nbright optical sources are the most ultra-violet luminous at high redshift and\r\nso bias against the detection of cool, neutral gas. Adding another 44, mostly\r\nnewly detected absorbers, to the previous sample of 92, we test four different\r\nmachine learning algorithms, again using the line properties (width, depth and\r\nnumber of Gaussian fits) as features. Of these algorithms, three gave a some\r\nimprovement over the previous sample, with a logistic regression model giving\r\nthe best results. This suggests that the inclusion of further training data, as\r\nnew absorbers are detected, will further increase the prediction accuracy above\r\nthe current 80%. We use the logistic regression model to classify the z = 0.42\r\nabsorption towards PKS 1657-298 and find this to be associated, which is\r\nconsistent with a previous study which determined a similar redshift from the\r\nK-band magnitude-redshift relation.","2045":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.","2046":"Beta Basis Function Neural Network (BBFNN) is a special kind of kernel basis\r\nneural networks. It is a feedforward network typified by the use of beta\r\nfunction as a hidden activation function. Beta is a flexible transfer function\r\nrepresenting richer forms than the common existing functions. As in every\r\nnetwork, the architecture setting as well as the learning method are two main\r\ngauntlets faced by BBFNN. In this paper, new architecture and training\r\nalgorithm are proposed for the BBFNN. An Extreme Learning Machine (ELM) is used\r\nas a training approach of BBFNN with the aim of quickening the training\r\nprocess. The peculiarity of ELM is permitting a certain decrement of the\r\ncomputing time and complexity regarding the already used BBFNN learning\r\nalgorithms such as backpropagation, OLS, etc. For the architectural design, a\r\nrecurrent structure is added to the common BBFNN architecture in order to make\r\nit more able to deal with complex, non linear and time varying problems.\r\nThroughout this paper, the conceived recurrent ELM-trained BBFNN is tested on a\r\nnumber of tasks related to time series prediction, classification and\r\nregression. Experimental results show noticeable achievements of the proposed\r\nnetwork compared to common feedforward and recurrent networks trained by ELM\r\nand using hyperbolic tangent as activation function. These achievements are in\r\nterms of accuracy and robustness against data breakdowns such as noise signals.","2047":"The potential for machine learning (ML) systems to amplify social inequities\r\nand unfairness is receiving increasing popular and academic attention. A surge\r\nof recent work has focused on the development of algorithmic tools to assess\r\nand mitigate such unfairness. If these tools are to have a positive impact on\r\nindustry practice, however, it is crucial that their design be informed by an\r\nunderstanding of real-world needs. Through 35 semi-structured interviews and an\r\nanonymous survey of 267 ML practitioners, we conduct the first systematic\r\ninvestigation of commercial product teams' challenges and needs for support in\r\ndeveloping fairer ML systems. We identify areas of alignment and disconnect\r\nbetween the challenges faced by industry practitioners and solutions proposed\r\nin the fair ML research literature. Based on these findings, we highlight\r\ndirections for future ML and HCI research that will better address industry\r\npractitioners' needs.","2048":"The Gumbel-max trick is a method to draw a sample from a categorical\r\ndistribution, given by its unnormalized (log-)probabilities. Over the past\r\nyears, the machine learning community has proposed several extensions of this\r\ntrick to facilitate, e.g., drawing multiple samples, sampling from structured\r\ndomains, or gradient estimation for error backpropagation in neural network\r\noptimization. The goal of this survey article is to present background about\r\nthe Gumbel-max trick, and to provide a structured overview of its extensions to\r\nease algorithm selection. Moreover, it presents a comprehensive outline of\r\n(machine learning) literature in which Gumbel-based algorithms have been\r\nleveraged, reviews commonly-made design choices, and sketches a future\r\nperspective.","2049":"a variety of machine learning techniques to a\n                 difficult modelling problem, the spatial distribution\n                 of an endangered Australian marsupial, the southern\n                 brown bandicoot (Isoodon obesulus). Four learning\n                 techniques decision trees\/rules, neural networks,\n                 support vector machines and genetic programming were\n                 applied to the problem. Support vector and neural\n                 network approaches gave marginally better predictivity,\n                 but in the context of low overall accuracy, decision\n                 trees and genetic programming gave more useful results\n                 because of the human comprehensibility of their\n                 models.","2050":"We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data.","2051":"This book is a practical tutorial introduction to the field of data science and machine learning, with a focus on building and deploying predictive models. The book provides a thorough overview of the Microsoft Azure Machine Learning service released for general availability on February 18th, 2015 with practical guidance for building recommenders, propensity models, and churn and predictive maintenance models.","2052":null,"2053":null,"2054":"Despite recent efforts to achieve a high level of interoperability of Machine Learning (ML) experiments, positively collaborating with the Reproducible Research context, we still run into the problems created due to the existence of different ML platforms: each of those have a specific conceptualization or schema for representing data and metadata. This scenario leads to an extra coding-effort to achieve both the desired interoperability and a better provenance level as well as a more automatized environment for obtaining the generated results. Hence, when using ML libraries, it is a common task to re-design specific data models (schemata) and develop wrappers to manage the produced outputs. In this article, we discuss this gap focusing on the solution for the question: ``What is the cleanest and lowest-impact solution to achieve both higher interoperability and provenance metadata levels in the Integrated Development Environments (IDE) context and how to facilitate the inherent data querying task?''. We introduce a novel and low impact methodology specifically designed for code built in that context, combining semantic web concepts and reflection in order to minimize the gap for exporting ML metadata in a structured manner, allowing embedded code annotations that are, in run-time, converted in one of the state-of-the-art ML schemas for the Semantic Web: the MEX Vocabulary.","2055":"Data mining techniques are used to power intelligent software, both on and off the Internet. <I>Data Mining: Practical Machine Learning Tools<\/I> explains the magic behind information extraction in a book that succeeds at bringing the latest in computer science research to any IS manager or developer. In addition, this book provides an opportunity for the authors to showcase their powerful reusable Java class library for building custom data mining software.<p> This text is remarkable with its comprehensive review of recent research on machine learning, all told in a very approachable style. (While there is plenty of math in some sections, the authors' explanations are always clear.) The book tours the nature of machine learning and how it can be used to find predictive patterns in data comprehensible to managers and developers alike. And they use sample data (for such topics as weather, contact lens prescriptions, and flowers) to illustrate key concepts. <p> After setting out to explain the types of machine learning models (like decision trees and classification rules), the book surveys algorithms used to implement them, plus strategies for improving performance and the reliability of results. Later the book turns to the authors' downloadable Weka (rhymes with \"Mecca\") Java class library, which lets you experiment with data mining hands-on and gets you started with this technology in custom applications. Final sections look at the bright prospects for data mining and machine learning on the Internet (for example, in Web search engines). <p> Precise but never pedantic, this admirably clear title delivers a real-world perspective on advantages of data mining and machine learning. Besides a programming how-to, it can be read profitably by any manager or developer who wants to see what leading-edge machine learning techniques can do for their software. <I>--Richard Dragan<\/I><p> <B>Topics covered<\/B>: Data mining and machine learning basics, sample datasets and applications for data mining, machine learning vs. statistics, the ethics of data mining, generalization, concepts, attributes, missing values, decision tables and trees, classification rules, association rules, exceptions, numeric prediction, clustering, algorithms and implementations in Java, inferring rules, statistical modeling, covering algorithms, linear models, support vector machines, instance-based learning, credibility, cross-validation, probability, costs (lift charts and ROC curves), selecting attributes, data cleansing, combining multiple models (bagging, boosting, and stacking), Weka (reusable Java classes for machine learning), customizing Weka, visualizing machine learning, working with massive datasets, text mining, and e-mail and the Internet.","2056":"Accurate forecasts of the number of newly infected people during an epidemic\r\nare critical for making effective timely decisions. This paper addresses this\r\nchallenge using the SIMLR model, which incorporates machine learning (ML) into\r\nthe epidemiological SIR model. For each region, SIMLR tracks the changes in the\r\npolicies implemented at the government level, which it uses to estimate the\r\ntime-varying parameters of an SIR model for forecasting the number of new\r\ninfections 1- to 4-weeks in advance.It also forecasts the probability of\r\nchanges in those government policies at each of these future times, which is\r\nessential for the longer-range forecasts. We applied SIMLR to data from regions\r\nin Canada and in the United States,and show that its MAPE (mean average\r\npercentage error) performance is as good as SOTA forecasting models, with the\r\nadded advantage of being an interpretable model. We expect that this approach\r\nwill be useful not only for forecasting COVID-19 infections, but also in\r\npredicting the evolution of other infectious diseases.","2057":"Jet substructure has emerged to play a central role at the Large Hadron\r\nCollider (LHC), where it has provided numerous innovative new ways to search\r\nfor new physics and to probe the Standard Model in extreme regions of phase\r\nspace. In this article we provide a comprehensive review of state of the art\r\ntheoretical and machine learning developments in jet substructure. This article\r\nis meant both as a pedagogical introduction, covering the key physical\r\nprinciples underlying the calculation of jet substructure observables, the\r\ndevelopment of new observables, and cutting edge machine learning techniques\r\nfor jet substructure, as well as a comprehensive reference for experts. We hope\r\nthat it will prove a useful introduction to the exciting and rapidly developing\r\nfield of jet substructure at the LHC.\r\n  This constitutes the theory and machine learning sections of a review on jet\r\nsubstructure at the LHC for Reviews of Modern Physics. An overview of recent\r\nexperimental progress in jet substructure will appear separately, and the\r\ncomplete review will be submitted to Reviews of Modern Physics.","2058":"Interest in the analysis of user behaviour on the Internet has been increasing rapidly, especially since the advent of electronic commerce. In this context, we argue here for the usefulness of constructing communities of users with common behaviour, making use of machine learning techniques. In particular, we assume that the users of any service on the Internet constitute a large community and we aim to construct smaller communities of users with common characteristics. The paper presents the results of three case studies for three different types of Internet service: a digital library, an information broker and a Web site. Particular attention is paid on the different types of information access involved in the three case studies: query-based information retrieval, profile-based information filtering and Web-site navigation. Each type of access imposes different constraints on the representation of the learning task. Two different unsupervised learning methods are evaluated: conceptual clustering and cluster mining. One of our main concerns is the construction of meaningful communities that can be used for improving information access on the Internet. Analysis of the results in the three case studies brings to surface some of the important properties of the task, suggesting the feasibility of a common methodology for the three different types of information access on the Internet.","2059":null,"2060":"Definition of measures from Machine Learning. Theses are especially interesting for comparison and evaluation of association rule algorithms.","2061":null,"2062":"Distributed optimization algorithms are essential for training machine\r\nlearning models on very large-scale datasets. However, they often suffer from\r\ncommunication bottlenecks. Confronting this issue, a communication-efficient\r\nprimal-dual coordinate ascent framework (CoCoA) and its improved variant CoCoA+\r\nhave been proposed, achieving a convergence rate of $O(1\/t)$ for\r\nsolving empirical risk minimization problems with Lipschitz continuous losses.\r\nIn this paper, an accelerated variant of CoCoA+ is proposed and shown to\r\npossess a convergence rate of $O(1\/t^2)$ in terms of reducing\r\nsuboptimality. The analysis of this rate is also notable in that the\r\nconvergence rate bounds involve constants that, except in extreme cases, are\r\nsignificantly reduced compared to those previously provided for CoCoA+. The\r\nresults of numerical experiments are provided to show that acceleration can\r\nlead to significant performance gains.","2063":null,"2064":null,"2065":"Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how.","2066":"The cosmic 21-cm line of hydrogen is expected to be measured in detail by the\r\nnext generation of radio telescopes. The enormous dataset from future 21-cm\r\nsurveys will revolutionize our understanding of early cosmic times. We present\r\na machine learning approach that uses emulation in order to uncover the\r\nastrophysics in the epoch of reionization and cosmic dawn. Using a\r\nseven-parameter astrophysical model that covers a very wide range of possible\r\n21-cm signals, over the redshift range $6$ to $30$ and wavenumber range $0.05 \\\r\nMpc^-1$ to $1 \\ Mpc^-1$ we emulate the 21-cm power spectrum with\r\na typical accuracy of $10 - 20\\%$. As a realistic example, we train an emulator\r\nusing the 21-cm power spectrum with an optimistic model for observational noise\r\nas expected for the Square Kilometre Array (SKA). Fitting to mock SKA data\r\nresults in a typical measurement accuracy of $5\\%$ in the optical depth to the\r\nCMB, $30\\%$ in the star-formation efficiency of galactic halos, and a factor of\r\n$3.5$ in the X-ray efficiency of galactic halos; the latter two parameters are\r\ncurrently uncertain by orders of magnitude. In addition to standard\r\nastrophysical models, we also consider two exotic possibilities of strong\r\nexcess radio backgrounds at high redshifts. We use a neural network to identify\r\nthe type of radio background present in the 21-cm power spectrum, with an\r\naccuracy of $87\\%$ for mock SKA data.","2067":"Reflective writing is an important educational practice to train reflective thinking. Currently, researchers must manually analyze these writings, limiting practice and research because the analysis is time and resource consuming. This study evaluates whether machine learning can be used to automate this manual analysis. The study investigates eight categories that are often used in models to assess reflective writing, and the evaluation is based on 76 student essays (5080 sentences) that are largely from third- and second-year health, business, and engineering students. To test the automated analysis of reflection in writings, machine learning models were built based on a random sample of 80\\% of the sentences. These models were then tested on the remaining 20\\% of the sentences. Overall, the standardized evaluation shows that five out of eight categories can be detected automatically with substantial or almost perfect reliability, while the other three categories can be detected with moderate reliability (Cohen's $\\kappa$ ranges between .53 and .85). The accuracies of the automated analysis were on average 10\\% lower than the accuracies of the manual analysis. These findings enable reflection analytics that is immediate and scalable.","2068":null,"2069":null,"2070":"This paper characterizes and investigates, from the perspective of machine learning and, particularly, classifier systems, the learning problem faced by animals and autonomous robots (here collectively termed animats). We suggest that, to survive in their environments, animats must in effect learn multiple disjunctive concepts incrementally under payoff (needs-satisfying) feedback. A review of machine learning techniques indicates that most relax at least one of these constraints. In theory, classifier systems satisfy the constraints, but tests have been limited. We show how the standard classifier system model applies to the animat learning problem. Then, in the experimental part of the paper, we specialize the model and test it in a problem environment satisfying the constraints and consisting of a difficult, disjunctive Boolean function drawn from the machine learning literature. Results include: learning the function in significantly fewer trials than a neural-network method; learning under payoff regimes that include both noisy payoff and partial reward for suboptimal performance; demonstration, in a classifier system, of a theoretically predicted property of genetic algorithms: the superiority of crossovers to point mutations; and automatic control of variation (search) rate based on system entropy. We conclude that the results support the classifier system approach to the animat problem, but suggest work aimed at the emergence of behavioral hierarchies of classifiers to offset slower learning rates in larger problems.","2071":null,"2072":"A chemical-genetic interaction matrix (CGM) of compounds against genotypes A systematic chemical interaction matrix between genotype-specific inhibitors Machine learning models of structural features and CGM interactions that predict synergism Synergistic combinations that exhibit species-selective effects against pathogenic fungi The structure of genetic interaction networks predicts that, analogous to synthetic lethal interactions between non-essential genes, combinations of compounds with latent activities may exhibit potent synergism. To test this hypothesis, we generated a chemical-genetic matrix of 195 diverse yeast deletion strains treated with 4,915 compounds. This approach uncovered 1,221 genotype-specific inhibitors, which we termed cryptagens. Synergism between 8,128 structurally disparate cryptagen pairs was assessed experimentally and used to benchmark predictive algorithms. A model based on the chemical-genetic matrix and the genetic interaction network failed to accurately predict synergism. However, a combined random forest and Naive Bayesian learner that associated chemical structural features with genotype-specific growth inhibition had strong predictive power. This approach identified previously unknown compound combinations that exhibited species-selective toxicity toward human fungal pathogens. This work demonstrates that machine learning methods trained on unbiased chemical-genetic interaction data may be widely applicable for the discovery of synergistic combinations in different species.","2073":"In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn, et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem where we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT\u201916 by seeing only 16,000 translated words (~600 parallel sentences)","2074":null,"2075":"Deep learning models with convolutional and recurrent networks are now\r\nubiquitous and analyze massive amounts of audio, image, video, text and graph\r\ndata, with applications in automatic translation, speech-to-text, scene\r\nunderstanding, ranking user preferences, ad placement, etc. Competing\r\nframeworks for building these networks such as TensorFlow, Chainer, CNTK,\r\nTorch\/PyTorch, Caffe1\/2, MXNet and Theano, explore different tradeoffs between\r\nusability and expressiveness, research or production orientation and supported\r\nhardware. They operate on a DAG of computational operators, wrapping\r\nhigh-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for\r\nvarious CPUs), and automate memory allocation, synchronization, distribution.\r\nCustom operators are needed where the computation does not fit existing\r\nhigh-performance library calls, usually at a high engineering cost. This is\r\nfrequently required when new operators are invented by researchers: such\r\noperators suffer a severe performance penalty, which limits the pace of\r\ninnovation. Furthermore, even if there is an existing runtime call these\r\nframeworks can use, it often doesn't offer optimal performance for a user's\r\nparticular network architecture and dataset, missing optimizations between\r\noperators as well as optimizations that can be done knowing the size and shape\r\nof data. Our contributions include (1) a language close to the mathematics of\r\ndeep learning called Tensor Comprehensions offering both imperative and\r\ndeclarative styles, (2) a polyhedral Just-In-Time compiler to convert a\r\nmathematical description of a deep learning DAG into a CUDA kernel with\r\ndelegated memory management and synchronization, also providing optimizations\r\nsuch as operator fusion and specialization for specific sizes, (3) a\r\ncompilation cache populated by an autotuner. Abstract cutoff","2076":null,"2077":"Machine learning has been a fast growing field of research in several areas\ndealing with large datasets. We report recent attempts to use Renormalization\nGroup (RG) ideas in the context of machine learning. We examine coarse graining\nprocedures for perceptron models designed to identify the digits of the MNIST\ndata. We discuss the correspondence between principal components analysis (PCA)\nand RG flows across the transition for worm configurations of the 2D Ising\nmodel. Preliminary results regarding the logarithmic divergence of the leading\nPCA eigenvalue were presented at the conference and have been improved after.\nMore generally, we discuss the relationship between PCA and observables in\nMonte Carlo simulations and the possibility of reduction of the number of\nlearning parameters in supervised learning based on RG inspired hierarchical\nansatzes.","2078":null,"2079":"This paper investigates how intersectional critical theoretical concepts from social sciences and humanities research can be worked with in machine learning systems design. It does so by presenting a case study of a series of speculative design workshops, conducted in 2021. These workshops drew on intersectional feminist methodologies to construct interdisciplinary interventions in the design of machine learning systems, towards more inclusive, accountable, and contextualized systems design. The concepts of \u201csituating\/situated knowledges\u201d, \"figuration\", \"diffraction\", and \u201ccritical fabulation\/speculation\u201d were taken up as theoretical and methodological tools for concept-led design workshops. This paper presents the design framework of the workshops and highlights tensions and possibilities with regards to interdisciplinary machine learning systems design towards more inclusive, contextualized, and accountable systems. It discusses the role that critical theoretical concepts can play in a design process and shows how such concepts can work as methodological tools that nonetheless require an open-ended experimental space to function. It presents insights and discussion points regarding what it means to work with critical intersectional knowledge that is inextricably connected to its historical and socio-political roots, and how this reframes what it might mean to design fair and accountable systems.","2080":"When Newell introduced the concept of the knowledge level as a\nuseful level of description for computer systems, he focused on\nthe representation of knowledge. This paper applies the knowledge\nlevel notion to the problem of knowledge acquisition. Two\ninteresting issues arise. First, some existing machine learning\nprograms appear to be completely static when viewed at the\nknowledge level. These programs improve their performance without\nchanging their ``knowledge\". Second, the behaviour of some other\nmachine learning programs cannot be predicted or described at the\nknowledge level. These programs take unjustified inductive leaps.\nThe first programs are called symbol level learning (SLL)\nprograms; the second, non-deductive knowledge level learning\n(NKLL) programs. The paper analyzes both of these classes of\nlearning programs and speculates on the possibility of developing\ncoherent theories of each. A theory of symbol level learning is\nsketched, and some reasons are presented for believing that a\ntheory of NKLL will be difficult to obtain.","2081":"Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g., multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g., resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.","2082":null,"2083":null,"2084":null,"2085":null,"2086":null,"2087":null,"2088":null,"2089":null,"2090":null,"2091":"In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.","2092":null,"2093":null,"2094":"A central problem in machine learning is identifying a representative set of features from\r\nwhich to construct a classification model for a particular task. This thesis addresses the\r\nproblem of feature selection for machine learning through a correlation based approach.\r\nThe central hypothesis is that good feature sets contain features that are highly correlated\r\nwith the class, yet uncorrelated with each other. A feature evaluation formula, based\r\non ideas from test theory, provides an operational definition of this hypothesis. CFS\r\n(Correlation based Feature Selection) is an algorithm that couples this evaluation formula\r\nwith an appropriate correlation measure and a heuristic search strategy.\r\nCFS was evaluated by experiments on artificial and natural datasets. Three machine learn-\r\ning algorithms were used: C4.5 (a decision tree learner), IB1 (an instance based learner),\r\nand naive Bayes. Experiments on artificial datasets showed that CFS quickly identifies\r\nand screens irrelevant, redundant, and noisy features, and identifies relevant features as\r\nlong as their relevance does not strongly depend on other features. On natural domains,\r\nCFS typically eliminated well over half the features. In most cases, classification accuracy\r\nusing the reduced feature set equaled or bettered accuracy using the complete feature set.\r\nFeature selection degraded machine learning performance in cases where some features\r\nwere eliminated which were highly predictive of very small areas of the instance space.\r\nFurther experiments compared CFS with a wrapper\u2014a well known approach to feature\r\nselection that employs the target learning algorithm to evaluate feature sets. In many cases\r\nCFS gave comparable results to the wrapper, and in general, outperformed the wrapper\r\non small datasets. CFS executes many times faster than the wrapper, which allows it to\r\nscale to larger datasets.\r\nTwo methods of extending CFS to handle feature interaction are presented and exper-\r\nimentally evaluated. The first considers pairs of features and the second incorporates\r\nfeature weights calculated by the RELIEF algorithm. Experiments on artificial domains\r\nshowed that both methods were able to identify interacting features. On natural domains,\r\nthe pairwise method gave more reliable results than using weights provided by RELIEF.","2095":"This book is a thorough introduction to the most important topics in data mining and machine learning. It begins with a detailed review of classical function estimation and proceeds with chapters on nonlinear regression, classification, and ensemble methods. The final chapters focus on clustering, dimension reduction, variable selection, and multiple comparisons. All these topics have undergone extraordinarily rapid development in recent years and this treatment offers a modern perspective emphasizing the most recent contributions. The presentation of foundational results is detailed and includes many accessible proofs not readily available outside original sources. While the orientation is conceptual and theoretical, the main points are regularly reinforced by computational comparisons. Intended primarily as a graduate level textbook for statistics, computer science, and electrical engineering students, this book assumes only a strong foundation in undergraduate statistics and mathematics, and facility with using R packages. The text has a wide variety of problems, many of an exploratory nature. There are numerous computed examples, complete with code, so that further computations can be carried out readily. The book also serves as a handbook for researchers who want a conceptual overview of the central topics in data mining and machine learning. Bertrand Clarke is a Professor of Statistics in the Department of Medicine, Department of Epidemiology and Public Health, and the Center for Computational Sciences at the University of Miami. He has been on the Editorial Board of the Journal of the American Statistical Association, the Journal of Statistical Planning and Inference, and Statistical Papers. He is co-winner, with Andrew Barron, of the 1990 Browder J. Thompson Prize from the Institute of Electrical and Electronic Engineers. Ernest Fokoue is an Assistant Professor of Statistics at Kettering University. He has also taught at Ohio State University and been a long term visitor at the Statistical and Mathematical Sciences Institute where he was a Post-doctoral Research Fellow in the Data Mining and Machine Learning Program. In 2000, he was the winner of the Young Researcher Award from the International Association for Statistical Computing. Hao Helen Zhang is an Associate Professor of Statistics in the Department of Statistics at North Carolina State University. For 2003-2004, she was a Research Fellow at SAMSI and in 2007, she won a Faculty Early Career Development Award from the National Science Foundation. She is on the Editorial Board of the Journal of the American Statistical Association and Biometrics.","2096":"We reduce measurement errors in a quantum computer using machine learning\r\ntechniques. We exploit a simple yet versatile neural network to classify\r\nmulti-qubit quantum states, which is trained using experimental data. This\r\nflexible approach allows the incorporation of any number of features of the\r\ndata with minimal modifications to the underlying network architecture. We\r\nexperimentally illustrate this approach in the readout of trapped-ion qubits\r\nusing additional spatial and temporal features in the data. Using this neural\r\nnetwork classifier, we efficiently treat qubit readout crosstalk, resulting in\r\na 30\\% improvement in detection error over the conventional threshold method.\r\nOur approach does not depend on the specific details of the system and can be\r\nreadily generalized to other quantum computing platforms.","2097":null,"2098":"In recent years many works have shown that unsupervised Machine Learning (ML)\r\ncan help detect unusual objects and uncover trends in large astronomical\r\ndatasets, but a few challenges remain. We show here, for example, that\r\ndifferent methods, or even small variations of the same method, can produce\r\nsignificantly different outcomes. While intuitively somewhat surprising, this\r\ncan naturally occur when applying unsupervised ML to highly dimensional data,\r\nwhere there can be many reasonable yet different answers to the same question.\r\nIn such a case the outcome of any single unsupervised ML method should be\r\nconsidered a sample from a conceivably wide range of possibilities. We\r\ntherefore suggest an approach that eschews finding an optimal outcome, instead\r\nfacilitating the production and examination of many valid ones. This can be\r\nachieved by incorporating unsupervised ML into data visualisation portals. We\r\npresent here such a portal that we are developing, applied to the sample of\r\nSDSS spectra of galaxies. The main feature of the portal is interactive 2D maps\r\nof the data. Different maps are constructed by applying dimensionality\r\nreduction to different subspaces of the data, so that each map contains\r\ndifferent information that in turn gives a different perspective on the data.\r\nThe interactive maps are intuitive to use, and we demonstrate how peculiar\r\nobjects and trends can be detected by means of a few button clicks. We believe\r\nthat including tools in this spirit in next generation astronomical surveys\r\nwill be important for making unexpected discoveries, either by professional\r\nastronomers or by citizen scientists, and will generally enable the benefits of\r\nvisual inspection even when dealing with very complex and extensive datasets.\r\nOur portal is available online at galaxyportal.space.","2099":null,"2100":"Simulation of a geostratigraphic unit is of vital importance for the study of geoinformatics, as well as geoengineering planning and design. A traditional method depends on the guidance of expert experience, which is subjective and limited, thereby making the effective evaluation of a stratum simulation quite impossible. To solve this problem, this study proposes a machine learning method for a geostratigraphic series simulation. On the basis of a recurrent neural network, a sequence model of the stratum type and a sequence model of the stratum thickness is successively established. The performance of the model is improved in combination with expert-driven learning. Finally, a machine learning model is established for a geostratigraphic series simulation, and a three-dimensional (3D) geological modeling evaluation method is proposed which considers the stratum type and thickness. The results show that we can use machine learning in the simulation of a series. The series model based on machine learning can describe the real situation at wells, and it is a complimentary tool to the traditional 3D geological model. The prediction ability of the model is improved to a certain extent by including expert-driven learning. This study provides a novel approach for the simulation and prediction of a series by 3D geological modeling.","2101":"While there is currently a lot of enthusiasm about \"big data\", useful data is\r\nusually \"small\" and expensive to acquire. In this paper, we present a new\r\nparadigm of learning partial differential equations from small data. In\r\nparticular, we introduce hidden physics models, which are essentially\r\ndata-efficient learning machines capable of leveraging the underlying laws of\r\nphysics, expressed by time dependent and nonlinear partial differential\r\nequations, to extract patterns from high-dimensional data generated from\r\nexperiments. The proposed methodology may be applied to the problem of\r\nlearning, system identification, or data-driven discovery of partial\r\ndifferential equations. Our framework relies on Gaussian processes, a powerful\r\ntool for probabilistic inference over functions, that enables us to strike a\r\nbalance between model complexity and data fitting. The effectiveness of the\r\nproposed approach is demonstrated through a variety of canonical problems,\r\nspanning a number of scientific domains, including the Navier-Stokes,\r\nSchr\u00f6dinger, Kuramoto-Sivashinsky, and time dependent linear fractional\r\nequations. The methodology provides a promising new direction for harnessing\r\nthe long-standing developments of classical methods in applied mathematics and\r\nmathematical physics to design learning machines with the ability to operate in\r\ncomplex domains without requiring large quantities of data.","2102":null,"2103":"Federated machine learning frameworks, which take into account confidentiality of distributed data sources are of increasing interest in smart manufacturing. However, the scope of applicability of most such frameworks is restricted in industrial settings due to limitations in the assumptions on the data sources involved. In this work, first, we shed light on the nature of this arising gap between current federated learning and requirements in industrial settings. Our discussion aims at clarifying related notions in emerging sub-disciplines of machine learning, which are partially overlapping. Second, we envision a new confidentiality-preserving approach for smart manufacturing applications based on the more general setting of transfer learning, and envision its implementation in a module-based platform.","2104":"Most of the existing classification techniques concentrate on learning the datasets as a single similar unit, in spite of so many differentiating attributes and complexities involved. However, traditional classification techniques, require to analysis the dataset prior to learning and for not doing so they loss their performance in terms of accuracy and AUC. To this end, many of the machine learning problems can be very easily solved just by careful observing human learning and training nature and then mimic the same in the machine learning. \nThis paper presents an updated literature survey of current and novel machine learning strategies inducing models efficiently for supervised and unsupervised learning in data mining.","2105":null,"2106":"Although a large body of work is devoted to finding communities in static social networks, only a few studies examined the dynamics of communities in evolving social networks. In this paper, we propose a dynamic stochastic block model for finding communities and their evolution in a dynamic social network. The proposed model captures the evolution of communities by explicitly modeling the transition of community memberships for individual nodes in the network. Unlike many existing approaches for modeling social networks that estimate parameters by their most likely values (i.e., point estimation), in this study, we employ a Bayesian treatment for parameter estimation that computes the posterior distributions for all the unknown parameters. This Bayesian treatment allows us to capture the uncertainty in parameter values and therefore is more robust to data noise than point estimation. In addition, an efficient algorithm is developed for Bayesian inference to handle large sparse social networks. Extensive experimental studies based on both synthetic data and real-life data demonstrate that our model achieves higher accuracy and reveals more insights in the data than several state-of-the-art algorithms.","2107":"In the last few decades a computational approach to\n                 machine learning has emerged based on paradigms from\n                 recursion theory and the theory of computation. Such\n                 ideas include learning in the limit, learning by\n                 enumeration, and probably approximately correct (pac)\n                 learning. These models usually are not suitable in\n                 practical situations. In contrast, statistics based\n                 inference methods have enjoyed a long and distinguished\n                 career. Currently, Bayesian reasoning in various forms,\n                 minimum message length (MML) and minimum description\n                 length (MDL), are widely applied approaches. They are\n                 the tools to use with particular machine learning\n                 praxis such as simulated annealing, genetic algorithms,\n                 genetic programming, artificial neural networks, and\n                 the like. These statistical inference methods select\n                 the hypothesis which minimizes the sum of the length of\n                 the description of the hypothesis (also called `model')\n                 and the length of the description of the data relative\n                 to the hypothesis. It app...","2108":null,"2109":"To make machine-learning analyses in the life sciences more computationally reproducible, we propose standards based on data, model and code publication, programming best practices and workflow automation. By meeting these standards, the community of researchers applying machine-learning methods in the life sciences can ensure that their analyses are worthy of trust.","2110":"In this investigation, we have developed a graphical user interface application to perform the diagnostic of pathology on the column vertebral based on the Cluster K-Nearest Neighbor (CKNN) classifier. The system is implemented and simulated in Anaconda, and its performance is tested on real dataset that contains 6 features and two (02) classes.  Each class, abnormal and normal class consists of 210 instances, and 100 instances, respectively. A comparison of the performance of the test measurement under various test sizes (10%~50%) is carried out to predict the class label when the nearest neighbor k changes from 1 to 19. The results show that the accuracy depends on both independent parameters, the test size and k-neighbors, which gives better training accuracy than the test accuracy, in the range of 82.5% ~ 100% and 70%~84%, respectively. When k varies from 1 to 4, a higher training accuracy, larger than 90% is observed. While the test set shows a low accuracy in the range of 74% ~ 82.5%. Increasing the test size or\/and k, does not affect significantly the accuracy.  When k is larger 1, the training accuracy is approximately equal to 0.925\u00b10.05, the test accuracy (except for k=6 and 17) is about 0.79\u00b10.05. The prediction of the class status maybe optimized by combining the dataset set size with the k-neighbors parameters. The GUI can be useful to help the medical doctors to diagnostic the patient effectively to take a rapid decision and predict results in a reduced time lapse.","2111":null,"2112":null,"2113":null,"2114":null,"2115":"The arms race between attacks and defenses for machine learning models has\r\ncome to a forefront in recent years, in both the security community and the\r\nprivacy community. However, one big limitation of previous research is that the\r\nsecurity domain and the privacy domain have typically been considered\r\nseparately. It is thus unclear whether the defense methods in one domain will\r\nhave any unexpected impact on the other domain.\r\n  In this paper, we take a step towards resolving this limitation by combining\r\nthe two domains. In particular, we measure the success of membership inference\r\nattacks against six state-of-the-art defense methods that mitigate the risk of\r\nadversarial examples (i.e., evasion attacks). Membership inference attacks\r\ndetermine whether or not an individual data record has been part of a model's\r\ntraining set. The accuracy of such attacks reflects the information leakage of\r\ntraining algorithms about individual members of the training set. Adversarial\r\ndefense methods against adversarial examples influence the model's decision\r\nboundaries such that model predictions remain unchanged for a small area around\r\neach input. However, this objective is optimized on training data. Thus,\r\nindividual data records in the training set have a significant influence on\r\nrobust models. This makes the models more vulnerable to inference attacks.\r\n  To perform the membership inference attacks, we leverage the existing\r\ninference methods that exploit model predictions. We also propose two new\r\ninference methods that exploit structural properties of robust models on\r\nadversarially perturbed data. Our experimental evaluation demonstrates that\r\ncompared with the natural training (undefended) approach, adversarial defense\r\nmethods can indeed increase the target model's risk against membership\r\ninference attacks.","2116":"Machine learning methods, a family of statistical techniques with origins in the field of artificial intelligence, are recognized as holding great promise for the advancement of understanding and prediction about ecological phenomena. These modeling techniques are flexible enough to handle complex problems with multiple interacting elements and typically outcompete traditional approaches (e.g., generalized linear models), making them ideal for modeling ecological systems. Despite their inherent advantages, a review of the literature reveals only a modest use of these approaches in ecology as compared to other disciplines. One potential explanation for this lack of interest is that machine learning techniques do not fall neatly into the class of statistical modeling approaches with which most ecologists are familiar. In this paper, we provide an introduction to three machine learning approaches that can be broadly used by ecologists: classification and regression trees, artificial neural networks, and evolutionary computation. For each approach, we provide a brief background to the methodology, give examples of its application in ecology, describe model development and implementation, discuss strengths and weaknesses, explore the availability of statistical software, and provide an illustrative example. Although the ecological application of machine learning approaches has increased, there remains considerable skepticism with respect to the role of these techniques in ecology. Our review encourages a greater understanding of machin learning approaches and promotes their future application and utilization, while also providing a basis from which ecologists can make informed decisions about whether to select or avoid these approaches in their future modeling endeavors.","2117":null,"2118":null,"2119":null,"2120":"Synaptic vesicles (SVs) are a key component of neuronal signaling and fulfil different roles depending on their composition. In electron micrograms of neurites, two types of vesicles can be distinguished by morphological criteria, the classical \u201cclear core\u201d vesicles (CCV) and the typically larger \u201cdense core\u201d vesicles (DCV), with differences in electron density due to their diverse cargos. Compared to CCVs, the precise function of DCVs is less defined. DCVs are known to store neuropeptides, which function as neuronal messengers and modulators 1. In C. elegans, they play a role in locomotion, dauer formation, egg-laying, and mechano- and chemosensation 2. Another type of DCVs, also referred to as granulated vesicles, are known to transport Bassoon, Piccolo and further constituents of the presynaptic density in the center of the active zone (AZ), and therefore are important for synaptogenesis 3. To better understand the role of different types of SVs, we present here a new automated approach to classify vesicles. We combine machine learning with an extension of our previously developed vesicle segmentation workflow, the ImageJ macro 3D ART VeSElecT. With that we reliably distinguish CCVs and DCVs in electron tomograms of C. elegans NMJs using image-based features. Analysis of the underlying ground truth data shows an increased fraction of DCVs as well as a higher mean distance between DCVs and AZs in dauer larvae compared to young adult hermaphrodites. Our machine learning based tools are adaptable and can be applied to study properties of different synaptic vesicle pools in electron tomograms of diverse model organisms.","2121":null,"2122":"MapReduce is emerging as a generic parallel programming paradigm for large clusters of machines. This trend combined with the growing need to run machine learning (ML) algorithms on massive datasets has led to an increased interest in implementing ML algorithms on MapReduce. However, the cost of implementing a large class of ML algorithms as low-level MapReduce jobs on varying data and machine cluster sizes can be prohibitive. In this paper, we propose SystemML in which ML algorithms are expressed in a higher-level language and are compiled and executed in a MapReduce environment. This higher-level language exposes several constructs including linear algebra primitives that constitute key building blocks for a broad class of supervised and unsupervised ML algorithms. The algorithms expressed in SystemML are compiled and optimized into a set of MapReduce jobs that can run on a cluster of machines. We describe and empirically evaluate a number of optimization strategies for efficiently executing these algorithms on Hadoop, an open-source MapReduce implementation. We report an extensive performance evaluation on three ML algorithms on varying data and cluster sizes.","2123":"Abstract\n            Research in computer analysis of medical images bears many promises to improve patients\u2019 health. However, a number of systematic challenges are slowing down the progress of the field, from limitations of the data, such as biases, to research incentives, such as optimizing for publication. In this paper we review roadblocks to developing and assessing methods. Building our analysis on evidence from the literature and data challenges, we show that at every step, potential biases can creep in. On a positive note, we also discuss on-going efforts to counteract these problems. Finally we provide recommendations on how to further address these problems in the future.","2124":"In this paper, we present a web-based, machine-learning enhanced news reader (PersoNews). The main advantages of PersoNews are the aggregation of many different news sources, machine learning filtering offering personalization not only per user but also for every feed a user is subscribed to, and finally the ability for every user to watch a more abstracted topic of interest by employing a simple form of semantic filtering through a taxonomy of topics.","2125":"We describe a novel technique for evolving a machine\n                 that can learn. The machine is evolved using a Genetic\n                 Programming (GP) algorithm that incorporates in its\n                 function set what we have called a learning node. Such\n                 a node is tuned by a second optimisation algorithm (in\n                 this case Simulated Annealing), mimicking a natural\n                 learning process and providing the GP tree with added\n                 flexibility and adaptability. The result of the\n                 evolution is a system with a fixed structure but with\n                 some variable parameters. The system can then learn new\n                 tasks in new environments without undergoing further\n                 evolution.","2126":null,"2127":null,"2128":null,"2129":"Machine learning (ML) expands traditional data analysis and presents a range of opportunities in ecosystem service (ES) research, offering rapid processing of \u2018big data\u2019 and enabling significant advances in data description and predictive modelling. Descriptive ML techniques group data with little or no prior domain specific assumptions; they can generate hypotheses and automatically sort data prior to other analyses. Predictive ML techniques allow for the predictive modelling of highly non-linear systems where casual mechanisms are poorly understood, as is often the case for ES. We conducted a review to explore how ML is used in ES research and to identify and quantify trends in the different ML approaches that are used. We reviewed 308 peer-reviewed publications and identified that ES studies implemented machine learning techniques in data description (64\\%; n = 308) and predictive modelling (44\\%), with some papers containing both categories. Classification and Regression Trees were the most popular techniques (60\\%), but unsupervised learning techniques were also used for descriptive tasks such as clustering to group or split data without prior assumptions (19\\%). Whilst there are examples of ES publications that apply ML with rigour, many studies do not have robust or repeatable methods. Some studies fail to report model settings (43\\%) or software used (28\\%), and many studies do not report carrying out any form of model hyperparameter tuning (67\\%) or test model generalisability (59\\%). Whilst studies use ML to analyse very large and complex datasets, ES research is generally not taking full advantage of the capacity of ML to model big data (1138 medium number of data points; 13 median quantity of variables). There is great further opportunity to utilise ML in ES research, to make better use of big data and to develop detailed modelling of spatial-temporal dynamics that meet stakeholder demands.","2130":null,"2131":"The stock market is the primary entity driving every major economy across the globe, with each investment designed to capitalize profit while decreasing its associated risks. As a result of the stock market\u2019s importance, there have been enumerable studies conducted with the goal of predicting the stock market through data analysis techniques including machine learning, neural networks, and time series analysis. This paper uses machine learning algorithms to perform stock market indices classification using fundamental data, while classifying the indices using technical indicators, with data derived from Yahoo Finance on the top 100 indices in the NASDAQ stock market from January 2000 to December 2020.","2132":"This paper describes an application of machine learning to student modelling. Unlike previous machine learning approaches to student modelling, the new approach is based on attribute-value machine learning. In contrast to many previous approaches it is not necessary for the lesson author to identify all forms of error that may be detected or to identify the possible approaches to problem solving in the domain that may be adopted. Rather, the lesson author need only identify the relevant attributes both of the tasks to be performed by the student and of the student's actions. The values of these attributes are automatically processed by the student modeler to produce the student model.","2133":null,"2134":"We propose using neural networks to detect data departures from a given\r\nreference model, with no prior bias on the nature of the new physics\r\nresponsible for the discrepancy. The virtues of neural networks as unbiased\r\nfunction approximants make them particularly suited for this task. An algorithm\r\nthat implements this idea is constructed, as a straightforward application of\r\nthe likelihood-ratio hypothesis test. The algorithm compares observations with\r\nan auxiliary set of reference-distributed events, possibly obtained with a\r\nMonte Carlo event generator. It returns a p-value, which measures the\r\ncompatibility of the reference model with the data. It also identifies the most\r\ndiscrepant phase-space region of the data set, to be selected for further\r\ninvestigation. The most interesting potential applications are\r\nmodel-independent new physics searches, although our approach could also be\r\nused to compare the theoretical predictions of different Monte Carlo event\r\ngenerators, or for data validation algorithms. In this work we study the\r\nperformance of our algorithm on a few simple examples. The results confirm the\r\nmodel-independence of the approach, namely that it displays good sensitivity to\r\na variety of putative signals. Furthermore, we show that the reach does not\r\ndepend much on whether a favorable signal region is selected based on prior\r\nexpectations. We identify directions for improvement towards applications to\r\nreal experimental data sets.","2135":"Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 82.2% to 87.8% and from 88% accuracy to 95% accuracy on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.","2136":null,"2137":"Ransomware is a known threat that had a severe impact on computer security in the past five years. This type of malware has caused financial losses of about $13 Billion in 2017 and 2018 combined. Ransomware makes a user\u2019s data unavailable to them, only granting access again when they pay a ransom. Traditionally, ransomware targeted the computer\u2019s filesystem. Database ransomware is a new variant of the same principle. Instead of targeting individual files, it logs into DBMSs remotely and destroys the data, leaving only a ransom message behind. In most cases, attackers do not create a backup copy of the data. In this case, the data cannot be restored by the attackers, even if the ransom is paid. In 2018, Jobst et al. presented DIMAQS, a MySQL plugin to mitigate these attacks by detecting malicious activity through a Petri net classifier. Our work recognizes the main drawback of this approach: The Petri net cannot be easily adapted to new attack scenarios and has to be re-engineered manually. To solve this problem, we design a machine learning classifier to replace the original one. This approach yields a model that detects all attacks in our tests. Unfortunately, the model also produces a high number of false positives when trying to detect attacks before any harmful queries are issued. Overall, our approach achieves a 85.23% f1-score. The performance impact of the revised plugin is nonexistent for OLAP workloads and stays under 15% for OLTP tasks.","2138":"The implementation of a vast majority of machine learning (ML) algorithms\r\nboils down to solving a numerical optimization problem. In this context,\r\nStochastic Gradient Descent (SGD) methods have long proven to provide good\r\nresults, both in terms of convergence and accuracy. Recently, several\r\nparallelization approaches have been proposed in order to scale SGD to solve\r\nvery large ML problems. At their core, most of these approaches are following a\r\nmap-reduce scheme. This paper presents a novel parallel updating algorithm for\r\nSGD, which utilizes the asynchronous single-sided communication paradigm.\r\nCompared to existing methods, Asynchronous Parallel Stochastic Gradient Descent\r\n(ASGD) provides faster (or at least equal) convergence, close to linear scaling\r\nand stable accuracy.","2139":"Accurate and effective diagnosis of actual injury severity can be problematic in trauma patients. Inherent physiologic compensatory mechanisms may prevent accurate diagnosis and mask true severity in many circumstances. The objective of this project was the development and validation of a multiparameter machine learning algorithm and system capable of predicting the need for life-saving interventions (LSIs) in trauma patients. Statistics based on means, slopes, and maxima of various vital sign measurements corresponding to 79 trauma patient records generated over 110,000 feature sets, which were used to develop, train, and implement the system. Comparisons among several machine learning models proved that a multilayer perceptron would best implement the algorithm in a hybrid system consisting of a machine learning component and basic detection rules. Additionally, 295,994 feature sets from 82\u00a0h of trauma patient data showed that the system can obtain 89.8\u00a0\\% accuracy within 5\u00a0min of recorded LSIs. Use of machine learning technologies combined with basic detection rules provides a potential approach for accurately assessing the need for LSIs in trauma patients. The performance of this system demonstrates that machine learning technology can be implemented in a real-time fashion and potentially used in a critical care environment.","2140":null,"2141":null,"2142":"Abstract Topological phases of matter are conventionally characterized by the bulk-boundary correspondence in Hermitian systems. The topological invariant of the bulk in d dimensions corresponds to the number of (d \u2212 1)-dimensional boundary states. By extension, higher-order topological insulators reveal a bulk-edge-corner correspondence, such that nth order topological phases feature (d \u2212 n)-dimensional boundary states. The advent of non-Hermitian topological systems sheds new light on the emergence of the non-Hermitian skin effect (NHSE) with an extensive number of boundary modes under open boundary conditions. Still, the higher-order NHSE remains largely unexplored, particularly in the experiment. An unsupervised approach\u2014physics-graph-informed machine learning (PGIML)\u2014to enhance the data mining ability of machine learning with limited domain knowledge is introduced. Through PGIML, the second-order NHSE in a 2D non-Hermitian topoelectrical circuit is experimentally demonstrated. The admittance spectra of the circuit exhibit an extensive number of corner skin modes and extreme sensitivity of the spectral flow to the boundary conditions. The violation of the conventional bulk-boundary correspondence in the second-order NHSE implies that modification of the topological band theory is inevitable in higher dimensional non-Hermitian\u00a0systems.","2143":"We present a new method based on information theory to find the optimal\r\nnumber of bands required to measure the physical properties of galaxies with a\r\ndesired accuracy. As a proof of concept, using the recently updated COSMOS\r\ncatalog (COSMOS2020), we identify the most relevant wavebands for measuring the\r\nphysical properties of galaxies in a Hawaii Two-0 (H20)- and UVISTA-like survey\r\nfor a sample of $i<25$ AB mag galaxies. We find that with available $i$-band\r\nfluxes, $r$, $u$, IRAC\/$ch2$ and $z$ bands provide most of the information\r\nregarding the redshift with importance decreasing from $r$-band to $z$-band. We\r\nalso find that for the same sample, IRAC\/$ch2$, $Y$, $r$ and $u$ bands are the\r\nmost relevant bands in stellar mass measurements with decreasing order of\r\nimportance. Investigating the inter-correlation between the bands, we train a\r\nmodel to predict UVISTA observations in near-IR from H20-like observations. We\r\nfind that magnitudes in $YJH$ bands can be simulated\/predicted with an accuracy\r\nof $1\\sigma$ mag scatter $0.2$ for galaxies brighter than 24 AB mag in\r\nnear-IR bands. One should note that these conclusions depend on the selection\r\ncriteria of the sample. For any new sample of galaxies with a different\r\nselection, these results should be remeasured. Our results suggest that in the\r\npresence of a limited number of bands, a machine learning model trained over\r\nthe population of observed galaxies with extensive spectral coverage\r\noutperforms template-fitting. Such a machine learning model maximally comprises\r\nthe information acquired over available extensive surveys and breaks\r\ndegeneracies in the parameter space of template-fitting inevitable in the\r\npresence of a few bands.","2144":null,"2145":"We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists' trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies - qualitative interviews, a controlled experiment, and a card-sorting task - to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool.","2146":null,"2147":"Investigation of physical well-motivated parameter space in the theories of\nBeyond the Standard Model (BSM) plays an important role in new physics\ndiscoveries. However, a large-scale scan of high dimensional (HD) parameter\nspace under vast experimental constraints is typically a time-consuming and\nexpensive task. In this Letter, we propose a novel self-learning scan strategy,\nnamed Machine Learning Scan (MLS), to achieve a fast and reliable analysis of\nHD parameter space by using machine learning models to evaluate the quality of\nrandom parameter sets. As a proof-of-concept, we apply MLS to find the light\nHiggs and light neutralino dark matter scenario in pMSSM and find that such a\nmethod can significantly reduce the computational cost (two orders faster than\nthe standard random scan) and ensure a full coverage of survived regions.","2148":null,"2149":null,"2150":null,"2151":null,"2152":null,"2153":null,"2154":"We review the current state of data mining and machine learning in astronomy.\n'Data Mining' can have a somewhat mixed connotation from the point of view of a\nresearcher in this field. If used correctly, it can be a powerful approach,\nholding the potential to fully exploit the exponentially increasing amount of\navailable data, promising great scientific advance. However, if misused, it can\nbe little more than the black-box application of complex computing algorithms\nthat may give little physical insight, and provide questionable results. Here,\nwe give an overview of the entire data mining process, from data collection\nthrough to the interpretation of results. We cover common machine learning\nalgorithms, such as artificial neural networks and support vector machines,\napplications from a broad range of astronomy, emphasizing those where data\nmining techniques directly resulted in improved science, and important current\nand future directions, including probability density functions, parallel\nalgorithms, petascale computing, and the time domain. We conclude that, so long\nas one carefully selects an appropriate algorithm, and is guided by the\nastronomical problem at hand, data mining can be very much the powerful tool,\nand not the questionable black box.","2155":"Web-based social systems enable new community-based opportunities for participants to engage, share, and interact. This community value and related services like search and advertising are threatened by spammers, content polluters, and malware disseminators. In an effort to preserve community value and ensure longterm success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deployment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives.","2156":null,"2157":null,"2158":null,"2159":null,"2160":null,"2161":null,"2162":null,"2163":null,"2164":null,"2165":null,"2166":null,"2167":"The number of smartphones connected to wireless networks and the volume of wireless network traffic generated by such devices have dramatically increased in the last few years, making it more challenging to tackle wireless network monitoring applications. The high-dimensionality of network data provided by current smartphone devices opens the door to the massive application of machine learning approaches to improve different wireless networking applications. In this paper we study the specific problem of Quality of Experience (QoE) prediction for popular smartphone apps, using machine learning models and in-smartphone measurements. We evaluate and compare different models for the analysis of smartphone generated data, including single models as well as machine learning ensembles such as bagging, boosting and stacking. Results suggest that, while decision-tree based models are the most accurate single models to predict QoE, ensemble learning models, and in particular stacking ones, are capable to significantly increase accuracy prediction and overall classification performance.","2168":null,"2169":null,"2170":null,"2171":null,"2172":null,"2173":null,"2174":null,"2175":null,"2176":"This study presents a novel application of machine learning to deliver optimised, multi-model combinations (MMCs) of Global Hydrological Model (GHM) simulations. We exemplify the approach using runoff simulations from five GHMs across 40 large global catchments. The benchmarked, median performance gain of the MMC solutions is 45% compared to the best performing GHM and exceeds 100% when compared to the ensemble mean (EM). The performance gain offered by MMC suggests that future multi-model applications consider reporting MMCs, alongside the EM and intermodal range, to provide end-users of GHM ensembles with a better contextualised estimate of runoff. Importantly, the study highlights the difficulty of interpreting complex, non-linear MMC solutions in physical terms. This indicates that a pragmatic approach to future MMC studies based on machine learning methods is required, in which the allowable solution complexity is carefully constrained.","2177":"Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced  sibling  precision metric, where our method also obtains excellent results.","2178":null,"2179":"We present a new trace estimator of the matrix whose explicit form is not\ngiven but its matrix multiplication to a vector is available. The form of the\nestimator is similar to the Hutchison stochastic trace estimator, but instead\nof the random noise vectors in Hutchison estimator, we use small number of\nprobing vectors determined by machine learning. Evaluation of the quality of\nestimates and bias correction are discussed. An unbiased estimator is proposed\nfor the calculation of the expectation value of a function of traces. In the\nnumerical experiments with random matrices, it is shown that the precision of\ntrace estimates with \\$O(10)\\$ probing vectors determined by the\nmachine learning is similar to that with \\$O(10000)\\$ random noise\nvectors.","2180":"Today many information sources---including sensor networks, financial markets, social networks, and healthcare monitoring---are so-called data streams, arriving sequentially and at high speed. Analysis must take place in real time, with partial data and without the capacity to store the entire data set. This book presents algorithms and techniques used in data stream mining and real-time analytics. Taking a hands-on approach, the book demonstrates the techniques using MOA (Massive Online Analysis), a popular, freely available open-source software framework, allowing readers to try out the techniques after reading the explanations.","2181":"In this paper we describe a new hybrid distributed\/shared memory parallel software for support vector machine learning on\r\nlarge data sets. The support vector machine (SVM) method is a well-known and reliable machine learning technique for classificationand regression tasks. Based on a recently developed shared memory decomposition algorithm for support vector machine classifierdesign we increased the level of parallelism by implementing a cross validation routine based on message passing. With thisextention we obtained a flexible parallel SVM software that can be used on high-end machines with SMP architectures to processthe large data sets that arise more and more in bioinformatics and other fields of research.","2182":"Machine learning (ML)-based software\u2019s deployment has raised serious concerns about its pervasive and harmful consequences for users, business, and society inflicted through bias. While approaches to address bias are increasingly recognized and developed, our understanding of debiasing remains nascent. Research has yet to provide a comprehensive coverage of this vast growing field, much of which is not embedded in theoretical understanding. Conceptualizing and structuring the nature, effect, and implementation of debiasing instruments could provide necessary guidance for practitioners investing in debiasing efforts. We develop a taxonomy that classifies debiasing instrument characteristics into seven key dimensions. We evaluate and refine our taxonomy through nine experts and apply our taxonomy to three actual debiasing instruments, drawing lessons for the design and choice of appropriate instruments. Bridging the gaps between our conceptual understanding of debiasing for ML-based software and its organizational implementation, we discuss contributions and future research.","2183":null,"2184":"Machine learning is becoming more and more prevalent in networking, e.g, network monitoring and managament. Though, expert knowledge is always valuable, sometimes irreplaceable. As the machine learning model may not always be confident, one idea is to relay such inconfident decisions to an expert, i.e., the admin, which can decide what to do with this decision and potentially relabel it, called user-based active learning. The goal of this thesis is to perform a simulative analysis regarding the influence of various parameters, such as the confidence threshold (i.e., when to relay the information) or the processing time and qualification level of the admin (i.e., a fast but not always correct expert, slow but accurate etc.). For more information, read the linked presentation slide and\/or message katharina.dietz@uni-wuerzburg.de.","2185":"RATIONALE, AIMS AND OBJECTIVES In evaluating non-randomized interventions, propensity scores (PS) estimate the probability of assignment to the treatment group given observed characteristics. Machine learning algorithms have been proposed as an alternative to conventional logistic regression for modelling PS in order to avoid limitations of linear methods. We introduce classification tree analysis (CTA) to generate PS which is a \"decision-tree\"-like classification model that provides accurate, parsimonious decision rules that are easy to display and interpret, reports P values derived via permutation tests, and evaluates cross-generalizability. METHOD Using empirical data, we identify all statistically valid CTA PS models and then use them to compute strata-specific, observation-level PS weights that are subsequently applied in outcomes analyses. We compare findings obtained using this framework to logistic regression and boosted regression, by evaluating covariate balance using standardized differences, model predictive accuracy, and treatment effect estimates obtained using median regression and a weighted CTA outcomes model. RESULTS While all models had some imbalanced covariates, main-effects logistic regression yielded the lowest average standardized difference, whereas CTA yielded the greatest predictive accuracy. Nevertheless, treatment effect estimates were generally consistent across all models. CONCLUSIONS Assessing standardized differences in means as a test of covariate balance is inappropriate for machine learning algorithms that segment the sample into two or more strata. Because the CTA algorithm identifies all statistically valid PS models for a sample, it is most likely to identify a correctly specified PS model, and should be considered as an alternative approach to modeling the PS.","2186":null,"2187":"In this paper we describe and analyze a three week\n                 assignment that was given in a Machine Learning course\n                 at Columbia University. The assignment presented\n                 students with an introduction to machine learning\n                 research. The assignment required students to apply\n                 Genetic Programming to evolve algorithms that play the\n                 board game Othello. The students were provided with an\n                 implemented experimental approach as a starting point.\n                 The students were required to perform their own\n                 experimental modifications corresponding to research\n                 issues in machine learning. The results of student\n                 experiments were good both in terms of research and in\n                 terms of student learning. All relevant code,\n                 documentation and information about GPOthello is\n                 available at the following url:\n                 http:\/\/www.cs.columbia.edu\/~evs\/ml\/othello.html .","2188":null,"2189":"In the context of science, the well-known adage \u00e4 picture is worth a\r\nthousand words\" might well be \u00e4 model is worth a thousand datasets.\"\r\nScientific models, such as Newtonian physics or biological gene regulatory\r\nnetworks, are human-driven simplifications of complex phenomena that serve as\r\nsurrogates for the countless experiments that validated the models. Recently,\r\nmachine learning has been able to overcome the inaccuracies of approximate\r\nmodeling by directly learning the entire set of nonlinear interactions from\r\ndata. However, without any predetermined structure from the scientific basis\r\nbehind the problem, machine learning approaches are flexible but\r\ndata-expensive, requiring large databases of homogeneous labeled training data.\r\nA central challenge is reconciling data that is at odds with simplified models\r\nwithout requiring \"big data\".\r\n  In this work we develop a new methodology, universal differential equations\r\n(UDEs), which augments scientific models with machine-learnable structures for\r\nscientifically-based learning. We show how UDEs can be utilized to discover\r\npreviously unknown governing equations, accurately extrapolate beyond the\r\noriginal data, and accelerate model simulation, all in a time and\r\ndata-efficient manner. This advance is coupled with open-source software that\r\nallows for training UDEs which incorporate physical constraints, delayed\r\ninteractions, implicitly-defined events, and intrinsic stochasticity in the\r\nmodel. Our examples show how a diverse set of computationally-difficult\r\nmodeling issues across scientific disciplines, from automatically discovering\r\nbiological mechanisms to accelerating the training of physics-informed neural\r\nnetworks and large-eddy simulations, can all be transformed into UDE training\r\nproblems that are efficiently solved by a single software methodology.","2190":null,"2191":null,"2192":"Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using <em>pool-based active learning<\/em>. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a <em>version space<\/em>. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.","2193":"Knowledge elicitation from experts and empirical machine learning are two distinct approaches to knowledge acquisition with differing and mutually complementary capabilities. Learning apprentices have provided environments in which a knowledge engineer may collaborate with a machine learning system allowing, for a synergy between the complementary approaches. The Knowledge Factory is a knowledge acquisition environment that allows a domain expert to collaborate directly with a machine learning system without the need for assistance from a knowledge engineer. This requires a different form of environment to the learning apprentice. This paper describes techniques for supporting such interactions and their implementation in a knowledge acquisition environment called The Knowledge Factory.","2194":null,"2195":"The NASA\/IPAC Extragalactic Database (NED) is a comprehensive online service\r\nthat combines fundamental multi-wavelength information for known objects beyond\r\nthe Milky Way and provides value-added, derived quantities and tools to search\r\nand access the data. The contents and relationships between measurements in the\r\ndatabase are continuously augmented and revised to stay current with\r\nastrophysics literature and new sky surveys. The conventional process of\r\ndistilling and extracting data from the literature involves human experts to\r\nreview the journal articles and determine if an article is of extragalactic\r\nnature, and if so, what types of data it contains. This is both labor intensive\r\nand unsustainable, especially given the ever-increasing number of publications\r\neach year. We present here a machine learning (ML) approach developed and\r\nintegrated into the NED production pipeline to help automate the classification\r\nof journal article topics and their data content for inclusion into NED. We\r\nshow that this ML application can successfully reproduce the classifications of\r\na human expert to an accuracy of over 90% in a fraction of the time it takes a\r\nhuman, allowing us to focus human expertise on tasks that are more difficult to\r\nautomate.","2196":"Neural Machine Translation (NMT) is no-\r\ntorious  for  its  need  for  large  amounts  of\r\nbilingual  data.   An  effective  approach  to\r\ncompensate for this requirement is Multi-\r\nTask  Learning  (MTL)  to  leverage  differ-\r\nent  linguistic  resources  as  a  source  of\r\ninductive  bias.\r\nCurrent  MTL  architec-\r\ntures are based on the S\r\nEQ\r\n2S\r\nEQ\r\ntransduc-\r\ntion,  and  (partially)  share  different  com-\r\nponents  of  the  models  among  the  tasks.\r\nHowever, this MTL approach often suffers\r\nfrom task interference,  and is not able to\r\nfully  capture  commonalities  among  sub-\r\nsets of tasks.  We address this issue by ex-\r\ntending  the  recurrent  units  with  multiple\r\nblocks\r\nalong with a trainable\r\nrouting net-\r\nwork\r\n.  The routing network enables adap-\r\ntive  collaboration  by  dynamic  sharing  of\r\nblocks conditioned on the task at hand, in-\r\nput, and model state. Empirical evaluation\r\nof two low-resource translation tasks, En-\r\nglish  to  Vietnamese  and  Farsi,  show  +1\r\nBLEU  score  improvements  compared  to\r\nstrong baselines.","2197":"Medical imaging is an important research field with many opportunities for\r\nimproving patients' health. However, there are a number of challenges that are\r\nslowing down the progress of the field as a whole, such optimizing for\r\npublication. In this paper we reviewed several problems related to choosing\r\ndatasets, methods, evaluation metrics, and publication strategies. With a\r\nreview of literature and our own analysis, we show that at every step,\r\npotential biases can creep in. On a positive note, we also see that initiatives\r\nto counteract these problems are already being started. Finally we provide a\r\nbroad range of recommendations on how to further these address problems in the\r\nfuture. For reproducibility, data and code for our analyses are available on\r\nhttps:\/\/github.com\/GaelVaroquaux\/ml_med_imaging_failures","2198":null,"2199":null,"2200":null,"2201":null,"2202":"In this research, we outline a user modeling framework that uses both unsupervised and supervised machine learning in order to reduce development costs of building user models, and facilitate transferability. We apply the framework to model student learning during interaction with the Adaptive Coach for Exploration (ACE) learning environment (using both interface and eye-tracking data). In addition to demonstrating framework effectiveness, we also compare results from previous research on applying the framework to a different learning environment and data type. Our results also confirm previous research on the value of using eye-tracking data to assess student learning.","2203":null,"2204":null,"2205":null,"2206":"Alcoholism is one of the most common diseases in the world. This type of\r\nsubstance abuse leads to mental and physical dependence on ethanol-containing\r\ndrinks. Alcoholism is accompanied by progressive degradation of the personality\r\nand damage to the internal organs. Today still not exists a quick diagnosis\r\nmethod to detect this disease. This article presents the method for the quick\r\nand anonymous alcoholism diagnosis by neural networks. For this method, don't\r\nneed any private information about the subject. For the implementation, we\r\nconsidered various algorithms of machine learning and deep neural networks. In\r\ndetail analyzed the correlation of the signals from electrodes by neural\r\nnetworks. The wavelet transforms and the fast Fourier transform was considered.\r\nThe manuscript demonstrates that the deep neural network which operates only\r\nwith a dataset of EEG correlation signals can anonymously classify the\r\nalcoholic and control groups with high accuracy. On the one hand, this method\r\nwill allow subjects to be tested for alcoholism without any personal data,\r\nwhich will not cause inconvenience or shame in the subject, and on the other\r\nhand, the subject will not be able to deceive specialists who diagnose the\r\nsubject for the presence of the disease.","2207":"State-of-the-art decision tree methods apply heuristics recursively to create each split in isolation, which may not capture well the underlying characteristics of the dataset. The optimal decision tree problem attempts to resolve this by creating the entire decision tree at once to achieve global optimality. In the last 25\u00a0years, algorithmic advances in integer optimization coupled with hardware improvements have resulted in an astonishing 800 billion factor speedup in mixed-integer optimization (MIO). Motivated by this speedup, we present optimal classification trees, a novel formulation of the decision tree problem using modern MIO techniques that yields the optimal decision tree for axes-aligned splits. We also show the richness of this MIO formulation by adapting it to give optimal classification trees with hyperplanes that generates optimal decision trees with multivariate splits. Synthetic tests demonstrate that these methods recover the true decision tree more closely than heuristics, refuting the notion that optimal methods overfit the training data. We comprehensively benchmark these methods on a sample of 53 datasets from the UCI machine learning repository. We establish that these MIO methods are practically solvable on real-world datasets with sizes in the 1000s, and give average absolute improvements in out-of-sample accuracy over CART of 1--2 and 3--5\\% for the univariate and multivariate cases, respectively. Furthermore, we identify that optimal classification trees are likely to outperform CART by 1.2--1.3\\% in situations where the CART accuracy is high and we have sufficient training data, while the multivariate version outperforms CART by 4--7\\% when the CART accuracy or dimension of the dataset is low.","2208":null,"2209":null,"2210":null,"2211":null,"2212":null,"2213":null,"2214":null,"2215":null,"2216":null,"2217":null,"2218":null,"2219":null,"2220":"We have constructed ADVISOR, a two-agent machine learning architecture\n\tfor intelligent tutoring systems (ITS). The purpose of this architecture\n\tis to centralize the reasoning of an ITS into a single component\n\tto allow customization of teaching goals and to simplify improving\n\tthe ITS. The first agent is responsible for learning a model of how\n\tstudents perform using the tutor in a variety of contexts. The second\n\tagent is provided this model of student behavior and a goal specifying\n\tthe desired educational objective. Reinforcement learning is used\n\tby this agent to derive a teaching policy that meets the specified\n\teducational goal. Component evaluation studies show each agent performs\n\tadequately in isolation. We have also conducted an evaluation with\n\tactual students of the complete architecture. Results show ADVISOR\n\twas successful in learning a teaching policy that met the educational\n\tobjective provided. Although this set of machine learning agents\n\thas been integrated with a specific intelligent tutor, the general\n\ttechnique could be applied to a broad class of ITS.","2221":"Quantum Machine Learning (QML) shows how it maintains certain significant\r\nadvantages over machine learning methods. It now shows that hybrid quantum\r\nmethods have great scope for deployment and optimisation, and hold promise for\r\nfuture industries. As a weakness, quantum computing does not have enough qubits\r\nto justify its potential. This topic of study gives us encouraging results in\r\nthe improvement of quantum coding, being the data preprocessing an important\r\npoint in this research we employ two dimensionality reduction techniques LDA\r\nand PCA applying them in a hybrid way Quantum Support Vector Classifier (QSVC)\r\nand Variational Quantum Classifier (VQC) in the classification of Diabetes.","2222":"Benefiting from advances in online sketchy shape recognition system\r\n\tcould not be expected before the problem of its adaptability is well\r\n\tsolved. In this paper, three methods of machine learning are compared\r\n\tfor adaptive sketch recognition with some experiments based on various\r\n\tfeature representations of sketches and collected samples of multi-users.\r\n\tThe experimental results reveal elementally some important matters\r\n\tof sketch recognition based on machine learning methods and have\r\n\ta significant reference to further researches of adaptive sketch\r\n\trecognition.","2223":null,"2224":"The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data. After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem.","2225":null,"2226":null,"2227":null,"2228":"The impact of the network performance on the quality of experience (QoE) for various services is not well-understood. Assessing the impact of different network and channel conditions on the user experience is important for improving the telecommunication services. The QoE for various wireless services including VoIP, video streaming, and web browsing, has been in the epicenter of recent networking activities. The majority of such efforts aim to characterize the user experience, analyzing various types of measurements often in an aggregate manner. This paper proposes the MLQoE, a modular algorithm for user-centric QoE prediction. The MLQoE employs multiple machine learning (ML) algorithms, namely, Artificial Neural Networks, Support Vector Regression machines, Decision Trees, and Gaussian Naive Bayes classifiers, and tunes their hyper-parameters. It uses the Nested Cross Validation (nested CV) protocol for selecting the best classifier and the corresponding best hyper-parameter values and estimates the performance of the final model. The MLQoE is conservative in the performance estimation despite multiple induction of models. The MLQoE is modular, in that, it can be easily extended to include other ML algorithms. The MLQoE selects the ML algorithm that exhibits the best performance and its parameters automatically given the dataset used as input. It uses empirical measurements based on network metrics (e.g., packet loss, delay, and packet interarrival) and subjective opinion scores reported by actual users. This paper extensively evaluates the MLQoE using three unidirectional datasets containing VoIP calls over wireless networks under various network conditions and feedback from subjects (collected in field studies). Moreover, it performs a preliminary analysis to assess the generality of our methodology using bidirectional VoIP and video traces. The MLQoE outperforms several state-of-the-art algorithms, resulting in fairly accurate predictions.","2229":"Over the last decades many machine learning experiments have been published, giving benefit to the scientific progress. In order to compare machine-learning experiment results with each other and collaborate positively, they need to be performed thoroughly on the same computing environment, using the same sample datasets and algorithm configurations. Besides this, practical experience shows that scientists and engineers tend to have large output data in their experiments, which is both difficult to analyze and archive properly without provenance metadata. However, the Linked Data community still misses a light-weight specification for interchanging machine-learning metadata over different architectures to achieve a higher level of interoperability. In this paper, we address this gap by presenting a novel vocabulary dubbed MEX. We show that MEX provides a prompt method to describe experiments with a special focus on data provenance and fulfills the requirements for a long-term maintenance.","2230":"Despite the increased sensor-based data collection in Industry 4.0, the practical use of this data is still in its infancy. In contrast, academic literature provides several approaches to detect machine failures but, in most cases, relies on simulations and vast amounts of training data. Since it is often not practical to collect such amounts of data in an industrial context, we propose an approach to detect the current production mode and machine degradation states on a comparably small data set. Our approach integrates domain knowledge about manufacturing systems into a highly generalizable end-to-end workflow ranging from raw data processing, phase segmentation, data resampling, and feature extraction to machine tool anomaly detection. The workflow applies unsupervised clustering techniques to identify the current production mode and supervised classification models for detecting the present degradation. A resampling strategy and classical machine learning models enable the workflow to handle small data sets and distinguish between normal and abnormal machine tool behavior. To the best of our knowledge, there exists no such end-to-end workflow in the literature that uses the entire machine signal as input to identify anomalies for individual tools. Our evaluation with data from a real multi-purpose machine shows that the proposed workflow detects anomalies with an average F1-score of almost 93%.","2231":null,"2232":null,"2233":null,"2234":null,"2235":null,"2236":"A theory of \u201cmegacitation\u201d is introduced and used in an experiment to demonstrate how a qualitative scholarly book review can be converted into a weighted bibliometric indicator. We employ a manual human-coding approach to classify book reviews in the field of history based on reviewers' assessments of a book author's scholarly credibility (SC) and writing style (WS). In total, 100 book reviews were selected from the American Historical Review and coded for their positive\/negative valence on these two dimensions. Most were coded as positive (68% for SC and 47% for WS), and there was also a small positive correlation between SC and WS (r\u2009=\u20090.2). We then constructed a classifier, combining both manual design and machine learning, to categorize sentiment-based sentences in history book reviews. The machine classifier produced a matched accuracy (matched to the human coding) of approximately 75% for SC and 64% for WS. WS was found to be more difficult to classify by machine than SC because of the reviewers' use of more subtle language. With further training data, a machine-learning approach could be useful for automatically classifying a large number of history book reviews at once. Weighted megacitations can be especially valuable if they are used in conjunction with regular book\/journal citations, and \u201clibcitations\u201d (i.e., library holding counts) for a comprehensive assessment of a book\/monograph's scholarly impact.","2237":"The development of computer models for water quality index forecasting has been a leading research topic worldwide which has been considerably recognized over the last two decades; the balance between efficient water quality requires a good water management technique. The balance is said to be achieved through various procedures many of which require the application of computer-aided forecasting tools. In this paper, a decade research review on the water quality index in the field of artificial intelligence was carried out with the aim to present the most viable or most suitable methods and models to be adopted for future researchers in the field of water quality. The review incorporates the developed models such as ANN, ANFIS, SVM, other regression or time-series, and other soft computing models. This research shows that the study focused on a decade review of the methods and models, and also, there is room for long-term forecasts. It also shows that there is no single AI model that outperforms all the remaining AI models but It is necessary to evaluate the strength of each model combination for each region thus to know what type of method or model that works best for the country or region. The use of AI has grown significantly in recent decades however there is enough room for researchers to duel in and improve in the field of water quality index.","2238":null,"2239":"A key challenge in systems biology is the reconstruction of an organism's metabolic network from its genome sequence. One strategy for addressing this problem is to predict which metabolic pathways, from a reference database of known pathways, are present in the organism, based on the annotated genome of the organism.\n                To quantitatively validate methods for pathway prediction, we developed a large \"gold standard\" dataset of 5,610 pathway instances known to be present or absent in curated metabolic pathway databases for six organisms. We defined a collection of 123 pathway features, whose information content we evaluated with respect to the gold standard. Feature data were used as input to an extensive collection of machine learning (ML) methods, including na\u00efve Bayes, decision trees, and logistic regression, together with feature selection and ensemble methods. We compared the ML methods to the previous PathoLogic algorithm for pathway prediction using the gold standard dataset. We found that ML-based prediction methods can match the performance of the PathoLogic algorithm. PathoLogic achieved an accuracy of 91\\% and an F-measure of 0.786. The ML-based prediction methods achieved accuracy as high as 91.2\\% and F-measure as high as 0.787. The ML-based methods output a probability for each predicted pathway, whereas PathoLogic does not, which provides more information to the user and facilitates filtering of predicted pathways.\n                ML methods for pathway prediction perform as well as existing methods, and have qualitative advantages in terms of extensibility, tunability, and explainability. More advanced prediction methods and\/or more sophisticated input features may improve the performance of ML methods. However, pathway prediction performance appears to be limited largely by the ability to correctly match enzymes to the reactions they catalyze based on genome annotations.","2240":"Deep learning methods are recognised as state-of-the-art for many applications of machine learning. Recently, deep learning methods have emerged as a solution to the task of automatic music generation (AMG) using symbolic tokens in a target style, but their superiority over non-deep learning methods has not been demonstrated. Here, we conduct a listening study to comparatively evaluate several music generation systems along six musical dimensions: stylistic success, aesthetic pleasure, repetition or self-reference, melody, harmony, and rhythm. A range of models, both deep learning algorithms and other methods, are used to generate 30-s excerpts in the style of Classical string quartets and classical piano improvisations. Fifty participants with relatively high musical knowledge rate unlabelled samples of computer-generated and human-composed excerpts for the six musical dimensions. We use non-parametric Bayesian hypothesis testing to interpret the results, allowing the possibility of finding meaningful non-differences between systems' performance. We find that the strongest deep learning method, a reimplemented version of Music Transformer, has equivalent performance to a non-deep learning method, MAIA Markov, demonstrating that to date, deep learning does not outperform other methods for AMG. We also find there still remains a significant gap between any algorithmic method and human-composed excerpts.","2241":null,"2242":"Word sense disambiguation (WSD) is critical in the biomedical domain for improving the precision of natural language processing (NLP), text mining, and information retrieval systems because ambiguous words negatively impact accurate access to literature containing biomolecular entities, such as genes, proteins, cells, diseases, and other important entities. Automated techniques have been developed that address the WSD problem for a number of text processing situations, but the problem is still a challenging one. Supervised WSD machine learning (ML) methods have been applied in the biomedical domain and have shown promising results, but the results typically incorporate a number of confounding factors, and it is problematic to truly understand the effectiveness and generalizability of the methods because these factors interact with each other and affect the final results. Thus, there is a need to explicitly address the factors and to systematically quantify their effects on performance.","2243":"In Socially Guided Machine Learning we explore the ways in which machine\n\tlearning can more fully take advantage of natural human interaction.\n\tIn this work we are studying the role real-time human interaction\n\tplays in training assistive robots to perform new tasks. We describe\n\tan experimental platform, Sophie's World, and present descriptive\n\tanalysis of human teaching behavior found in a user study. We report\n\tthree important observations of how people administer reward and\n\tpunishment to teach a simulated robot a new task through Reinforcement\n\tLearning. People adjust their behavior as they develop a model of\n\tthe learner, they use the reward channel for guidance as well as\n\tfeedback, and they may also use it as a motivational channel.","2244":null,"2245":null,"2246":"Comparing probability distributions is a fundamental problem in data sciences. Simple norms and divergences such as the total variation and the relative entropy only compare densities in a point-wise manner and fail to capture the geometric nature of the problem. In sharp contrast, Maximum Mean Discrepancies (MMD) and Optimal Transport distances (OT) are two classes of distances between measures that take into account the geometry of the underlying space and metrize the convergence in law.  This paper studies the Sinkhorn divergences, a family of geometric divergences that interpolates between MMD and OT. Relying on a new notion of geometric entropy, we provide theoretical guarantees for these divergences: positivity, convexity and metrization of the convergence in law. On the practical side, we detail a numerical scheme that enables the large scale application of these divergences for machine learning: on the GPU, gradients of the Sinkhorn loss can be computed for batches of a million samples.","2247":null,"2248":null,"2249":null,"2250":null,"2251":"The rapid growth in the number of autism disorder among toddlers needs for the development of easily implemented and effective screening methods. In this current era, the causes of Autism Spectrum Disorder (ASD) do not know yet, however, the diagnosis and detection of ASD is based on behaviours and symptoms. This paper aims to improve ASD disease prediction accuracy among toddlers by using the Logistic Regression model of Machine Learning, through the collected health care dataset and by using an algorithm for rapid classification of the behaviours to check whether the children are having autism diseases or not according to information in the dataset. Therefore, Machine Learning decreasing the time needed to detect the disorder, then providing the necessary health services early for infected toddlers to enhance their lifestyle. In healthcare, most machine learning applications are in the research stage, and to take the advantage of emerging software tools that incorporate artificial intelligence, healthcare organizations first need to overcome a variety of challenges.","2252":null,"2253":null,"2254":"Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and\/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedforward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM.","2255":null,"2256":null,"2257":null,"2258":null,"2259":null,"2260":"Automated classification of email messages into user-specific folders and information extraction from chronologically ordered email streams have become interesting areas in text learning research. However, the lack of large benchmark collections has been an obstacle for studying the problems and evaluating the solutions. In this paper, we introduce the Enron corpus as a new test bed. We analyze its suitability with respect to email folder prediction, and provide the baseline results of a state-of-the-art classifier (Support Vector Machines) under various conditions, including the cases of using individual sections (From, To, Subject and body) alone as the input to the classifier, and using all the sections in combination with regression weights.","2261":null,"2262":null,"2263":null,"2264":null,"2265":"We present a modelling framework for predicting forest areas. The framework is obtained by integrating a machine learning software suite within the GRASS Geographical Information System (GIS) and by providing additional methods for predictive habitat modelling. Three machine learning techniques (Tree-Based Classification, Neural Networks and Random Forest) are available in parallel for modelling from climatic and topographic variables. Model evaluation and parameter selection are measured by sensitivity-specificity ROC analysis, while the final presence and absence maps are obtained through maximisation of the kappa statistic. The modelling framework is applied at a resolution of 1\u00a0km with Iberian subpopulations of Pinus sylvestris L. forests. For this data set, the most accurate algorithm is Breiman's random forest, an ensemble method which provides automatic combination of tree-classifiers trained on bootstrapped subsamples and randomised variable sets. All models show a potential area of P. sylvestris for the Iberian Peninsula which is larger than the present one, a result corroborated by regional pollen analyses.","2266":null,"2267":null,"2268":"An intrusion detection system detects various malicious behaviors and abnormal activities that might harm security and trust of computer system. IDS operate either on host or network level via utilizing anomaly detection or misuse detection. Main problem is to correctly detect intruder attack against computer network. The key point of successful detection of intrusion is choice of proper features. To resolve the problems of IDS scheme this research work propose \u201can improved method to detect intrusion using machine learning algorithms\u201d. In our paper we use KDDCUP 99 dataset to analyze efficiency of intrusion detection with different machine learning algorithms like Bayes, NaiveBayes, J48, J48Graft and Random forest. To identify network based IDS with KDDCUP 99 dataset, experimental results shows that the three algorithms J48, J48Graft and Random forest gives much better results than other machine learning algorithms. We use WEKA to check the accuracy of classified dataset via our proposed method. We have considered all the parameter for computation of result i.e. precision, recall, F \u2013 measure and ROC.","2269":"This article describes an approach to combining symbolic and connectionist\r\n\tapproaches to machine learning. A three-stage framework is presented\r\n\tand the research of several groups is reviewed with respect to this\r\n\tframework. The first stage involves the insertion of symbolic knowledge\r\n\tinto neural networks, the second addresses the refinement of this\r\n\tprior knowledge in its neural representation, while the third concerns\r\n\tthe extraction of the refined symbolic knowledge. Experimental results\r\n\tand open research issues are discussed","2270":null,"2271":"Because of the complexity of gene-phenotype relationships machine learning approaches have considerable appeal as a strategy for modelling interactions. A number of such methods have been developed and applied in recent years with some modest success. Progress is hampered by the challenges presented by the complexity of the disease genetic data, including phenotypic and genetic heterogeneity, polygenic forms of inheritance and variable penetrance, combined with the analytical and computational issues arising from the enormous number of potential interactions. We review here recent and current approaches focusing, wherever possible, on applications to real data (particularly in the context of genome-wide association studies) and looking ahead to the further challenges posed by next generation sequencing data.","2272":"How can end users efficiently influence the predictions that machine learning systems make on their behalf? This paper presents Explanatory Debugging, an approach in which the system explains to users how it made each of its predictions, and the user then explains any necessary corrections back to the learning system. We present the principles underlying this approach and a prototype instantiating it. An empirical evaluation shows that Explanatory Debugging increased participants' understanding of the learning system by 52\\% and allowed participants to correct its mistakes up to twice as efficiently as participants using a traditional learning system.","2273":null,"2274":"Gathering labeled data to train well-performing machine learning models is one of the critical challenges in many applications. Active learning aims at reducing the labeling costs by an efficient and effective allocation of costly labeling resources. In this article, we propose a decision-theoretic selection strategy that (1) directly optimizes the gain in misclassification error, and (2) uses a Bayesian approach by introducing a conjugate prior distribution to determine the class posterior to deal with uncertainties. By reformulating existing selection strategies within our proposed model, we can explain which aspects are not covered in current state-of-the-art and why this leads to the superior performance of our approach. Extensive experiments on a large variety of datasets and different kernels validate our claims.","2275":"We introduce the task of learning to pick a single preferred example out a finite set of examples, an\r\n\u201coptimal choice problem\u201d, as a supervised machine learning problem with complex input. Problems of\r\noptimal choice emerge often in various practical applications. We formalize the problem, show that it does\r\nnot satisfy the assumptions of statistical learning theory, yet it can be solved efficiently in some cases. We\r\npropose two approaches to solve the problem. Both of them reach good solutions on real life data from a\r\nsignal processing application.","2276":"We propose a new framework for supervised machine learning. Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with \u201crationales.\u201d When annotating an example, the human teacher will also highlight evidence supporting  this annotation\u2014thereby teaching the machine learner why the example belongs to the category. We provide some rationale-annotated data and present a learning method that exploits the rationales during training to boost performance significantly on a sample task, namely sentiment classification of movie reviews. We hypothesize that in some situations, providing rationales is a more fruitful use of an annotator\u2019s time than annotating more examples.","2277":null,"2278":"Deep learning uses multilayered artificial neural networks to learn digitally from large datasets. It then performs advanced identification and classification tasks. To date, these multilayered neural networks have been implemented on a computer. Lin et al. demonstrate all-optical machine learning that uses passive optical components that can be patterned and fabricated with 3D-printing. Their hardware approach comprises stacked layers of diffractive optical elements analogous to an artificial neural network that can be trained to execute complex functions at the speed of light.Science, this issue p. 1004Deep learning has been transforming our ability to execute advanced inference tasks using computers. Here we introduce a physical mechanism to perform machine learning by demonstrating an all-optical diffractive deep neural network (D2NN) architecture that can implement various functions following the deep learning\\\u2013based design of passive diffractive layers that work collectively. We created 3D-printed D2NNs that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum. Our all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can execute; will find applications in all-optical image analysis, feature detection, and object classification; and will also enable new camera designs and optical components that perform distinctive tasks using D2NNs.","2279":null,"2280":"Data mining techniques are used to power intelligent software, both on and off the Internet. <I>Data Mining: Practical Machine Learning Tools<\/I> explains the magic behind information extraction in a book that succeeds at bringing the latest in computer science research to any IS manager or developer. In addition, this book provides an opportunity for the authors to showcase their powerful reusable Java class library for building custom data mining software.<p> This text is remarkable with its comprehensive review of recent research on machine learning, all told in a very approachable style. (While there is plenty of math in some sections, the authors' explanations are always clear.) The book tours the nature of machine learning and how it can be used to find predictive patterns in data comprehensible to managers and developers alike. And they use sample data (for such topics as weather, contact lens prescriptions, and flowers) to illustrate key concepts. <p> After setting out to explain the types of machine learning models (like decision trees and classification rules), the book surveys algorithms used to implement them, plus strategies for improving performance and the reliability of results. Later the book turns to the authors' downloadable Weka (rhymes with \"Mecca\") Java class library, which lets you experiment with data mining hands-on and gets you started with this technology in custom applications. Final sections look at the bright prospects for data mining and machine learning on the Internet (for example, in Web search engines). <p> Precise but never pedantic, this admirably clear title delivers a real-world perspective on advantages of data mining and machine learning. Besides a programming how-to, it can be read profitably by any manager or developer who wants to see what leading-edge machine learning techniques can do for their software. <I>--Richard Dragan<\/I><p> <B>Topics covered<\/B>: Data mining and machine learning basics, sample datasets and applications for data mining, machine learning vs. statistics, the ethics of data mining, generalization, concepts, attributes, missing values, decision tables and trees, classification rules, association rules, exceptions, numeric prediction, clustering, algorithms and implementations in Java, inferring rules, statistical modeling, covering algorithms, linear models, support vector machines, instance-based learning, credibility, cross-validation, probability, costs (lift charts and ROC curves), selecting attributes, data cleansing, combining multiple models (bagging, boosting, and stacking), Weka (reusable Java classes for machine learning), customizing Weka, visualizing machine learning, working with massive datasets, text mining, and e-mail and the Internet.","2281":"High-resolution cosmological hydrodynamic simulations are currently limited\r\nto relatively small volumes due to their computational expense. However, much\r\nlarger volumes are required to probe rare, overdense environments, and measure\r\nclustering statistics of the large scale structure. Typically, zoom simulations\r\nof individual regions are used to study rare environments, and semi-analytic\r\nmodels and halo occupation models applied to dark matter only (DMO) simulations\r\nare used to study the Universe in the large-volume regime. We propose a new\r\napproach, using a machine learning framework to explore the halo-galaxy\r\nrelationship in the periodic EAGLE simulations, and zoom C-EAGLE simulations of\r\ngalaxy clusters. We train a tree based machine learning method to predict the\r\nbaryonic properties of galaxies based on their host dark matter halo\r\nproperties. The trained model successfully reproduces a number of key\r\ndistribution functions for an infinitesimal fraction of the computational cost\r\nof a full hydrodynamic simulation. By training on both periodic simulations as\r\nwell as zooms of overdense environments, we learn the bias of galaxy evolution\r\nin differing environments. This allows us to apply the trained model to a\r\nlarger DMO volume than would be possible if we only trained on a periodic\r\nsimulation. We demonstrate this application using the $(800 \\; Mpc)^3$\r\nP-Millennium simulation, and present predictions for key baryonic distribution\r\nfunctions and clustering statistics from the EAGLE model in this large volume.","2282":null,"2283":null,"2284":null,"2285":null,"2286":"A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks.","2287":null,"2288":"Cancer is a diseases in which a set of cells has not able controlled their growth, attack that interrupts upon and destroy the nearest tissues or spreading to other locations in the body. Cancer has become one of the perilous diseases in the present scenario. In this paper, the recently developed Extreme Learning Machine is used for classification problems in cancer diagnosis area. ELM is an available learning algorithm for single layer feed forward neural network. The advanced and developed methodology known for cancer multi classification using ELM microarray gene expression cancer diagnosis, this used for directing multi category classification problems in the cancer diagnosis area. ELM avoids many problems, improper learning rate and over fitting commonly faced by iterative learning methods and completes the training very fast. The performance of classification ELM on three benchmark microarray data for cancer diagnosis, namely Lymphoma data set, Leukemia data set, SRBCT data set. The results of experiments with RVM and ELM shows that for many categories of ELM still outperformer with RVM","2289":"Stock market prediction is the act of trying to determine the future value of\r\na company stock or other financial instrument traded on a financial exchange.","2290":"Missing value research has been around for at least two to three decades, but imputation of missing values is a major challenge in keeping databases intact. Statistics-oriented imputation and non-statistics-oriented imputation are two types of missing value imputation. Numerous flaws in the statistics-based imputation method make it difficult to fine-tune or expect perfect imputation; it also has a number of execution-related limitations. This is a clue to the non-statistical practise known as machine learning, which we examined in this work. The Deep Belief Network (DBN) is a type of unsupervised probabilistic generative model used in machine learning applications. Restricted Boltzmann Machines are used to build it, and they perform a contrastive divergence and backpropagation to fine-tune the weights for the imputation process. DBN's stable imputation value is based on the contrastive divergences. Data from the UCI Repository's PIMA medical dataset was used in the experimentation Up to 90% of the time, the DBM with backpropagation is accurate. A maximum of 10% mean square error rate is supported by this method (DBN) compared to earlier imputation techniques. In order to evaluate the accuracy of DBN, nearly five additional imputation methods are linked to it. In comparison to other methods, DBN imputation provides 90% accuracy.","2291":"Objectives The authors study two approaches to assertion classification. One of these approaches, Extended NegEx (ENegEx), extends the rule-based NegEx algorithm to cover alter-association assertions; the other, Statistical Assertion Classifier (StAC), presents a machine learning solution to assertion classification.  Design For each mention of each medical problem, both approaches determine whether the problem, as asserted by the context of that mention, is present, absent, or uncertain in the patient, or associated with someone other than the patient. The authors use these two systems to (1) extend negation and uncertainty extraction to recognition of alter-association assertions, (2) determine the contribution of lexical and syntactic context to assertion classification, and (3) test if a machine learning approach to assertion classification can be as generally applicable and useful as its rule-based counterparts.  Measurements The authors evaluated assertion classification approaches with precision, recall, and F-measure.  Results The ENegEx algorithm is a general algorithm that can be directly applied to new corpora. Despite being based on machine learning, StAC can also be applied out-of-the-box to new corpora and achieve similar generality.  Conclusion The StAC models that are developed on discharge summaries can be successfully applied to radiology reports. These models benefit the most from words found in the \u00b1 4 word window of the target and can outperform ENegEx.","2292":"High-resolution nowcasting is an essential tool needed for effective\r\nadaptation to climate change, particularly for extreme weather. As Deep\r\nLearning (DL) techniques have shown dramatic promise in many domains, including\r\nthe geosciences, we present an application of DL to the problem of\r\nprecipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1\r\nhour) predictions of precipitation. We treat forecasting as an image-to-image\r\ntranslation problem and leverage the power of the ubiquitous UNET convolutional\r\nneural network. We find this performs favorably when compared to three commonly\r\nused models: optical flow, persistence and NOAA's numerical one-hour HRRR\r\nnowcasting prediction.","2293":"Over the past decade, unprecedented progress in the development of neural\r\nnetworks influenced dozens of different industries, including weed recognition\r\nin the agro-industrial sector. The use of neural networks in agro-industrial\r\nactivity in the task of recognizing cultivated crops is a new direction. The\r\nabsence of any standards significantly complicates the understanding of the\r\nreal situation of the use of the neural network in the agricultural sector. The\r\nmanuscript presents the complete analysis of researches over the past 10 years\r\non the use of neural networks for the classification and tracking of weeds due\r\nto neural networks. In particular, the analysis of the results of using various\r\nneural network algorithms for the task of classification and tracking was\r\npresented. As a result, we presented the recommendation for the use of neural\r\nnetworks in the tasks of recognizing a cultivated object and weeds. Using this\r\nstandard can significantly improve the quality of research on this topic and\r\nsimplify the analysis and understanding of any paper.","2294":null,"2295":null,"2296":"The gene expression process in nature involves several\n                 representation transformations of the genome.\n                 Translation is one among them; it constructs the amino\n                 acid sequence in proteins from the nucleic acid-based\n                 mRNA sequence. Translation is defined by a code book,\n                 known as the universal genetic code. This paper\n                 explores the role of genetic code and similar\n                 representation transformations for enhancing the\n                 performance of inductive machine learning algorithms.\n                 It considers an abstract model of genetic code-like\n                 transformations (GCTs) introduced elsewhere 21 and\n                 develops the notion of randomised GCTs. It shows that\n                 randomized GCTs can construct a representation of the\n                 learning problem where the mean-square-error surface is\n                 almost convex quadratic and therefore easier to\n                 minimise. It considers the functionally complete\n                 Fourier representation of Boolean functions to analyse\n                 this effect of such representation transformations. It\n                 offers experimental results to substantiate this claim.\n                 It shows that a linear classifier like the Perceptron\n                 38 can learn non-linear XOR and DNF functions using a\n                 gradient-descent algorithm in a representation\n                 constructed by randomized GCTs. The paper also\n                 discusses the immediate challenges that must be solved\n                 before the proposed technique can be used as a viable\n                 approach for representation construction in machine\n                 learning.","2297":"Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method\u2014Deep Support Vector Data Description\u2014, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.","2298":null,"2299":null,"2300":null,"2301":null,"2302":null,"2303":null,"2304":"The internet community has been benefitting tremendously from the works of various researchers in the field of Natural Language Processing. Semantic orientation analysis, sentiment analysis, etc. has served the social networks as well as companies relying on user reviews well. Flame identification has made the internet less hostile for some users. Spam filtering has made the electronic mail a more efficient means of communication. But with the incessant growth of the internet, NLP using machine learning working on massive sets of raw and unprocessed data is an ever-growing challenge. Semi-supervised machine learning can overcome this problem by using a large set of unlabeled data in conjunction with a small set of labeled data. Also, focusing on developing NLP systems that can contribute to developing a unified architecture could pave the way towards General Intelligence in the future.","2305":null,"2306":null,"2307":null,"2308":null,"2309":null,"2310":null,"2311":"Big data validation and system verification are crucial for ensuring the quality of big data applications. However, a rigorous technique for such tasks is yet to emerge. During the past decade, we have developed a big data system called CMA for investigating the classification of biological cells based on cell morphology that is captured in diffraction images. CMA includes a group of scientific software tools, machine learning algorithms, and a large scale cell image repository. We have also developed a framework for rigorous validation of the massive scale image data and verification of both the software systems and machine learning algorithms. Different machine learning algorithms integrated with image processing techniques were used to automate the selection and validation of the massive scale image data in CMA. An experiment based technique guided by a feature selection algorithm was introduced in the framework to select optimal machine learning features. An iterative metamorphic testing approach is applied for testing the scientific software. Due to the non-testable characteristic of the scientific software, a machine learning approach is introduced for developing test oracles iteratively to ensure the adequacy of the test coverage criteria. Performance of the machine learning algorithms is evaluated with the stratified N-fold cross validation and confusion matrix. We describe the design of the proposed framework with CMA as the case study. The effectiveness of the framework is demonstrated through verifying and validating the data set, software systems and algorithms in CMA.","2312":"Laut Vorhersagen 1 soll mobiles Video Streaming im Jahr 2021 etwa dreiviertel des gesamten Internetverkehrs ausmachen. Schon in der heutigen Zeit nimmt es bereits einen gro\u00dfen Teil ein. Dies f\u00fchrt dazu, dass Netzwerkprovider herausfinden m\u00fcssen, wie sie mit der Masse an Verkehr umgehen sollen, um gleichzeitig ihre Kunden zufrieden stellen zu k\u00f6nnen. Eine Metrik, um die Nutzerzufriedenheit festzustellen, ist Quality of Experience (QoE). Mit Hilfe von QoE-Management k\u00f6nnen Provider anhand des Datenverkehrs die Zufriedenheit der Kunden beeinflussen und trotzdem die Kosteneffizienz zu wahren. Ein wichtiger Teil vom QoE-Management ist das QoE-Monitoring. Da allerdings der Netzwerkverkehr verschl\u00fcsselt ist und deshalb deep packet inspection nicht mehr verwendet werden kann, muss auf statistische Analysen zur\u00fcck gegriffen werden. Das Ziel der Arbeit ist es, wichtige QoE-Einflussfaktoren f\u00fcr Mobile Video Stream- ing, speziell am Beispiel YouTube, zu finden mit Hilfe von Machine Learning (ML). Um die Daten zu sammeln, wurde ein Versuchsaufbau bestehend aus einer Linux- Maschine und einem Smartphone benutzt, wobei der PC als Router f\u00fcr das Smartphone zum Internet diente. Auf dem Smartphone lief die originale YouTube-App, welche auch im Google Play Store erh\u00e4ltlich ist, sowie eine Wrapper-App f\u00fcr das Monitoring. Diese kann autonom die YouTube-App starten und Videos abspielen. W\u00e4hrend ein Video l\u00e4uft werden Netzwerk- und Anwendungsinformationen gesam- melt und auf dem Ger\u00e4t in einer separaten Datei gespeichert. Mit diesen Daten konnten drei verschiedene Datens\u00e4tze gemessen werden, wobei zwei zur QoE-Vorhersage f\u00fcr initial delay und der dritte Datensatz zur QoE-Vorhersage for stalling events verwendet wurden. Der erste Datensatz bestand aus drei ver- schiedenen Videos und 1070 Instanzen, der zweite Datensatz aus 18 Videos und 1400 Instanzen und der letzte Datensatz aus 930 Instanzen sowie drei Videos. Des Weiteren wurden die Videos mit verschiedenen Bandbreitenbegrenzungen abgespielt, welche von 0.5 Mbits\/s bis 5 Mbits\/s reichten. Um die Datens\u00e4tze mit ML klas- sifizieren zu k\u00f6nnen, mussten die Instanzen zuvor in vordefinierte Klassen anhand ihres initial delay oder ihrer Anzahl an stallings aufgeteilt werden. Versuche mit dem ersten Datensatz zeigten, dass QoE-Sch\u00e4tzungen mit drei Videos nicht aussagekr\u00e4ftig genug sind. Anhand der Ergebnisse mit dem zweiten Datensatz konnten aber folgende wichtige Einflussparameter festgestellt werden: der gesamte Durchsatz von Up- und Downlink, sowie der Durchsatz in den ersten 20 Sekunden. Einflussreiche Parameter sind au\u00dferdem der Durchsatz der ersten f\u00fcnf Segmente, welche nicht leer sind, und deren Gr\u00f6\u00dfe. Im Anschluss daran wurden weitere QoE-Vorhersagen mit verschiedenen Klassen- aufteilungen gemacht. Es stellte sich heraus, dass die Genauigkeit der Vorhersagen mit steigender Klassenzahl stark nachl\u00e4sst. Des Weiteren wurden Sch\u00e4tzungen mit Klassenaufteilungen aus anderen QoE-Studien auf unsere Datens\u00e4tze angewendet, welche allerdings keine weiteren Erkenntnisse lieferten. Zum Schluss wurden QoE-Sch\u00e4tzungen mit dem dritten Datensatz f\u00fcr stalling gemacht und die daraus folgenden Ergebnisse mit den Resultaten der QoE-Vorhersagen mit dem zweiten Datensatz verglichen. Die Faktoren, welche zuvor bei den QoE- Sch\u00e4tzungen f\u00fcr initial delay als wichtige QoE-Einflussfaktoren galten, erwiesen sich bei den QoE-Sch\u00e4tzungen f\u00fcr stalling als nicht einflussreich. Weitere QoE- Vorhersagen zeigten, dass die Anzahl an Qualit\u00e4ts\u00e4nderungen w\u00e4hrend des Videos, sowie die mittlere Ankuftszeit und die kumulative Verteilung der Ankunftszeit der Uplink Requests, gro\u00dfen Einfluss auf die QoE haben.","2313":null,"2314":null,"2315":null,"2316":null,"2317":"Compiler writers are expected to create effective and\n                 inexpensive solutions to NP-hard problems such as\n                 instruction scheduling and register allocation. To make\n                 matters worse, separate optimisation phases have strong\n                 interactions and competing resource constraints.\n                 Compiler writers deal with system complexity by\n                 dividing the problem into multiple phases and devising\n                 approximate heuristics for each phase. However, to\n                 achieve satisfactory performance, developers are forced\n                 to manually tweak their heuristics with trial-and-error\n                 experimentation.\n\n                 In this dissertation I present meta optimization, a\n                 methodology for automatically constructing high quality\n                 compiler heuristics using machine learning techniques.\n                 This thesis describes machine-learned heuristics for\n                 three important compiler optimisations: hyperblock\n                 formation, register allocation, and loop unrolling. The\n                 machine-learned heuristics outperform (by as much as 3x\n                 in some cases) their state-of-the-art hand-crafted\n                 counterparts. By automatically collecting data and\n                 systematically analysing them, my techniques discover\n                 subtle interactions that even experienced engineers\n                 would likely overlook. In addition to improving\n                 performance, my techniques can significantly reduce the\n                 human effort involved in compiler design. Machine\n                 learning algorithms can design critical portions of\n                 compiler heuristics, thereby freeing the human designer\n                 to focus on compiler correctness.\n\n                 The progression of experiments I conduct in this thesis\n                 leads to collaborative compilation, an approach which\n                 enables ordinary users to transparently train compiler\n                 heuristics by running their applications as they\n                 normally would. The collaborative system automatically\n                 adapts itself to the applications in which a community\n                 of users is interested.","2318":"This paper presents DistRDF2ML, the generic, scalable, and distributed framework for creating in-memory data preprocessing pipelines for Spark-based machine learning on RDF knowledge graphs. This framework introduces software modules that transform large-scale RDF data into ML-ready fixed-length numeric feature vectors. The developed modules are optimized to the multi-modal nature of knowledge graphs. DistRDF2ML provides aligned software design and usage principles as common data science stacks that offer an easy-to-use package for creating machine learning pipelines. The modules used in the pipeline, the hyper-parameters and the results are exported as a semantic structure that can be used to enrich the original knowledge graph. The semantic representation of metadata and machine learning results offers the advantage of increasing the machine learning pipelines' reusability, explainability, and reproducibility. The entire framework of DistRDF2ML is open source, integrated into the holistic SANSA stack, documented in scala-docs, and covered by unit tests. DistRDF2ML demonstrates its scalable design across different processing power configurations and (hyper-)parameter setups within various experiments. The framework brings the three worlds of knowledge graph engineers, distributed computation developers, and data scientists closer together and offers all of them the creation of explainable ML pipelines using a few lines of code.","2319":null,"2320":"There are considerably more breast cancer fatalities each year. The most common kind of cancer and the main cause of death in women worldwide is this one. A healthy life depends on every development in the prognosis and diagnosis of cancer sickness. The standard of treatment and patient survival rate must be updated, thus an accurate cancer prognosis is crucial. Research has demonstrated that machine learning approaches are effective for the early detection and prediction of breast cancer and have grown in popularity. Random Forest, Logistic Regression, Xtreme Gradient, and AdaBoost Classifier are trained on the Breast Cancer Wisconsin Diagnostic dataset, and their efficacy is assessed and compared in this study using ensemble classifier and machine learning. The major objective of this study is to identify the most effective ensemble and machine learning classifiers for breast cancer detection and diagnosis in terms of Accuracy and AUC Score.","2321":null,"2322":"One or more machine code entities such as functions\n                 are created which represent solutions to a problem and\n                 are directly executable by a computer. The programs are\n                 created and altered by a program in a higher level\n                 language such as \"C\" which is not directly\n                 executable, but requires translation into executable\n                 machine code through compilation, interpretation,\n                 translation, etc. The entities are initially created as\n                 an integer array that can be altered by the program as\n                 data, and are executed by the program by recasting a\n                 pointer to the array as a function type. The entities\n                 are evaluated by executing them with training data as\n                 inputs, and calculating fitnesses based on a\n                 predetermined criterion. The entities are then altered\n                 based on their fitnesses using a machine learning\n                 algorithm by recasting the pointer to the array as a\n                 data (e.g. integer) type. This process is iteratively\n                 repeated until an end criterion is reached. The\n                 entities evolve in such a manner as to improve their\n                 fitness, and one entity is ultimately produced which\n                 represents an optimal solution to the problem. Each\n                 entity includes a plurality of directly executable\n                 machine code instructions, a header, a footer, and a\n                 return instruction. The instructions include branch\n                 instructions which enable subroutines, leaf functions,\n                 external function calls, recursion, and loops. The\n                 system can be implemented on an integrated circuit\n                 chip, with the entities stored in high speed memory in\n                 a central processing unit.","2323":null,"2324":"We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.","2325":"In many online applications of machine learning, the computational resources available for classification will vary from time to time. Most techniques are designed to operate within the constraints of the minimum expected resources and fail to utilize further resources when they are available. We propose a novel anytime classification algorithm, anytime averaged probabilistic estimators (AAPE), which is capable of delivering strong prediction accuracy with little CPU time and utilizing additional CPU time to increase classification accuracy. The idea is to run an ordered sequence of very efficient Bayesian probabilistic estimators (single improvement steps) until classification time runs out. Theoretical studies and empirical validations reveal that by properly identifying, ordering, invoking and ensembling single improvement steps, AAPE is able to accomplish accurate classification whenever it is interrupted. It is also able to output class probability estimates beyond simple 0\/1-loss classifications, as well as adeptly handle incremental learning.","2326":null,"2327":"A novel computational methodology for large-scale screening of MOFs is applied to gas storage with the use of machine learning technologies. This approach is a promising trade-off between the accuracy of ab initio methods and the speed of classical approaches, strategically combined with chemical intuition. The results demonstrate that the chemical properties of MOFs are indeed predictable (stochastically, not deterministically) using machine learning methods and automated analysis protocols, with the accuracy of predictions increasing with sample size. Our initial results indicate that this methodology is promising to apply not only to gas storage in MOFs but in many other material science projects.","2328":null,"2329":null,"2330":"Hydrothermal carbonization (HTC) is a process that converts biomass into\r\nversatile hydrochar without the need for prior drying. The physicochemical\r\nproperties of hydrochar are influenced by biomass properties and processing\r\nparameters, making it challenging to optimize for specific applications through\r\ntrial-and-error experiments. To save time and money, machine learning can be\r\nused to develop a model that characterizes hydrochar produced from different\r\nbiomass sources under varying reaction processing parameters. Thus, this study\r\naims to develop an inclusive model to characterize hydrochar using a database\r\ncovering a range of biomass types and reaction processing parameters. The\r\nquality and quantity of hydrochar are predicted using two models (decision tree\r\nregression and support vector regression). The decision tree regression model\r\noutperforms the support vector regression model in terms of forecast accuracy\r\n(R2 > 0.88, RMSE < 6.848, and MAE < 4.718). Using an evolutionary algorithm,\r\noptimum inputs are identified based on cost functions provided by the selected\r\nmodel to optimize hydrochar for energy production, soil amendment, and\r\npollutant adsorption, resulting in hydrochar yields of 84.31%, 84.91%, and\r\n80.40%, respectively. The feature importance analysis reveals that biomass\r\nash\/carbon content and operating temperature are the primary factors affecting\r\nhydrochar production in the HTC process.","2331":"In this paper, we propose two new neuro-fuzzy schemes, one for classification\r\n\tand one for clustering problems. The classification scheme is based\r\n\ton Simpson's fuzzy min-max method (1992, 1993) and relaxes some assumptions\r\n\the makes. This enables our scheme to handle mutually nonexclusive\r\n\tclasses. The neuro-fuzzy clustering scheme is a multiresolution algorithm\r\n\tthat is modeled after the mechanics of human pattern recognition.\r\n\tWe also present data from an exhaustive comparison of these techniques\r\n\twith neural, statistical, machine learning, and other traditional\r\n\tapproaches to pattern recognition applications. The data sets used\r\n\tfor comparisons include those from the machine learning repository\r\n\tat the University of California, Irvine. We find that our proposed\r\n\tschemes compare quite well with the existing techniques, and in addition\r\n\toffer the advantages of one-pass learning and online adaptation","2332":"The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.","2333":"Harmful algal blooms, which are considered a serious\n                 environmental problem nowadays, occur in coastal waters\n                 in many parts of the world. They cause acute ecological\n                 damage and ensuing economic losses, due to fish kills\n                 and shellfish poisoning as well as public health\n                 threats posed by toxic blooms. Recently, data-driven\n                 models including machine-learning (ML) techniques have\n                 been employed to mimic dynamics of algal blooms. One of\n                 the most important steps in the application of a ML\n                 technique is the selection of significant model input\n                 variables. In the present paper, we use two extensively\n                 used ML techniques, artificial neural networks (ANN)\n                 and genetic programming (GP) for selecting the\n                 significant input variables. The efficacy of these\n                 techniques is first demonstrated on a test problem with\n                 known dependence and then they are applied to a\n                 real-world case study of water quality data from Tolo\n                 Harbour, Hong Kong. These ML techniques overcome some\n                 of the limitations of the currently used techniques for\n                 input variable selection, a review of which is also\n                 presented. The interpretation of the weights of the\n                 trained ANN and the GP evolved equations demonstrate\n                 their ability to identify the ecologically significant\n                 variables precisely. The significant variables\n                 suggested by the ML techniques also indicate\n                 chlorophyll-a (Chl-a) itself to be the most significant\n                 input in predicting the algal blooms, suggesting an\n                 auto-regressive nature or persistence in the algal\n                 bloom dynamics, which may be related to the long\n                 flushing time in the semi-enclosed coastal waters. The\n                 study also confirms the previous understanding that the\n                 algal blooms in coastal waters of Hong Kong often occur\n                 with a life cycle of the order of 1-2 weeks.","2334":"We are well recognised that the vast majority of Indians work in agriculture. Most farmers always grow the same thing, always use the same amount of fertilizer, and always plant what the people want. Recently, there have been many breakthroughs in the use of machine learning in many fields of study and business. Thus, we intend to establish a framework for the application of machine learning in agriculture for the benefit of farmers. India's economy relies heavily on the nation's agricultural output. Agriculture, then, has the potential to serve as the backbone of the Philippine economy. Choosing the right crop every time is crucial when making agricultural plans. Researchers have utilised machine learning to explore agricultural issues such as crop yield, weather prediction, soil categorization, and crop labelling. Our Indian economy really needs the agricultural sector to undergo significant reforms. Simple applications of machine learning systems in agriculture have the potential to significantly enhance this industry. In addition to the considerable role played by improvements in farming machinery and technology, functional information about many subjects also plays an important role. The main idea of this study is to use the crop selection approach in order to address numerous issues in farming. This increases the wealth of India by increasing crop yields to their maximum potential. In our study, we use the method Random Forest (RF) to estimate a crop and then evaluate its performance relative to that of competing techniques","2335":"Due to big data progress in biomedical and healthcare communities, accurate study of medical data benefits early disease recognition, patient care and community services. When the quality of medical data is incomplete the exactness of study is reduced. Moreover, different regions exhibit unique appearances of certain regional diseases, which may results in weakening the prediction of disease outbreaks. In the proposed system, it provides machine learning algorithms for effective prediction of various disease occurrences in disease-frequent societies. It experiment the altered estimate models over real-life hospital data collected. To overcome the difficulty of incomplete data, it use a latent factor model to rebuild the missing data. It experiment on a regional chronic illness of cerebral infarction. Using structured and unstructured data from hospital it use Machine Learning Decision Tree algorithm and Map Reduce algorithm. To the best of our knowledge in the area of medical big data analytics none of the existing work focused on both data types. Compared to several typical estimate algorithms, the calculation exactness of our proposed algorithm reaches 94.8% with a convergence speed which is faster than that of the CNNbased unimodal disease risk prediction (CNN-UDRP) algorithm.","2336":"We present ReGroup, a novel end-user interactive machine learning system for helping people create custom, on demand groups in online social networks. As a person adds members to a group, ReGroup iteratively learns a probabilistic model of group membership specific to that group. ReGroup then uses its currently learned model to suggest additional members and group characteristics for filtering. Our evaluation shows that ReGroup is effective for helping people create large and varied groups, whereas traditional methods (searching by name or selecting from an alphabetical list) are better suited for small groups whose members can be easily recalled by name. By facilitating on demand group creation, ReGroup can enable in-context sharing and potentially encourage better online privacy practices. In addition, applying interactive machine learning to social network group creation introduces several challenges for designing effective end-user interaction with machine learning. We identify these challenges and discuss how we address them in ReGroup.","2337":"Automatic Speech Recognition (ASR) has historically\n                 been a driving force behind many machine learning (ML)\n                 techniques, including the ubiquitously used hidden\n                 Markov model, discriminative learning, structured\n                 sequence learning, Bayesian learning, and adaptive\n                 learning. Moreover, ML can and occasionally does use\n                 ASR as a large-scale, realistic application to\n                 rigorously test the effectiveness of a given technique,\n                 and to inspire new problems arising from the inherently\n                 sequential and dynamic nature of speech. On the other\n                 hand, even though ASR is available commercially for\n                 some applications, it is largely an unsolved problem -\n                 for almost all applications, the performance of ASR is\n                 not on par with human performance. New insight from\n                 modern ML methodology shows great promise to advance\n                 the state-of-the-art in ASR technology. This overview\n                 article provides readers with an overview of modern ML\n                 techniques as utilized in the current and as relevant\n                 to future ASR research and systems. The intent is to\n                 foster further cross-pollination between the ML and ASR\n                 communities than has occurred in the past. The article\n                 is organized according to the major ML paradigms that\n                 are either popular already or have potential for making\n                 significant contributions to ASR technology. The\n                 paradigms presented and elaborated in this overview\n                 include: generative and discriminative learning;\n                 supervised, unsupervised, semi-supervised, and active\n                 learning; adaptive and multi-task learning; and\n                 Bayesian learning. These learning paradigms are\n                 motivated and discussed in the context of ASR\n                 technology and applications. We finally present and\n                 analyze recent developments of deep learning and\n                 learning with sparse representations, focusing on their\n                 direct relevance to advancing ASR technology.","2338":"Three-point and high-order clustering statistics of the high-redshift 21cm\r\nsignal contain valuable information about the Epoch of Reionization. We present\r\n3PCF-Fast, an optimised code for estimating the three-point correlation\r\nfunction of 3D pixelised data such as the outputs from numerical and\r\nsemi-numerical simulations. After testing 3PCF-Fast on data with known analytic\r\nthree-point correlation function, we use machine learning techniques to recover\r\nthe mean bubble size and global ionisation fraction from correlations in the\r\noutputs of the publicly available 21cmFAST code. We assume that foregrounds\r\nhave been perfectly removed and negligible instrumental noise. Using ionisation\r\nfraction data, our best MLP model recovers the mean bubble size with a median\r\nprediction error of around 10%, or from the 21cm differential brightness\r\ntemperature with median prediction error of around 14%. A further two MLP\r\nmodels recover the global ionisation fraction with median prediction errors of\r\naround 4% (using ionisation fraction data) or around 16% (using brightness\r\ntemperature). Our results indicate that clustering in both the ionisation\r\nfraction field and the brightness temperature field encode useful information\r\nabout the progress of the Epoch of Reionization in a complementary way to other\r\nsummary statistics. Using clustering would be particularly useful in regimes\r\nwhere high signal-to-noise ratio prevents direct measurement of bubble size\r\nstatistics. We compare the quality of MLP models using the power spectrum, and\r\nfind that using the three-point correlation function outperforms the power\r\nspectrum at predicting both global ionisation fraction and mean bubble size.","2339":"Background: The concept of boosting emerged from the field of machine learning.\nThe basic idea is to boost the accuracy of a weak classifying tool by combining various instances into a more accurate prediction. This general concept was later adapted to the field of statistical modelling. Nowadays, boosting algorithms are often applied to estimate and select predictor effects in statistical regression models.\nObjectives: This review article attempts to highlight the evolution of boosting algorithms from machine learning to statistical modelling.\nMethods: We describe the AdaBoost algorithm for classification as well as the two most prominent statistical boosting approaches, gradient boosting and likelihood-based\nboosting for statistical modelling. We highlight the methodological background and present the most common software implementations.\nResults: Although gradient boosting and likelihood-based boosting are typically\ntreated separately in the literature, they share the same methodological roots and follow the same fundamental concepts. Compared to the initial machine learning algorithms, which must be seen as black-box prediction schemes, they result in statistical models with a straight-forward interpretation.\nConclusions: Statistical boosting algorithms have gained substantial interest during the last decade and offer a variety of options to address important research questions in modern biomedicine.","2340":null,"2341":"Machine learning algorithms, when applied to sensitive data, pose a distinct\r\nthreat to privacy. A growing body of prior work demonstrates that models\r\nproduced by these algorithms may leak specific private information in the\r\ntraining data to an attacker, either through the models' structure or their\r\nobservable behavior. However, the underlying cause of this privacy risk is not\r\nwell understood beyond a handful of anecdotal accounts that suggest overfitting\r\nand influence might play a role.\r\n  This paper examines the effect that overfitting and influence have on the\r\nability of an attacker to learn information about the training data from\r\nmachine learning models, either through training set membership inference or\r\nattribute inference attacks. Using both formal and empirical analyses, we\r\nillustrate a clear relationship between these factors and the privacy risk that\r\narises in several popular machine learning algorithms. We find that overfitting\r\nis sufficient to allow an attacker to perform membership inference and, when\r\nthe target attribute meets certain conditions about its influence, attribute\r\ninference attacks. Interestingly, our formal analysis also shows that\r\noverfitting is not necessary for these attacks and begins to shed light on what\r\nother factors may be in play. Finally, we explore the connection between\r\nmembership inference and attribute inference, showing that there are deep\r\nconnections between the two that lead to effective new attacks.","2342":"Rapidly discovering functional materials remains an open challenge because the traditional trial-and-error methods are usually inefficient especially when thousands of candidates are treated. Here, we develop a target-driven method to predict undiscovered hybrid organic-inorganic perovskites (HOIPs) for photovoltaics. This strategy, combining machine learning techniques and density functional theory calculations, aims to quickly screen the HOIPs based on bandgap and solve the problems of toxicity and poor environmental stability in HOIPs. Successfully, six orthorhombic lead-free HOIPs with proper bandgap for solar cells and room temperature thermal stability are screened out from 5158 unexplored HOIPs and two of them stand out with direct bandgaps in the visible region and excellent environmental stability. Essentially, a close structure-property relationship mapping the HOIPs bandgap is established. Our method can achieve high accuracy in a flash and be applicable to a broad class of functional material design.","2343":"Online communities are attractive sources of ideas relevant for new product development and innovation. However, making sense of the \u2018big data\u2019 in these communities is a complex analytical task. A systematic way of dealing with these data is needed to exploit their potential for boosting companies' innovation performance. We propose a method for analysing online community data with a special focus on identifying ideas. We employ a research design where two human raters classified 3,000 texts extracted from an online community, according to whether the text contained an idea. Among the 3,000, 137 idea texts and 2,666 non-idea texts were identified. The human raters could not agree on the remaining 197 texts. These texts were omitted from the analysis. The remaining 2,803 texts were processed by using text mining techniques and used to train a classification model. We describe how to tune the model and which text mining steps to perform. We conclude that machine learning and text mining can be useful for detecting ideas in online communities. The method can help researchers and firms identify ideas hidden in large amounts of texts. Also, it is interesting in its own right that machine learning can be used to detect ideas.","2344":null,"2345":null,"2346":null,"2347":null,"2348":null,"2349":null,"2350":null,"2351":"In designing a spoken dialogue system, developers need to specify the actions a system should take in response to user speech input and the state of the environment based on observed or inferred events, states, and beliefs. This is the fundamental task of dialogue management. Researchers have recently pursued methods for automating the design of spoken dialogue management using machine learning techniques such as reinforcement learning. In this paper, we discuss how dialogue management is handled in industry and critically evaluate to what extent current state-of-the-art machine learning methods can be of practical benefit to application developers who are deploying commercial production systems. In examining the strengths and weaknesses of these methods, we highlight what academic researchers need to know about commercial deployment if they are to influence the way industry designs and practices dialogue management.","2352":"Author Summary The genetic basis of recent adaptation can be uncovered from genomic patterns of variation, which are perturbed in predictable ways when a beneficial mutation \u201csweeps\u201d through a population. However, the detection of such \u201cselective sweeps\u201d is complicated by demographic events, such as population expansion, which can produce similar skews in genetic diversity. Here, we present a method for detecting selective sweeps that is remarkably powerful and robust to potentially confounding demographic histories. This method, called S\/HIC, operates using a machine learning paradigm to combine many different features of population genetic variation, and examine their values across a large genomic region in order to infer whether a selective sweep has recently occurred near its center. S\/HIC is also able to accurately distinguish between selection acting on de novo beneficial mutations (\u201chard sweeps\u201d) and selection on previously standing variants (\u201csoft sweeps\u201d). We demonstrate S\/HIC\u2019s power on a variety of simulated datasets as well as human population data wherein we recover several previously discovered targets of recent adaptation as well as a novel selective sweep.","2353":null,"2354":"This paper brings together two strands of machine learning of increasing importance: kernel methods and highly structured data. We propose a general method for constructing a kernel following the syntactic structure of the data, as defined by its type signature in a higher-order logic. Our main theoretical result is the positive definiteness of any kernel thus defined. We report encouraging experimental results on a range of real-world data sets. By converting our kernel to a distance pseudo-metric for 1-nearest neighbour, we were able to improve the best accuracy from the literature on the Diterpene data set by more than 10%.","2355":null,"2356":null,"2357":null,"2358":"We use techniques from sample-complexity in machine learning to reduce\nproblems of incentive-compatible mechanism design to standard algorithmic\nquestions, for a wide variety of revenue-maximizing pricing problems.\nOur reductions imply that for these problems, given an optimal (or\n#-approximation) algorithm for the standard algorithmic problem,\nwe can convert it into a (1 + #)-approximation (or #(1+#)-approximation)\nfor the incentive-compatible mechanism design problem, so long as\nthe number of bidders is sufficiently large as a function of an appropriate\nmeasure of complexity of the comparison class of solutions. We apply\nthese results to the problem of auctioning a digital good, the attribute\nauction problem, and to the problem of itempricing in unlimited-supply\ncombinatorial auctions. From a learning perspective, these settings\npresent several challenges: in particular, the loss function is discontinuous\nand asymmetric, and the range of bidders' valuations may be large.","2359":null,"2360":"DNA-binding proteins are a class of proteins which have a specific or general affinity to DNA and include three important components: transcription factors; nucleases, and histones. DNA-binding proteins also perform important roles in many types of cellular activities. In this paper we describe machine learning systems for the prediction of DNA- binding proteins where a Support Vector Machine and a Cascade Correlation Neural Network are optimized and then compared to determine the learning algorithm that achieves the best prediction performance. The information used for classification is derived from characteristics that include overall charge, patch size and amino acids composition. In total 121 DNA- binding proteins and 238 non-binding proteins are used to build and evaluate the system. For SVM using the ANOVA Kernel with Jack-knife evaluation, an accuracy of 86.7% has been achieved with 91.1% for sensitivity and 85.3% for specificity. For CCNN optimized over the entire dataset with Jack\r\nknife evaluation we report an accuracy of 75.4%, while the values of specificity and sensitivity achieved were 72.3% and 82.6%, respectively.","2361":null,"2362":"Today many schools, universities and institutions recognize the necessity and importance of using Learning Management Systems (LMS) as part of their educational services. This research work has applied LMS in the teaching and learning process of Bulacan State University (BulSU) Graduate School (GS) Program that enhances the face-to-face instruction with online components. The researchers uses an LMS that provides educators a platform that can motivate and engage students to new educational environment through manage online classes. The LMS allows educators to distribute information, manage learning materials, assignments, quizzes, and communications. Aside from the basic functions of the LMS, the researchers uses Machine Learning (ML) Algorithms applying Support Vector Machine (SVM) that will classify and identify the best related videos per topic. SVM is a supervised machine learning algorithm that analyzes data for classification and regression analysis by Maity 1. The results of this study showed that integration of video tutorials in LMS can significantly contribute knowledge and skills in the learning process of the students.","2363":null,"2364":null,"2365":null,"2366":"Multiple sclerosis disease is a main cause of non-traumatic disabilities and one of the most common neurological disorders in young adults over many countries. In this work, we introduce a survey study of the utilization of machine learning methods in Multiple Sclerosis early genetic disease detection methods incorporating Microarray data analysis and Single Nucleotide Polymorphism data analysis and explains in details the machine learning methods used in literature. In addition, this study demonstrates the future trends of Next Generation Sequencing data analysis in disease detection and sample datasets of each genetic detection method was included .in addition, the challenges facing genetic disease detection were elaborated.","2367":"One of the challenges in machine learning research is to ensure that\r\npresented and published results are sound and reliable. Reproducibility, that\r\nis obtaining similar results as presented in a paper or talk, using the same\r\ncode and data (when available), is a necessary step to verify the reliability\r\nof research findings. Reproducibility is also an important step to promote open\r\nand accessible research, thereby allowing the scientific community to quickly\r\nintegrate new findings and convert ideas to practice. Reproducibility also\r\npromotes the use of robust experimental workflows, which potentially reduce\r\nunintentional errors. In 2019, the Neural Information Processing Systems\r\n(NeurIPS) conference, the premier international conference for research in\r\nmachine learning, introduced a reproducibility program, designed to improve the\r\nstandards across the community for how we conduct, communicate, and evaluate\r\nmachine learning research. The program contained three components: a code\r\nsubmission policy, a community-wide reproducibility challenge, and the\r\ninclusion of the Machine Learning Reproducibility checklist as part of the\r\npaper submission process. In this paper, we describe each of these components,\r\nhow it was deployed, as well as what we were able to learn from this\r\ninitiative.","2368":null,"2369":null,"2370":null,"2371":null,"2372":null,"2373":"We analyze the growth of dataset sizes used in machine learning for natural\r\nlanguage processing and computer vision, and extrapolate these using two\r\nmethods; using the historical growth rate and estimating the compute-optimal\r\ndataset size for future predicted compute budgets. We investigate the growth in\r\ndata usage by estimating the total stock of unlabeled data available on the\r\ninternet over the coming decades. Our analysis indicates that the stock of\r\nhigh-quality language data will be exhausted soon; likely before 2026. By\r\ncontrast, the stock of low-quality language data and image data will be\r\nexhausted only much later; between 2030 and 2050 (for low-quality language) and\r\nbetween 2030 and 2060 (for images). Our work suggests that the current trend of\r\never-growing ML models that rely on enormous datasets might slow down if data\r\nefficiency is not drastically improved or new sources of data become available.","2374":null,"2375":"The rise of antibiotic resistance in pathogenic bacteria is a significant problem for the treatment of infectious diseases. Resistance is usually selected by the antibiotic itself; however, biocides might also co-select for resistance to antibiotics. Although resistance to biocides is poorly defined, different in vitro studies have shown that mutants presenting low susceptibility to biocides also have reduced susceptibility to antibiotics. However, studies with natural bacterial isolates are more limited and there are no clear conclusions as to whether the use of biocides results in the development of multidrug resistant bacteria.The main goal is to perform an unbiased blind-based evaluation of the relationship between antibiotic and biocide reduced susceptibility in natural isolates of Staphylococcus aureus. One of the largest data sets ever studied comprising 1632 human clinical isolates of S. aureus originated worldwide was analysed. The phenotypic characterization of 13 antibiotics and 4 biocides was performed for all the strains. Complex links between reduced susceptibility to biocides and antibiotics are difficult to elucidate using the standard statistical approaches in phenotypic data. Therefore, machine learning techniques were applied to explore the data.In this pioneer study, we demonstrated that reduced susceptibility to two common biocides, chlorhexidine and benzalkonium chloride, which belong to different structural families, is associated to multidrug resistance. We have consistently found that a minimum inhibitory concentration greater than 2 mg\/L for both biocides is related to antibiotic non-susceptibility in S. aureus.Two important results emerged from our work, one methodological and one other with relevance in the field of antibiotic resistance. We could not conclude on whether the use of antibiotics selects for biocide resistance or vice versa. However, the observation of association between multiple resistance and two biocides commonly used may be of concern for the treatment of infectious diseases in the future.","2376":"Detecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs. In this work, we show that this issue can be mitigated through Logit Normalization (LogitNorm)\u2014a simple fix to the cross-entropy loss\u2014by enforcing a constant vector norm on the logits in training. Our method is motivated by the analysis that the norm of the logit keeps increasing during training, leading to overconfident output. Our key idea behind LogitNorm is thus to decouple the influence of output\u2019s norm during network optimization. Trained with LogitNorm, neural networks produce highly distinguishable confidence scores between in- and out-of-distribution data. Extensive experiments demonstrate the superiority of LogitNorm, reducing the average FPR95 by up to 42.30% on common benchmarks.","2377":"Due to the industry 4.0 revolution massive automation is being carried out all over. One important aspect of this revolution\r\nis to determine the Tool wear must. This is due to the fact that a worn tool leads to reduced work piece surface quality,\r\nhigher production costs and times. Tool wear is non-linear and depends on many influencing variables. In practice, the\r\noptical determination of tool wear is a time-consuming and costly process, and the quality of manual wear determination is\r\nprone to errors. Our approach is to apply machine learning and deep learning technologies to make this process\r\neconomical and free of human errors.","2378":null,"2379":"Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures.  We focus in this paper on Sinkhorn divergences (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength $\\varepsilon$, between OT ($\\varepsilon=0$) and MMD ($\\varepsilon=\u0131nfty$). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively $O(n^3n)$, $O(n^2)$ and $n^2$ operations given a sample size $n$), much less is known in terms of their sample complexity, namely the gap between these quantities, when evaluated using finite samples vs. their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, $1\/n^1\/d$ for OT in dimension $d$ and $1\/n$ for MMD, that for SDs has only been studied empirically. In this paper, we (i) derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer $\\varepsilon$, (ii) prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and (iii) provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in $1\/n$ (as in MMD), with a constant that depends however on $\\varepsilon$, making the bridge between OT and MMD complete.","2380":null,"2381":"Monitoring the Quality of Experience (QoE) undergone by cellular network customers has become paramount for cellular ISPs, who need to ensure high quality levels to limit customer churn due to quality dissatisfaction. This paper tackles the problem of QoE monitoring, assessment and prediction in cellular networks, relying on end-user device (i.e., smartphone) QoS passive traffic measurements and QoE crowdsourced feedback. We conceive different QoE assessment models based on supervised machine learning techniques, which are capable to predict the QoE experienced by the end user of popular smartphone apps (e.g., YouTube and Facebook), using as input the passive in-device measurements. Using a rich QoE dataset derived from field trials in operational cellular networks, we benchmark the performance of multiple machine learning based predictors, and construct a decision-tree based model which is capable to predict the per-user overall experience and service acceptability with a success rate of 91% and 98% respectively. To the best of our knowledge, this is the first paper using end-user, in-device passive measurements and machine learning models to predict the QoE of smartphone users in operational cellular networks.","2382":"Pharmaceutical discovery and development is a cascade\n                 of extremely complex and costly research encompassing\n                 many facets from: therapeutic target identification and\n                 bioinformatics study, candidate drug discovery and\n                 optimisation to pre-clinical organism-level evaluations\n                 and beyond to extensive clinical trials assessing\n                 effectiveness and safety of new medicines. Machine\n                 learning, in particular support vector machines SVM,\n                 particle swarm optimisation PSO and genetic programming\n                 GP, is increasingly used.","2383":null,"2384":"13C metabolic flux analysis (13C-MFA) has been widely used to measure in vivo enzyme reaction rates (i.e., metabolic flux) in microorganisms. Mining the relationship between environmental and genetic factors and metabolic fluxes hidden in existing fluxomic data will lead to predictive models that can significantly accelerate flux quantification. In this paper, we present a web-based platform MFlux (http:\/\/mflux.org) that predicts the bacterial central metabolism via machine learning, leveraging data from approximately 100 13C-MFA papers on heterotrophic bacterial metabolisms. Three machine learning methods, namely Support Vector Machine (SVM), k-Nearest Neighbors (k-NN), and Decision Tree, were employed to study the sophisticated relationship between influential factors and metabolic fluxes. We performed a grid search of the best parameter set for each algorithm and verified their performance through 10-fold cross validations. SVM yields the highest accuracy among all three algorithms. Further, we employed quadratic programming to adjust flux profiles to satisfy stoichiometric constraints. Multiple case studies have shown that MFlux can reasonably predict fluxomes as a function of bacterial species, substrate types, growth rate, oxygen conditions, and cultivation methods. Due to the interest of studying model organism under particular carbon sources, bias of fluxome in the dataset may limit the applicability of machine learning models. This problem can be resolved after more papers on 13C-MFA are published for non-model species. Metabolic information is important for disease treatment, bioprocess optimization, environmental remediation, biogeochemical cycle regulation, and our understanding of life's origin and evolution. 13C-MFA can quantify microbial physiology at the level of metabolic reaction rates. To speed up microbial characterizations and fluxomic studies, we hypothesize that genetic and environmental factors generate specific fluxome patterns that can be recognized by machine learning. Aided by constraint programming and quadratic optimization, our platform based on machine learning (ML) can predict meaningful metabolic information about bacterial species in their environments. Further, it can offer constraints to improve the accuracy of flux balance analysis. This study infers that the bacterial metabolic network has a certain degree of rigidity in allocating carbon fluxes, and different microbial species may share common regulatory strategies for balancing carbon and energy metabolisms. As a proof of concept, we demonstrate that the use of data-driven artificial intelligence (AI) approaches, e.g., ML, may assist mechanistic based models to elucidate the topology of microbial fluxomes.","2385":"OpenStreetMap is a unique source of openly available worldwide map data, increasingly adopted in real-world applications. Vandalism Detection in OpenStreetMap is critical and remarkably challenging due to the large scale of the dataset, the sheer number of contributors, various vandalism forms, and the lack of annotated data to train machine learning algorithms. This paper presents Ovid - a novel machine learning method for vandalism detection in OpenStreetMap. Ovid relies on a neural network architecture that adopts a multi-head attention mechanism to effectively summarize information indicating vandalism from OpenStreetMap changesets. To facilitate automated vandalism detection, we introduce a set of original features that capture changeset, user, and edit information. Our evaluation results on real-world vandalism data demonstrate that the proposed Ovid method outperforms the baselines by 4.7 percentage points in accuracy.","2386":null,"2387":"Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.","2388":null,"2389":"Abstract Using datasets from several sources, a list of more than 450 materials is generated and related them with their thermoelectric properties. This is obtained by generating a set of features using only the molecular formula. Subsequently, a machine learning algorithm classifies the materials in specific, binary classes, for example, possessing high or low Seebeck coefficients or electrical conductivity. After adjusting the threshold values and grouping the materials into clusters, the thermoelectric performance of more than 25k materials is predicted. Finally, the results are filtered to obtain only the sustainable materials, that is, neither toxic nor critical, (ideally) inexpensive, and isotropic with regard to their transport properties to simplify the preparation\u00a0procedure.","2390":"Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a \ufb02exible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature as well as on data from a real-world knowledge base (WordNet). In addition, we present how our method can be applied to perform word-sense disambiguation in a context of open-text semantic parsing, where the goal is to learn to assign a structured meaning representation to almost any sentence of free text, demonstrating that it can scale up to tens of thousands of nodes and thousands of types of relation.","2391":"Research in machine learning (ML) has argued that models trained on incomplete or biased datasets can lead to discriminatory outputs. In this commentary, we propose moving the research focus beyond bias-oriented framings by adopting a power-aware perspective to \"study up\" ML datasets. This means accounting for historical inequities, labor conditions, and epistemological standpoints inscribed in data. We draw on HCI and CSCW work to support our argument, critically analyze previous research, and point at two co-existing lines of work within our research community \\,---\\,one bias-centered, the other power-aware. We highlight the need for dialogue and cooperation in three areas: data quality, data work, and data documentation. In the first area, we argue that reducing societal problems to \"bias\" misses the context-based nature of data. In the second one, we highlight the corporate forces and market imperatives involved in the labor of data workers that subsequently shape ML datasets. Finally, we propose expanding current transparency-oriented efforts in dataset documentation to reflect the social contexts of data design and production.","2392":null,"2393":"A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy.","2394":"Machine learning has revolutionized drug discovery by speeding up the process and improving therapeutic interventions, transforming the pharmaceutical research and development landscape.","2395":null,"2396":null,"2397":null,"2398":null,"2399":"This work focuses on the prediction of hot deformation behavior of thermo-mechanically processed precipitation hardenable aluminum alloy AA7075. Data available are focusing on a novel hot forming process at different tool temperatures ranging from 24\u00b0C to 350\u00b0C  to set different cooling rates after solution-heat-treatment. Isothermal uniaxial tensile tests in the temperature range from 200\u00b0C to 400\u00b0C and at strain rates ranging from 0.001 s^-1 to 0.1 s^-1 were carried out on four different material conditions. The present paper mainly focuses on a comparative study of modeling techniques based on Machine Learning (ML) and the well-known Zerilli-Armstrong model (Z-A) as an empirically based reference. Work focused on predicting single data points of curves that the model was trained on. Due to the novel way data were split with respect to training and testing data, it becomes possible to predict entire stress-strain curves which leads to a reduction in the number of required laboratory experiments, finally saving costs and time in future experiments. While all investigated ML methods showed a higher performance than the Z-A model, the extreme Gradient Boosting model (XGB) showed the superior results, i.e., highest error reduction of 91% with respect to the Mean Squared Error.","2400":null,"2401":"Deep neural networks (DNNs) are powerful black-box predictors that have\r\nachieved impressive performance on a wide variety of tasks. However, their\r\naccuracy comes at the cost of intelligibility: it is usually unclear how they\r\nmake their decisions. This hinders their applicability to high stakes\r\ndecision-making domains such as healthcare. We propose Neural Additive Models\r\n(NAMs) which combine some of the expressivity of DNNs with the inherent\r\nintelligibility of generalized additive models. NAMs learn a linear combination\r\nof neural networks that each attend to a single input feature. These networks\r\nare trained jointly and can learn arbitrarily complex relationships between\r\ntheir input feature and the output. Our experiments on regression and\r\nclassification datasets show that NAMs are more accurate than widely used\r\nintelligible models such as logistic regression and shallow decision trees.\r\nThey perform similarly to existing state-of-the-art generalized additive models\r\nin accuracy, but can be more easily applied to real-world problems.","2402":"In recent years, researchers proposed a variety of deep learning models for wind power forecasting. These models predict the wind power generation of wind farms or entire regions more accurately than traditional machine learning algorithms or physical models. However, latest research has shown that deep learning models can often be manipulated by adversarial attacks. Since wind power forecasts are essential for the stability of modern power systems, it is important to protect them from this threat. In this work, we investigate the vulnerability of two different forecasting models to targeted, semi-targeted, and untargeted adversarial attacks. We consider a Long Short-Term Memory (LSTM) network for predicting the power generation of individual wind farms and a Convolutional Neural Network (CNN) for forecasting the wind power generation throughout Germany. Moreover, we propose the Total Adversarial Robustness Score (TARS), an evaluation metric for quantifying the robustness of regression models to targeted and semi-targeted adversarial attacks. It assesses the impact of attacks on the model's performance, as well as the extent to which the attacker's goal was achieved, by assigning a score between 0 (very vulnerable) and 1 (very robust). In our experiments, the LSTM forecasting model was fairly robust and achieved a TARS value of over 0.78 for all adversarial attacks investigated. The CNN forecasting model only achieved TARS values below 0.10 when trained ordinarily, and was thus very vulnerable. Yet, its robustness could be significantly improved by adversarial training, which always resulted in a TARS above 0.46.","2403":"The diagnosis of heart disease depends mostly on the combination of clinical and pathological data. It leads to the quality of medical care provided for the patient. In this paper, three machine learning (ML) techniques \u2212Classification and Regression tree (CART), Neural Networks (NN), and Support vector machine (SVM)\u2212 are utilized to find the best attributes for estimating the severity of heart failure. The data is collected from three different resources, then each input attribute used for assessing the severity of heart\r\nfailure is analyzed individually after implementing the machine learning techniques. Finally, the most important supportive attributes are presented in this paper by which medical staffs can identify heart failure severity fast and more accurately. In fact, by screening important attributes, clinicians can make better decision about right treatment procedures or preventive actions that reduce risk of heart attacks.","2404":"Symbolic regression, i.e. predicting a function from the observation of its values, is well-known to be a challenging task. In this paper, we train Transformers to infer the function or recurrence relation underlying sequences of integers or floats, a typical task in human IQ tests which has hardly been tackled in the machine learning literature. We evaluate our integer model on a subset of OEIS sequences, and show that it outperforms built-in Mathematica functions for recurrence prediction. We also demonstrate that our float model is able to yield informative approximations of out-of-vocabulary functions and constants, e.g. $bessel0(x)\\sin(x)+\\cos(x)x$ and $1.644934\\pi^2\/6$.","2405":"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.","2406":"The diagnosis of heart disease depends mostly on the combination of clinical and pathological data. It leads to the quality of medical care provided for the patient. In this paper, three machine learning (ML) techniques \u2212Classification and Regression tree (CART), Neural Networks (NN), and Support vector machine (SVM)\u2212 are utilized to find the best attributes for estimating the severity of heart failure. The data is collected from three different resources, then each input attribute used for assessing the severity of heart failure is analyzed individually after implementing the machine learning techniques. Finally, the most important supportive attributes are presented in this paper by which medical staffs can identify heart failure severity fast and more accurately. In fact, by screening important attributes, clinicians can make\r\nbetter decision about right treatment procedures or preventive actions that reduce risk of heart attacks","2407":"Domain-specific search engines are becoming\nincreasingly popular because they o\u000ber increased\naccuracy and extra features not possible\nwith general, Web-wide search engines.\nUnfortunately, they are also difficult and timeconsuming\nto maintain. This paper proposes\nthe use of machine learning techniques to\ngreatly automate the creation and maintenance\nof domain-speci\fc search engines. We describe\nnew research in reinforcement learning, text\nclassi\fcation and information extraction that\nenables e\u000ecient spidering, populates topic hierarchies,\nand identies informative text segments.\nUsing these techniques, we have built\na demonstration system: a search engine for\ncomputer science research papers available at\nwww.cora.justresearch.com.","2408":"Measuring and monitoring YouTube Quality of Experience is a challenging task, especially when dealing with cellular networks and smartphone users. Using a large-scale database of crowdsourced YouTube-QoE measurements in smartphones, we conceive multiple machine-learning models to infer different YouTube-QoE-relevant metrics and userbehavior- related metrics from network-level measurements, without requiring root access to the smartphone, video-player embedding, or any other reverse-engineering-like approaches. The dataset includes measurements from more than 360 users worldwide, spanning over the last five years. Our preliminary results suggest that QoE-based monitoring of YouTube mobile can be realized through machine learning models with high accuracy, relying only on network-related features and without accessing any higher-layer metric to perform the estimations.","2409":"The diagnosis of heart disease depends mostly on the combination of clinical and pathological data. It leads to the quality of medical care provided for the patient. In this paper, three machine learning (ML) techniques \u2212Classification and Regression tree (CART), Neural Networks (NN), and Support vector machine (SVM)\u2212 are utilized to find the best attributes for estimating the severity of heart failure. The data is collected from three different resources, then each input attribute used for assessing the severity of heart failure is analyzed individually after implementing the machine learning techniques. Finally, the most important supportive attributes are presented in this paper by which medical staffs can identify heart failure severity fast and more accurately. In fact, by screening important attributes, clinicians can make better decision about right treatment procedures or preventive actions that reduce risk of heart attacks.","2410":null,"2411":null,"2412":"Data-driven soft sensors have been widely utilized in industrial processes to estimate the critical quality variables which are intractable to directly measure online through physical devices. Due to the low sampling rate of quality variables, most of the soft sensors are developed on small number of labeled samples and the large number of unlabeled process data is discarded. The loss of information greatly limits the improvement of quality prediction accuracy. One of the main issues of data-driven soft sensor is to furthest exploit the information contained in all available process data. This paper proposes a semisupervised deep learning model for soft sensor development based on the hierarchical extreme learning machine (HELM). First, the deep network structure of autoencoders is implemented for unsupervised feature extraction with all the process samples. Then, extreme learning machine is utilized for regression through appending the quality variable. Meanwhile, the manifold regularization method is introduced for semisupervised model training. The new method can not only deeply extract the information that the data contains, but learn more from the extra unlabeled samples as well. The proposed semisupervised HELM method is applied in a high-low transformer to estimate the carbon monoxide content, which shows a significant improvement of the prediction accuracy, compared to traditional methods.","2413":null,"2414":"BACKGROUND:Identifying essential genes in bacteria supports to identify potential drug targets and an understanding of minimal requirements for a synthetic cell. However, experimentally assaying the essentiality of their coding genes is resource intensive and not feasible for all bacterial organisms, in particular if they are infective.RESULTS:We developed a machine learning technique to identify essential genes using the experimental data of genome-wide knock-out screens from one bacterial organism to infer essential genes of another related bacterial organism. We used a broad variety of topological features, sequence characteristics and co-expression properties potentially associated with essentiality, such as flux deviations, centrality, codon frequencies of the sequences, co-regulation and phyletic retention. An organism-wise cross-validation on bacterial species yielded reliable results with good accuracies (area under the receiver-operator-curve of 75\\% - 81\\%). Finally, it was applied to drug target predictions for Salmonella typhimurium. We compared our predictions to the viability of experimental knock-outs of S. typhimurium and identified 35 enzymes, which are highly relevant to be considered as potential drug targets. Specifically, we detected promising drug targets in the non-mevalonate pathway.CONCLUSIONS:Using elaborated features characterizing network topology, sequence information and microarray data enables to predict essential genes from a bacterial reference organism to a related query organism without any knowledge about the essentiality of genes of the query organism. In general, such a method is beneficial for inferring drug targets when experimental data about genome-wide knockout screens is not available for the investigated organism.","2415":"The purpose of this research was to implement an application improving the process of optimization for efficient results estimation in possible quick time using the concepts of adaptive machine learning. Back Propagated Neural Network algorithm was implemented to take advantage in developing a learning behavior in system and Genetic algorithm was implemented for data optimization. Going into the details of this research paper, we briefly discuss the implemented methodology in the form of software application i.e. Optimal Data Classifier (DOC), using Java programming language. Furthermore we describes two performed experimentation processes using two different datasets with DOC, and presented observed results helping in improving learning and data optimization behavior.","2416":"The creative potential from innovative contributions of the crowd constitutes some critical challenges. The quantity of contributions and the resource demands to identify valuable ideas is high and remains challenging for firms that apply open innovation initiatives. To solve these problems, research on algorithmic approaches proved to be a valuable way by identifying metrics to distinguish between high and low-quality ideas. However, such filtering approaches always risk missing promising ideas by classifying good ideas as bad ones. In response, organizations have turned to the crowd to not just for generating ideas but also to evaluate them to filter high quality contributions. However, such crowd-based filtering approaches tend to perform poorly in practice as they make unrealistic demands on the crowd. We, therefore, conduct a design science research project to provide prescriptive knowledge on how to combine machine learning techniques with crowd evaluation to adaptively assign humans to ideas.","2417":null,"2418":null,"2419":null,"2420":"Improving predictive understanding of Earth system variability and change\r\nrequires data-model integration. Efficient data-model integration for complex\r\nmodels requires surrogate modeling to reduce model evaluation time. However,\r\nbuilding a surrogate of a large-scale Earth system model (ESM) with many output\r\nvariables is computationally intensive because it involves a large number of\r\nexpensive ESM simulations. In this effort, we propose an efficient surrogate\r\nmethod capable of using a few ESM runs to build an accurate and\r\nfast-to-evaluate surrogate system of model outputs over large spatial and\r\ntemporal domains. We first use singular value decomposition to reduce the\r\noutput dimensions, and then use Bayesian optimization techniques to generate an\r\naccurate neural network surrogate model based on limited ESM simulation\r\nsamples. Our machine learning based surrogate methods can build and evaluate a\r\nlarge surrogate system of many variables quickly. Thus, whenever the quantities\r\nof interest change such as a different objective function, a new site, and a\r\nlonger simulation time, we can simply extract the information of interest from\r\nthe surrogate system without rebuilding new surrogates, which significantly\r\nsaves computational efforts. We apply the proposed method to a regional\r\necosystem model to approximate the relationship between 8 model parameters and\r\n42660 carbon flux outputs. Results indicate that using only 20 model\r\nsimulations, we can build an accurate surrogate system of the 42660 variables,\r\nwhere the consistency between the surrogate prediction and actual model\r\nsimulation is 0.93 and the mean squared error is 0.02. This highly-accurate and\r\nfast-to-evaluate surrogate system will greatly enhance the computational\r\nefficiency in data-model integration to improve predictions and advance our\r\nunderstanding of the Earth system.","2421":"Programming by demonstration enables users to easily personalize their applications, automating repetitive tasks simply by executing a few examples. We formalize programming by demonstration as a machine learning problem: given the changes in the application state that result from the user's demonstrated actions, learn the general program that maps from one application state to the next. We present a methodology for learning in this space of complex functions. First we extend version spaces to learn arbitrary functions, not just concepts. Then we introduce the version space algebra, a method for composing simpler version spaces to construct more complex spaces. Finally, we apply our version space algebra to the text-editing domain and describe an implemented system called SMARTedit that learns repetitive text-editing procedures by example. We evaluate our approach by measuring the number of examples required for the system to learn a procedure that works on the remainder of examples, and by an informal user study measuring the effort users spend using our system versus performing the task by hand. The results show that SMARTedit is capable of generalizing correctly from as few as one or two examples, and that users generally save a significant amount of effort when completing tasks with SMARTedit's help.","2422":null,"2423":null,"2424":null,"2425":null,"2426":null,"2427":null,"2428":null,"2429":"This chapter studies ontology matching: the problem of finding the semantic mappings between two given ontologies. This problem lies at the heart of numerous information processing applications. Virtually any application that involves multiple ontologies must establish semantic mappings among them, to ensure interoperability. Examples of such applications arise in myriad domains, including e-commerce, knowledge management, e-learning, information extraction, bio-informatics, web services, and tourism (see Part D of this book on ontology applications).\r\n\r\nDespite its pervasiveness, today ontology matching is still largely conducted by hand, in a labor-intensive and error-prone process. The manual matching has now become a key bottleneck in building large-scale information management systems. The advent of technologies such as the WWW, XML, and the emerging Semantic Web will further fuel information sharing applications and exacerbate the problem. Hence, the development of tools to assist in the ontology matching process has become crucial for the success of a wide variety of information management applications.\r\n\r\nIn response to the above challenge, we have developed GLUE, a system that employs learning techniques to semi-automatically create semantic mappings between ontologies. We shall begin the chapter by describing a motivating example: ontology matching on the Semantic Web. Then we present our GLUE solution. Finally, we describe a set of experiments on several real-world domains, and show that GLUE proposes highly accurate semantic mappings.","2430":null,"2431":"Heart illnesses are among the most significant contributors to mortality in the world in the modern era. Heart attacks are responsible for the death of one person every 33 seconds. disease of the cardiovascular system by disclosing the proportion of mortality all over the world that are caused by heart attacks. In order to forecast instances of heart disease, a supervised machine learning method is utilised. Because the incidence of heart strokes in younger people is growing at an alarming rate, we need to establish a method that can identify the warning signs of a heart attack at an early stage and stop the stroke before it occurs. Because it is impractical for the average person to often undertake expensive tests like the electrocardiogram (ECG), there is a need for a system that is convenient and, at the same time, accurate in forecasting the likelihood of developing heart disease. Therefore, our plan is to create a programme that, given basic symptoms such as age, sex, pulse rate, etc., can determine whether or not a person is at risk for developing a cardiac condition. The machine learning algorithm neural networks that are used in the suggested system are the most accurate and dependable.","2432":null,"2433":null,"2434":null,"2435":"Housing prices are increasing every year, necessitating the creation of a long term housing price strategy. Predicting a homes price will assist a developer in determining a homes purchase price, as well as a consumer in determining the best time to buy a home. The sale price of real estate in major cities depends on the specific circumstances. Housing prices are constantly changing from day to day and are sometimes fired rather than based on estimates. Predicting real estate prices by real factors is a key element as part of our analysis. We want to make our test dependent on all of the simple metrics that are taken into account when deciding the significance. In this research we use linear regression techniques pathway and our results are not self inflicted process rather is a weighted method of various techniques to give the most accurate results. There are fifteen features in the data collection. In this research. There has been an effort to build a forecasting model for determining the price based on the variables that influence the price.The results have proven to be effective lower error and higher accuracy than individual algorithms are used. Jakir Khan | Dr. Ganesh D \"House Price Estimates Based on Machine Learning Algorithm\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https:\/\/www.ijtsrd.compapers\/ijtsrd42367.pdf Paper URL: https:\/\/www.ijtsrd.comcomputer-science\/other\/42367\/house-price-estimates-based-on-machine-learning-algorithm\/jakir-khan","2436":"It is important yet hard to identify navigational queries in Web search due to a lack of sufficient information in Web queries, which are typically very short. In this paper we study several machine learning methods, including naive Bayes model, maximum entropy model, support vector machine (SVM), and stochastic gradient boosting tree (SGBT), for navigational query identification in Web search. To boost the performance of these machine techniques, we exploit several feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance. Different from most prior work that uses a small number of features, in this paper, we study the problem of identifying navigational queries with thousands of available features, extracted from major commercial search engine results, Web search user click data, query log, and the whole Web's relational content. A multi-level feature extraction system is constructed.Our results on real search data show that 1) Among all the features we tested, user click distribution features are the most important set of features for identifying navigational queries. 2) In order to achieve good performance, machine learning approaches have to be coupled with good feature selection methods. We find that gradient boosting tree, coupled with linear SVM feature selection is most effective. 3) With carefully coupled feature selection and classification approaches, navigational queries can be accurately identified with 88.1\\% F1 score, which is 33\\% error rate reduction compared to the best uncoupled system, and 40\\% error rate reduction compared to a well tuned system without feature selection.","2437":"Physical Unclonable Functions (PUFs) are promising security primitives for resource-constrained network nodes. The XOR Arbiter PUF (XOR PUF or XPUF) is an intensively studied PUF invented to improve the security of the Arbiter PUF, probably the most lightweight delay-based PUF. Recently, highly powerful machine learning attack methods were discovered and were able to easily break large-sized XPUFs, which were highly secure against earlier machine learning attack methods. Component-differentially-challenged XPUFs (CDC-XPUFs) are XPUFs with different component PUFs receiving different challenges. Studies showed they were much more secure against machine learning attacks than the conventional XPUFs, whose component PUFs receive the same challenge. But these studies were all based on earlier machine learning attack methods, and hence it is not clear if CDC-XPUFs can remain secure under the recently discovered powerful attack methods. In this paper, the two current most powerful two machine learning methods for attacking XPUFs are adapted by fine-tuning the parameters of the two methods for CDCXPUFs. Attack experiments using both simulated PUF data and silicon data generated from PUFs implemented on field-programmable gate array (FPGA) were carried out, and the experimental results showed that some previously secure CDC-XPUFs of certain circuit parameter values are no longer secure under the adapted new attack methods, while many more CDC-XPUFs of other circuit parameter values remain secure. Thus, our experimental attack study has re-defined the boundary between the secure region and the insecure region of the PUF circuit parameter space, providing PUF manufacturers and IoT security application developers with valuable information in choosing PUFs with secure parameter values.","2438":"While explanations may help people learn by providing information about why an answer is correct, many problems on online platforms lack high-quality explanations. This paper presents AXIS (Adaptive eXplanation Improvement System), a system for obtaining explanations. AXIS asks learners to generate, revise, and evaluate explanations as they solve a problem, and then uses machine learning to dynamically determine which explanation to present to a future learner, based on previous learners' collective input. Results from a case study deployment and a randomized experiment demonstrate that AXIS elicits and identifies explanations that learners find helpful. Providing explanations from AXIS also objectively enhanced learning, when compared to the default practice where learners solved problems and received answers without explanations. The rated quality and learning benefit of AXIS explanations did not differ from explanations generated by an experienced instructor.","2439":null,"2440":null,"2441":null,"2442":"In this paper we introduce JuliaSim, a high-performance programming environment designed to blend traditional modeling and simulation with machine learning. JuliaSim can build accelerated surrogates from component-based\r\nmodels, such as those conforming to the FMI standard, using continuous-time echo state networks (CTESN). The foundation of this environment, ModelingToolkit.jl, is an acausal modeling language which can compose the trained surrogates as components within its staged compilation process. As a complementary factor we present the JuliaSim model library, a standard library with differential-algebraic equations and pre-trained surrogates, which can be composed using the modeling system for design, optimization, and control. We demonstrate the effectiveness of the surrogate-accelerated modeling and simulation approach on HVAC dynamics by showing that the CTESN surrogates accurately capture the dynamics of a HVAC cycle at less than 4\\% error while accelerating its simulation by 340x. We illustrate the use of surrogate acceleration in the design process via global optimization of simulation parameters using the embedded surrogate, yielding a speedup of two orders of magnitude to find the optimum. We showcase the surrogate deployed in a co-simulation loop, as a drop-in replacement for one of the coupled FMUs, allowing engineers to effectively explore the design space of a coupled system.\r\nTogether this demonstrates a workflow for automating the integration of machine learning techniques into traditional modeling and simulation processes.","2443":null,"2444":null,"2445":"Distributed Artificial Intelligence (DAI) has\r\n                  existed as a subfield of AI for less than two\r\n                  decades. DAI is concerned with systems that consist\r\n                  of multiple independent entities that interact in a\r\n                  domain. Traditionally, DAI has been divided into two\r\n                  sub-disciplines: Distributed Problem Solving (DPS)\r\n                  focuses on the information management aspects of\r\n                  systems with several components working together\r\n                  towards a common goal; Multiagent Systems (MAS)\r\n                  deals with behavior management in collections of\r\n                  several independent entities, or agents. This survey\r\n                  of MAS is intended to serve as an introduction to\r\n                  the field and as an organizational framework. A\r\n                  series of general multiagent scenarios are\r\n                  presented. For each scenario, the issues that arise\r\n                  are described along with a sampling of the\r\n                  techniques that exist to deal with them. The\r\n                  presented techniques are not exhaustive, but they\r\n                  highlight how multiagent systems can be and have\r\n                  been used to build complex systems. When options\r\n                  exist, the techniques presented are biased towards\r\n                  machine learning approaches. Additional\r\n                  opportunities for applying machine learning to MAS\r\n                  are highlighted and robotic soccer is presented as\r\n                  an appropriate test bed for MAS. This survey does\r\n                  not focus exclusively on robotic systems. However,\r\n                  we believe that much of the prior research in\r\n                  non-robotic MAS is relevant to robotic MAS, and we\r\n                  explicitly discuss several robotic MAS, including\r\n                  all of those presented in this issue.","2446":null,"2447":"Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the \u03b5-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.","2448":null,"2449":null,"2450":null,"2451":"Linked Data repositories have become a popular source of publicly-available data. Users accessing this data through SPARQL endpoints usually launch several restrictive yet similar consecutive queries, either to find the information they need through trial-and-error or to query related resources. However, instead of executing each individual query separately, query augmentation aims at modifying the incoming queries to retrieve more data that is potentially relevant to subsequent requests. In this paper, we propose a novel approach to query augmentation for SPARQL endpoints based on machine learning. Our approach separates the structure of the query from its contents and measures two types of similarity, which are then used to predict the structure and contents of the augmented query. We test the approach on the real-world query logs of the Spanish and English DBpedia and show that our approach yields high-accuracy prediction. We also show that, by caching the results of the predicted augmented queries, we can retrieve data relevant to several subsequent queries at once, achieving a higher cache hit rate than previous approaches.","2452":"This paper presents a method that assists in maintaining a rule-based named-entity recognition and classification system. The underlying idea is to use a separate system, constructed with the use of machine learning, to monitor the performance of the rule-based system. The training data for the second system is generated with the use of the rule-based system, thus avoiding the need for manual tagging. The disagreement of the two systems acts as a signal for updating the rule-based system. The generality of the approach is illustrated by applying it to large corpora in two different languages: Greek and French. The results are very encouraging, showing that this alternative use of machine learning can assist significantly in the maintenance of rule-based systems.","2453":null,"2454":"Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.\r\nER  -","2455":"Structured Prediction or Structured Classification is the task of predicting a collection of related variables given some input. The relationship between the variables to be predicted is often complex. An example of such complex dependencies is machine translation, where the input is a sequence of words in the source natural language and the output is a sequence of words in the target natural language. Here, each word in the target language relates not only to the words in the source language, but also to the other (arbitrarily far) words in the target sequence. As a field, structured prediction has some unique challenges, several of which are addressed by the papers in this issue. One of the most obvious of these is that the output spaces in question are often exponential in size, and so the complexity, both of these spaces and the learned models, can result in computationally infeasible learning and inference algorithms. In many cases, therefore, efficient optimization remains an open problem in structured prediction.","2456":"In this paper an ensemble of supervised machine learning methods has been investigated to virtually and dynamically calibrate the cosmic ray sensors measuring area wise bulk soil moisture. Main focus of this study was to find an alternative to the currently available field calibration method; based on expensive and time consuming soil sample collection methodology. Data from the Australian Water Availability Project (AWAP) database was used as independent soil moisture ground truth and results were compared against the conventionally estimated soil moisture using a Hydroinnova CRS-1000 cosmic ray probe deployed in Tullochgorum, Australia. Prediction performance of a complementary ensemble of four supervised estimators, namely Sugano type Adaptive Neuro-Fuzzy Inference System (S-ANFIS), Cascade Forward Neural Network (CFNN), Elman Neural Network (ENN) and Learning Vector Quantization Neural Network (LVQN) was evaluated using training and testing paradigms. An AWAP trained ensemble of four estimators was able to predict bulk soil moisture directly from cosmic ray neutron counts with 94.4\\% as best accuracy. The ensemble approach outperformed the individual performances from these networks. This result proved that an ensemble machine learning based paradigm could be a valuable alternative data driven calibration method for cosmic ray sensors against the current expensive and hydrological assumption based field calibration method.","2457":null,"2458":null,"2459":null,"2460":"Until now, computer vision and machine learning techniques barely\n\tcontributed to the archaeological domain. The use of these techniques\n\tcan support archaeologists in their assessment and classification\n\tof archaeological finds. The paper illustrates the use of computer\n\tvision techniques for archaeology with two examples: (1) a content-based\n\timage retrieval system for historical glass and (2) an automatic\n\tsystem for medieval coin classification. The content-based image\n\tretrieval system automatically finds artifact drawings in a reference\n\tcollection that are most similar to a photograph or drawing of an\n\texcavated historical glass. The similarity measurements are based\n\ton the outer shape contours of the artifacts. The system can speed\n\tup the process of classifying historical glass, and make it more\n\tobjective and controllable. The coin classification system will be\n\ttrained on a collection of Dutch early-medieval coins. For this system,\n\twe present preliminary results on modern coin data.","2461":null,"2462":null,"2463":"As decision-making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data-driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness-aware ML solutions have been proposed which involve fairness-related interventions in the data, learning algorithms, and\/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware ML. We focus on tabular data as the most common data representation for fairness-aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis.","2464":"We present the discovery and preliminary characterization of a\r\ngravitationally lensed quasar with a source redshift $z_s=2.74$ and image\r\nseparation of $2.9\"$ lensed by a foreground $z_l=0.40$ elliptical galaxy.\r\nSince the images of gravitationally lensed quasars are the superposition of\r\nmultiple point sources and a foreground lensing galaxy, we have developed a\r\nmorphology independent multi-wavelength approach to the photometric selection\r\nof lensed quasar candidates based on Gaussian Mixture Models (GMM) supervised\r\nmachine learning. Using this technique and $gi$ multicolour photometric\r\nobservations from the Dark Energy Survey (DES), near IR $JK$ photometry from\r\nthe VISTA Hemisphere Survey (VHS) and WISE mid IR photometry, we have\r\nidentified a candidate system with two catalogue components with $i_AB=18.61$\r\nand $i_AB=20.44$ comprised of an elliptical galaxy and two blue point\r\nsources. Spectroscopic follow-up with NTT and the use of an archival AAT\r\nspectrum show that the point sources can be identified as a lensed quasar with\r\nan emission line redshift of $z=2.739\\pm0.003$ and a foreground early type\r\ngalaxy with $z=0.400\\pm0.002$. We model the system as a single isothermal\r\nellipsoid and find the Einstein radius $\u00feeta_E 1.47\"$, enclosed mass\r\n$M_enc 4 10^11$M$_\u00f8dot$ and a time delay of $\\sim$52 days.\r\nThe relatively wide separation, month scale time delay duration and high\r\nredshift make this an ideal system for constraining the expansion rate beyond a\r\nredshift of 1.","2465":null,"2466":null,"2467":null,"2468":"BACKGROUND: Most of the existing in silico phosphorylation site prediction systems use machine learning approach that requires preparing a good set of classification data in order to build the classification knowledge. Furthermore, phosphorylation is catalyzed by kinase enzymes and hence the kinase information of the phosphorylated sites has been used as major classification data in most of the existing systems. Since the number of kinase annotations in protein sequences is far less than that of the proteins being sequenced to date, the prediction systems that use the information found from the small clique of kinase annotated proteins can not be considered as completely perfect for predicting outside the clique. Hence the systems are certainly not generalized. In this paper, a novel generalized prediction system, PPRED (Phosphorylation PREDictor) is proposed that ignores the kinase information and only uses the evolutionary information of proteins for classifying phosphorylation sites. RESULTS: Experimental results based on cross validations and an independent benchmark reveal the significance of using the evolutionary information alone to classify phosphorylation sites from protein sequences. The prediction performance of the proposed system is better than those of the existing prediction systems that also do not incorporate kinase information. The system is also comparable to systems that incorporate kinase information in predicting such sites. CONCLUSIONS: The approach presented in this paper provides an efficient way to identify phosphorylation sites in a given protein primary sequence that would be a valuable information for the molecular biologists working on protein phosphorylation sites and for bioinformaticians developing generalized prediction systems for the post translational modifications like phosphorylation or glycosylation. PPRED is publicly available at the URL http:\/\/www.cse.univdhaka.edu\/\u00e3shis\/ppred\/index.php.","2469":"The development of microarray technology has supplied a large\nvolume of data to many fields. In particular, it has been applied\nto prediction and diagnosis of cancer, so that it expectedly helps\nus to exactly predict and diagnose cancer. To precisely classify\ncancer we have to select genes related to cancer because\nextracted genes from microarray have many noises. In this paper,\nwe attempt to explore many features and classifiers using three\nbenchmark datasets to systematically evaluate the performances\nof the feature selection methods and machine learning classifiers.\nThree benchmark datasets are Leukemia cancer dataset, Colon\ncancer dataset and Lymphoma cancer data set. Pearson\u2019s and\nSpearman\u2019s correlation coefficients, Euclidean distance, cosine\ncoefficient, information gain, mutual information and signal to\nnoise ratio have been used for feature selection. Multi-layer\nperceptron, k-nearest neighbour, support vector machine and\nstructure adaptive self\u2013organizing map have been used for\nclassification. Also, we have combined the classifiers to improve\nthe performance of classification. Experimental results show that\nthe ensemble with several basis classifiers produces the best\nrecognition rate on the benchmark dataset.","2470":"Digital libraries desire automatic subject indexing as a scalable provider of high-quality semantic document representations. The task is, however, complex and challenging, thus many issues are still unsolved. For instance, certain concepts are not detected accurately, and confidence estimates are often unreliable. Accurate quality estimates are, however, crucial in practice, for example, to filter results and ensure highest standards before subsequent use. The proposed thesis studies applications of machine learning for automatic subject indexing, which faces considerable challenges like class imbalance, concept drift, and zero-shot learning. Special attention will be paid to architecture design and automatic quality estimation, with experiments on scholarly publications in economics and business studies. First results indicate the importance of knowledge transfer between concepts and point out the value of so-called fusion approaches that carefully combine lexical and associative subsystems. This extended abstract summarizes the main topic and status of the thesis and provides an outlook on future directions.","2471":null,"2472":"Abstract&nbsp;&nbsp;In this study we analyzed the possible context-specific and individual-specific features of dog barks using a new machine-learning\r\nalgorithm. A pool containing more than 6,000 barks, which were recorded in six different communicative situations was usedas the sound sample. The algorithm\u00e2\u0080\u0099s task was to learn which acoustic features of the barks, which were recorded in differentcontexts and from different individuals, could be distinguished from another. The program conducted this task by analyzingbarks emitted in previously identified contexts by identified dogs. After the best feature set had been obtained (with whichthe highest identification rate was achieved), the efficiency of the algorithm was tested in a classification task in whichunknown barks were analyzed. The recognition rates we found were highly above chance level: the algorithm could categorizethe barks according to their recorded situation with an efficiency of 43% and with an efficiency of 52% of the barking individuals.These findings suggest that dog barks have context-specific and individual-specific acoustic features. In our opinion, thismachine learning method may provide an efficient tool for analyzing acoustic data in various behavioral studies.","2473":null,"2474":null,"2475":"Building models using machine learning techniques\n                 requires data. For some projects, gathering data is\n                 very expensive. In this type of project, there are two\n                 significant costs to using machine learning techniques\n                 in this type of project: (1) Machine learning models\n                 cannot even begin to make predictions until the project\n                 has already spent a lot of money gathering data; and\n                 (2) While the data is being gathered to train the\n                 machine learning system, unnecessary costs are incurred\n                 in making inefficient decisions. Engineers may address\n                 this type of problem efficiently when enough human\n                 expertise exists about the problem domain to be\n                 modelled. This work proposes an approach to combining\n                 human expertise, machine learning and information\n                 theory that makes efficient and effective decisions\n                 from the start of the project, while project data is\n                 being gathered.","2476":"Genetic algorithms have given rise to two new fields\n                 of research where (global) optimisation is of crucial\n                 importance: 'Genetic Programming' and 'Genetic based\n                 Machine Learning' (GBML). An overview of one of the\n                 first GBML implementations by Holland, also known as\n                 the Learning Classifier Systems (LCS) will be given.\n                 After describing and solving a well-known basic\n                 (educational) problem a more complex application of\n                 GBML is presented. The goal of this application is the\n                 automatic development of a rule set for an industrial\n                 production process. To this end, the case study on\n                 generating a rule set for predicting the spinnability\n                 in the fibre-to-yarn production process will be\n                 presented. A largely modified LCS, called Fuzzy\n                 Efficiency based Classifier System (FECS), originally\n                 designed by one of the authors, is used to solve this\n                 problem successfully.","2477":"Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. Despite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, \"powerful,\" \"strong\"  and \"Paris\" are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classification and sentiment analysis tasks.","2478":null,"2479":"Simulation and optimisation of industrial processes is\n                 cost effective and profit productive. Often, high\n                 fidelity models require extensive resources to code and\n                 require long execution times. In this work, we examine\n                 using machine learning techniques to replace simulation\n                 models with high fidelity approximations. We test\n                 linear genetic programming, linear regression, and\n                 machine learning paradigms. The results show that high\n                 fidelity approximations (R2 of 0.99) are possible that\n                 execute in a fraction of the time required by the\n                 original simulator. These solutions are coded into web\n                 services so that a plant manager can input standard\n                 information into a user friendly web page, but produce\n                 results in a few milliseconds as opposed to hours. This\n                 advantage allows for real-time dynamic planning and\n                 optimization on the plant floor.","2480":"One of the most challenging machine learning problems is a particular case of data classification in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with applications in text classification, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC.","2481":"This article analyzes the development of artificial intelligence, discusses the use of machine learning and neural networks as one of the areas of artificial intelligence.","2482":"This pilot study has applied machine learning (artificial intelligence derived qualitative analysis procedures) to yield non-invasive techniques for the assessment and interpretation of clinical and laboratory data in glomerular disease. To evaluate the appropriateness of these techniques, they were applied to subsets of a small database of 284 case histories and the resulting procedures evaluated against the remaining cases. Over such evaluations, the following average diagnostic accuracies were obtained: microscopic polyarteritis, 95.37%; minimal lesion nephrotic syndrome, 96.50%; immunoglobulin A nephropathy, 81.26%; minor changes, 93.66%; lupus nephritis, 96.27%; focal glomerulosclerosis, 92.06%; mesangial proliferative glomerulonephritis, 92.56%; and membranous nephropathy, 92.56%. Although in general the new diagnostic system is not yet as accurate as the histological evaluation of renal biopsy specimens, it shows promise of adding a further dimension to the diagnostic process. When the machine learning techniques are applied to a larger database, greater diagnostic accuracy should be obtained. It may allow accurate non- invasive diagnosis of some cases of glomerular disease without the need for renal biopsy. This may reduce both the cost and the morbidity of the investigation of glomerular disease and may be of particular value in situations where renal biopsy is considered hazardous or contraindicated.","2483":"In this work we present the experiments which lead to the creation of our\r\nBERT and ELECTRA based German language models, GBERT and GELECTRA. By varying\r\nthe input training data, model size, and the presence of Whole Word Masking\r\n(WWM) we were able to attain SoTA performance across a set of document\r\nclassification and named entity recognition (NER) tasks for both models of base\r\nand large size. We adopt an evaluation driven approach in training these models\r\nand our results indicate that both adding more data and utilizing WWM improve\r\nmodel performance. By benchmarking against existing German models, we show that\r\nthese models are the best German models to date. Our trained models will be\r\nmade publicly available to the research community.","2484":"Prompted models have demonstrated impressive few-shot learning abilities.\r\nRepeated interactions at test-time with a single model, or the composition of\r\nmultiple models together, further expands capabilities. These compositions are\r\nprobabilistic models, and may be expressed in the language of graphical models\r\nwith random variables whose values are complex data types such as strings.\r\nCases with control flow and dynamic structure require techniques from\r\nprobabilistic programming, which allow implementing disparate model structures\r\nand inference strategies in a unified language. We formalize several existing\r\ntechniques from this perspective, including scratchpads \/ chain of thought,\r\nverifiers, STaR, selection-inference, and tool use. We refer to the resulting\r\nprograms as language model cascades.","2485":"Recent studies have demonstrated the efficiency of generative pretraining for\r\nEnglish natural language understanding. In this work, we extend this approach\r\nto multiple languages and show the effectiveness of cross-lingual pretraining.\r\nWe propose two methods to learn cross-lingual language models (XLMs): one\r\nunsupervised that only relies on monolingual data, and one supervised that\r\nleverages parallel data with a new cross-lingual language model objective. We\r\nobtain state-of-the-art results on cross-lingual classification, unsupervised\r\nand supervised machine translation. On XNLI, our approach pushes the state of\r\nthe art by an absolute gain of 4.9% accuracy. On unsupervised machine\r\ntranslation, we obtain 34.3 BLEU on WMT'16 German-English, improving the\r\nprevious state of the art by more than 9 BLEU. On supervised machine\r\ntranslation, we obtain a new state of the art of 38.5 BLEU on WMT'16\r\nRomanian-English, outperforming the previous best approach by more than 4 BLEU.\r\nOur code and pretrained models will be made publicly available.","2486":"Large-scale language models show promising text generation capabilities, but\r\nusers cannot easily control particular aspects of the generated text. We\r\nrelease CTRL, a 1.63 billion-parameter conditional transformer language model,\r\ntrained to condition on control codes that govern style, content, and\r\ntask-specific behavior. Control codes were derived from structure that\r\nnaturally co-occurs with raw text, preserving the advantages of unsupervised\r\nlearning while providing more explicit control over text generation. These\r\ncodes also allow CTRL to predict which parts of the training data are most\r\nlikely given a sequence. This provides a potential method for analyzing large\r\namounts of data via model-based source attribution. We have released multiple\r\nfull-sized, pretrained versions of CTRL at https:\/\/github.com\/salesforce\/ctrl.","2487":"Formal languages let us define the textual representation of data with\r\nprecision. Formal grammars, typically in the form of BNF-like productions,\r\ndescribe the language syntax, which is then annotated for syntax-directed\r\ntranslation and completed with semantic actions. When, apart from the textual\r\nrepresentation of data, an explicit representation of the corresponding data\r\nstructure is required, the language designer has to devise the mapping between\r\nthe suitable data model and its proper language specification, and then develop\r\nthe conversion procedure from the parse tree to the data model instance.\r\nUnfortunately, whenever the format of the textual representation has to be\r\nmodified, changes have to propagated throughout the entire language processor\r\ntool chain. These updates are time-consuming, tedious, and error-prone.\r\nBesides, in case different applications use the same language, several copies\r\nof the same language specification have to be maintained. In this paper, we\r\nintroduce a model-based parser generator that decouples language specification\r\nfrom language processing, hence avoiding many of the problems caused by\r\ngrammar-driven parsers and parser generators.","2488":"A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.","2489":"We propose a new document vector representation specifically designed for the document clustering task. Instead of the traditional term-based vectors, a document is represented as an n-dimensional vector, where n is the number of documents in the cluster. The value at each dimension of the vector is closely related to the generation probability based on the language model of the corresponding document. Inspired by the recent graph-based NLP methods, we reinforce the generation probabilities by iterating random walks on the underlying graph representation. Experiments with k-means and hierarchical clustering algorithms show significant improvements over the alternative tf\u00b7idf vector representation.","2490":"This article provides information on education models. The role of the use of an interactive model of education in the teaching of Uzbek language in schools where education is conducted in another language is also analyzed. The results of research conducted using an interactive learning model are presented. D. A. Khidoyatova \"Teaching the Uzbek Language is a Topical Issue: An Interactive Learning Model\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-6 , October 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd47546.pdf Paper URL : https:\/\/www.ijtsrd.com\/humanities-and-the-arts\/philosophy\/47546\/teaching-the-uzbek-language-is-a-topical-issue-an-interactive-learning-model\/d-a-khidoyatova","2491":"It has recently been observed that neural language models trained on\r\nunstructured text can implicitly store and retrieve knowledge using natural\r\nlanguage queries. In this short paper, we measure the practical utility of this\r\napproach by fine-tuning pre-trained models to answer questions without access\r\nto any external context or knowledge. We show that this approach scales\r\nsurprisingly well with model size and outperforms models that explicitly look\r\nup knowledge on the open-domain variants of Natural Questions and WebQuestions.\r\nTo facilitate reproducibility and future work, we release our code and trained\r\nmodels.","2492":"Pre-trained language models (LMs) have made significant advances in various\r\nNatural Language Processing (NLP) domains, but it is unclear to what extent\r\nthey can infer formal semantics in ontologies, which are often used to\r\nrepresent conceptual knowledge and serve as the schema of data graphs. To\r\ninvestigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of\r\ninference-based probing tasks and datasets from ontology subsumption axioms\r\ninvolving both atomic and complex concepts. We conduct extensive experiments on\r\nontologies of different domains and scales, and our results demonstrate that\r\nLMs encode relatively less background knowledge of Subsumption Inference (SI)\r\nthan traditional Natural Language Inference (NLI) but can improve on SI\r\nsignificantly when a small number of samples are given. We will open-source our\r\ncode and datasets.","2493":"Inductive transfer learning has greatly impacted computer vision, but\r\nexisting approaches in NLP still require task-specific modifications and\r\ntraining from scratch. We propose Universal Language Model Fine-tuning\r\n(ULMFiT), an effective transfer learning method that can be applied to any task\r\nin NLP, and introduce techniques that are key for fine-tuning a language model.\r\nOur method significantly outperforms the state-of-the-art on six text\r\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\r\nFurthermore, with only 100 labeled examples, it matches the performance of\r\ntraining from scratch on 100x more data. We open-source our pretrained models\r\nand code.","2494":"Advances in neural network language models have demonstrated that these models can effectively learn representations of words meaning. In this paper, we explore a variation of neural language models that can learn on concepts taken from structured ontologies and extracted from free-text, rather than directly from terms in free-text. This model is employed for the task of measuring semantic similarity between medical concepts, a task that is central to a number of techniques in medical informatics and information retrieval. The model is built with two medical corpora (journal abstracts and patient records) and empirically validated on two ground-truth datasets of human-judged concept pairs assessed by medical professionals. Empirically, our approach correlates closely with expert human assessors (\u22480.9) and outperforms a number of state-of-the-art benchmarks for medical semantic similarity. The demonstrated superiority of this model for providing an effective semantic similarity measure is promising in that this may translate into effectiveness gains for techniques in medical information retrieval and medical informatics (e.g., query expansion and literature-based discovery).","2495":"In this work, we propose a permutation invariant language model, SymphonyNet,\r\nas a solution for symbolic symphony music generation. We propose a novel\r\nMulti-track Multi-instrument Repeatable (MMR) representation for symphonic\r\nmusic and model the music sequence using a Transformer-based auto-regressive\r\nlanguage model with specific 3-D positional embedding. To overcome length\r\noverflow when modeling extra-long symphony tokens, we also propose a modified\r\nByte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel\r\nlinear transformer decoder architecture as a backbone. Meanwhile, we train the\r\ndecoder to learn automatic orchestration as a joint task by masking instrument\r\ninformation from the input. We also introduce a large-scale symbolic symphony\r\ndataset for the advance of symphony generation research. Empirical results show\r\nthat the proposed approach can generate coherent, novel, complex and harmonious\r\nsymphony as a pioneer solution for multi-track multi-instrument symbolic music\r\ngeneration.","2496":"Building recognition systems for historical documents is a difficult task. Especially, when it comes to medieval scripts. The complexity is mainly affected by the poor quality and the small quantity of the data available. In this paper we apply an HMM based recognition system to medieval manuscripts from the 13th century written in Middle High German. The recognition system, which was originally developed for modern scripts, has been adapted to medieval scripts. Beside the data processing, one of the major challenges is to create a suitable language model. Because of the lack of appropriate independent text corpora for medieval languages, the language model has to be created on the base of a rather small number of manuscripts only. Due to the small size of the corpus, optimizing the language model parameters can quickly lead to the problem of overfitting. In this paper we describe a strategy to integrate all available information into the language model and to optimize the language model parameters without suffering from this problem.","2497":null,"2498":null,"2499":"Task-oriented dialogue is often decomposed into three tasks: understanding\r\nuser input, deciding actions, and generating a response. While such\r\ndecomposition might suggest a dedicated model for each sub-task, we find a\r\nsimple, unified approach leads to state-of-the-art performance on the MultiWOZ\r\ndataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a\r\nsingle causal language model trained on all sub-tasks recast as a single\r\nsequence prediction problem. This allows SimpleTOD to fully leverage transfer\r\nlearning from pre-trained, open domain, causal language models such as GPT-2.\r\nSimpleTOD improves over the prior state-of-the-art by 0.49 points in joint goal\r\naccuracy for dialogue state tracking. More impressively, SimpleTOD also\r\nimproves the main metrics used to evaluate action decisions and response\r\ngeneration in an end-to-end setting for task-oriented dialog systems: inform\r\nrate by 8.1 points, success rate by 9.7 points, and combined score by 7.2\r\npoints.","2500":"Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.","2501":"This article provides information on education models. The role of the use of an interactive model of education in the teaching of Uzbek language in schools where education is conducted in another language is also analyzed. The results of research conducted using an interactive learning model are presented. D. A. Khidoyatova \"Teaching the Uzbek Language is a Topical Issue: On the Example of an Interactive Model of Education\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-6 , October 2021, URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd47644.pdf Paper URL : https:\/\/www.ijtsrd.com\/humanities-and-the-arts\/other\/47644\/teaching-the-uzbek-language-is-a-topical-issue-on-the-example-of-an-interactive-model-of-education\/d-a-khidoyatova","2502":null,"2503":null,"2504":"This article sets out to parse code switched discourse samples involving Bassa and French using the Matrix Language Frame Model proposed by Scotton. The model is based on the assumption that in code switching there must be a dominant language referred to as the Matrix Language ML and a gap filler language referred to as the Embedded Language EL . It is further claimed in this model that the ML is always the first language of the speaker who code switches and also the dominant language in the linguistic environment where code switching occurs. The model was applied to two codes Swahili English and French Arabic. In the present article the endeavor is to find out how Scotton's model unravels code switching in a multilingual context where three or four codes may be involved in modules considered to be two codes at surface level. The endeavor is also to find out how the model sorts out the dominant language i.e. the Matrix Language in Code switched discourse samples with a 50 50 morpheme split. It has equally been argued in the present contribution that discourse dominance or the morpheme frequency criterion is not the only variable to be used to predict and carve out the ML in switched or mixed discourse. There are indeed switched data where focus prevails over morpheme dominance.R\u00c3\u2030SUM\u00c3\u2030Cet article a pour but d'analyser des exemples de m\u00c3\u00a9lange de codes impliquant le Bassa et le fran\u00c3\u00a7ais en se servant du mod\u00c3\u00a8le propos\u00c3\u00a9 par Scotton lequel porte sur l'ossature de productions orales entrem\u00c3\u00aal\u00c3\u00a9es. Selon ce mod\u00c3\u00a8le dans chaque m\u00c3\u00a9lange de codes il y a n\u00c3\u00a9cesssairement une langue dominante appel\u00c3\u00a9e la Matrice et une langue secondaire celle des \u00c3\u00a9l\u00c3\u00a9ments enchas\u00c3\u00a9s. Le mod\u00c3\u00a8le soutient en outre que la matrice est toujours la premi\u00c3\u00a8re langue de l'individu qui proc\u00c3\u00a8de au m\u00c3\u00a9lange de codes et aussi la langue la plus en vue dans la communaut\u00c3\u00a9 linguistique o\u00c3\u00b9 le m\u00c3\u00a9lange de codes s'effectue. Ce mod\u00c3\u00a8le a \u00c3\u00a9t\u00c3\u00a9 test\u00c3\u00a9 sur deux codes le Swahili et l'anglais et l'arabe et le fran\u00c3\u00a7ais. Dans le pr\u00c3\u00a9sent article notre souci est de voir comment le mod\u00c3\u00a8le propos\u00c3\u00a9 par Scotton traite la question du m\u00c3\u00a9lange de codes dans un context multilingue o\u00c3\u00b9 trois voire quatre codes peuvent \u00c3\u00aatre impliqu\u00c3\u00a9s dans des modules consid\u00c3\u00a9r\u00c3\u00a9s comme deux codes du point de vue de la surface. Notre souci est aussi de voir comment le mod\u00c3\u00a8le de Scotton parvient \u00c3\u00a0 d\u00c3\u00a9gager la langue dominante c'est \u00c3\u00a0 dire la matrice dans les productions orales entrem\u00c3\u00aal\u00c3\u00a9es o\u00c3\u00b9 le m\u00c3\u00aame nombre de morph\u00c3\u00a8mes est enregistr\u00c3\u00a9 dans les deux langues ou codes m\u00c3\u00a9lang\u00c3\u00a9s . La pr\u00c3\u00a9sente contribution se propose aussi d'\u00c3\u00a9mettre l'id\u00c3\u00a9e selon laquelle la matrice d'une production orale m\u00c3\u00aal\u00c3\u00a9e ne se di\u00c3\u00a9tecte pas seulement sur la base de la dominance morphologique mais aussi et parfois sur le focus de la production. Mbeng Sampson Tambe \"Beyond Scotton's Matrix Language Frame Model: A Look into Some Bassa-French Code-Switched Discourse Samples\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd) ISSN: 2456-6470 Volume-3 | Issue-5  August 2019 URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd26561.pdfPaper URL: https:\/\/www.ijtsrd.com\/other-scientific-research-area\/other\/26561\/beyond-scotton%E2%80%99s-matrix-language-frame-model-a-look-into-some-bassa-french-code-switched-discourse-samples\/mbeng-sampson-tambe","2505":"State-of-the-art natural language understanding classification models follow\r\ntwo-stages: pre-training a large language model on an auxiliary task, and then\r\nfine-tuning the model on a task-specific labeled dataset using cross-entropy\r\nloss. Cross-entropy loss has several shortcomings that can lead to sub-optimal\r\ngeneralization and instability. Driven by the intuition that good\r\ngeneralization requires capturing the similarity between examples in one class\r\nand contrasting them with examples in other classes, we propose a supervised\r\ncontrastive learning (SCL) objective for the fine-tuning stage. Combined with\r\ncross-entropy, the SCL loss we propose obtains improvements over a strong\r\nRoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both the\r\nhigh-data and low-data regimes, and it does not require any specialized\r\narchitecture, data augmentation of any kind, memory banks, or additional\r\nunsupervised data. We also demonstrate that the new objective leads to models\r\nthat are more robust to different levels of noise in the training data, and can\r\ngeneralize better to related tasks with limited labeled task data.","2506":"Model checking is a successful formal verification technique; however, its application to fault-tolerant distributed algorithms is still not common practice. One major reason for this is that model checking requires non-negligible users\u2019 efforts in representing the algorithm to be verified in the input language of a model checker. To alleviate this problem we propose an approach which encompasses (i) a language for concisely describing fault-tolerant distributed algorithms and (ii) a translator from the proposed language to PROMELA, the input language of the SPIN model checker. To demonstrate the feasibility of our approach, we show the results of an experiment where we described and verified several algorithms for consensus, a well-known distributed agreement problem.","2507":"We present a specification language called Action Language for model checking software specifications. Action Language forms an interface between transition system models that a model checker generates and high level specification languages such as Statecharts, RSML and SCR-similar to an assembly language between a microprocessor and a programming language. We show that Action Language translations of Statecharts and SCR specifications are compact and they preserve the structure of the original specification. Action Language allows specification of both synchronous and asynchronous systems. It also supports modular specifications to enable compositional model checking","2508":"Although machines perform much better than human beings in most of the tasks, it is not the case of natural language processing. Computational linguistic systems usually rely on mathematical and statistical formalisms, which are efficient and useful but far from human procedures and therefore not so skilled. This paper proposes a computational model of natural language reading, called Cognitive Reading Indexing Model (CRIM), inspired by some aspects of human cognition, that tries to become as more psychologically plausible as possible. The model relies on a semantic neural network and it produces not vectors but nets of activated concepts as text representations. Based on these representations, measures of semantic similarity are also defined. Human comparison results show that the system is suitable to model human reading. Additional results also point out that the system could be used in real applications concerning natural language processing tasks.","2509":"Unit tests play a key role in ensuring the correctness of software. However,\r\nmanually creating unit tests is a laborious task, motivating the need for\r\nautomation. This paper presents TestPilot, an adaptive test generation\r\ntechnique that leverages Large Language Models (LLMs). TestPilot uses Codex, an\r\noff-the-shelf LLM, to automatically generate unit tests for a given program\r\nwithout requiring additional training or few-shot learning on examples of\r\nexisting tests. In our approach, Codex is provided with prompts that include\r\nthe signature and implementation of a function under test, along with usage\r\nexamples extracted from documentation. If a generated test fails, TestPilot's\r\nadaptive component attempts to generate a new test that fixes the problem by\r\nre-prompting the model with the failing test and error message. We created an\r\nimplementation of TestPilot for JavaScript and evaluated it on 25 npm packages\r\nwith a total of 1,684 API functions to generate tests for. Our results show\r\nthat the generated tests achieve up to 93.1% statement coverage (median 68.2%).\r\nMoreover, on average, 58.5% of the generated tests contain at least one\r\nassertion that exercises functionality from the package under test. Our\r\nexperiments with excluding parts of the information included in the prompts\r\nshow that all components contribute towards the generation of effective test\r\nsuites. Finally, we find that TestPilot does not generate memorized tests:\r\n92.7% of our generated tests have $\u0142eq$ 50% similarity with existing tests (as\r\nmeasured by normalized edit distance), with none of them being exact copies.","2510":"Pretrained language models are now ubiquitous in Natural Language Processing.\r\nDespite their success, most available models have either been trained on\r\nEnglish data or on the concatenation of data in multiple languages. This makes\r\npractical use of such models --in all languages except English-- very limited.\r\nIn this paper, we investigate the feasibility of training monolingual\r\nTransformer-based language models for other languages, taking French as an\r\nexample and evaluating our language models on part-of-speech tagging,\r\ndependency parsing, named entity recognition and natural language inference\r\ntasks. We show that the use of web crawled data is preferable to the use of\r\nWikipedia data. More surprisingly, we show that a relatively small web crawled\r\ndataset (4GB) leads to results that are as good as those obtained using larger\r\ndatasets (130+GB). Our best performing model CamemBERT reaches or improves the\r\nstate of the art in all four downstream tasks.","2511":"Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a\r\npowerful technique for natural language understanding and generation. Existing\r\npre-training techniques employ autoencoding and\/or autoregressive objectives to\r\ntrain Transformer-based models by recovering original word tokens from\r\ncorrupted text with some masked tokens. The training goals of existing\r\ntechniques are often inconsistent with the goals of many language generation\r\ntasks, such as generative question answering and conversational response\r\ngeneration, for producing new text given context.\r\nThis work presents PALM with a novel scheme that jointly pre-trains an\r\nautoencoding and autoregressive language model on a large unlabeled corpus,\r\nspecifically designed for generating new text conditioned on context. The new\r\nscheme alleviates the mismatch introduced by the existing denoising scheme\r\nbetween pre-training and fine-tuning where generation is more than\r\nreconstructing original text. An extensive set of experiments show that PALM\r\nachieves new state-of-the-art results on a variety of language generation\r\nbenchmarks covering generative question answering (Rank 1 on the official MARCO\r\nleaderboard), abstractive summarization on CNN\/DailyMail as well as Gigaword,\r\nquestion generation on SQuAD, and conversational response generation on Cornell\r\nMovie Dialogues.","2512":null,"2513":null,"2514":"Despite the success of large language models (LLMs) in various natural\r\nlanguage processing (NLP) tasks, the stored knowledge in these models may\r\ninevitably be incomplete, out-of-date, or incorrect. This motivates the need to\r\nutilize external knowledge to assist LLMs. Unfortunately, current methods for\r\nincorporating external knowledge often require additional training or\r\nfine-tuning, which can be costly and may not be feasible for LLMs. To address\r\nthis issue, we propose a novel post-processing approach, rethinking with\r\nretrieval (RR), which retrieves relevant external knowledge based on the\r\ndecomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\r\nThis lightweight approach does not require additional training or fine-tuning\r\nand is not limited by the input length of LLMs. We evaluate the effectiveness\r\nof RR through extensive experiments with GPT-3 on three complex reasoning\r\ntasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\r\nresults show that RR can produce more faithful explanations and improve the\r\nperformance of LLMs.","2515":"This paper investigates model merging, a tech-nique for deriving Markov models from text or speech corpora. Models are derived by starting with a large and specific model and by successi-vely combining states to build smaller and more general models. We present methods to reduce the time complexity of the algorithm and report on experiments on deriving language models for a speech recognition task. The experiments show the advantage of model merging over the standard bigram approach. The merged model assigns a lower perplexity to the test set and uses consi-derably fewer states.","2516":"Given the massive cost of language model pre-training, a non-trivial\r\nimprovement of the optimization algorithm would lead to a material reduction on\r\nthe time and cost of training. Adam and its variants have been state-of-the-art\r\nfor years, and more sophisticated second-order (Hessian-based) optimizers often\r\nincur too much per-step overhead. In this paper, we propose Sophia,\r\nSecond-order Clipped Stochastic Optimization, a simple scalable second-order\r\noptimizer that uses a light-weight estimate of the diagonal Hessian as the\r\npre-conditioner. The update is the moving average of the gradients divided by\r\nthe moving average of the estimated Hessian, followed by element-wise clipping.\r\nThe clipping controls the worst-case update size and tames the negative impact\r\nof non-convexity and rapid change of Hessian along the trajectory. Sophia only\r\nestimates the diagonal Hessian every handful of iterations, which has\r\nnegligible average per-step time and memory overhead. On language modeling with\r\nGPT-2 models of sizes ranging from 125M to 770M, Sophia achieves a 2x speed-up\r\ncompared with Adam in the number of steps, total compute, and wall-clock time.\r\nTheoretically, we show that Sophia adapts to the curvature in different\r\ncomponents of the parameters, which can be highly heterogeneous for language\r\nmodeling tasks. Our run-time bound does not depend on the condition number of\r\nthe loss.","2517":"We describe a simple neural language model that relies only on\r\ncharacter-level inputs. Predictions are still made at the word-level. Our model\r\nemploys a convolutional neural network (CNN) and a highway network over\r\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\r\nneural network language model (RNN-LM). On the English Penn Treebank the model\r\nis on par with the existing state-of-the-art despite having 60% fewer\r\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\r\nSpanish, Russian), the model outperforms word-level\/morpheme-level LSTM\r\nbaselines, again with fewer parameters. The results suggest that on many\r\nlanguages, character inputs are sufficient for language modeling. Analysis of\r\nword representations obtained from the character composition part of the model\r\nreveals that the model is able to encode, from characters only, both semantic\r\nand orthographic information.","2518":"This paper proposes the use of a language representation that specifies the relationship between terms of a sentence using question words. The proposed representation is tailored to help the search for documents containing an answer for a natural language question. This study presents the construction of this language model, the framework where it is used, and its evaluation.","2519":"While usage-based approaches to language development enjoy considerable support from\r\ncomputational studies, there have been few attempts to answer a key computational challenge posed by\r\nusage-based theory: the successful modeling of language learning as language use. We present a usagebased computational model of language acquisition which learns in a purely incremental fashion,\r\nthrough on-line processing based on chunking, and which offers broad, cross-linguistic coverage while\r\nuniting key aspects of comprehension and production within a single framework. The model\u2019s design\r\nreflects memory constraints imposed by the real-time nature of language processing, and is inspired by\r\npsycholinguistic evidence for children's sensitivity to the distributional properties of multi-word\r\nsequences and for shallow language comprehension based on local information. It learns from corpora\r\nof child-directed speech, chunking incoming words together to incrementally build an item-based\r\n\u201cshallow parse.\u201d When the model encounters an utterance made by the target child, it attempts to\r\ngenerate an identical utterance using the same chunks and statistics involved during comprehension.\r\nHigh performance is achieved on both comprehension- and production-related tasks: the model\u2019s\r\nshallow parsing is evaluated across 79 single-child corpora spanning English, French, and German,\r\nwhile its production performance is evaluated across over 200 single-child corpora representing 29\r\nlanguages from the CHILDES database. The model also succeeds in capturing findings from children\u2019s\r\nproduction of complex sentence types. Together, our modeling results suggest that much of children's\r\nearly linguistic behavior may be supported by item-based learning through on-line processing of simple\r\ndistributional cues, consistent with the notion that acquisition can be understood as learning to process\r\nlanguage.","2520":"The phenomenon of human language is widely studied from various points of view. It is interesting not only for social scientists, antropologists or philosophers, but also for those, interested in the network dynamics. In several recent papers word web, or language as a graph has been investigated R.F. Cancho, R. Sole, The small world of human language, Proc. R. Soc. London B 268 (2001) 2261-2265; A.E. Motter, P.S. de Moura, Lai Ying-Cheng, P. Dasgupta, Topology of the conceptual network of language, Phys. Rev. E 65 (2002) R 065102; M. Steyvers, J.B. Tenenbaum, The large-scale structure of semantic networks: Statistical analysis and a model of semantic growth, Cogn. Sci. 29 (2005) 41-78. In this paper I revise recent studies of syntactical word web R.E Cancho, R. Sole, The small world of human language, Proc. R. Soc. London B 268 (2001) 2261-2265; S.N. Dorogovtsev, J.F.F Mendes, Language as an evolving word web, Proc. R. Soc. London B 268 (2001) 2603-2606. 1 present a model of growing network in which such processes as node addition, edge rewiring and new link creation are taken into account. I argue, that this model is a satisfactory minimal model explaining measured data R.F. Cancho, R. Sole, The small world of human language, Proc. R. Soc. London B 268 (2001) 2261-2265; M. Markosova, P. Nather, Language as a graph, in: V Kvasnicka, P. Trebaticky, J. Pospichal (Eds.), Mind, Intelligence and Life, Kelemen, STU Bratislava, 2007, pp. 298-307 (in Slovak). (c) 2007 Elsevier B.V. All rights reserved.","2521":"Cost-effective development of large, integratedcomputer-based systems can be realized through systematicreuse of development experiences throughout thedevelopment procss. In this paper we describe a techniquefor representing reusable modeling experiences. The techniqueallows developers to express domain-specific designpatterns as a sub-language of the modeling language,the UML. Use of the sub-language to build application-specificUML models results in the reuse of the embeddeddesign experiences. We use a notation called the (meta-)Role-Based Modeling Language (RBML) to define UMLsub-languages. A (meta-)Role Model is a specializationof the UML (Unified Modeling Language) meta-model,that is, it determines a sub-language of the UML. We showhow RBML can be used to define domain-specific designpatterns.Keywords: Object-oriented design models, role-basedmodeling language, software reuse, UML","2522":"We present the model transformation language of the VIATRA2 framework, which provides a rule- and pattern-based transformation language for manipulating graph models by combining graph transformation and abstract state machines into a single specification paradigm. This language offers advanced constructs for querying (e.g. recursive graph patterns) and manipulating models (e.g. generic transformation and meta-transformation rules) in unidirectional model transformations frequently used in formal model analysis to carry out powerful abstractions.","2523":"In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based\r\nmodel, pretrained on a large corpus of Twitter messages on the topic of\r\nCOVID-19. Our model shows a 10-30% marginal improvement compared to its base\r\nmodel, BERT-Large, on five different classification datasets. The largest\r\nimprovements are on the target domain. Pretrained transformer models, such as\r\nCT-BERT, are trained on a specific target domain and can be used for a wide\r\nvariety of natural language processing tasks, including classification,\r\nquestion-answering and chatbots. CT-BERT is optimised to be used on COVID-19\r\ncontent, in particular social media posts from Twitter.","2524":"Recent work in unsupervised language modeling demonstrates that training\r\nlarge neural language models advances the state of the art in Natural Language\r\nProcessing applications. However, for very large models, memory constraints\r\nlimit the size of models that can be practically trained. Model parallelism\r\nallows us to train larger models, because the parameters can be split across\r\nmultiple processors. In this work, we implement a simple, efficient intra-layer\r\nmodel parallel approach that enables training state of the art transformer\r\nlanguage models with billions of parameters. Our approach does not require a\r\nnew compiler or library changes, is orthogonal and complimentary to pipeline\r\nmodel parallelism, and can be fully implemented with the insertion of a few\r\ncommunication operations in native PyTorch. We illustrate this approach by\r\nconverging an 8.3 billion parameter transformer language model using 512 GPUs,\r\nmaking it the largest transformer model ever trained at 24x times the size of\r\nBERT and 5.6x times the size of GPT-2. We sustain up to 15.1 PetaFLOPs per\r\nsecond across the entire application with 76% scaling efficiency, compared to a\r\nstrong single processor baseline that sustains 39 TeraFLOPs per second, which\r\nis 30% of peak FLOPs. The model is trained on 174GB of text, requiring 12\r\nZettaFLOPs over 9.2 days to converge. Transferring this language model achieves\r\nstate of the art (SOTA) results on the WikiText103 (10.8 compared to SOTA\r\nperplexity of 16.4) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)\r\ndatasets. We release training and evaluation code, as well as the weights of\r\nour smaller portable model, for reproducibility.","2525":null,"2526":null,"2527":null,"2528":"Purpose \u2013 The purpose of this paper is to develop a cross-language personalized recommendation model based on web log mining, which can recommend academic articles, in different languages, to users according to their demands. Design\/methodology\/approach \u2013 The proposed model takes advantage of web log data archived in digital libraries and learns user profiles by means of integration analysis of a user's multiple online behaviors. Moreover, keyword translation was carried out to eliminate language dissimilarity between user and item profiles. Finally, article recommendation can be achieved using various existing algorithms. Findings \u2013 The proposed model can recommend articles in different languages to users according to their demands, and the integration analysis of multiple online behaviors can help to better understand a user's interests. Practical implications \u2013 This study has a significant implication for digital libraries in non-English countries, since English is the most popular language in current academic articles and it is a very common phenomenon for users in these countries to obtain literatures presented by more than one language. Furthermore, this approach is also useful for other text-based item recommendation systems. Originality\/value \u2013 A lot of research work has been done in the personalized recommendation area, but few works have discussed the recommendation problem under multiple linguistic circumstances. This paper deals with cross-language recommendation and, moreover, the proposed model puts forward an integration analysis method based on multiple online behaviors to understand users' interests, which can provide references for other recommendation systems in the digital age.","2529":null,"2530":"Pseudo relevance feedback has demonstrated to be in general an effective technique for improving retrieval effectiveness, but the noise in the top retrieved documents still can cause topic drift problem that affects the performance of certain topics. By viewing a document as an interaction of a set of independent hidden topics, we propose a novel semantic clustering technique using independent component analysis. Then within the language modeling framework, we apply the obtained semantic topic clusters into the query sampling process so that the sampling depends on the activated topics rather than on the individual document language model. Therefore, we obtain a semantic cluster based relevance language model, which uses pseudo relevance feedback technique without requiring any relevance training information. We applied the model on five TREC data sets. The experiments show that our model can significantly improve retrieval performance over traditional language models including relevance-based and clustering-based retrieval language models. The main contribution of the improvements comes from the estimation of the relevance model on the semantic clusters that are closely related to the query.","2531":null,"2532":"Multidimensional data analysis or On-line analytical processing (OLAP) offers a single subject-oriented source for analyzing summary data based on various dimensions. We demonstrate that the OLAP approach gives a promising starting point for advanced analysis and comparison among summary data in informetrics applications. At the moment there is no single precise, commonly accepted logical\/conceptual model for multidimensional analysis. This is because the requirements of applications vary considerably. We develop a conceptual\/logical multidimensional model for supporting the complex and unpredictable needs of informetrics. Summary data are considered with respect of some dimensions. By changing dimensions the user may construct other views on the same summary data. We develop a multidimensional query language whose basic idea is to support the definition of views in a way, which is natural and intuitive for lay users in the informetrics area. We show that this view-oriented query language has a great expressive power and its degree of declarativity is greater than in contemporary operation-oriented or SQL (Structured Query Language)-like OLAP query languages. ABSTRACT FROM AUTHOR\nMultidimensional data analysis or On-line analytical processing (OLAP) offers a single subject-oriented source for analyzing summary data based on various dimensions. We demonstrate that the OLAP approach gives a promising starting point for advanced analysis and comparison among summary data in informetrics applications. At the moment there is no single precise, commonly accepted logical\/conceptual model for multidimensional analysis. This is because the requirements of applications vary considerably. We develop a conceptual\/logical multidimensional model for supporting the complex and unpredictable needs of informetrics. Summary data are considered with respect of some dimensions. By changing dimensions the user may construct other views on the same summary data. We develop a multidimensional query language whose basic idea is to support the definition of views in a way, which is natural and intuitive for lay users in the informetrics area. We show that this view-oriented query language has a great expressive power and its degree of declarativity is greater than in contemporary operation-oriented or SQL (Structured Query Language)-like OLAP query languages. ABSTRACT FROM AUTHOR","2533":"Recent work has demonstrated substantial gains on many NLP tasks and\r\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\r\na specific task. While typically task-agnostic in architecture, this method\r\nstill requires task-specific fine-tuning datasets of thousands or tens of\r\nthousands of examples. By contrast, humans can generally perform a new language\r\ntask from only a few examples or from simple instructions - something which\r\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\r\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\r\neven reaching competitiveness with prior state-of-the-art fine-tuning\r\napproaches. Specifically, we train GPT-3, an autoregressive language model with\r\n175 billion parameters, 10x more than any previous non-sparse language model,\r\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\r\napplied without any gradient updates or fine-tuning, with tasks and few-shot\r\ndemonstrations specified purely via text interaction with the model. GPT-3\r\nachieves strong performance on many NLP datasets, including translation,\r\nquestion-answering, and cloze tasks, as well as several tasks that require\r\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\r\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\r\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\r\nas well as some datasets where GPT-3 faces methodological issues related to\r\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\r\nof news articles which human evaluators have difficulty distinguishing from\r\narticles written by humans. We discuss broader societal impacts of this finding\r\nand of GPT-3 in general.","2534":null,"2535":"Truffle is a Java-based framework for developing high-performance language runtimes. Language implementers aiming at developing new runtimes have to design all the runtime mechanisms for managing dynamically typed objects from scratch. This not only leads to potential code duplication, but also impacts the actual time needed to develop a fully-fledged runtime. In this paper we address this issue by introducing a common object storage model (OSM) for Truffle that can be used by language implementers to develop new runtimes. The OSM is generic, language-agnostic, and portable, as it can be used to implement a great variety of dynamic languages. It is extensible, featuring built-in support for custom extension mechanisms. It is also high-performance, as it is designed to benefit from the optimizing compiler in the Truffle framework. Our initial evaluation indicates that the Truffle OSM can be used to implement high-performance language runtimes, with no performance overhead when compared to language-specific solutions.","2536":null,"2537":"Web service discovery is the process of finding a suitable Web service for a given user\u2019s query through analyzing the web service\u2018s WSDL content and finding the best match for the user\u2019s query. The service query should be written in the same language of the WSDL, for example English. Cross Language Information Retrieval techniques does not exist in the web service discovery process. The absence of CLIR methods limits the search language to the English language keywords only, which raises the following question \u201cHow do people that do not know the English Language find a web service, This paper proposes the application of CLIR techniques and IR methods to support Bilingual Web service discovery process the second language that proposed here is Arabic. Text mining techniques were applied on WSDL content and user\u2019s query to be ready for CLIR methods. The proposed model was tested on a curated catalogue of Life Science Web Services http:\/\/www.biocatalogue.org\/ and used for solving the research problem with 99.87 \\% accuracy and 95.06 precision","2538":null,"2539":"Electronic health record (EHR) data contains most of the important patient\r\nhealth information and is typically stored in a relational database with\r\nmultiple tables. One important way for doctors to make use of EHR data is to\r\nretrieve intuitive information by posing a sequence of questions against it.\r\nHowever, due to a large amount of information stored in it, effectively\r\nretrieving patient information from EHR data in a short time is still a\r\nchallenging issue for medical experts since it requires a good understanding of\r\na query language to get access to the database. We tackle this challenge by\r\ndeveloping a deep learning based approach that can translate a natural language\r\nquestion on multi-relational EHR data into its corresponding SQL query, which\r\nis referred to as a Question-to-SQL generation task. Most of the existing\r\nmethods cannot solve this problem since they primarily focus on tackling the\r\nquestions related to a single table under the table-aware assumption. While in\r\nour problem, it is possible that questions asked by clinicians are related to\r\nmultiple unspecified tables. In this paper, we first create a new question to\r\nquery dataset designed for healthcare to perform the Question-to-SQL generation\r\ntask, named MIMICSQL, based on a publicly available electronic medical\r\ndatabase. To address the challenge of generating queries on multi-relational\r\ndatabases from natural language questions, we propose a TRanslate-Edit Model\r\nfor Question-to-SQL query (TREQS), which adopts the sequence-to-sequence model\r\nto directly generate SQL query for a given question, and further edits it with\r\nan attentive-copying mechanism and task-specific look-up tables. Both\r\nquantitative and qualitative experimental results indicate the flexibility and\r\nefficiency of our proposed method in tackling challenges that are unique in\r\nMIMICSQL.","2540":null,"2541":"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\textbackslash&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.","2542":null,"2543":null,"2544":"Meta programming is the act of reasoning about a computational system. For example, a program in Prolog can reason about a program written in Smalltalk. Reflection is a more powerful form of meta programming where the same language is used to reason about, and act upon, itself in a causally connected way. Thus on the one hand we have meta programming that allows different languages or paradigms to be used, but without causal connection, while on the other hand we have reflection that offers causal connection but only for a single language. This paper combines both and presents inter-language reflection that allows one language to reason about and change in a causally connected way another language and vice versa. The fundamental aspects of inter-language reflection and the language symbiosis used therein, are discussed. Moreover the implementation of two symbiotic reflective languages is discussed: Agora\/Java and SOUL\/Smalltalk.","2545":"The support systems for conceptual modeling of today lack natural\r\n\tlanguage feedback. The paper argues for the need of natural language\r\n\tdiscourse for the validation of a conceptual model. Based on this\r\n\tconclusion a suggestion is made on a natural language discourse generation\r\n\tsystem as a validation tool and also as a support tool in simulating\r\n\ta conceptual model. Various appropriate natural language discourses\r\n\tare then proposed in the paper. To conclude the paper a support system\r\n\tbased on the natural language generation techniques of today and\r\n\ton previous working systems constructed by the author is suggested.","2546":"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.","2547":"Aiming at shortcomings existing in Unified Modeling Language (UML) statecharts studies, a formal syntactics of UML statecharts was firstly described. Then, an extended Petri net model, StateChart Net (SC_Net), which could precisely describe the semantic of UML statecharts was defined, The statechart could precisely describe translation among layers, translation conflict and translation concurrence. Data processing of statechart translation and communication mechanism between statecharts could also be described. And the translation process from UML statechart diagrams to SC_Net was defined. Finally, verification of correctness and consistency of UML behaviour models was discussed.","2548":null,"2549":"Technologically enhanced learning environments raise complex challenges for their designers, developers and users. Design patterns and pattern languages have recently emerged as a potential framework for addressing some of these challenges. However, the uptake of design patterns has been slow outside of the computer science community. We argue that this is largely a consequence of a weak positioning of pattern languages, as a form of delivering expert knowledge to layperson, and suggest an alternative view: the development of a pattern language as a community endeavour. In terms of open education, the workshop model can be viewed as an open production process for developing educational resources, in our case design patterns. We propose a model of pattern elicitation workshops, in which collaborative development of a pattern language provides a framework for sharing design knowledge within interdisciplinary communities. This model was iteratively developed at five international conferences. It was then postulated as a design pattern itself, encompassing a series of practices and a set of supporting tools.  We believe this model could be applied in a broad range of communities concerned with the development of open digital educational resources.","2550":"OWL 1.1 extends the W3C OWL Web Ontology Language with a small but useful set of features that have been requested by users, for which effective reasoning algorithms are now available, and that OWL tool developers are willing to support. The new features include extra syntactic sugar, additional property and qualified cardinality constructors, extended datatype support, simple metamodeling, and extended annotations. This document provides a model-theoretic semantics for OWL 1.1.","2551":"In this paper, we propose a novel dependency language modeling approach for information retrieval. The approach extends the existing language modeling approach by relaxing the independence assumption. Our goal is to build a language model in which various word relationships can be integrated. In this work, we integrate two types of relationship extracted from WordNet and co-occurrence relationships respectively. The integrated model has been tested on several TREC collections. The results show that our model achieves substantial and significant improvements with respect to the models without these relationships. These results clearly show the benefit of integrating word relationships into language models for IR.","2552":"We propose a new approach for thematic text clustering. The text clusters are used to generate domain specific language models in order to address the problem of language model adaptation. The method relies on a new discriminative n-gram based term selection process (n>l), which reduces the influence of the corpus inhomogeneity, and outputs only semantically focused n-grams as being the most representative key terms in the corpus. These key terms are then used to automatically cluster the whole document collection and generate LM out of these text clusters. Different key term selection methods are evaluated using perplexity as a measure. Automatically computed clusters are compared with manually assigned labelling according to genre information. The results of these experimental studies are presented and discussed. Compared to the manual clustering a significant performance improvement between 21.87 % and 53.12 % is observed depending on the chosen key term selection method.","2553":"Query\/Views\/Transformation (QVT) is the OMG standard language for specifying model transformations in the context of MDA. It is regarded as one of the most important standards since model transformations are proposed as major operations for manipulating models. In the first part of the paper we briefly summarize the typical transformation scenarios that developers encounter in software development and formulate key requirements for each scenario. This allows a comparison between the desirable and the formulated requirements for QVT. Such a comparison helps us to initially evaluate the adequacy of the QVT language.The second part of the paper focuses on the current state of the standard: the language architecture, specification, paradigm, and open issues. The three QVT sublanguages Operational Mappings, Relations, and Core are briefly described. Special attention is given to the currently available and expected tool support.","2554":"Model-based performance predictions and reconfigurations enable optimizing resource efficiency while ensuring that Quality-of-Service demands are met in today's complex ITsystems. The Descartes Modeling Language (DML) and the Palladio Component Model (PCM) are two architectural performance modeling formalisms applied in this context. This paper compares DML to PCM concerning similarities, differences and semantic gaps. Based on this, we propose a mapping from DML to PCM for which we implemented a tool realizing an automated transformation.","2555":"This paper aims to present an easy way of communication for speech impaired and hearing impaired people. This paper focuses on the use of neural network techniques, specifically MediaPipe Holistic and the Long Short-Term Memory (LSTM) module, to recognize sign language for individuals with disabilities. The utilization of MediaPipe Holistic, which integrates pose, hand, and face keypoints with precise levels, was used due to its low latency and high tracking accuracy in real-world scenarios. This paper deals with commonly used words of American Standard Sign Language. The result obtained shows that the proposed model can detect American Standard Sign Language with accuracy of 98.50 percentage.","2556":"Automatic prediction tools play a key role in enabling the application of non-functional analysis to the selection and the\r\nassembly of components for component-based systems, without requiring extensive knowledge of analysis methodologies to theapplication designer. A key idea to achieve this goal is to define a model transformation that takes as input some \u201cdesign-oriented\u201d model of the component assembly and produces as a result an \u201canalysis-oriented\u201dmodel that lends itself to the application of some analysis methodology. For this purpose, we define a model-driven transformationframework, centered around a kernel language whose aim is to capture the relevant information for the analysis of non-functionalattributes of component-based systems, with a focus on performance and reliability. Using this kernel language as a bridgebetween design-oriented and analysis-oriented notations we reduce the burden of defining a variety of direct transformationsfrom the former to the latter to the less complex problem of defining transformations to\/from the kernel language. The proposedkernel language is defined within the MOF (Meta-Object Facility) framework, to allow the exploitation of existing model transformationfacilities. In this chapter, we present the key concepts of our methodology and we show its application to the CoCoME casestudy.","2557":"Visual languages (VLs) play a central role in modelling various system aspects. Besides standard languages like UML, a variety of domain-specific languages exist which are the more used the more tool support is available for them. Different kinds of generators have been developed which produce visual modelling environments based on VL specifications. To define a VL, declarative as well as constructive approaches are used. The meta modelling approach is a declarative one where classes of symbols and relations are defined and associated to each other. Constraints describe additional language properties. Defining a VL by a graph grammar, the constructive way is followed where graphs describe the abstract syntax of models and graph rules formulate the language grammar. In this paper, we extend algebraic graph grammars by a node type inheritance concept which opens up the possibility to integrate both approaches by identifying symbol classes with node types and associations with edge types of some graph class. In this way, declarative as well as constructive elements may be used for language definition and model manipulation. Two concrete approaches, the GenGED and the AToM 3 approach, illustrate how VLs can be defined and models can be manipulated by the techniques described above.\r\nER  -","2558":null,"2559":null,"2560":"As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and\/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.","2561":null,"2562":"The process of Natural Language Generation for a Conversational Agent translates some semantic language to its surface form expressed in natural language. In this paper, we are going to show a Case Based Reasoning technique which is easily extensible and adaptable to multiple domains and languages, that generates coherent phrases and produces a natural outcome in the context of a Conversational Agent that maintains a dialogue with the user.","2563":"With the rise of Systems Biology as a new paradigm for understanding biological processes, the development of quantitative models is no longer restricted to a small circle of theoreticians. The dramatic increase in the number of these models precipitates the need to exchange and reuse both existing and newly created models. The Systems Biology Markup Language (SBML) is a free, open, XML-based format for representing quantitative models of biological interest that advocates the consistent specification of such models and thus facilitates both software development and model exchange.Principally oriented towards describing systems of biochemical reactions, such as cell signalling pathways, metabolic networks and gene regulation etc., SBML can also be used to encode any kinetic model. SBML offers mechanisms to describe biological components by means of compartments and reacting species, as well as their dynamic behaviour, using reactions, events and arbitrary mathematical rules. SBML also offers all the housekeeping structures needed to ensure an unambiguous understanding of quantitative descriptions.This is Release 1 of the specification for SBML Level 2 Version 4, describing the structures of the language and the rules used to build a valid model. SBML XML Schema and other related documents and software are also available from the SBML project web site, http:\/\/sbml.org\/.","2564":"The process of Natural Language Generation for a Conversational Agent translates some semantic language to its surface form expressed in natural language. In this paper, we are going to show a Case Based Reasoning technique which is easily extensible and adaptable to multiple domains and languages, that generates coherent phrases and produces a natural outcome in the context of a Conversational Agent that maintains a dialogue with the user.","2565":null,"2566":"Good explanations are essential to efficiently learning introductory programming concepts 10. To provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code 8, 12, defining terms 9, giving hints 16, and providing error-specific feedback 10, 16. However, these approaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code 7. For example, Github's Copilot can generate code for programmers 4, leading researchers to raise concerns about cheating 7. Instead, our work focuses on LLMs' potential to support learning by explaining numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code.","2567":"Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\\\\% F1 score improvement), biomedical relation extraction (2.80\\\\% F1 score improvement) and biomedical question answering (12.24\\\\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of BioBERT freely available at https:\/\/github.com\/naver\/biobert-pretrained, and the source code for fine-tuning BioBERT available at https:\/\/github.com\/dmis-lab\/biobert.","2568":null,"2569":"Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model\u2019s responses. We developed a prototype system \u2013 the Programmer\u2019s Assistant \u2013 in order to explore the utility of conversational interactions grounded in code, as well as software engineers\u2019 receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant\u2019s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.","2570":"Everyday life would be easier if we could simply talk with machines instead of having to program them. Before such talking robots can be built, however, there must be a theory of how communicating with natural language works. This requires not only a grammatical analysis of the language signs, but also a model of the cognitive agent, with interfaces for recognition and action, an internal database, and an algorithm for reading content in and out. In Database Semantics, these ingredients are used for reconstructing natural language communication as a mechanism for transferring content from the database of the speaker to the database of the hearer. Part I of this book presents a high-level description of an artificial agent which humans can freely communicate with in their accustomed language. Part II analyzes the major constructions of natural language, i.e., intra- and extrapropositional functor - argument structure, coordination, and coreference, in the speaker and the hearer mode. Part III defines declarative specifications for fragments of English, which are used for an implementation in Java. The book provides researchers, graduate students and software engineers with a functional framework for the theoretical analysis of natural language communication and for all practical applications of natural language processing.","2571":null,"2572":"Sarcasm identification on text documents is one of the most challenging tasks in natural language processing (NLP), has become an essential research direction, due to its prevalence on social media data. The purpose of our research is to present an effective sarcasm identification framework on social media data by pursuing the paradigms of neural language models and deep neural networks. To represent text documents, we introduce inverse gravity moment based term weighted word embedding model with trigrams. In this way, critical words\/terms have higher values by keeping the word-ordering information. In our model, we present a three-layer stacked bidirectional long short-term memory architecture to identify sarcastic text documents. For the evaluation task, the presented framework has been evaluated on three-sarcasm identification corpus. In the empirical analysis, three neural language models (i.e., word2vec, fastText and GloVe), two unsupervised term weighting functions (i.e., term-frequency, and TF-IDF) and eight supervised term weighting functions (i.e., odds ratio, relevance frequency, balanced distributional concentration, inverse question frequency-question frequency-inverse category frequency, short text weighting, inverse gravity moment, regularized entropy and inverse false negative-true positive-inverse category frequency) have been evaluated. For sarcasm identification task, the presented model yields promising results with a classification accuracy of 95.30%.","2573":"Knowledge Distillation (KD) is a promising technique for reducing the high\r\ncomputational demand of large language models (LLMs). However, previous KD\r\nmethods are primarily applied to white-box classification models or training\r\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\r\ndistill the knowledge from white-box generative LLMs is still under-explored,\r\nwhich becomes more and more important with the prosperity of LLMs. In this\r\nwork, we propose MiniLLM that distills smaller language models from generative\r\nlarger language models. We first replace the forward Kullback-Leibler\r\ndivergence (KLD) objective in the standard KD approaches with reverse KLD,\r\nwhich is more suitable for KD on generative language models, to prevent the\r\nstudent model from overestimating the low-probability regions of the teacher\r\ndistribution. Then, we derive an effective optimization approach to learn this\r\nobjective. Extensive experiments in the instruction-following setting show that\r\nthe MiniLLM models generate more precise responses with the higher overall\r\nquality, lower exposure bias, better calibration, and higher long-text\r\ngeneration performance. Our method is also scalable for different model\r\nfamilies with 120M to 13B parameters. We will release our code and model\r\ncheckpoints at https:\/\/aka.ms\/MiniLLM.","2574":"Intuitively, any \u2018bag of words\u2019 approach in IR should benefit from taking term dependencies into account. Unfortunately, for years the results of exploiting such dependencies have been mixed or inconclusive. To improve the situation, this paper shows how the natural language properties of the target documents can be used to transform and enrich the term dependencies to more useful statistics. This is done in three steps. The term co-occurrence statistics of queries and documents are each represented by a Markov chain. The paper proves that such a chain is ergodic, and therefore its asymptotic behavior is unique, stationary, and independent of the initial state. Next, the stationary distribution is taken to model queries and documents, rather than their initial distributions. Finally, ranking is achieved following the customary language modeling paradigm. The main contribution of this paper is to argue why the asymptotic behavior of the document model is a better representation then just the document\u2019s initial distribution. A secondary contribution is to investigate the practical application of this representation in case the queries become increasingly verbose. In the experiments (based on Lemur\u2019s search engine substrate) the default query model was replaced by the stable distribution of the query. Just modeling the query this way already resulted in significant improvements over a standard language model baseline. The results were on a par or better than more sophisticated algorithms that use fine-tuned parameters or extensive training. Moreover, the more verbose the query, the more effective the approach seems to become.","2575":"Domain-specific visual languages (DSVLs) are concise and useful tools that allow the rapid development of the behavior and\/or structure of applications in well-defined domains. These languages are typically developed specifically for a domain, and have a strong cohesion to the domain concepts, which often appear as primitives in the language. The strong cohesion between DSVL language primitives and the domain is a benefit for development by domain experts, but can be a drawback when the domain evolves--even when that evolution appears to be insignificant. This paper presents a domain-specific visual language developed expressly for the evolution of domain-specific visual languages, and uses concepts from graph rewriting to specify and carry out the transformation of the models built using the original DSVL.","2576":"Large language models (LMs) of code have recently shown tremendous promise in\r\ncompleting code and synthesizing code from natural language descriptions.\r\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\r\n2021)) are not publicly available, leaving many questions about their model and\r\ndata design decisions. We aim to fill in some of these blanks through a\r\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\r\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\r\nCodex itself is not open-source, we find that existing open-source models do\r\nachieve close results in some programming languages, although targeted mainly\r\nfor natural language modeling. We further identify an important missing piece\r\nin the form of a large open-source model trained exclusively on a multi-lingual\r\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\r\non the GPT-2 architecture, which was trained on 249GB of code across 12\r\nprogramming languages on a single machine. In the C programming language,\r\nPolyCoder outperforms all models including Codex. Our trained models are\r\nopen-source and publicly available at https:\/\/github.com\/VHellendoorn\/Code-LMs,\r\nwhich enables future research and application in this area.","2577":"With the growing importance of model-driven development, the ability of transforming\r\nmodels into well-defined semantic domains becomes a key to automated\r\ncode generation or verification in the software development process.\r\nIn this paper, we describe a high-level concept for specifying model transformations\r\nby means of typed, attributed graph transformation at the level of formal\r\nvisual language specifications for the source and the target language. At the implementation\r\nlevel, a graph-transformation based generator of visual editor Eclipse\r\nplug-ins from formal visual language specifications has been developed. On the\r\nbasis of this generator we discuss concepts for an implementation of the presented\r\nmodel transformation concepts and for an integration with the generated Eclipse\r\nplug-ins.\r\nWe explain the concepts for model transformation and their implementation along\r\na concrete model transformation from activity diagrams to Petri nets.","2578":"A Real Dynamic Cyber Trust Model is an application that is proposed in order to distinguish the trust belief among the trustees who have been marketing in today's digital world by authenticated users. Though we are happy with the developing technology still we are worrying about the security issues in every scenario. In the same way if we shop online by trusting some products there are few chances of getting bad products. We can experience these types of scenarios when we shop online in some interfaces like amazon e bay flip kart etc. Of course there are many existing systems which give a rating to the product that help the buyer to trust the seller and buy the product. Still there is an issue of being cheated by some wrong reviews given by unauthenticated users. In order to overcome that type of issues this Real Dynamic Cyber Trust Model has been proposed. A Real Dynamic Cyber Trust Model has taken the scenario of seller and buyer who goes shopping for products online. This Real Dynamic Cyber Trust model acts as an interface between the Seller and Buyer. Both Modules of Seller and Buyer have the opportunity of getting registered and log in to the application. Buyer can give the feedback of any product he buys and checks the trust factor of any seller. Whereas the Seller can add update or delete the products he sells. Based on the feedback given by the buyer the Real Dynamic Cyber Trust Model calculates the trust factor of the seller which helps the buyer to find out whether the seller is trustworthy or not. Kuchillapati Chinnari | Dr. Adusumalli Balaji \u00c4 Real Dynamic Cyber Trust Model\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd) ISSN: 2456-6470 Volume-3 | Issue-5  August 2019 URL: https:\/\/www.ijtsrd.com\/papers\/ijtsrd26420.pdfPaper URL: https:\/\/www.ijtsrd.com\/computer-science\/programming-language\/26420\/a-real-dynamic-cyber-trust-model\/kuchillapati-chinnari","2579":"We introduce REPLUG, a retrieval-augmented language modeling framework that\r\ntreats the language model (LM) as a black box and augments it with a tuneable\r\nretrieval model. Unlike prior retrieval-augmented LMs that train language\r\nmodels with special cross attention mechanisms to encode the retrieved text,\r\nREPLUG simply prepends retrieved documents to the input for the frozen\r\nblack-box LM. This simple design can be easily applied to any existing\r\nretrieval and language models. Furthermore, we show that the LM can be used to\r\nsupervise the retrieval model, which can then find documents that help the LM\r\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\r\nretriever significantly improves the performance of GPT-3 (175B) on language\r\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\r\n5.1%.","2580":"We investigate the optimal model size and number of tokens for training a\r\ntransformer language model under a given compute budget. We find that current\r\nlarge language models are significantly undertrained, a consequence of the\r\nrecent focus on scaling language models whilst keeping the amount of training\r\ndata constant. By training over 400 language models ranging from 70 million to\r\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\r\ncompute-optimal training, the model size and the number of training tokens\r\nshould be scaled equally: for every doubling of model size the number of\r\ntraining tokens should also be doubled. We test this hypothesis by training a\r\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\r\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\r\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\r\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\r\nevaluation tasks. This also means that Chinchilla uses substantially less\r\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\r\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\r\non the MMLU benchmark, greater than a 7% improvement over Gopher.","2581":"In the context of Model Engineering, work has focused on operations such as model validation and model transformation. By contrast, other model management operations of significant importance remain underdeveloped. One of the least elaborated operations is model merging. In this paper we discuss the special requirements of model merging and introduce the Epsilon Merging Language (EML), a rule-based language, with tool support, for merging models of diverse metamodels and technologies. Moreover, we identify special cases of model merging that are of particular interest and provide a working example through which we demonstrate the practicality and usefulness of the proposed language.","2582":"We present a new approach to extracting keyphrases based on statistical language models. Our approach is to use pointwise KL-divergence between multiple language models for scoring both phraseness and informativeness, which can be unified into a single score to rank extracted phrases.","2583":"Modern IT systems have increasingly distributed and dynamic architectures providing flexibility to adapt to changes in the environment and thus enabling higher resource efficiency. However, these benefits come at the cost of higher system complexity and dynamics. Thus, engineering systems that manage their end-to-end application performance and resource efficiency in an autonomic manner is a challenge. In this article, we present a holistic model-based approach for self-aware performance and resource management leveraging the Descartes Modeling Language (DML), an architecture-level modeling language for online performance and resource management. We propose a novel online performance prediction process that dynamically tailors the model solving depending on the requirements regarding accuracy and overhead. Using these prediction capabilities, we implement a generic model-based control loop for proactive system adaptation. We evaluate our model-based approach in the context of two representative case studies showing that with the proposed methods, significant resource efficiency gains can be achieved while maintaining performance requirements. These results represent the first end-to-end validation of our approach, demonstrating its potential for self-aware performance and resource management in the context of modern IT systems and infrastructures.","2584":"Language is essentially a complex, intricate system of human expressions\r\ngoverned by grammatical rules. It poses a significant challenge to develop\r\ncapable AI algorithms for comprehending and grasping a language. As a major\r\napproach, language modeling has been widely studied for language understanding\r\nand generation in the past two decades, evolving from statistical language\r\nmodels to neural language models. Recently, pre-trained language models (PLMs)\r\nhave been proposed by pre-training Transformer models over large-scale corpora,\r\nshowing strong capabilities in solving various NLP tasks. Since researchers\r\nhave found that model scaling can lead to performance improvement, they further\r\nstudy the scaling effect by increasing the model size to an even larger size.\r\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\r\nlanguage models not only achieve a significant performance improvement but also\r\nshow some special abilities that are not present in small-scale language\r\nmodels. To discriminate the difference in parameter scale, the research\r\ncommunity has coined the term large language models (LLM) for the PLMs of\r\nsignificant size. Recently, the research on LLMs has been largely advanced by\r\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\r\nwhich has attracted widespread attention from society. The technical evolution\r\nof LLMs has been making an important impact on the entire AI community, which\r\nwould revolutionize the way how we develop and use AI algorithms. In this\r\nsurvey, we review the recent advances of LLMs by introducing the background,\r\nkey findings, and mainstream techniques. In particular, we focus on four major\r\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\r\ncapacity evaluation. Besides, we also summarize the available resources for\r\ndeveloping LLMs and discuss the remaining issues for future directions.","2585":null,"2586":"Context-aware Web services are emerging as a promising technology for the electronic businesses in mobile and pervasive environments. Unfortunately, complex context-aware services are still hard to build. In this paper, we present a modeling language for the model-driven development of context-aware Web services based on theUnified Modeling Language (UML). Specifically, we show how UML can be used to specify information related to the design of context-aware services. We present the abstract syntax and notation of the language and illustrate its usage using an example service. Our language offers significant design flexibility that considerably simplifies the development of context-aware Web services.","2587":null,"2588":null,"2589":null,"2590":null,"2591":null,"2592":"Building accurate language models that capture meaningful long-term\r\ndependencies is a core challenge in natural language processing. Towards this\r\nend, we present a calibration-based approach to measure long-term discrepancies\r\nbetween a generative sequence model and the true distribution, and use these\r\ndiscrepancies to improve the model. Empirically, we show that state-of-the-art\r\nlanguage models, including LSTMs and Transformers, are miscalibrated:\r\nthe entropy rates of their generations drift dramatically upward over time. We\r\nthen provide provable methods to mitigate this phenomenon. Furthermore, we show\r\nhow this calibration-based approach can also be used to measure the amount of\r\nmemory that language models use for prediction.","2593":null,"2594":"We propose an extension to neural network language models to adapt their\r\nprediction to the recent history. Our model is a simplified version of memory\r\naugmented networks, which stores past hidden activations as memory and accesses\r\nthem through a dot product with the current hidden activation. This mechanism\r\nis very efficient and scales to very large memory sizes. We also draw a link\r\nbetween the use of external memory in neural network and cache models used with\r\ncount based language models. We demonstrate on several language model datasets\r\nthat our approach performs significantly better than recent memory augmented\r\nnetworks.","2595":"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.","2596":"We investigate interoperability aspects of scientific workflow systems and argue that the workflow execution environment, the model of computation (MoC), and the workflow language form three dimensions that must be considered depending on the type of interoperability sought: at the activity, sub-workflow, or workflow levels. With a focus on the problems that affect interoperability, we illustrate how these issues are tackled by current scientific workflows as well as how similar problems have been addressed in related areas. Our long-term objective is to achieve (logical) interoperability between workflow systems operating under different MoCs, using distinct language features, and sharing activities running on different execution environments. (C) 2009 Elsevier B.V. All rights reserved.","2597":"We introduce factored language models\n\n(FLMs) and generalized parallel backoff\n\n(GPB). An FLM represents words as bundles\n\nof features (e.g., morphological classes, stems,\n\ndata-driven clusters, etc.), and induces a probability\n\nmodel covering sequences of bundles\n\nrather than just words. GPB extends standard\n\nbackoff to general conditional probability\n\ntables where variables might be heterogeneous\n\ntypes, where no obvious natural (temporal)\n\nbackoff order exists, and where multiple\n\ndynamic...","2598":null,"2599":null,"2600":"Many theories and solutions have been proposed for the improvement\r\nof the learning efficiency of secondary language(L2) learning. However neither an\r\nunified view on the functionality of semantics in aiding learning nor an objective\r\nmeasure of the efficiency improvement at theoretical level has been presented in existing\r\nliterature. This situation hinders the efficient adoption of the semantics aided\r\nlearning and the explicit planning of a semantics aided learning process. We aim to\r\nfill these gaps by adopting an evolutionary strategy towards approaching a holistic\r\nsolution. Firstly we model the general learning process from cognitive linguistic perspective\r\nat the memory level. Then we justify the functionality of semantics aided\r\napproach according to specific conditions. Thereafter we propose the quantity measure\r\nfor the improvement of learning efficiency in terms of reuse level for semantics\r\naided secondary language learning from the perspective of value based analysis.","2601":"Making language models bigger does not inherently make them better at\r\nfollowing a user's intent. For example, large language models can generate\r\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\r\nwords, these models are not aligned with their users. In this paper, we show an\r\navenue for aligning language models with user intent on a wide range of tasks\r\nby fine-tuning with human feedback. Starting with a set of labeler-written\r\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\r\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\r\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\r\noutputs, which we use to further fine-tune this supervised model using\r\nreinforcement learning from human feedback. We call the resulting models\r\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\r\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\r\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\r\nimprovements in truthfulness and reductions in toxic output generation while\r\nhaving minimal performance regressions on public NLP datasets. Even though\r\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\r\nhuman feedback is a promising direction for aligning language models with human\r\nintent.","2602":"Computer-based systems for communication with humans are a cornerstone of AI research since the 1950s. So far, the most effective way to assess the quality of the dialogues produced by these systems is to use resource-intensive manual labor instead of automated means. In this work, we investigate whether language models (LM) based on transformer neural networks can indicate the quality of a conversation. In a general sense, language models are methods that learn to predict one or more words based on an already given context. Due to their unsupervised nature, they are candidates for efficient, automatic indication of dialogue quality. We demonstrate that human evaluators have a positive correlation between the output of the language models and scores. We also provide some insights into their behavior and inner-working in a conversational context.","2603":"Large neural language models trained on massive amounts of text have emerged\r\nas a formidable strategy for Natural Language Understanding tasks. However, the\r\nstrength of these models as Natural Language Generators is less clear. Though\r\nanecdotal evidence suggests that these models generate better quality text,\r\nthere has been no detailed study characterizing their generation abilities. In\r\nthis work, we compare the performance of an extensively pretrained model,\r\nOpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story\r\ngeneration model (Fan et al., 2018). By evaluating the generated text across a\r\nwide variety of automatic metrics, we characterize the ways in which pretrained\r\nmodels do, and do not, make better storytellers. We find that although GPT2-117\r\nconditions more strongly on context, is more sensitive to ordering of events,\r\nand uses more unusual words, it is just as likely to produce repetitive and\r\nunder-diverse text when using likelihood-maximizing decoding algorithms.","2604":null,"2605":"Previous research on cluster-based retrieval has been inconclusive as to whether it does bring improved retrieval effectiveness over document-based retrieval. Recent developments in the language modeling approach to IR have motivated us to re-examine this problem within this new retrieval framework. We propose two new models for cluster-based retrieval and evaluate them on several TREC collections. We show that cluster-based retrieval can perform consistently across collections of realistic size, and significant improvements over document-based retrieval can be obtained in a fully automatic manner and without relevance information provided by human.","2606":"The article develops a scientific and pedagogical content and model of the formation of media culture in future foreign language teachers on the basis of interdisciplinary cooperation.","2607":"The recent advent of large language models has reinvigorated debate over\r\nwhether human cognitive capacities might emerge in such generic models given\r\nsufficient training data. Of particular interest is the ability of these models\r\nto reason about novel problems zero-shot, without any direct training. In human\r\ncognition, this capacity is closely tied to an ability to reason by analogy.\r\nHere, we performed a direct comparison between human reasoners and a large\r\nlanguage model (the text-davinci-003 variant of GPT-3) on a range of analogical\r\ntasks, including a non-visual matrix reasoning task based on the rule structure\r\nof Raven's Standard Progressive Matrices. We found that GPT-3 displayed a\r\nsurprisingly strong capacity for abstract pattern induction, matching or even\r\nsurpassing human capabilities in most settings; preliminary tests of GPT-4\r\nindicated even better performance. Our results indicate that large language\r\nmodels such as GPT-3 have acquired an emergent ability to find zero-shot\r\nsolutions to a broad range of analogy problems.","2608":"Although scaling up language model size has reliably improved performance on\r\na range of NLP tasks, even the largest models currently struggle with certain\r\nreasoning tasks such as math word problems, symbolic manipulation, and\r\ncommonsense reasoning. This paper explores the ability of language models to\r\ngenerate a coherent chain of thought -- a series of short sentences that mimic\r\nthe reasoning process a person might have when responding to a question.\r\nExperiments show that inducing a chain of thought via prompting can enable\r\nsufficiently large language models to better perform reasoning tasks that\r\notherwise have flat scaling curves. When combined with the 540B parameter PaLM\r\nmodel, chain of thought prompting achieves new state of the art of 58.1\\% on\r\nthe GSM8K benchmark of math word problems.","2609":null,"2610":null,"2611":null,"2612":"Enriching existing medical terminology knowledge bases (KBs) is an important\r\nand never-ending work for clinical research because new terminology alias may\r\nbe continually added and standard terminologies may be newly renamed. In this\r\npaper, we propose a novel automatic terminology enriching approach to\r\nsupplement a set of terminologies to KBs. Specifically, terminology and entity\r\ncharacters are first fed into pre-trained language model to obtain semantic\r\nembedding. The pre-trained model is used again to initialize the terminology\r\nand entity representations, then they are further embedded through graph\r\nconvolutional network to gain structure embedding. Afterwards, both semantic\r\nand structure embeddings are combined to measure the relevancy between the\r\nterminology and the entity. Finally, the optimal alignment is achieved based on\r\nthe order of relevancy between the terminology and all the entities in the KB.\r\nExperimental results on clinical indicator terminology KB, collected from 38\r\ntop-class hospitals of Shanghai Hospital Development Center, show that our\r\nproposed approach outperforms baseline methods and can effectively enrich the\r\nKB.","2613":"This document describes the CLIPS experiments done for the CLEF 2005 campaign. We use surface-syntactic parser in order to extract new indexing terms. These terms are syntactic dependencies. Our goal is to evaluate their interest for an information retrieval task. We used them under different forms in different information retrieval models, particularly in a language model. For the bilingual part we tried two simple tests on Spanish and German to French evaluation, for the translation we use a lemmatization and a dictionary.","2614":null,"2615":null,"2616":null,"2617":null,"2618":null,"2619":null,"2620":null,"2621":null,"2622":null,"2623":null,"2624":null,"2625":null,"2626":"Determining the intended sense of words in text -- word sense disambiguation\r\n(WSD) -- is a long-standing problem in natural language processing. In this\r\npaper, we present WSD algorithms which use neural network language models to\r\nachieve state-of-the-art precision. Each of these methods learns to\r\ndisambiguate word senses using only a set of word senses, a few example\r\nsentences for each sense taken from a licensed lexicon, and a large unlabeled\r\ntext corpus. We classify based on cosine similarity of vectors derived from the\r\ncontexts in unlabeled query and labeled example sentences. We demonstrate\r\nstate-of-the-art results when using the WordNet sense inventory, and\r\nsignificantly better than baseline performance using the New Oxford American\r\nDictionary inventory. The best performance was achieved by combining an LSTM\r\nlanguage model with graph label propagation.","2627":null,"2628":null,"2629":null,"2630":null,"2631":null,"2632":null,"2633":null,"2634":null,"2635":"Retrieval-Augmented Language Modeling (RALM) methods, which condition a\r\nlanguage model (LM) on relevant documents from a grounding corpus during\r\ngeneration, were shown to significantly improve language modeling performance.\r\nIn addition, they can mitigate the problem of factually inaccurate text\r\ngeneration and provide natural source attribution mechanism. Existing RALM\r\napproaches focus on modifying the LM architecture in order to facilitate the\r\nincorporation of external information, significantly complicating deployment.\r\nThis paper considers a simple alternative, which we dub In-Context RALM:\r\nleaving the LM architecture unchanged and prepending grounding documents to the\r\ninput, without any further training of the LM. We show that In-Context RALM\r\nthat builds on off-the-shelf general purpose retrievers provides surprisingly\r\nlarge LM gains across model sizes and diverse corpora. We also demonstrate that\r\nthe document retrieval and ranking mechanism can be specialized to the RALM\r\nsetting to further boost performance. We conclude that In-Context RALM has\r\nconsiderable potential to increase the prevalence of LM grounding, particularly\r\nin settings where a pretrained LM must be used without modification or even via\r\nAPI access.","2636":"Originally, meta-models were used to specify the structure (abstract syntax) of modelling languages. This is reflected both in meta-languages like MOF and Ecore, and the four-layer meta-model architecture. Presently, meta-modelling is used for specification of complete languages. In this situation, it turns out that the traditional meta-languages are not always expressive enough to capture all language aspects. This usually implies the use of more than one metalanguage in the meta-model architecture to cover the different language aspects. There are many approaches to address this challenge. In this paper, we analyze these approaches, and based on this analysis, we re-think the meta-model architecture focusing on complete language specifications. In our meta-model architecture, each aspect of a language conforms to an aspect-specific meta-language at the level above, and models can reside at different levels depending on their context and use. This meta-model architecture is easier to understand, more flexible and more extensible; therefore it may be useful in the design of meta-model-based language specification platforms, as well as for promoting the understanding of the principles of meta-modelling.","2637":"An important paradigm of natural language processing consists of large-scale\r\npre-training on general domain data and adaptation to particular tasks or\r\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\r\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\r\ndeploying independent instances of fine-tuned models, each with 175B\r\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\r\nLoRA, which freezes the pre-trained model weights and injects trainable rank\r\ndecomposition matrices into each layer of the Transformer architecture, greatly\r\nreducing the number of trainable parameters for downstream tasks. Compared to\r\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\r\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\r\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\r\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\r\ntraining throughput, and, unlike adapters, no additional inference latency. We\r\nalso provide an empirical investigation into rank-deficiency in language model\r\nadaptation, which sheds light on the efficacy of LoRA. We release a package\r\nthat facilitates the integration of LoRA with PyTorch models and provide our\r\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\r\nhttps:\/\/github.com\/microsoft\/LoRA.","2638":null,"2639":null,"2640":null,"2641":null,"2642":null,"2643":null,"2644":"We present CodeBERT, a bimodal pre-trained model for programming language\r\n(PL) and nat-ural language (NL). CodeBERT learns general-purpose\r\nrepresentations that support downstream NL-PL applications such as natural\r\nlanguage codesearch, code documentation generation, etc. We develop CodeBERT\r\nwith Transformer-based neural architecture, and train it with a hybrid\r\nobjective function that incorporates the pre-training task of replaced token\r\ndetection, which is to detect plausible alternatives sampled from generators.\r\nThis enables us to utilize both bimodal data of NL-PL pairs and unimodal data,\r\nwhere the former provides input tokens for model training while the latter\r\nhelps to learn better generators. We evaluate CodeBERT on two NL-PL\r\napplications by fine-tuning model parameters. Results show that CodeBERT\r\nachieves state-of-the-art performance on both natural language code search and\r\ncode documentation generation tasks. Furthermore, to investigate what type of\r\nknowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and\r\nevaluate in a zero-shot setting where parameters of pre-trained models are\r\nfixed. Results show that CodeBERT performs better than previous pre-trained\r\nmodels on NL-PL probing.","2645":null,"2646":"Large language models (LLMs) have shown the potential of revolutionizing\r\nnatural language processing tasks in diverse domains, sparking great interest\r\nin finance. Accessing high-quality financial data is the first challenge for\r\nfinancial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken\r\nadvantage of their unique data accumulation, such privileged access calls for\r\nan open-source alternative to democratize Internet-scale financial data.\r\n  In this paper, we present an open-source large language model, FinGPT, for\r\nthe finance sector. Unlike proprietary models, FinGPT takes a data-centric\r\napproach, providing researchers and practitioners with accessible and\r\ntransparent resources to develop their FinLLMs. We highlight the importance of\r\nan automatic data curation pipeline and the lightweight low-rank adaptation\r\ntechnique in building FinGPT. Furthermore, we showcase several potential\r\napplications as stepping stones for users, such as robo-advising, algorithmic\r\ntrading, and low-code development. Through collaborative efforts within the\r\nopen-source AI4Finance community, FinGPT aims to stimulate innovation,\r\ndemocratize FinLLMs, and unlock new opportunities in open finance. Two\r\nassociated code repos are https:\/\/github.com\/AI4Finance-Foundation\/FinGPT\r\nand https:\/\/github.com\/AI4Finance-Foundation\/FinNLP","2647":"Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural re-ranking approach to ad hoc information retrieval: we reorder the documents in an initially retrieved set by exploiting asymmetric relationships between them. Specifically, we consider generation links, which indicate that the language model induced from one document assigns high probability to the text of another; in doing so, we take care to prevent bias against long documents. We study a number of re-ranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks.","2648":"Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT).Supplementary data are available at Bioinformatics online.","2649":null,"2650":null,"2651":null,"2652":"Model transformation languages have matured to a point where people have started experimenting with model transformation definitions themselves in addition to the language they are written in. In addition to the transformation language properties, the properties of model transformation definitions themselves become important, such as scalability, maintainability and reusability. Composition of model transformations allows for the creation of smaller, maintainable and reusable model transformation definitions that can scale up to a larger model transformation. There are two kinds of composition for model transformations. External composition deals with chaining separate model transformations together by passing models from one transformation to another. Internal composition composes two model transformation definitions into one new model transformation, which typically requires knowledge of the transformation language. This paper focuses on internal composition for two rule-based model transformation languages. One is the ATLAS Transformation Language, which serves as our implementation vehicle. The other is the QVT Relations language, which is a standard transformation language for MOF. We propose a composition technique called module superimposition. We discuss how module superimposition interacts with other composition techniques in ATL, such as helpers, called rules and rule inheritance. Together, these techniques allow for powerful composition of entire transformation modules as well as individual transformation rules. By applying superimposition to QVT Relations, we demonstrate that our composition technique is relevant outside the ATL language as well.","2653":null,"2654":"SIBI (Sistem Isyarat Bahasa Indonesia) is the commonly used sign language in Indonesia. SIBI, which follows Indonesian language's grammatical structure, is a complex and unique sign language. A method to recognize SIBI gestures in a rapid, precise and efficient manner needs to be developed for the SIBI machine translation system. Feature extraction method with space-efficient feature set and at the same time retained its capability to recognize different types of SIBI gestures is the ultimate goal. There are four types of SIBI gestures: root, affix, inflectional and function word gestures. This paper proposed to use heuristic Hidden Markov Model and a feature extraction system to separate inflectional gesture into its constituents, prefix, suffix and root. The separation reduces the amount of feature sets that would otherwise as big as the product of the prefixes, suffixes and root words feature sets of the inflectional word gestures.","2655":"We explore how generating a chain of thought -- a series of intermediate\r\nreasoning steps -- significantly improves the ability of large language models\r\nto perform complex reasoning. In particular, we show how such reasoning\r\nabilities emerge naturally in sufficiently large language models via a simple\r\nmethod called chain of thought prompting, where a few chain of thought\r\ndemonstrations are provided as exemplars in prompting. Experiments on three\r\nlarge language models show that chain of thought prompting improves performance\r\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\r\nempirical gains can be striking. For instance, prompting a 540B-parameter\r\nlanguage model with just eight chain of thought exemplars achieves state of the\r\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\r\nfinetuned GPT-3 with a verifier.","2656":null,"2657":null,"2658":null,"2659":null,"2660":null,"2661":null,"2662":null,"2663":null,"2664":null,"2665":null,"2666":null,"2667":null,"2668":null,"2669":null,"2670":null,"2671":null,"2672":null,"2673":null,"2674":null,"2675":null,"2676":null,"2677":null,"2678":null,"2679":null,"2680":null,"2681":null,"2682":null,"2683":null,"2684":null,"2685":null,"2686":null,"2687":null,"2688":null,"2689":null,"2690":null,"2691":null,"2692":null,"2693":null,"2694":null,"2695":null,"2696":null,"2697":null,"2698":null,"2699":null,"2700":null,"2701":null,"2702":null,"2703":null,"2704":null,"2705":null,"2706":null,"2707":"Large language models (LLMs) have demonstrated remarkable potential in\r\nhandling multilingual machine translation (MMT). In this paper, we\r\nsystematically investigate the advantages and challenges of LLMs for MMT by\r\nanswering two questions: 1) How well do LLMs perform in translating a massive\r\nnumber of languages? 2) Which factors affect LLMs' performance in translation?\r\nWe evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102\r\nlanguages. Our empirical results show that even the best model ChatGPT still\r\nlags behind the supervised baseline NLLB in 83.33% of translation directions.\r\nThrough further analysis, we discover that LLMs exhibit new working patterns\r\nwhen used for MMT. First, prompt semantics can surprisingly be ignored when\r\ngiven in-context exemplars, where LLMs still show strong performance even with\r\nunreasonable prompts. Second, cross-lingual exemplars can provide better task\r\ninstruction for low-resource translation than exemplars in the same language\r\npairs. Third, we observe the overestimated performance of BLOOMZ on dataset\r\nFlores-101, indicating the potential risk when using public datasets for\r\nevaluation.","2708":"Large language models have shown impressive few-shot results on a wide range\r\nof tasks. However, when knowledge is key for such results, as is the case for\r\ntasks such as question answering and fact checking, massive parameter counts to\r\nstore knowledge seem to be needed. Retrieval augmented models are known to\r\nexcel at knowledge intensive tasks without the need for as many parameters, but\r\nit is unclear whether they work in few-shot settings. In this work we present\r\nAtlas, a carefully designed and pre-trained retrieval augmented language model\r\nable to learn knowledge intensive tasks with very few training examples. We\r\nperform evaluations on a wide range of tasks, including MMLU, KILT and\r\nNaturalQuestions, and study the impact of the content of the document index,\r\nshowing that it can easily be updated. Notably, Atlas reaches over 42% accuracy\r\non Natural Questions using only 64 examples, outperforming a 540B parameters\r\nmodel by 3% despite having 50x fewer parameters.","2709":null,"2710":null,"2711":null,"2712":null,"2713":null,"2714":null,"2715":"Intelligent applications and agents on the Semantic Web typically need to be specified with, or interact with specifications written in, many different kinds of formal languages. Such languages include ontology languages, data and metadata query languages, as well as transformation languages. As learnt from years of experience in development of complex software systems, languages need to support some form of component-based development. Components enable higher software quality, better understanding and reusability of already developed artifacts. Any component approach contains an underlying component model, a description detailing what valid components are and how components can interact. With the multitude of languages developed for the Semantic Web, what are their underlying component models? Do we need to develop one for each language, or is a more general and reusable approach achievable? We present a language-driven component model specification approach. This means that a component model can be (automatically) generated from a given base language (actually, its specification, e.g. its grammar). As a consequence, we can provide components for different languages and simplify the development of software artifacts used on the Semantic Web.","2716":null,"2717":null,"2718":null,"2719":null,"2720":null,"2721":"Neural language representation models such as BERT 1 have recently shown state of the art performance in downstream NLP tasks and bio-medical domain adaptation of BERT (Bio-BERT 2) has shown same behavior on biomedical text mining tasks. However, due to their large model size and resulting increased computational need, practical application of models such as BERT is challenging making smaller models with comparable performance desirable for real word applications. Recently, a new language transformers based language representation model named ELECTRA 3 is introduced, that makes efficient usage of training data in a generative-discriminative neural model setting that shows performance gains over BERT. These gains are especially impressive for smaller models. Here, we introduce a small ELECTRA based model named Bio-ELECTRA that is eight times smaller than BERT BASE and achieves comparable performance on biomedical question answering and yes\/no question answer classification tasks. The model is pre-trained from scratch on PubMed abstracts using a consumer grade GPU with only 8GB memory. For biomedical named entity recognition, however, large BERT Base model outperforms both Bio-ELECTRA and ELECTRA-Small++.Competing Interest StatementThe authors have declared no competing interest.","2722":null,"2723":null,"2724":null,"2725":null,"2726":null,"2727":"Language Models (LMs) and Knowledge Graphs (KGs) are both active research areas in Machine Learning and Semantic Web. While LMs have brought great improvements for many downstream tasks on their own, they are often combined with KGs providing additionally aggregated, well structured knowledge. Usually, this is done by leveraging KGs to improve LMs. But what happens if we turn this around and use LMs to improve KGs?","2728":"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo\/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","2729":null,"2730":null,"2731":null,"2732":null,"2733":null,"2734":null,"2735":"We give an in-depth account of compositional matrix-space models (CMSMs), a type of generic models for natural language, wherein compositionality is realized via matrix multiplication. We argue for the structural plausibility of this model and show that it is able to cover and combine various common compositional natural language processing approaches. Then, we consider efficient task-specific learning methods for training CMSMs and evaluate their performance in compositionality prediction and sentiment analysis.","2736":null,"2737":"Transforming models is a crucial activity in Model Driven Engineering (MDE). With the adoption of the OMG QVT standard for model transformation languages, it is anticipated that the experience in applying model transformations in various domains will increase. However, the QVT standard is just one possible approach for solving model transformation problems. In parallel with the QVT activity, many research groups and companies have been working on their own model transformation approaches and languages. It is important for software developers to be able to compare and select the most suitable languages and tools for a particular problem. This paper compares several model-to-model transformation languages as a step in the direction of gathering knowledge about the existing model transformation approaches. The focus is on the major language components (sublanguages and their features, execution tools, etc.) and how they are related. The major goal is to motivate the need for language interoperability and to explore options and obstacles for such interoperability. We propose a set of heuristics to reason about the problems that must be addressed when translators between languages have to be developed. These heuristics are applied on several examples. The experience from these examples shows that achieving a large degree of interoperability is difficult since some languages expose incompatible features. We managed to identify, however, cases where the interoperability between languages is feasible and brings certain benefits.","2738":"In this paper, we investigate the relationship between smoothing in language models and idf weights. Language models regard the relative within-document-frequency and the relative collection frequency; idf weights are very similar to the latter, but yield higher weights for rare terms. Regarding the correlation between the language model parameters and relevance for two test collections, we find that the idf type of weighting seems to be more appropriate. Based on the observed correlation, we devise empirical smoothing as a new type of term weighting for language models, and retrieval experiments confirm the general applicability of our method. Finally, we show that the most appropriate form of describing the relationship between the language model parameters and relevance seems to be a product form, which confirms a language model proposed before.","2739":"Language models demonstrate both quantitative improvement and new qualitative\r\ncapabilities with increasing scale. Despite their potentially transformative\r\nimpact, these new capabilities are as yet poorly characterized. In order to\r\ninform future research, prepare for disruptive new model capabilities, and\r\nameliorate socially harmful effects, it is vital that we understand the present\r\nand near-future capabilities and limitations of language models. To address\r\nthis challenge, we introduce the Beyond the Imitation Game benchmark\r\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450\r\nauthors across 132 institutions. Task topics are diverse, drawing problems from\r\nlinguistics, childhood development, math, common-sense reasoning, biology,\r\nphysics, social bias, software development, and beyond. BIG-bench focuses on\r\ntasks that are believed to be beyond the capabilities of current language\r\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\r\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\r\nacross model sizes spanning millions to hundreds of billions of parameters. In\r\naddition, a team of human expert raters performed all tasks in order to provide\r\na strong baseline. Findings include: model performance and calibration both\r\nimprove with scale, but are poor in absolute terms (and when compared with\r\nrater performance); performance is remarkably similar across model classes,\r\nthough with benefits from sparsity; tasks that improve gradually and\r\npredictably commonly involve a large knowledge or memorization component,\r\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\r\ninvolve multiple steps or components, or brittle metrics; social bias typically\r\nincreases with scale in settings with ambiguous context, but this can be\r\nimproved with prompting.","2740":null,"2741":null,"2742":"A difficult challenge in the industrialisation of Model-Driven Development is managing different versions of models. Different versions may arise at any time during the development process, due to different individuals or teams working on different parts of the overall model. To manage these versions it is necessary to be able to identify differences and reconcile these differences in a single, integrated model. We describe the use of model merging technology for managing different versions of a model in an industrial software development process. The use of automated model merging technology is contrasted with an alternative, semi-automated approach. The contributions of model merging to helping to solve this problem are outlined.","2743":null,"2744":"Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora---Penn Treebank, WikiText-2, and CNN\/Daily Mail---resulting in similar conclusions.","2745":null,"2746":null,"2747":null,"2748":null,"2749":null,"2750":null,"2751":null,"2752":null,"2753":null,"2754":null,"2755":null,"2756":null,"2757":null,"2758":null,"2759":null,"2760":null,"2761":null,"2762":null,"2763":null,"2764":null,"2765":null,"2766":null,"2767":null,"2768":null,"2769":null,"2770":null,"2771":null,"2772":null,"2773":null,"2774":null,"2775":null,"2776":null,"2777":null,"2778":null,"2779":null,"2780":null,"2781":null,"2782":null,"2783":null,"2784":null,"2785":null,"2786":null,"2787":null,"2788":null,"2789":null,"2790":null,"2791":null,"2792":null,"2793":null,"2794":null,"2795":null,"2796":null,"2797":null,"2798":null,"2799":null,"2800":null,"2801":null,"2802":null,"2803":null,"2804":null,"2805":null,"2806":null,"2807":null,"2808":null,"2809":null,"2810":null,"2811":null,"2812":null,"2813":null,"2814":null,"2815":null,"2816":null,"2817":null,"2818":null,"2819":null,"2820":null,"2821":null,"2822":null,"2823":null,"2824":null,"2825":null,"2826":null,"2827":null,"2828":null,"2829":null,"2830":null,"2831":null,"2832":null,"2833":null,"2834":null,"2835":null,"2836":null,"2837":null,"2838":null,"2839":null,"2840":null,"2841":"Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.","2842":"In this paper, we define and apply representational stability analysis\r\n(ReStA), an intuitive way of analyzing neural language models. ReStA is a\r\nvariant of the popular representational similarity analysis (RSA) in cognitive\r\nneuroscience. While RSA can be used to compare representations in models, model\r\ncomponents, and human brains, ReStA compares instances of the same model, while\r\nsystematically varying single model parameter. Using ReStA, we study four\r\nrecent and successful neural language models, and evaluate how sensitive their\r\ninternal representations are to the amount of prior context. Using RSA, we\r\nperform a systematic study of how similar the representational spaces in the\r\nfirst and second (or higher) layers of these models are to each other and to\r\npatterns of activation in the human brain. Our results reveal surprisingly\r\nstrong differences between language models, and give insights into where the\r\ndeep linguistic processing, that integrates information over multiple\r\nsentences, is happening in these models. The combination of ReStA and RSA on\r\nmodels and brains allows us to start addressing the important question of what\r\nkind of linguistic processes we can hope to observe in fMRI brain imaging data.\r\nIn particular, our results suggest that the data on story reading from Wehbe et\r\nal. (2014) contains a signal of shallow linguistic processing, but show no\r\nevidence on the more interesting deep linguistic processing.","2843":"Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT\/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.","2844":null,"2845":null,"2846":null,"2847":null,"2848":null,"2849":null,"2850":null,"2851":null,"2852":null,"2853":null,"2854":null,"2855":null,"2856":"Transformers have a potential of learning longer-term dependency, but are\r\nlimited by a fixed-length context in the setting of language modeling. We\r\npropose a novel neural architecture Transformer-XL that enables learning\r\ndependency beyond a fixed length without disrupting temporal coherence. It\r\nconsists of a segment-level recurrence mechanism and a novel positional\r\nencoding scheme. Our method not only enables capturing longer-term dependency,\r\nbut also resolves the context fragmentation problem. As a result,\r\nTransformer-XL learns dependency that is 80\\% longer than RNNs and 450\\% longer\r\nthan vanilla Transformers, achieves better performance on both short and long\r\nsequences, and is up to 1,800+ times faster than vanilla Transformers during\r\nevaluation. Notably, we improve the state-of-the-art results of bpc\/perplexity\r\nto 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion\r\nWord, and 54.5 on Penn Treebank (without finetuning). When trained only on\r\nWikiText-103, Transformer-XL manages to generate reasonably coherent, novel\r\ntext articles with thousands of tokens. Our code, pretrained models, and\r\nhyperparameters are available in both Tensorflow and PyTorch.","2857":null,"2858":null,"2859":"While large language models (LLMs) have demonstrated impressive capabilities\r\nacross tasks in language understanding and interactive decision making, their\r\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\r\naction plan generation) have primarily been studied as separate topics. In this\r\npaper, we explore the use of LLMs to generate both reasoning traces and\r\ntask-specific actions in an interleaved manner, allowing for greater synergy\r\nbetween the two: reasoning traces help the model induce, track, and update\r\naction plans as well as handle exceptions, while actions allow it to interface\r\nwith external sources, such as knowledge bases or environments, to gather\r\nadditional information. We apply our approach, named ReAct, to a diverse set of\r\nlanguage and decision making tasks and demonstrate its effectiveness over\r\nstate-of-the-art baselines, as well as improved human interpretability and\r\ntrustworthiness over methods without reasoning or acting components.\r\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\r\nReAct overcomes issues of hallucination and error propagation prevalent in\r\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\r\ngenerates human-like task-solving trajectories that are more interpretable than\r\nbaselines without reasoning traces. On two interactive decision making\r\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\r\nreinforcement learning methods by an absolute success rate of 34% and 10%\r\nrespectively, while being prompted with only one or two in-context examples.\r\nProject site with code: https:\/\/react-lm.github.io","2860":"Pretrained language models, especially masked language models (MLMs) have\r\nseen success across many NLP tasks. However, there is ample evidence that they\r\nuse the cultural biases that are undoubtedly present in the corpora they are\r\ntrained on, implicitly creating harm with biased representations. To measure\r\nsome forms of social bias in language models against protected demographic\r\ngroups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark\r\n(CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing\r\nwith nine types of bias, like race, religion, and age. In CrowS-Pairs a model\r\nis presented with two sentences: one that is more stereotyping and another that\r\nis less stereotyping. The data focuses on stereotypes about historically\r\ndisadvantaged groups and contrasts them with advantaged groups. We find that\r\nall three of the widely-used MLMs we evaluate substantially favor sentences\r\nthat express stereotypes in every category in CrowS-Pairs. As work on building\r\nless biased models advances, this dataset can be used as a benchmark to\r\nevaluate progress.","2861":null,"2862":null,"2863":null,"2864":null,"2865":null,"2866":"Multi-emotion sentiment classification is a natural language processing (NLP)\r\nproblem with valuable use cases on real-world data. We demonstrate that\r\nlarge-scale unsupervised language modeling combined with finetuning offers a\r\npractical solution to this task on difficult datasets, including those with\r\nlabel class imbalance and domain-specific context. By training an\r\nattention-based Transformer network (Vaswani et al. 2017) on 40GB of text\r\n(Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our\r\nmodel achieves a 0.69 F1 score on the SemEval Task 1:E-c multi-dimensional\r\nemotion classification problem (Mohammad et al. 2018), based on the Plutchik\r\nwheel of emotions (Plutchik 1979). These results are competitive with state of\r\nthe art models, including strong F1 scores on difficult (emotion) categories\r\nsuch as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive\r\nresults on rare categories such as Anticipation (0.42) and Surprise (0.37).\r\nFurthermore, we demonstrate our application on a real world text classification\r\ntask. We create a narrowly collected text dataset of real tweets on several\r\ntopics, and show that our finetuned model outperforms general purpose\r\ncommercially available APIs for sentiment and multidimensional emotion\r\nclassification on this dataset by a significant margin. We also perform a\r\nvariety of additional studies, investigating properties of deep learning\r\narchitectures, datasets and algorithms for achieving practical multidimensional\r\nsentiment classification. Overall, we find that unsupervised language modeling\r\nand finetuning is a simple framework for achieving high quality results on\r\nreal-world sentiment classification.","2867":null,"2868":null,"2869":"Existing pre-trained large language models have shown unparalleled generative\r\ncapabilities. However, they are not controllable. In this paper, we propose\r\nMEGATRON-CNTRL, a novel framework that uses large-scale language models and\r\nadds control to text generation by incorporating an external knowledge base.\r\nOur framework consists of a keyword predictor, a knowledge retriever, a\r\ncontextual knowledge ranker, and a conditional text generator. As we do not\r\nhave access to ground-truth supervision for the knowledge ranker, we make use\r\nof weak supervision from sentence embedding. The empirical results show that\r\nour model generates more fluent, consistent, and coherent stories with less\r\nrepetition and higher diversity compared to prior work on the ROC story\r\ndataset. We showcase the controllability of our model by replacing the keywords\r\nused to generate stories and re-running the generation process. Human\r\nevaluation results show that 77.5% of these stories are successfully controlled\r\nby the new keywords. Furthermore, by scaling our model from 124 million to 8.3\r\nbillion parameters we demonstrate that larger models improve both the quality\r\nof generation (from 74.5% to 93.0% for consistency) and controllability (from\r\n77.5% to 91.5%).","2870":null,"2871":"We introduce a new language representation model called BERT, which stands\r\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\r\nlanguage representation models, BERT is designed to pre-train deep\r\nbidirectional representations from unlabeled text by jointly conditioning on\r\nboth left and right context in all layers. As a result, the pre-trained BERT\r\nmodel can be fine-tuned with just one additional output layer to create\r\nstate-of-the-art models for a wide range of tasks, such as question answering\r\nand language inference, without substantial task-specific architecture\r\nmodifications.\r\n  BERT is conceptually simple and empirically powerful. It obtains new\r\nstate-of-the-art results on eleven natural language processing tasks, including\r\npushing the GLUE score to 80.5\\% (7.7\\% point absolute improvement), MultiNLI\r\naccuracy to 86.7\\% (4.6\\% absolute improvement), SQuAD v1.1 question answering\r\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\r\n(5.1 point absolute improvement).","2872":null,"2873":null,"2874":null,"2875":null,"2876":null,"2877":null,"2878":null,"2879":null,"2880":null,"2881":null,"2882":null,"2883":null,"2884":null,"2885":null,"2886":null,"2887":null,"2888":null,"2889":null,"2890":null,"2891":null,"2892":null,"2893":null,"2894":null,"2895":null,"2896":null,"2897":null,"2898":null,"2899":null,"2900":null,"2901":null,"2902":null,"2903":null,"2904":null,"2905":null,"2906":null,"2907":null,"2908":null,"2909":null,"2910":null,"2911":null,"2912":null,"2913":null,"2914":null,"2915":null,"2916":null,"2917":null,"2918":null,"2919":null,"2920":null,"2921":null,"2922":null,"2923":null,"2924":null,"2925":null,"2926":null,"2927":null,"2928":null,"2929":null,"2930":null,"2931":null,"2932":null,"2933":null,"2934":null,"2935":null,"2936":null,"2937":null,"2938":"A very important decision was made in 1989 the so called ' Zero Version ' which wasn't as harsh as in the other Baltic republics. It is stated that everyone living in Lithuania, until the enactment of the Citizenship Law, has the right to become a citizen of the Republic of Lithuania. This Law gave all inhabitants of Lithuania the possibility to gain citizenship without qualification even though international rights don't include such an obligation. On December 5, 1991 the new Citizenship Law of the Republic of Lithuania was accepted and it meet international standards.","2939":null,"2940":null,"2941":null,"2942":null,"2943":null,"2944":null,"2945":null,"2946":null,"2947":null,"2948":null,"2949":null,"2950":null,"2951":null,"2952":null,"2953":null,"2954":null,"2955":null,"2956":null,"2957":null,"2958":null,"2959":null,"2960":null,"2961":null,"2962":null,"2963":null,"2964":null,"2965":null,"2966":null,"2967":null,"2968":null,"2969":null,"2970":null,"2971":null,"2972":null,"2973":null,"2974":null,"2975":null,"2976":null,"2977":null,"2978":null,"2979":null,"2980":null,"2981":null,"2982":null,"2983":null,"2984":null,"2985":null,"2986":null,"2987":null,"2988":null,"2989":null,"2990":null,"2991":null,"2992":null,"2993":null,"2994":null,"2995":null,"2996":null,"2997":null,"2998":null,"2999":null,"3000":null,"3001":null,"3002":null,"3003":null,"3004":null,"3005":null,"3006":null,"3007":null,"3008":null,"3009":null,"3010":null,"3011":null,"3012":null,"3013":null,"3014":null,"3015":null,"3016":null,"3017":null,"3018":null,"3019":null,"3020":null,"3021":null,"3022":null,"3023":null,"3024":null,"3025":null,"3026":null,"3027":null,"3028":null,"3029":null,"3030":null,"3031":null,"3032":null,"3033":null,"3034":null,"3035":null,"3036":null,"3037":null,"3038":null,"3039":null,"3040":null,"3041":"Previous research has shown that passage-level evidence can bring added benefits to document retrieval when documents are long or span different subject areas. Recent developments in language modeling approach to IR provided a new effective alternative to traditional retrieval models. These two streams of research motivate us to examine the use of passages in a language model framework. This paper reports on experiments using passages in a simple language model and a relevance model, and compares the results with document-based retrieval. Results from the INQUERY search engine, which is not based on a language modeling approach, are also given for comparison. Test data include two heterogeneous and one homogeneous document collections. Our experiments show that passage retrieval is feasible in the language modeling context, and more importantly, it can provide more reliable performance than retrieval based on full documents.","3042":null,"3043":null,"3044":null,"3045":null,"3046":null,"3047":null,"3048":null,"3049":null,"3050":null,"3051":null,"3052":null,"3053":null,"3054":null,"3055":null,"3056":null,"3057":null,"3058":null,"3059":null,"3060":null,"3061":null,"3062":null,"3063":null,"3064":null,"3065":null,"3066":null,"3067":null,"3068":null,"3069":null,"3070":null,"3071":null,"3072":null,"3073":null,"3074":null,"3075":null,"3076":null,"3077":null,"3078":null,"3079":null,"3080":null,"3081":null,"3082":null,"3083":null,"3084":null,"3085":null,"3086":null,"3087":null,"3088":null,"3089":null,"3090":"We develop a new Low-level, First-order Probabilistic Programming Language\r\n(LF-PPL) suited for models containing a mix of continuous, discrete, and\/or\r\npiecewise-continuous variables. The key success of this language and its\r\ncompilation scheme is in its ability to automatically distinguish parameters\r\nthe density function is discontinuous with respect to, while further providing\r\nruntime checks for boundary crossings. This enables the introduction of new\r\ninference engines that are able to exploit gradient information, while\r\nremaining efficient for models which are not everywhere differentiable. We\r\ndemonstrate this ability by incorporating a discontinuous Hamiltonian Monte\r\nCarlo (DHMC) inference engine that is able to deliver automated and efficient\r\ninference for non-differentiable models. Our system is backed up by a\r\nmathematical formalism that ensures that any model expressed in this language\r\nhas a density with measure zero discontinuities to maintain the validity of the\r\ninference engine.","3091":"In this article we show how the problem of neural text generation can be\r\nconstructively reformulated in terms of transitions between the states of a\r\nfinite-state machine. This framework leads to an efficient approach to guiding\r\ntext generation with regular expressions and context-free grammars by allowing\r\nthe construction of an index over a language model's vocabulary. The approach\r\nis model agnostic, allows one to enforce domain-specific knowledge and\r\nconstraints, and enables the construction of reliable interfaces by\r\nguaranteeing the structure of the generated text. It adds little overhead to\r\nthe token sequence generation process and significantly outperforms existing\r\nsolutions. An implementation is provided in the open source Python library\r\nOutlines","3092":null,"3093":null,"3094":null,"3095":null,"3096":null,"3097":null,"3098":null,"3099":null,"3100":null,"3101":null,"3102":null,"3103":null,"3104":null,"3105":null,"3106":null,"3107":"Access controls is an important IT security issue and has accordingly been a huge research topic for the last decade. Many models and role engineering methods have been provided since then, and RBAC has appeared to be one of the most significant contributions. In parallel to those developments, new requirements have appeared in the field of IT governance and they provide new constraints for the elicitation of access control policies. One of those requirements is to have access rights strictly aligned with the business process and to have the responsibility of the employees involved in those processes strictly defined and suitably assigned to the employee. RBAC doesn?t permit to integrate these new requirements. In this paper we propose a responsibility modeling language to align access rights with business processes requirements. To achieve that, our approach uses the concept of employees? responsibility as a means to bridge the gap through frameworks from the business layer down to frameworks from the technical layer.","3108":null,"3109":"\"In this thoroughly revised edition, Jos and Anneke offer a concise, pragmatic, and pedagogic explanation of the Object Constraint Language (OCL) and its different applications. Their discussion of OCL's potential role in Model Driven Architecture (MDA) is timely and offers great insight into the way that UML can be taken to the next level of automated software development practice. I highly recommend this book to anyone who is looking to get the most out of UML.\"\r\n\r\n\u2014Shane Sendall, PhD, Senior Researcher, Swiss Federal Institute of Technology in Lausanne\r\n\r\nThe release of Unified Modeling Language (UML) 2.0 places renewed emphasis on the Object Constraint Language (OCL). Within UML, OCL is the standard for specifying expressions that add vital information to object-oriented models and other object-modeling artifacts. Model Driven Architecture (MDA) relies on OCL to add the level of programming detail necessary to enable platform-specific models (PSM) to communicate with platform-independent models (PIM).\r\n\r\nThis book is a practical, accessible guide to OCL for software architects, designers, and developers. Much care has been taken during the redesign of OCL to ensure that the syntax remains readable and writable by the average software modeler. The Object Constraint Language, Second Edition, utilizes a case study to show how to exercise these compact but powerful expressions for maximum effect.\r\n\r\nThis newly updated edition\r\n# Explains why OCL is critical to MDA--and why UML alone is not enough\r\n# Introduces an SQL-like syntax to OCL\r\n# Defines the new language constructs of OCL 2.0\r\n# Demonstrates how OCL can be incorporated into code\r\n# Shares tips and tricks for applying OCL to real-world modeling challenges\u2014showing which can be solved with UML and which require OCL\r\n\r\nUsing a combination of UML and OCL allows developers to realize the effective, consistent, and coherent models that are critical to working with MDA. The authors' pragmatic approach and illustrative use of examples will help application developers come quickly up to speed with this important object-modeling method\u2014and will serve as a ready reference thereafter.","3110":null,"3111":null,"3112":null,"3113":"Modern language models can generate high-quality short texts. However, they\r\noften meander or are incoherent when generating longer texts. These issues\r\narise from the next-token-only language modeling objective. To address these\r\nissues, we introduce Time Control (TC), a language model that implicitly plans\r\nvia a latent stochastic process. TC does this by learning a representation\r\nwhich maps the dynamics of how text changes in a document to the dynamics of a\r\nstochastic process of interest. Using this representation, the language model\r\ncan generate text by first implicitly generating a document plan via a\r\nstochastic process, and then generating text that is consistent with this\r\nlatent plan. Compared to domain-specific methods and fine-tuning GPT2 across a\r\nvariety of text domains, TC improves performance on text infilling and\r\ndiscourse coherence. On long text generation settings, TC preserves the text\r\nstructure both in terms of ordering (up to +40% better) and text length\r\nconsistency (up to +17% better). Human evaluators also prefer TC's output 28.6%\r\nmore than the baselines.","3114":"Understanding the causal relationships that underlie a system is a\r\nfundamental prerequisite to accurate decision-making. In this work, we explore\r\nhow expert knowledge can be used to improve the data-driven identification of\r\ncausal graphs, beyond Markov equivalence classes. In doing so, we consider a\r\nsetting where we can query an expert about the orientation of causal\r\nrelationships between variables, but where the expert may provide erroneous\r\ninformation. We propose strategies for amending such expert knowledge based on\r\nconsistency properties, e.g., acyclicity and conditional independencies in the\r\nequivalence class. We then report a case study, on real data, where a large\r\nlanguage model is used as an imperfect expert.","3115":null,"3116":null,"3117":null,"3118":null,"3119":null,"3120":null,"3121":null,"3122":null,"3123":null,"3124":null,"3125":null,"3126":null,"3127":null,"3128":null,"3129":null,"3130":null,"3131":null,"3132":null,"3133":null,"3134":null,"3135":null,"3136":null,"3137":null,"3138":null,"3139":null,"3140":null,"3141":null,"3142":null,"3143":null,"3144":null,"3145":null,"3146":null,"3147":null,"3148":null,"3149":null,"3150":null,"3151":null,"3152":null,"3153":null,"3154":null,"3155":null,"3156":null,"3157":null,"3158":null,"3159":null,"3160":null,"3161":null,"3162":null,"3163":null,"3164":null,"3165":null,"3166":null,"3167":null,"3168":null,"3169":null,"3170":null,"3171":null,"3172":null,"3173":null,"3174":null,"3175":null,"3176":null,"3177":null,"3178":null,"3179":null,"3180":null,"3181":null,"3182":null,"3183":null,"3184":null,"3185":null,"3186":null,"3187":null,"3188":null,"3189":null,"3190":null,"3191":null,"3192":null,"3193":null,"3194":null,"3195":null,"3196":null,"3197":null,"3198":null,"3199":null,"3200":null,"3201":null,"3202":null,"3203":null,"3204":null,"3205":null,"3206":null,"3207":null,"3208":null,"3209":null,"3210":null,"3211":null,"3212":null,"3213":null,"3214":null,"3215":null,"3216":null,"3217":null,"3218":null,"3219":null,"3220":null,"3221":null,"3222":null,"3223":null,"3224":null,"3225":null,"3226":null,"3227":null,"3228":null,"3229":null,"3230":null,"3231":null,"3232":null,"3233":null,"3234":null,"3235":null,"3236":null,"3237":null,"3238":null,"3239":null,"3240":null,"3241":null,"3242":null,"3243":null,"3244":null,"3245":null,"3246":null,"3247":null,"3248":null,"3249":null,"3250":null,"3251":null,"3252":null,"3253":null,"3254":null,"3255":null,"3256":null,"3257":null,"3258":null,"3259":null,"3260":null,"3261":null,"3262":null,"3263":null,"3264":null,"3265":null,"3266":null,"3267":null,"3268":null,"3269":null,"3270":null,"3271":null,"3272":null,"3273":null,"3274":null,"3275":null,"3276":null,"3277":null,"3278":null,"3279":null,"3280":null,"3281":null,"3282":null,"3283":null,"3284":null,"3285":null,"3286":null,"3287":null,"3288":null,"3289":null,"3290":null,"3291":null,"3292":null,"3293":null,"3294":null,"3295":null,"3296":null,"3297":null,"3298":null,"3299":null,"3300":null,"3301":null,"3302":null,"3303":null,"3304":null,"3305":null,"3306":"This paper introduces an open computational\r\nframework for visual perception and grounded\r\nlanguage acquisition called Experience-Based\r\nLanguage Acquisition (EBLA). EBLA can\r\n\"watch\" a series of short videos and acquire a\r\nsimple language of nouns and verbs corresponding to the objects and object-object relations in those videos. Upon acquiring this\r\nprotolanguage, EBLA can perform basic\r\nscene analysis to generate descriptions of\r\nnovel videos.\r\nThe performance of EBLA has been evaluated\r\nbased on accuracy and speed of protolanguage\r\nacquisition as well as on accuracy of generated scene descriptions. For a test set of simple animations, EBLA had average acquisition\r\nsuccess rates as high as 100% and average description success rates as high as 96.7%. For\r\na larger set of real videos, EBLA had average\r\nacquisition success rates as high as 95.8% and\r\naverage description success rates as high as\r\n65.3%. The lower description success rate for\r\nthe videos is attributed to the wide variance in\r\nthe appearance of objects across the test set.\r\nWhile there have been several systems capable of learning object or event labels for videos, EBLA is the first known system to\r\nacquire both nouns and verbs using a\r\ngrounded computer vision system.","3307":null,"3308":null,"3309":null,"3310":null,"3311":null,"3312":null,"3313":null,"3314":null,"3315":null,"3316":null,"3317":null,"3318":null,"3319":null,"3320":null,"3321":null,"3322":null,"3323":null,"3324":null,"3325":null,"3326":null,"3327":null,"3328":null,"3329":null,"3330":null,"3331":null,"3332":null,"3333":null,"3334":null,"3335":null,"3336":null,"3337":null,"3338":null,"3339":null,"3340":null,"3341":null,"3342":null,"3343":null,"3344":null,"3345":null,"3346":null,"3347":null,"3348":null,"3349":null,"3350":null,"3351":null,"3352":null,"3353":null,"3354":null,"3355":null,"3356":null,"3357":null,"3358":null,"3359":null,"3360":null,"3361":null,"3362":null,"3363":null,"3364":"Explainable Artificial Intelligence (XAI) has emerged as a critical facet in the realm of machine learning and artificial intelligence, responding to the increasing complexity of models, particularly deep neural networks, and the subsequent need for transparent decision making processes. This research paper delves into the essence of XAI, unraveling its significance across diverse domains such as healthcare, finance, and criminal justice. As a countermeasure to the opacity of intricate models, the paper explores various XAI methods and techniques, including LIME and SHAP, weighing their interpretability against computational efficiency and accuracy. Through an examination of real-world applications, the research elucidates how XAI not only enhances decision-making processes but also influences user trust and acceptance in AI systems. However, the paper also scrutinizes the delicate balance between interpretability and performance, shedding light on instances where the pursuit of accuracy may compromise explain-ability. Additionally, it navigates through the current challenges and limitations in XAI, the regulatory landscape surrounding AI explain-ability, and offers insights into future trends and directions, fostering a comprehensive understanding of XAI's present state and future potential.","3365":null,"3366":null,"3367":null,"3368":null,"3369":null,"3370":"Generative artificial intelligence (AI) is ushering in an era of potential transformation of journalism and media content. This essay considers one notable generative AI platform called ChatGPT made available to the public in 2022 for free use. ChatGPT allows users to enter text prompts and rapidly generates text responses drawn from its knowledge acquired via machine learning in engagement with the internet. This essay is coauthored by a human journalism and media professor in collaboration with ChatGPT. The essay demonstrates the capacity and limitations of ChatGPT and offers reflections on the implications of generative AI for journalism and media education.","3371":null,"3372":null,"3373":null,"3374":null,"3375":null,"3376":"\n This study examines the appropriateness of artificial intelligence model responses to fundamental cardiovascular disease prevention questions.\n","3377":null,"3378":"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49\u00b115 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","3379":"Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach","3380":null,"3381":"Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications.","3382":"Artificial intelligence (AI) and algorithmic decision making are having a profound impact on our daily lives. These systems are vastly used in different high-stakes applications like healthcare, business, government, education, and justice, moving us toward a more algorithmic society. However, despite so many advantages of these systems, they sometimes directly or indirectly cause harm to the users and society. Therefore, it has become essential to make these systems safe, reliable, and trustworthy. Several requirements, such as fairness, explainability, accountability, reliability, and acceptance, have been proposed in this direction to make these systems trustworthy. This survey analyzes all of these different requirements through the lens of the literature. It provides an overview of different approaches that can help mitigate AI risks and increase trust and acceptance of the systems by utilizing the users and society. It also discusses existing strategies for validating and verifying these systems and the current standardization efforts for trustworthy AI. Finally, we present a holistic view of the recent advancements in trustworthy AI to help the interested researchers grasp the crucial facets of the topic efficiently and offer possible future research directions.","3383":"supported AI safety research and organized annual conferences bringing together hundreds of the world's top AI researchers to address key challenges in responsible development trajectories. More recently, FLI was honored to participate as a \u201cchampion\u201d for the U.N. Secretary-General\u2019s Digital Cooperation Roadmap, advising on AI-related global governance issues, along with two European Union Member States and the Governments of France and Finland. With this perspective, we commend the European Commission for pursuing a positive, 2","3384":null,"3385":null,"3386":null,"3387":null,"3388":"The pandemic of coronavirus disease 2019 (COVID-19) is spreading all over the world. Medical imaging such as X-ray and computed tomography (CT) plays an essential role in the global fight against COVID-19, whereas the recently emerging artificial intelligence (AI) technologies further strengthen the power of the imaging tools and help medical specialists. We hereby review the rapid responses in the community of medical imaging (empowered by AI) toward COVID-19. For example, AI-empowered image acquisition can significantly help automate the scanning procedure and also reshape the workflow with minimal contact to patients, providing the best protection to the imaging technicians. Also, AI can improve work efficiency by accurate delineation of infections in X-ray and CT images, facilitating subsequent quantification. Moreover, the computer-aided platforms help radiologists make clinical decisions, i.e., for disease diagnosis, tracking, and prognosis. In this review paper, we thus cover the entire pipeline of medical imaging and analysis techniques involved with COVID-19, including image acquisition, segmentation, diagnosis, and follow-up. We particularly focus on the integration of AI with X-ray and CT, both of which are widely used in the frontline hospitals, in order to depict the latest progress of medical imaging and radiology fighting against COVID-19.","3389":": This Agriculture plays an important role in our day to day life and sustains the existence of life in this planet. Application of Artificial Intelligence (AI) in this sector is clearly visible and is ever growing than before. Soil management, Crop management, Weeds management, Water management, Weather predictions are the crucial areas identified in Agriculture, to act on. Any impacts or distractions in these areas can pay way to disruptions economy growth. Expectations from industries and farmers are to provide a solution at less cost and effectively performing at scale. Robotics have been tested a lot in this field and have seen an improvement over the years. Automations are significant in Agriculture, so as to reduce the complexity and tedious human labour. Maximising the yield with high quality and lesser chemical influence, have been the hopes from consumer perspective. Such an application of AI in Agriculture, will prevent lives from natural disasters, can alert or warn about the calamities and take the safest route.","3390":"Artificial Intelligence: A Modern Approach, eBook, Global EditionArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceArtificial IntelligenceArtificial IntelligenceHuman CompatibleARTIFICIAL INTELLIGENCEIntelligent Help Systems for UNIXHuman CompatibleArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence: A Modern Approach, 2\/EDistributed Artificial IntelligenceIntroduction to Artificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern ApproachArtificial Intelligence: Pearson New International EditionArtificial Intelligence a Modern ApproachArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceMaritime Security and the Law of the SeaArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial Intelligence For BeginnersArtificial Intelligence: a Modern ApproachArtificial IntelligenceArtificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughDo the Right ThingArtificial IntelligenceIntroduction to Machine LearningArtificial IntelligenceMultiagent SystemsArtificial IntelligenceEssentials of Strategic ManagementArtificial Intelligence BusinessArtificial Intelligence","3391":"In the modern era, many terms related to artificial intelligence, machine learning, and deep learning are widely used in domains such as business, healthcare, industries, and military. In these fields, the accurate prediction and analysis of data are crucial, regardless of how large the data are. However, using big data is confusing due to the rapid growth and massive development in public life, which requires a tremendous human effort in order to deal with such type of data and extract worthy information from it. Thus, the role of artificial intelligence begins in analyzing big data based on scientific techniques, especially in machine learning, whereby it can identify patterns of decision-making and reduce human intervention. In this regard, the significance role of artificial intelligence, machine learning and deep learning is growing rapidly. In this article, the authors decide to highlight these sciences by discussing how to develop and apply them in many decision-making domains. In addition, the influence of artificial intelligence in healthcare and the gains this science provides in the face of the COVID-19 pandemic are highlighted. This article concludes that these sciences have a significant impact, especially in healthcare, as well as the ability to grow and improve their methodology in decision-making. Additionally, artificial intelligence is a vital science, especially in the face of COVID-19.","3392":null,"3393":"Artificial intelligence simply means machine learning that can sense, reason, act, and adapt based on experience with the goal of contributing to the economic growth of the country and contributing to the betterment of people's standards of living. The main aim of artificial intelligence in healthcare is to make machines more useful in solving ambiguous healthcare challenges, and by using the latest technology, it is possible to interpret data accurately and rapidly. It helps in the early detection of many chronic diseases like Alzheimer's, diabetes, cardiovascular diseases, and several types of cancers like breast cancer, colon cancer, etc., which simultaneously reduces the financial burden and severity of the disease. The key areas where AI can be applied medically include disease detection and treatment, patient connection and engagement, and managerial and security activities. The research has been aimed at a study of AI systems in the healthcare sector in India. The methodology used here consisted of a systematic literature review followed by live, on field interviews.","3394":"In this chapter, the authors present a profound literature review of artificial intelligence (AI). After defining it, they briefly cover its history and enumerate its principal fields of application. They name, for example, information system, commerce, image processing, human-computer interaction, data compression, robotics, route planning, etc. Moreover, the test that defines an artificially intelligent system, called the Turing test, is also defined and detailed. Afterwards, the authors describe some AI tools such as fuzzy logic, genetic algorithms, and swarm intelligence. Special attention will be given to neural networks and fuzzy logic. The authors also present the future research directions and ethics.","3395":null,"3396":"ABSTRACT The complexity and rise of data in healthcare means that artificial intelligence (AI) will increasingly be applied within the field. Several types of AI are already being employed by payers and providers of care, and life sciences companies. The key categories of applications involve diagnosis and treatment recommendations, patient engagement and adherence, and administrative activities. Although there are many instances in which AI can perform healthcare tasks as well or better than humans, implementation factors will prevent large-scale automation of healthcare professional jobs for a considerable period. Ethical issues in the application of AI to healthcare are also discussed.","3397":null,"3398":"Abstract Although academic production in intelligent automation (e.g. artificial intelligence, robotics) has grown rapidly, we still lack a comprehensive understanding of the impacts of the utilization of these technologies in human resource management (HRM) at an organizational (firms) and individual (employees) level. This study therefore aims to systematize the academic inputs on intelligent automation so far and to clarify what are its main contributions to and challenges for HRM. In a systematic search of 13,136 potentially relevant studies published in the top HRM, international business (IB), general management (GM) and information management (IM) journals, we found 45 articles studying artificial intelligence, robotics and other advanced technologies within HRM settings. Results show that intelligent automation technologies constitute a new approach to managing employees and enhancing firm performance, thus offering several opportunities for HRM but also considerable challenges at a technological and ethical level. The impact of these technologies has been identified to concentrate on HRM strategies, namely, job replacement, human-robot\/AI collaboration, decision-making and learning opportunities, and HRM activities, namely, recruiting, training and job performance. This study discusses these shifts in detail, along with the main contributions to theory and practice and directions for future research.","3399":"With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video\/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training\/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI.","3400":"The thriving of artificial intelligence (AI) applications is driving the further evolution of wireless networks. It has been envisioned that 6G will be transformative and will revolutionize the evolution of wireless from \u201cconnected things\u201d to \u201cconnected intelligence\u201d. However, state-of-the-art deep learning and big data analytics based AI systems require tremendous computation and communication resources, causing significant latency, energy consumption, network congestion, and privacy leakage in both of the training and inference processes. By embedding model training and inference capabilities into the network edge, edge AI stands out as a disruptive technology for 6G to seamlessly integrate sensing, communication, computation, and intelligence, thereby improving the efficiency, effectiveness, privacy, and security of 6G networks. In this paper, we shall provide our vision for scalable and trustworthy edge AI systems with integrated design of wireless communication strategies and decentralized machine learning models. New design principles of wireless networks, service-driven resource allocation optimization methods, as well as a holistic end-to-end system architecture to support edge AI will be described. Standardization, software and hardware platforms, and application scenarios are also discussed to facilitate the industrialization and commercialization of edge AI systems.","3401":"To the Editor: Artificial intelligence (AI) systems are now regularly being used in medical settings,1 although regulatory oversight is inconsistent and undeveloped.2,3 Safe deployment of clinical AI requires informed clinician-users, who are generally responsible for identifying and reporting emerging problems. Clinicians may also serve as administrators in governing the use of clinical AI. A natural question follows: are clinicians adequately prepared to identify circumstances in which AI systems fail to perform their intended function reliably? A major driver of AI system malfunction is known as \u201cdataset shift.\u201d4,5 Most clinical AI systems today use machine learning, algorithms that leverage statistical methods to learn key patterns from clinical data. Dataset shift occurs when a machine-learning system underperforms because of a mismatch between the data set with which it was developed and the data on which it is deployed.4 For example, the University of Michigan Hospital implemented the widely used sepsis-alerting model developed by Epic Systems; in April 2020, the model had to be deactivated because of spurious alerting owing to changes in patients\u2019 demographic characteristics associated with the coronavirus disease 2019 pandemic. This was a case in which dataset shift fundamentally altered the relationship between fevers and bacterial sepsis, leading the hospital\u2019s clinical AI governing committee (which one of the authors of this letter chairs) to decommission its use. This is an extreme example; many causes of dataset shift are more subtle. In Table 1, we present common causes of dataset shift, which we group into changes in technology (e.g., software vendors), changes in population and setting (e.g., new demographics), and changes in behavior (e.g., new reimbursement incentives); the list is not meant to be exhaustive. Successful recognition and mitigation of dataset shift require both vigilant clinicians and sound technical oversight through AI governance teams.4,5 When using an AI system, clinicians should note misalignment between the predictions of the model and their own clinical judgment, as in the sepsis example above. Clinicians who use AI systems must frequently consider whether relevant aspects of their own clinical practice are atypical or have recently changed. For their part, AI governance teams must be sure that it is easy for clinicians to report concerns about the function of AI systems and provide feedback so that the clinician who is reporting will understand that the registered concern has been noted and, if appropriate, actions to mitigate the concern have been taken. Teams must also establish AI monitoring and updating protocols that integrate technical solu-","3402":null,"3403":"This paper provides a brief analytical review of the current state\u2010of\u2010the\u2010art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested.","3404":null,"3405":"Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into...","3406":"Goal: We hypothesized that COVID-19 subjects, especially including asymptomatics, could be accurately discriminated only from a forced-cough cell phone recording using Artificial Intelligence. To train our MIT Open Voice model we built a data collection pipeline of COVID-19 cough recordings through our website (opensigma.mit.edu) between April and May 2020 and created the largest audio COVID-19 cough balanced dataset reported to date with 5,320 subjects. Methods: We developed an AI speech processing framework that leverages acoustic biomarker feature extractors to pre-screen for COVID-19 from cough recordings, and provide a personalized patient saliency map to longitudinally monitor patients in real-time, non-invasively, and at essentially zero variable cost. Cough recordings are transformed with Mel Frequency Cepstral Coefficient and inputted into a Convolutional Neural Network (CNN) based architecture made up of one Poisson biomarker layer and 3 pre-trained ResNet50's in parallel, outputting a binary pre-screening diagnostic. Our CNN-based models have been trained on 4256 subjects and tested on the remaining 1064 subjects of our dataset. Transfer learning was used to learn biomarker features on larger datasets, previously successfully tested in our Lab on Alzheimer's, which significantly improves the COVID-19 discrimination accuracy of our architecture. Results: When validated with subjects diagnosed using an official test, the model achieves COVID-19 sensitivity of 98.5% with a specificity of 94.2% (AUC: 0.97). For asymptomatic subjects it achieves sensitivity of 100% with a specificity of 83.2%. Conclusions: AI techniques can produce a free, non-invasive, real-time, any-time, instantly distributable, large-scale COVID-19 asymptomatic screening tool to augment current approaches in containing the spread of COVID-19. Practical use cases could be for daily screening of students, workers, and public as schools, jobs, and transport reopen, or for pool testing to quickly alert of outbreaks in groups. General speech biomarkers may exist that cover several disease categories, as we demonstrated using the same ones for COVID-19 and Alzheimer's.","3407":"Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.","3408":null,"3409":null,"3410":null,"3411":null,"3412":"Artificial intelligence (AI) is increasingly reshaping service by performing various tasks, constituting a major source of innovation, yet threatening human jobs. We develop a theory of AI job replacement to address this double-edged impact. The theory specifies four intelligences required for service tasks\u2014mechanical, analytical, intuitive, and empathetic\u2014and lays out the way firms should decide between humans and machines for accomplishing those tasks. AI is developing in a predictable order, with mechanical mostly preceding analytical, analytical mostly preceding intuitive, and intuitive mostly preceding empathetic intelligence. The theory asserts that AI job replacement occurs fundamentally at the task level, rather than the job level, and for \u201clower\u201d (easier for AI) intelligence tasks first. AI first replaces some of a service job\u2019s tasks, a transition stage seen as augmentation, and then progresses to replace human labor entirely when it has the ability to take over all of a job\u2019s tasks. The progression of AI task replacement from lower to higher intelligences results in predictable shifts over time in the relative importance of the intelligences for service employees. An important implication from our theory is that analytical skills will become less important, as AI takes over more analytical tasks, giving the \u201csofter\u201d intuitive and empathetic skills even more importance for service employees. Eventually, AI will be capable of performing even the intuitive and empathetic tasks, which enables innovative ways of human\u2013machine integration for providing service but also results in a fundamental threat for human employment.","3413":null,"3414":"Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA\u2019s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems\u2019 explanations improve user understanding, user trust, and user task performance.","3415":null,"3416":"Artificial intelligence, especially machine learning (ML) and deep learning (DL) algorithms, is becoming an important tool in the fields of materials and mechanical engineering, attributed to its power to predict materials properties, design de novo materials and discover new mechanisms beyond intuitions. As the structural complexity of novel materials soars, the material design problem to optimize mechanical behaviors can involve massive design spaces that are intractable for conventional methods. Addressing this challenge, ML models trained from large material datasets that relate structure, properties and function at multiple hierarchical levels have offered new avenues for fast exploration of the design spaces. The performance of a ML-based materials design approach relies on the collection or generation of a large dataset that is properly preprocessed using the domain knowledge of materials science underlying chemical and physical concepts, and a suitable selection of the applied ML model. Recent breakthroughs in ML techniques have created vast opportunities for not only overcoming long-standing mechanics problems but also for developing unprecedented materials design strategies. In this review, we first present a brief introduction of state-of-the-art ML models, algorithms and structures. Then, we discuss the importance of data collection, generation and preprocessing. The applications in mechanical property prediction, materials design and computational methods using ML-based approaches are summarized, followed by perspectives on opportunities and open challenges in this emerging and exciting field.","3417":"2 Structured summary of study design, methods, results, and conclusions","3418":null,"3419":"Artificial intelligence (AI) is rapidly reshaping cancer research and personalized clinical care. Availability of high-dimensionality datasets coupled with advances in high-performance computing, as well as innovative deep learning architectures, has led to an explosion of AI use in various aspects of oncology research. These applications range from detection and classification of cancer, to molecular characterization of tumors and their microenvironment, to drug discovery and repurposing, to predicting treatment outcomes for patients. As these advances start penetrating the clinic, we foresee a shifting paradigm in cancer care becoming strongly driven by AI. SIGNIFICANCE: AI has the potential to dramatically affect nearly all aspects of oncology-from enhancing diagnosis to personalizing treatment and discovering novel anticancer drugs. Here, we review the recent enormous progress in the application of AI to oncology, highlight limitations and pitfalls, and chart a path for adoption of AI in the cancer clinic.","3420":"This is a critical review of artificial intelligence\/machine learning (AI\/ML) methods applied to battery research. It aims at providing a comprehensive, authoritative, and critical, yet easily understandable, review of general interest to the battery community. It addresses the concepts, approaches, tools, outcomes, and challenges of using AI\/ML as an accelerator for the design and optimization of the next generation of batteries\u2014a current hot topic. It intends to create both accessibility of these tools to the chemistry and electrochemical energy sciences communities and completeness in terms of the different battery R&D aspects covered.","3421":"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","3422":null,"3423":null,"3424":null,"3425":"Over the years, the accounting profession has evolved greatly since its creation thousands of years prior. Arguably the most monumental reason for the amount of change and growth the industry has experienced is due to various technological advancements seen over the past few decades. The question posed in this paper relates to the analyzation of whether or not artificial intelligence is beneficial for the accounting industry or not. In this paper, I will present the potential negative and positive benefits associated with technology in general. I will also introduce the field of accounting and describe the diversity of positions offered within accounting and the evolution of the industry itself. Furthermore, I will relate the two topics of technology and accounting together and demonstrate how they are interrelated and have transformed into a coexistent state. Additionally, we will take a look at what the accounting industry is like in today(cid:182)s present age with the technology available to the industry and what this means for the profession moving forward. Finally, we will answer the question of whether or not this technological growth is vital or fatal for the profession of accounting.","3426":"The main aim of this paper was to review how artificial intelligence works in improving Citizen Services and Government. Several government agencies throughout the world are experimenting with artificial intelligence applications (AI). The most common use cases for citizen services are inquiries and information [1]. This article examined the many forms of artificial intelligence applications, as well as the existing and prospective software solutions of AI in the delivery of citizen services by the government, with a particular emphasis on citizen queries and information. It also provides solutions for governments considering the use of artificial intelligence. Most artificial intelligence (AI) published studies in citizen services now fall into the following categories: responding to inquiries, filling out and finding papers, routing requests, translating documents, and generating paperwork, among others [1]. These technologies have the potential to increase the efficiency of government operations while also providing personnel with more time to create stronger ties with residents. Artificial intelligence (AI) may be one solution to bridge the gap between the dissatisfaction of citizens with digital government products and enhancing citizen participation and service delivery [2].","3427":"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide \u201cobviously\u201d interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.","3428":null,"3429":"ABSTRACT Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe & Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility.","3430":"Conceptual abstraction and analogy\u2010making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","3431":null,"3432":null,"3433":null,"3434":"Taking three recent business books on artificial intelligence (AI) as a starting point, we explore the automation and augmentation concepts in the management domain. Whereas automation implies that...","3435":null,"3436":null,"3437":"\n \n \nCOVID-19, the disease caused by the SARS-CoV-2 virus, has been declared a pandemic by the World Health Organization, which has reported over 18 million confirmed cases as of August 5, 2020. In this review, we present an overview of recent studies using Machine Learning and, more broadly, Artificial Intelligence, to tackle many aspects of the COVID19 crisis. We have identified applications that address challenges posed by COVID-19 at different scales, including: molecular, by identifying new or existing drugs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. We also review datasets, tools, and resources needed to facilitate Artificial Intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. We highlight the need for international cooperation to maximize the potential of AI in this and future pandemics. \n \n \n","3438":null,"3439":null,"3440":"Artificial Intelligence (AI)\u2010based systems are widely employed nowadays to make decisions that have far\u2010reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well\u2010grounded in a legal frame. In this survey, we focus on data\u2010driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth.","3441":"Use of Artificial intelligence (AI) has increased in the healthcare in many sectors. Organizations from health care of different sizes, types and different specialties are now a days more interested in how artificial intelligence has evolved and is helping patient needs and their care, also reducing costs, and increasing efficiency. This study explores the implications of AI on healthcare management, and challenges involved with using AI in healthcare along with the review of several research papers that used AI models in different sectors of healthcare like Dermatology, Radiology, Drug design etc.","3442":"BACKGROUND An alternative to epidemiological models for transmission dynamics of Covid-19 in China, we propose the artificial intelligence (AI)-inspired methods for real-time forecasting of Covid-19 to estimate the size, lengths and ending time of Covid-19 across China. METHODS We developed a modified stacked auto-encoder for modeling the transmission dynamics of the epidemics. We applied this model to real-time forecasting the confirmed cases of Covid-19 across China. The data were collected from January 11 to February 27, 2020 by WHO. We used the latent variables in the auto-encoder and clustering algorithms to group the provinces\/cities for investigating the transmission structure. RESULTS We forecasted curves of cumulative confirmed cases of Covid-19 across China from Jan 20, 2020 to April 20, 2020. Using the multiple-step forecasting, the estimated average errors of 6-step, 7-step, 8-step, 9-step and 10-step forecasting were 1.64%, 2.27%, 2.14%, 2.08%, 0.73%, respectively. We predicted that the time points of the provinces\/cities entering the plateau of the forecasted transmission dynamic curves varied, ranging from Jan 21 to April 19, 2020. The 34 provinces\/cities were grouped into 9 clusters. CONCLUSIONS The accuracy of the AI-based methods for forecasting the trajectory of Covid-19 was high. We predicted that the epidemics of Covid-19 will be over by the middle of April. If the data are reliable and there are no second transmissions, we can accurately forecast the transmission dynamics of the Covid-19 across the provinces\/cities in China. The AI-inspired methods are a powerful tool for helping public health planning and policymaking.","3443":"The virus SARS-CoV2, which causes coronavirus disease (COVID-19) has become a pandemic and has spread to every inhabited continent Given the increasing caseload, there is an urgent need to augment clinical skills in order to identify from among the many mild cases the few that will progress to critical illness We present a first step towards building an artificial intelligence (AI) framework, with predictive analytics (PA) capabilities applied to real patient data, to provide rapid clinical decision-making support COVID-19 has presented a pressing need as a) clinicians are still developing clinical acumen to this novel disease and b) resource limitations in a surging pandemic require difficult resource allocation decisions The objectives of this research are: (1) to algorithmically identify the combinations of clinical characteristics of COVID-19 that predict outcomes, and (2) to develop a tool with AI capabilities that will predict patients at risk for more severe illness on initial presentation The predictive models learn from historical data to help predict who will develop acute respiratory distress syndrome (ARDS), a severe outcome in COVID-19 Our results, based on data from two hospitals in Wenzhou, Zhejiang, China, identified features on initial presentation with COVID-19 that were most predictive of later development of ARDS A mildly elevated alanine aminotransferase (ALT) (a liver enzyme), the presence of myalgias (body aches), and an elevated hemoglobin (red blood cells), in this order, are the clinical features, on presentation, that are the most predictive The predictive models that learned from historical data of patients from these two hospitals achieved 70% to 80% accuracy in predicting severe cases","3444":"In the Internet-of-Things (IoT) era, billions of sensors and devices collect and process data from the environment, transmit them to cloud centers, and receive feedback via the Internet for connectivity and perception. However, transmitting massive amounts of heterogeneous data, perceiving complex environments from these data, and then making smart decisions in a timely manner are difficult. Artificial intelligence (AI), especially deep learning, is now a proven success in various areas, including computer vision, speech recognition, and natural language processing. AI introduced into the IoT heralds the era of AI of things (AIoT). This article presents a comprehensive survey on AIoT to show how AI can empower the IoT to make it faster, smarter, greener, and safer. Specifically, we briefly present the AIoT architecture in the context of cloud computing, fog computing, and edge computing. Then, we present progress in AI research for IoT from four perspectives: 1) perceiving; 2) learning; 3) reasoning; and 4) behaving. Next, we summarize some promising applications of AIoT that are likely to profoundly reshape our world. Finally, we highlight the challenges facing AIoT and some potential research opportunities.","3445":null,"3446":"AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and \u201ccollaborate\u201d with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI \u201cpartners\u201d with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying \u2018psychological\u2019 mechanisms of AI. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed.","3447":"By leveraging deep learning-based technologies, industrial artificial intelligence (IAI) has been applied to solve various industrial challenging problems in Industry 4.0. However, for privacy reasons, traditional centralized training may be unsuitable for sensitive data-driven industrial scenarios, such as healthcare and autopilot. Recently, federated learning has received widespread attention, since it enables participants to collaboratively learn a shared model without revealing their local data. However, studies have shown that, by exploiting the shared parameters adversaries can still compromise industrial applications such as auto-driving navigation systems, medical data in wearable devices, and industrial robots\u2019 decision making. In this article, to solve this problem, we propose an efficient and privacy-enhanced federated learning (PEFL) scheme for IAI. Compared with existing solutions, PEFL is noninteractive, and can prevent private data from being leaked even if multiple entities collude with each other. Moreover, extensive experiments with real-world data demonstrate the superiority of PEFL in terms of accuracy and efficiency.","3448":"The SPIRIT 2013 statement aims to improve the completeness of clinical trial protocol reporting by providing evidence-based recommendations for the minimum set of items to be addressed. This guidance has been instrumental in promoting transparent evaluation of new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate their impact on health outcomes. The SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials\u2013Artificial Intelligence) extension is a new reporting guideline for clinical trial protocols evaluating interventions with an AI component. It was developed in parallel with its companion statement for trial reports: CONSORT-AI (Consolidated Standards of Reporting Trials\u2013Artificial Intelligence). Both guidelines were developed through a staged consensus process involving literature review and expert consultation to generate 26 candidate items, which were consulted upon by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed upon in a consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The SPIRIT-AI extension includes 15 new items that were considered sufficiently important for clinical trial protocols of AI interventions. These new items should be routinely reported in addition to the core SPIRIT 2013 items. SPIRIT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention will be integrated, considerations for the handling of input and output data, the human\u2013AI interaction and analysis of error cases. SPIRIT-AI will help promote transparency and completeness for clinical trial protocols for AI interventions. Its use will assist editors and peer reviewers, as well as the general readership, to understand, interpret and critically appraise the design and risk of bias for a planned clinical trial. The CONSORT-AI and SPIRIT-AI extensions improve the transparency of clinical trial design and trial protocol reporting for artificial intelligence interventions.","3449":null,"3450":"This article gives an overview of the artificial intelligence (AI) applications for power electronic systems. The three distinctive life-cycle phases, design, control, and maintenance are correlated with one or more tasks to be addressed by AI, including optimization, classification, regression, and data structure exploration. The applications of four categories of AI are discussed, which are expert system, fuzzy logic, metaheuristic method, and machine learning. More than 500 publications have been reviewed to identify the common understandings, practical implementation challenges, and research opportunities in the application of AI for power electronics. This article is accompanied by an Excel file listing the relevant publications for statistical analytics.","3451":"Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.","3452":null,"3453":null,"3454":null,"3455":null,"3456":"The development of artificial intelligence is accompanied by the progress of human society. Purpose of the article is to using artificial intelligence in coordination of physical education teaching...","3457":null,"3458":"INTRODUCTION\nArtificial intelligence (AI) and machine learning (ML) are rapidly evolving fields in various sectors, including healthcare. This article reviews AI's present applications in healthcare, including its benefits, limitations and future scope.\n\n\nSOURCES OF DATA\nA review of the English literature was conducted with search terms 'AI' or 'ML' or 'deep learning' and 'healthcare' or 'medicine' using PubMED and Google Scholar from 2000-2021.\n\n\nAREAS OF AGREEMENT\nAI could transform physician workflow and patient care through its applications, from assisting physicians and replacing administrative tasks to augmenting medical knowledge.\n\n\nAREAS OF CONTROVERSY\nFrom challenges training ML systems to unclear accountability, AI's implementation is difficult and incremental at best. Physicians also lack understanding of what AI implementation could represent.\n\n\nGROWING POINTS\nAI can ultimately prove beneficial in healthcare, but requires meticulous governance similar to the governance of physician conduct.\n\n\nAREAS TIMELY FOR DEVELOPING RESEARCH\nRegulatory guidelines are needed on how to safely implement and assess AI technology, alongside further research into the specific capabilities and limitations of its medical use.","3459":"Abstract Artificial intelligence (AI) has given the electrocardiogram (ECG) and clinicians reading them super-human diagnostic abilities. Trained without hard-coded rules by finding often subclinical patterns in huge datasets, AI transforms the ECG, a ubiquitous, non-invasive cardiac test that is integrated into practice workflows, into a screening tool and predictor of cardiac and non-cardiac diseases, often in asymptomatic individuals. This review describes the mathematical background behind supervised AI algorithms, and discusses selected AI ECG cardiac screening algorithms including those for the detection of left ventricular dysfunction, episodic atrial fibrillation from a tracing recorded during normal sinus rhythm, and other structural and valvular diseases. The ability to learn from big data sets, without the need to understand the biological mechanism, has created opportunities for detecting non-cardiac diseases as COVID-19 and introduced challenges with regards to data privacy. Like all medical tests, the AI ECG must be carefully vetted and validated in real-world clinical environments. Finally, with mobile form factors that allow acquisition of medical-grade ECGs from smartphones and wearables, the use of AI may enable massive scalability to democratize healthcare.","3460":"ABSTRACT The rapid developments in Artificial Intelligence (AI) and the intensification in the adoption of AI in domains such as autonomous vehicles, lethal weapon systems, robotics and alike pose serious challenges to governments as they must manage the scale and speed of socio-technical transitions occurring. While there is considerable literature emerging on various aspects of AI, governance of AI is a significantly underdeveloped area. The new applications of AI offer opportunities for increasing economic efficiency and quality of life, but they also generate unexpected and unintended consequences and pose new forms of risks that need to be addressed. To enhance the benefits from AI while minimising the adverse risks, governments worldwide need to understand better the scope and depth of the risks posed and develop regulatory and governance processes and structures to address these challenges. This introductory article unpacks AI and describes why the Governance of AI should be gaining far more attention given the myriad of challenges it presents. It then summarises the special issue articles and highlights their key contributions. This special issue introduces the multifaceted challenges of governance of AI, including emerging governance approaches to AI, policy capacity building, exploring legal and regulatory challenges of AI and Robotics, and outstanding issues and gaps that need attention. The special issue showcases the state-of-the-art in the governance of AI, aiming to enable researchers and practitioners to appreciate the challenges and complexities of AI governance and highlight future avenues for exploration.","3461":null,"3462":null,"3463":null,"3464":"On May 22, 2019, the Organisation for Economic Co-operation and Development (OECD) Ministerial Council Meeting adopted the Recommendation on Artificial Intelligence, signed by all 36 OECD member countries and non-member countries Argentina, Brazil, Columbia, Costa Rica, Peru, and Romania. Its aim is to foster innovation and trust in artificial intelligence (AI) by promoting the \u201cresponsible stewardship of trustworthy AI.\u201d","3465":"Artificial intelligence (AI) helps companies offer important benefits to consumers, such as health monitoring with wearable devices, advice with recommender systems, peace of mind with smart household products, and convenience with voice-activated virtual assistants. However, although AI can be seen as a neutral tool to be evaluated on efficiency and accuracy, this approach does not consider the social and individual challenges that can occur when AI is deployed. This research aims to bridge these two perspectives: on one side, the authors acknowledge the value that embedding AI technology into products and services can provide to consumers. On the other side, the authors build on and integrate sociological and psychological scholarship to examine some of the costs consumers experience in their interactions with AI. In doing so, the authors identify four types of consumer experiences with AI: (1) data capture, (2) classification, (3) delegation, and (4) social. This approach allows the authors to discuss policy and managerial avenues to address the ways in which consumers may fail to experience value in organizations\u2019 investments into AI and to lay out an agenda for future research.","3466":null,"3467":"Artificial intelligence (AI) can transform health care practices with its increasing ability to translate the uncertainty and complexity in data into actionable\u2014though imperfect\u2014clinical decisions or suggestions. In the evolving relationship between humans and AI, trust is the one mechanism that shapes clinicians\u2019 use and adoption of AI. Trust is a psychological mechanism to deal with the uncertainty between what is known and unknown. Several research studies have highlighted the need for improving AI-based systems and enhancing their capabilities to help clinicians. However, assessing the magnitude and impact of human trust on AI technology demands substantial attention. Will a clinician trust an AI-based system? What are the factors that influence human trust in AI? Can trust in AI be optimized to improve decision-making processes? In this paper, we focus on clinicians as the primary users of AI systems in health care and present factors shaping trust between clinicians and AI. We highlight critical challenges related to trust that should be considered during the development of any AI system for clinical use.","3468":null,"3469":"Artificial intelligence-powered medical technologies are rapidly evolving into applicable solutions for clinical practice. Deep learning algorithms can deal with increasing amounts of data provided by wearables, smartphones, and other mobile monitoring sensors in different areas of medicine. Currently, only very specific settings in clinical practice benefit from the application of artificial intelligence, such as the detection of atrial fibrillation, epilepsy seizures, and hypoglycemia, or the diagnosis of disease based on histopathological examination or medical imaging. The implementation of augmented medicine is long-awaited by patients because it allows for a greater autonomy and a more personalized treatment, however, it is met with resistance from physicians which were not prepared for such an evolution of clinical practice. This phenomenon also creates the need to validate these modern tools with traditional clinical trials, debate the educational upgrade of the medical curriculum in light of digital medicine as well as ethical consideration of the ongoing connected monitoring. The aim of this paper is to discuss recent scientific literature and provide a perspective on the benefits, future opportunities and risks of established artificial intelligence applications in clinical practice on physicians, healthcare institutions, medical education, and bioethics.","3470":"The term \u201cartificial intelligence\u201d (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which \u201clearns\u201d intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed\/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (\u201cexplainable AI\u201d). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","3471":"COVID-19 outbreak has put the whole world in an unprecedented difficult situation bringing life around the world to a frightening halt and claiming thousands of lives. Due to COVID-19\u2019s spread in 212 countries and territories and increasing numbers of infected cases and death tolls mounting to 5,212,172 and 334,915 (as of May 22 2020), it remains a real threat to the public health system. This paper renders a response to combat the virus through Artificial Intelligence (AI). Some Deep Learning (DL) methods have been illustrated to reach this goal, including Generative Adversarial Networks (GANs), Extreme Learning Machine (ELM), and Long\/Short Term Memory (LSTM). It delineates an integrated bioinformatics approach in which different aspects of information from a continuum of structured and unstructured data sources are put together to form the user-friendly platforms for physicians and researchers. The main advantage of these AI-based platforms is to accelerate the process of diagnosis and treatment of the COVID-19 disease. The most recent related publications and medical reports were investigated with the purpose of choosing inputs and targets of the network that could facilitate reaching a reliable Artificial Neural Network-based tool for challenges associated with COVID-19. Furthermore, there are some specific inputs for each platform, including various forms of the data, such as clinical data and medical imaging which can improve the performance of the introduced approaches toward the best responses in practical applications.","3472":null,"3473":"Artificial Intelligence has ameliorated in prominence during the last decade. In practically every area, Artificial Intelligence has had a consequential contribution. It has grown into a tremendous technology that has revolutionized the way human beings communicate and may transform the way human beings look to the future. Nowadays, discoveries in artificial intelligence (AI) that outperform humans in some tasks generate headlines. I exhibit a spiffing updated literature-review for Artificial Intelligence. Other works offered domain-specific plus non-comprehensive, as well as shortcomings on their introduction, background information, related work, and discussion and future directions. This research intends to provide diverse AI techniques, which can be implement to preclude cyber-assaults; the Artificial Intelligence and its uses in a variety of fields. This literature review will definitely assist scientists and readers in comprehending the technologies, fields, uses, and applications of AI. Furthermore, in terms of state of knowledge, introduction, background information, related work, discussion, and future directions, this literature review outperformed previous literature review publications.","3474":null,"3475":null,"3476":null,"3477":null,"3478":"Change detection based on remote sensing (RS) data is an important method of detecting changes on the Earth\u2019s surface and has a wide range of applications in urban planning, environmental monitoring, agriculture investigation, disaster assessment, and map revision. In recent years, integrated artificial intelligence (AI) technology has become a research focus in developing new change detection methods. Although some researchers claim that AI-based change detection approaches outperform traditional change detection approaches, it is not immediately obvious how and to what extent AI can improve the performance of change detection. This review focuses on the state-of-the-art methods, applications, and challenges of AI for change detection. Specifically, the implementation process of AI-based change detection is first introduced. Then, the data from different sensors used for change detection, including optical RS data, synthetic aperture radar (SAR) data, street view images, and combined heterogeneous data, are presented, and the available open datasets are also listed. The general frameworks of AI-based change detection methods are reviewed and analyzed systematically, and the unsupervised schemes used in AI-based change detection are further analyzed. Subsequently, the commonly used networks in AI for change detection are described. From a practical point of view, the application domains of AI-based change detection methods are classified based on their applicability. Finally, the major challenges and prospects of AI for change detection are discussed and delineated, including (a) heterogeneous big data processing, (b) unsupervised AI, and (c) the reliability of AI. This review will be beneficial for researchers in understanding this field.","3479":null,"3480":null,"3481":"Judgement, as one of the core tenets of medicine, relies upon the integration of multilayered data with nuanced decision making. Cancer offers a unique context for medical decisions given not only its variegated forms with evolution of disease but also the need to take into account the individual condition of patients, their ability to receive treatment, and their responses to treatment. Challenges remain in the accurate detection, characterization, and monitoring of cancers despite improved technologies. Radiographic assessment of disease most commonly relies upon visual evaluations, the interpretations of which may be augmented by advanced computational analyses. In particular, artificial intelligence (AI) promises to make great strides in the qualitative interpretation of cancer imaging by expert clinicians, including volumetric delineation of tumors over time, extrapolation of the tumor genotype and biological course from its radiographic phenotype, prediction of clinical outcome, and assessment of the impact of disease and treatment on adjacent organs. AI may automate processes in the initial interpretation of images and shift the clinical workflow of radiographic detection, management decisions on whether or not to administer an intervention, and subsequent observation to a yet to be envisioned paradigm. Here, the authors review the current state of AI as applied to medical imaging of cancer and describe advances in 4 tumor types (lung, brain, breast, and prostate) to illustrate how common clinical problems are being addressed. Although most studies evaluating AI applications in oncology to date have not been vigorously validated for reproducibility and generalizability, the results do highlight increasingly concerted efforts in pushing AI technology to clinical use and to impact future directions in cancer care.","3482":"\"The most important book I have read in quite some time\" (Daniel Kahneman); \"A must-read\" (Max Tegmark); \"The book we've all been waiting for\" (Sam Harris) LONGLISTED FOR THE 2019 FINANCIAL TIMES AND MCKINSEY BUSINESS BOOK OF THE YEAR; A FINANCIAL TIMES BEST BOOK OF THE YEAR 2019 Humans dream of super-intelligent machines. But what happens if we actually succeed? Creating superior intelligence would be the biggest event in human history. Unfortunately, according to the world's pre-eminent AI expert, it could also be the last. In this groundbreaking book on the biggest question facing humanity, Stuart Russell explains why he has come to consider his own discipline an existential threat to our species, and lays out how we can change course before it's too late. There is no one better placed to assess the promise and perils of the dominant technology of the future than Russell, who has spent decades at the forefront of AI research. Through brilliant analogies and crisp, lucid prose, he explains how AI actually works, how it has an enormous capacity to improve our lives - but why we must ensure that we never lose control of machines more powerful than we are. Here Russell shows how we can avert the worst threats by reshaping the foundations of AI to guarantee that machines pursue our objectives, not theirs. Profound, urgent and visionary, Human Compatible is the one book everyone needs to read to understand a future that is coming sooner than we think.","3483":"As artificial intelligence (AI) systems begin to make their way into clinical radiology practice, it is crucial to assure that they function correctly and that they gain the trust of experts. Toward this goal, approaches to make AI \"interpretable\" have gained attention to enhance the understanding of a machine learning algorithm, despite its complexity. This article aims to provide insights into the current state of the art of interpretability methods for radiology AI. This review discusses radiologists' opinions on the topic and suggests trends and challenges that need to be addressed to effectively streamline interpretability methods in clinical practice. Supplemental material is available for this article. \u00a9 RSNA, 2020 See also the commentary by Gastounioti and Kontos in this issue.","3484":null,"3485":null,"3486":null,"3487":null,"3488":null,"3489":null,"3490":"\n Artificial intelligence (AI) is revolutionizing healthcare, but little is known about consumer receptivity to AI in medicine. Consumers are reluctant to utilize healthcare provided by AI in real and hypothetical choices, separate and joint evaluations. Consumers are less likely to utilize healthcare (study 1), exhibit lower reservation prices for healthcare (study 2), are less sensitive to differences in provider performance (studies 3A\u20133C), and derive negative utility if a provider is automated rather than human (study 4). Uniqueness neglect, a concern that AI providers are less able than human providers to account for consumers\u2019 unique characteristics and circumstances, drives consumer resistance to medical AI. Indeed, resistance to medical AI is stronger for consumers who perceive themselves to be more unique (study 5).\u00a0Uniqueness neglect mediates resistance to medical AI (study 6), and is eliminated when AI provides care (a) that is framed as personalized (study 7), (b) to consumers other than the self (study 8), or (c) that only supports, rather than replaces, a decision made by a human healthcare provider (study 9). These findings make contributions to the psychology of automation and medical decision making, and suggest interventions to increase consumer acceptance of AI in medicine.","3491":"There is a substantial gap between the promise and reality of artificial intelligence in human resource (HR) management. This article identifies four challenges in using data science techniques for HR tasks: complexity of HR phenomena, constraints imposed by small data sets, accountability questions associated with fairness and other ethical and legal constraints, and possible adverse employee reactions to management decisions via data-based algorithms. It then proposes practical responses to these challenges based on three overlapping principles\u2014causal reasoning, randomization and experiments, and employee contribution\u2014that would be both economically efficient and socially appropriate for using data science in the management of employees.","3492":null,"3493":"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black\u2010box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use\u2010case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system","3494":null,"3495":null,"3496":"In medicine, artificial intelligence (AI) research is becoming increasingly focused on applying machine learning (ML) techniques to complex problems, and so allowing computers to make predictions from large amounts of patient data, by learning their own associations.1 Estimates of the impact of AI on the wider economy globally vary wildly, with a recent report suggesting a 14% effect on global gross domestic product by 2030, half of which coming from productivity improvements.2 These predictions create political appetite for the rapid development of the AI industry,3 and healthcare is a priority area where this technology has yet to be exploited.2 3 The digital health revolution described by Duggal et al 4 is already in full swing with the potential to \u2018disrupt\u2019 healthcare. Health AI research has demonstrated some impressive results,5\u201310 but its clinical value has not yet been realised, hindered partly by a lack of a clear understanding of how to quantify benefit or ensure patient safety, and increasing concerns about the ethical and medico-legal impact.11 \n\nThis analysis is written with the dual aim of helping clinical safety professionals to critically appraise current medical AI research from a quality and safety perspective, and supporting research and development in AI by highlighting some of the clinical safety questions that must be considered if medical application of these exciting technologies is to be successful.\n\nClinical decision support systems (DSS) are in widespread use in medicine and have had most impact providing guidance on the safe prescription of medicines,12 guideline adherence, simple risk screening13 or prognostic scoring.14 These systems use predefined rules, which have predictable behaviour and are usually shown to reduce clinical error,12 although sometimes inadvertently introduce safety issues themselves.15 16 Rules-based systems have also been developed to address diagnostic uncertainty17\u201319 \u2026","3497":null,"3498":"Machine learning, artificial intelligence, and other modern statistical methods are providing new opportunities to operationalise previously untapped and rapidly growing sources of data for patient benefit. Despite much promising research currently being undertaken, particularly in imaging, the literature as a whole lacks transparency, clear reporting to facilitate replicability, exploration for potential ethical concerns, and clear demonstrations of effectiveness. Among the many reasons why these problems exist, one of the most important (for which we provide a preliminary solution here) is the current lack of best practice guidance specific to machine learning and artificial intelligence. However, we believe that interdisciplinary groups pursuing research and impact projects involving machine learning and artificial intelligence for health would benefit from explicitly addressing a series of questions concerning transparency, reproducibility, ethics, and effectiveness (TREE). The 20 critical questions proposed here provide a framework for research groups to inform the design, conduct, and reporting; for editors and peer reviewers to evaluate contributions to the literature; and for patients, clinicians and policy makers to critically appraise where new findings may deliver patient benefit.","3499":null,"3500":null,"3501":"Along with the rapid developments in communication technologies and the surge in the use of mobile devices, a brand-new computation paradigm, edge computing, is surging in popularity. Meanwhile, the artificial intelligence (AI) applications are thriving with the breakthroughs in deep learning and the many improvements in hardware architectures. Billions of data bytes, generated at the network edge, put massive demands on data processing and structural optimization. Thus, there exists a strong demand to integrate edge computing and AI, which gives birth to edge intelligence. In this article, we divide edge intelligence into AI for edge (intelligence-enabled edge computing) and AI on edge (artificial intelligence on edge). The former focuses on providing more optimal solutions to key problems in edge computing with the help of popular and effective AI technologies while the latter studies how to carry out the entire process of building AI models, i.e., model training and inference, on the edge. This article provides insights into this new interdisciplinary field from a broader perspective. It discusses the core concepts and the research roadmap, which should provide the necessary background for potential future research initiatives in edge intelligence.","3502":"As Rob Atkinson writse in a book review for the New York Journal of Books, anyone who has experience with the U.S. healthcare system will find a lot to agree with in Dr. Eric Topol\u2019s \"Deep Medicine\". Topol, a cardiologist, medical researcher, and author, argues that medicine as it is practiced today is \u201cshallow\u201d rather than deep, and that accounts for many of its problems. By deep, Topol is referring to more meaningful encounters between clinician and patient, rather than a quick visit with a doctor. To get to a world of \u201cdeep medicine,\u201d Topol argues that artificial intelligence (AI) will play a key role.","3503":null,"3504":null,"3505":null,"3506":"Background: Artificial intelligence (AI) is the term used to describe the use of computers and technology to simulate intelligent behavior and critical thinking comparable to a human being. John McCarthy first described the term AI in 1956 as the science and engineering of making intelligent machines. Objective: This descriptive article gives a broad overview of AI in medicine, dealing with the terms and concepts as well as the current and future applications of AI. It aims to develop knowledge and familiarity of AI among primary care physicians. Materials and Methods: PubMed and Google searches were performed using the key words 'artificial intelligence'. Further references were obtained by cross-referencing the key articles. Results: Recent advances in AI technology and its current applications in the field of medicine have been discussed in detail. Conclusions: AI promises to change the practice of medicine in hitherto unknown ways, but many of its practical applications are still in their infancy and need to be explored and developed better. Medical professionals also need to understand and acclimatize themselves with these advances for better healthcare delivery to the masses.","3507":"Rapid advances in artificial intelligence (AI) and automation technologies have the potential to significantly disrupt labor markets. While AI and automation can augment the productivity of some workers, they can replace the work done by others and will likely transform almost all occupations at least to some degree. Rising automation is happening in a period of growing economic inequality, raising fears of mass technological unemployment and a renewed call for policy efforts to address the consequences of technological change. In this paper we discuss the barriers that inhibit scientists from measuring the effects of AI and automation on the future of work. These barriers include the lack of high-quality data about the nature of work (e.g., the dynamic requirements of occupations), lack of empirically informed models of key microlevel processes (e.g., skill substitution and human\u2013machine complementarity), and insufficient understanding of how cognitive technologies interact with broader economic dynamics and institutional mechanisms (e.g., urban migration and international trade policy). Overcoming these barriers requires improvements in the longitudinal and spatial resolution of data, as well as refinements to data on workplace skills. These improvements will enable multidisciplinary research to quantitatively monitor and predict the complex evolution of work in tandem with technological progress. Finally, given the fundamental uncertainty in predicting technological change, we recommend developing a decision framework that focuses on resilience to unexpected scenarios in addition to general equilibrium behavior.","3508":"The practice of medicine is changing with the development of new Artificial Intelligence (AI) methods of machine learning. Coupled with rapid improvements in computer processing, these AI-based systems are already improving the accuracy and efficiency of diagnosis and treatment across various specializations. The increasing focus of AI in radiology has led to some experts suggesting that someday AI may even replace radiologists. These suggestions raise the question of whether AI-based systems will eventually replace physicians in some specializations or will augment the role of physicians without actually replacing them. To assess the impact on physicians this research seeks to better understand this technology and how it is transforming medicine. To that end this paper researches the role of AI-based systems in performing medical work in specializations including radiology, pathology, ophthalmology, and cardiology. It concludes that AI-based systems will augment physicians and are unlikely to replace the traditional physician\u2013patient relationship.","3509":"Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.","3510":"\n Abstract\n \n This chapter will map the ethical and legal challenges posed by artificial intelligence (AI) in healthcare and suggest directions for resolving them. Section 1 will briefly clarify what AI is and Section 2 will give an idea of the trends and strategies in the United States (US) and Europe, thereby tailoring the discussion to the ethical and legal debate of AI-driven healthcare. This will be followed in Section 3 by a discussion of four primary ethical challenges, namely, (1) informed consent to use, (2) safety and transparency, (3) algorithmic fairness and biases, and (4) data privacy. Section 4 will then analyze five legal challenges in the US and Europe: (1) safety and effectiveness, (2) liability, (3) data protection and privacy, (4) cybersecurity, and (5) intellectual property law. Finally, Section 5 will summarize the major conclusions and especially emphasize the importance of building an AI-driven healthcare system that is successful and promotes trust and the motto Health AIs for All of Us.\n \n","3511":null,"3512":"Internet of Things (IoT) results in a massive amount of streaming data, often referred to as \u201cbig data,\u201d which brings new opportunities to monitor agricultural and food processes. Besides sensors, big data from social media is also becoming important for the food industry. In this review, we present an overview of IoT, big data, and artificial intelligence (AI), and their disruptive role in shaping the future of agri-food systems. Following an introduction to the fields of IoT, big data, and AI, we discuss the role of IoT and big data analysis in agriculture (including greenhouse monitoring, intelligent farm machines, and drone-based crop imaging), supply chain modernization, social media (for open innovation and sentiment analysis) in food industry, food quality assessment (using spectral methods and sensor fusion), and finally, food safety (using gene sequencing and blockchain-based digital traceability). A special emphasis is laid on the commercial status of applications and translational research outcomes.","3513":"Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.","3514":"Purpose To compare breast cancer detection performance of radiologists reading mammographic examinations unaided versus supported by an artificial intelligence (AI) system. Materials and Methods An enriched retrospective, fully crossed, multireader, multicase, HIPAA-compliant study was performed. Screening digital mammographic examinations from 240 women (median age, 62 years; range, 39-89 years) performed between 2013 and 2017 were included. The 240 examinations (100 showing cancers, 40 leading to false-positive recalls, 100 normal) were interpreted by 14 Mammography Quality Standards Act-qualified radiologists, once with and once without AI support. The readers provided a Breast Imaging Reporting and Data System score and probability of malignancy. AI support provided radiologists with interactive decision support (clicking on a breast region yields a local cancer likelihood score), traditional lesion markers for computer-detected abnormalities, and an examination-based cancer likelihood score. The area under the receiver operating characteristic curve (AUC), specificity and sensitivity, and reading time were compared between conditions by using mixed-models analysis dof variance and generalized linear models for multiple repeated measurements. Results On average, the AUC was higher with AI support than with unaided reading (0.89 vs 0.87, respectively; P = .002). Sensitivity increased with AI support (86% [86 of 100] vs 83% [83 of 100]; P = .046), whereas specificity trended toward improvement (79% [111 of 140]) vs 77% [108 of 140]; P = .06). Reading time per case was similar (unaided, 146 seconds; supported by AI, 149 seconds; P = .15). The AUC with the AI system alone was similar to the average AUC of the radiologists (0.89 vs 0.87). Conclusion Radiologists improved their cancer detection at mammography when using an artificial intelligence system for support, without requiring additional reading time. Published under a CC BY 4.0 license. See also the editorial by Bahl in this issue.","3515":"Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https:\/\/github.com\/thunlp\/CLAIM.","3516":"Artificial intelligence (AI) and robotics are digital technologies that will have significant impact on the development of humanity in the near future. They have raised fundamental questions about what we should do with these systems, what the systems themselves should do, what risks they involve, and how we can control these. - After the Introduction to the field (\u00a71), the main themes (\u00a72) of this article are: Ethical issues that arise with AI systems as objects, i.e., tools made and used by humans. This includes issues of privacy (\u00a72.1) and manipulation (\u00a72.2), opacity (\u00a72.3) and bias (\u00a72.4), human-robot interaction (\u00a72.5), employment (\u00a72.6), and the effects of autonomy (\u00a72.7). Then AI systems as subjects, i.e., ethics for the AI systems themselves in machine ethics (\u00a72.8) and artificial moral agency (\u00a72.9). Finally, the problem of a possible future AI superintelligence leading to a \u201csingularity\u201d (\u00a72.10). We close with a remark on the vision of AI (\u00a73). - For each section within these themes, we provide a general explanation of the ethical issues, outline existing positions and arguments, then analyse how these play out with current technologies and finally, what policy consequences may be drawn.","3517":"The advancement of artificial intelligence ( AI ) has truly stimulated the development and deployment of autonomous vehicles ( AVs ) in the transportation industry. Fueled by big data from various sensing devices and advanced computing resources, AI has become an essential component of AVs for perceiving the surrounding environment and making appropriate decision in motion. To achieve goal of full automation ( i.e., self-driving ( , it is important to know how AI works in AV systems. Existing research have made great efforts in investigating different aspects of applying AI in AV development. However, few studies have offered the research community a thorough examination of current practices in implementing AI in AVs. Thus, this paper aims to shorten the gap by providing a comprehensive survey of key studies in this research avenue. Specifically, it intends to analyze their use of AIs in supporting the primary applications in AVs: 1) perception; 2) localization and mapping; and 3) decision making. It investigates the current practices to understand how AI can be used and what are the challenges and issues associated with their implementation. Based on the exploration of current practices and technology advances, this paper further provides insights into potential opportunities regarding the use of AI in conjunction with other emerging technologies: 1) high definition maps, big data, and high performance computing; 2) augmented reality( AR ) \/ virtual reality ( VR ) enhanced simulation platform; and 3) 5G communication for connected AVs. This paper is expected to offer a quick reference for researchers interested in understanding the use of AI in AV research.","3518":"Artificial Intelligence (AI) is a potentially powerful tool in the fight against the COVID- 19 pandemic. Since the outbreak of the pandemic, there has been a scramble to use AI. This article provides an early, and necessarily selective review, discussing the contribution of AI to the fight against COVID-19, as well as the current constraints on these contributions. Six areas where AI can contribute to the fight against COVID-19 are discussed, namely i) early warnings and alerts, ii) tracking and prediction, iii) data dashboards, iv) diagnosis and prognosis, v) treatments and cures, and vi) social control. It is concluded that AI has not yet been impactful against COVID-19. Its use is hampered by a lack of data, and by too much data. Overcoming these constraints will require a careful balance between data privacy and public health, and rigorous human-AI interaction. It is unlikely that these will be addressed in time to be of much help during the present pandemic. In the meantime, extensive gathering of diagnostic data on who is infectious will be essential to save lives, train AI, and limit economic damages.","3519":null,"3520":"This diagnostic accuracy study evaluates whether artificial intelligence can overcome human mammography interpretation limits with a rigorous, unbiased evaluation of machine learning algorithms.","3521":"At the heart of any innovation process lies a fundamental practice: the way people create ideas and solve problems. This \u201cdecision making\u201d side of innovation is what scholars and practitioners refer to as \u201cdesign.\u201d Decisions in innovation processes have so far been taken by humans. What happens when they can be substituted by machines? Artificial Intelligence (AI) brings data and algorithms to the core of the innovation processes. What are the implications of this diffusion of AI for our understanding of design and innovation? Is AI just another digital technology that, akin to many others, will not significantly question what we know about design? Or will it create transformations in design that current theoretical frameworks cannot capture? This paper proposes a framework for understanding the design and innovation in the age of AI. We discuss the implications for design and innovation theory. Specifically, we observe that, as creative problem-solving is significantly conducted by algorithms, human design increasingly becomes an activity of sensemaking, that is, understanding which problems should or could be addressed. This shift in focus calls for the new theories and brings design closer to leadership, which is, inherently, an activity of sensemaking. Our insights are derived from and illustrated with two cases at the frontier of AI\u2014Netflix and Airbnb (com-plemented with analyses of Microsoft and Tesla)\u2014which point to two directions for the evolution of design and innovation in firms. First, AI enables an organization to overcome many past limitations of human-intensive design processes, by improving the scalability of the process, broadening its scope across traditional boundaries, and en-hancing its ability to learn and adapt on the fly. Second, and maybe more surprising, while removing these limitations, AI also appears to deeply enact several popular design principles. AI thus reinforces the principles of Design Thinking, namely: being people-centered, abductive, and iterative. In fact, AI enables the creation of solutions that are more highly user centered than human-based approaches (i.e., to an extreme level of granularity, designed for every single person); that are potentially more creative; and that are continuously updated through learning iterations across the entire product life cycle. In sum, while AI does not undermine the basic principles of design, it profoundly changes the practice of design. Problem-solving tasks, traditionally carried out by designers, are now automated into learning loops that operate without limitations of volume and speed. The algorithms embedded in these loops think in a radically different way than a designer who handles the complex problems holistically with a systemic perspective. Algorithms instead han-dle complexity through very simple tasks, which are iterated continuously. This paper discusses the implications of these insights for design and innovation management scholars and practitioners.","3522":"The increased relevance of social media in our daily life has been accompanied by efforts to manipulate online conversations and opinions. Deceptive social bots -- automated or semi-automated accounts designed to impersonate humans -- have been successfully exploited for these kinds of abuse. Researchers have responded by developing AI tools to arm the public in the fight against social bots. Here we review the literature on different types of bots, their impact, and detection methods. We use the case study of Botometer, a popular bot detection tool developed at Indiana University, to illustrate how people interact with AI countermeasures. A user experience survey suggests that bot detection has become an integral part of the social media experience for many users. However, barriers in interpreting the output of AI tools can lead to fundamental misunderstandings. The arms race between machine learning methods to develop sophisticated bots and effective countermeasures makes it necessary to update the training data and features of detection tools. We again use the Botometer case to illustrate both algorithmic and interpretability improvements of bot scores, designed to meet user expectations. We conclude by discussing how future AI developments may affect the fight between malicious bots and the public.","3523":null,"3524":null,"3525":null,"3526":null,"3527":"Background As a critical driving power to promote health care, the health care\u2013related artificial intelligence (AI) literature is growing rapidly. Objective The purpose of this analysis is to provide a dynamic and longitudinal bibliometric analysis of health care\u2013related AI publications. Methods The Web of Science (Clarivate PLC) was searched to retrieve all existing and highly cited AI-related health care research papers published in English up to December 2019. Based on bibliometric indicators, a search strategy was developed to screen the title for eligibility, using the abstract and full text where needed. The growth rate of publications, characteristics of research activities, publication patterns, and research hotspot tendencies were computed using the HistCite software. Results The search identified 5235 hits, of which 1473 publications were included in the analyses. Publication output increased an average of 17.02% per year since 1995, but the growth rate of research papers significantly increased to 45.15% from 2014 to 2019. The major health problems studied in AI research are cancer, depression, Alzheimer disease, heart failure, and diabetes. Artificial neural networks, support vector machines, and convolutional neural networks have the highest impact on health care. Nucleosides, convolutional neural networks, and tumor markers have remained research hotspots through 2019. Conclusions This analysis provides a comprehensive overview of the AI-related research conducted in the field of health care, which helps researchers, policy makers, and practitioners better understand the development of health care\u2013related AI research and possible practice implications. Future AI research should be dedicated to filling in the gaps between AI health care research and clinical applications.","3528":"Adaptation and innovation are extremely important to the manufacturing industry. This development should lead to sustainable manufacturing using new technologies. To promote sustainability, smart production requires global perspectives of smart production application technology. In this regard, thanks to intensive research efforts in the field of artificial intelligence (AI), a number of AI-based techniques, such as machine learning, have already been established in the industry to achieve sustainable manufacturing. Thus, the aim of the present research was to analyze, systematically, the scientific literature relating to the application of artificial intelligence and machine learning (ML) in industry. In fact, with the introduction of the Industry 4.0, artificial intelligence and machine learning are considered the driving force of smart factory revolution. The purpose of this review was to classify the literature, including publication year, authors, scientific sector, country, institution, and keywords. The analysis was done using the Web of Science and SCOPUS database. Furthermore, UCINET and NVivo 12 software were used to complete them. A literature review on ML and AI empirical studies published in the last century was carried out to highlight the evolution of the topic before and after Industry 4.0 introduction, from 1999 to now. Eighty-two articles were reviewed and classified. A first interesting result is the greater number of works published by the USA and the increasing interest after the birth of Industry 4.0.","3529":"Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health. Expected final online publication date for the Annual Review of Psychology, Volume 71 is January 4, 2020. Please see http:\/\/www.annualreviews.org\/page\/journal\/pubdates for revised estimates.","3530":"I develop a new method to predict the impacts of a technology on occupations. I use the overlap between the text of job task descriptions and the text of patents to construct a measure of the exposure of tasks to automation. I first apply the method to historical cases such as software and industrial robots. I establish that occupations I measure as highly exposed to previous automation technologies saw declines in employment and wages over the relevant periods. I use the fitted parameters from the case studies to predict the impacts of artificial intelligence. I find that, in contrast to software and robots, AI is directed at high-skilled tasks. Under the assumption that the historical pattern of long-run substitution will continue, I estimate that AI will reduce 90:10 wage inequality, but will not affect the top 1%.","3531":"During the last years, water quality has been threatened by various pollutants. Therefore, modeling and predicting water quality have become very important in controlling water pollution. In this work, advanced artificial intelligence (AI) algorithms are developed to predict water quality index (WQI) and water quality classification (WQC). For the WQI prediction, artificial neural network models, namely nonlinear autoregressive neural network (NARNET) and long short-term memory (LSTM) deep learning algorithm, have been developed. In addition, three machine learning algorithms, namely, support vector machine (SVM), K-nearest neighbor (K-NN), and Naive Bayes, have been used for the WQC forecasting. The used dataset has 7 significant parameters, and the developed models were evaluated based on some statistical parameters. The results revealed that the proposed models can accurately predict WQI and classify the water quality according to superior robustness. Prediction results demonstrated that the NARNET model performed slightly better than the LSTM for the prediction of the WQI values and the SVM algorithm has achieved the highest accuracy (97.01%) for the WQC prediction. Furthermore, the NARNET and LSTM models have achieved similar accuracy for the testing phase with a slight difference in the regression coefficient (RNARNET = 96.17% and RLSTM = 94.21%). This kind of promising research can contribute significantly to water management.","3532":"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the \u201chow\u201d and \u201cwhy\u201d of automated decision\u2010making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge\u2010based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural\u2010symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human\u2010understandable explainable systems.","3533":"We define hybrid intelligence (HI) as the combination of human and machine intelligence, augmenting human intellect and capabilities instead of replacing them and achieving goals that were unreachable by either humans or machines. HI is an important new research focus for artificial intelligence, and we set a research agenda for HI by formulating four challenges.","3534":null,"3535":"Artificial intelligence (AI) has contributed substantially to the resolution of a variety of biomedical problems, including cancer, over the past decade. Deep learning, a subfield of AI that is highly flexible and supports automatic feature extraction, is increasingly being applied in various areas of both basic and clinical cancer research. In this review, we describe numerous recent examples of the application of AI in oncology, including cases in which deep learning has efficiently solved problems that were previously thought to be unsolvable, and we address obstacles that must be overcome before such application can become more widespread. We also highlight resources and datasets that can help harness the power of AI for cancer research. The development of innovative approaches to and applications of AI will yield important insights in oncology in the coming decade.","3536":null,"3537":null,"3538":"Deep learning networks have been trained to recognize speech, caption photographs, and translate text between languages at high levels of performance. Although applications of deep learning networks to real-world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and nonconvex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures, and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.","3539":null,"3540":null,"3541":null,"3542":null,"3543":"Artificial intelligence (AI) is a technology that utilizes machines to mimic intelligent human behavior. To appreciate human-technology interaction in the clinical setting, augmented intelligence has been proposed as a cognitive extension of AI in health care, emphasizing its assistive and supplementary role to medical professionals. While truly autonomous medical robotic systems are still beyond reach, the virtual component of AI, known as software-type algorithms, is the main component used in dentistry. Because of their powerful capabilities in data analysis, these virtual algorithms are expected to improve the accuracy and efficacy of dental diagnosis, provide visualized anatomic guidance for treatment, simulate and evaluate prospective results, and project the occurrence and prognosis of oral diseases. Potential obstacles in contemporary algorithms that prevent routine implementation of AI include the lack of data curation, sharing, and readability; the inability to illustrate the inner decision-making process; the insufficient power of classical computing; and the neglect of ethical principles in the design of AI frameworks. It is necessary to maintain a proactive attitude toward AI to ensure its affirmative development and promote human-technology rapport to revolutionize dental practice. The present review outlines the progress and potential dental applications of AI in medical-aided diagnosis, treatment, and disease prediction and discusses their data limitations, interpretability, computing power, and ethical considerations, as well as their impact on dentists, with the objective of creating a backdrop for future research in this rapidly expanding arena.","3544":"ABSTRACT Emotional intelligence as personal intelligence and artificial intelligence as a machine intelligence have been popular in the relevant literature over the last two decades. The current study integrates these two concepts and explores how emotional and artificial intelligence influences employee retention and performance with a focus on service employees in the hotel industry. Employee performance is operationalised into internal and external dimensions that captures employees\u2019 task efficiency over both internal and external service encounters with co-workers and customers respectively. The data were collected from a variety of different ranking hotels. The results show that emotional intelligence has a significant effect on employee retention and performance; whereas artificial intelligence plays a significant moderating role in employee performance. A discussion of the findings and implications concludes this paper.","3545":"Technological innovation is constantly reshaping the materiality and mechanics of smart-city initiatives. Recently, innovation in artificial intelligence (AI) in the shape of self-driving cars, robots and city brains, has been pushing the so-called smart city to morph into an autonomous urban creature which is largely unknown. In this emerging strand of smart urbanism, artificially intelligent entities are taking the management of urban services as well as urban governance out of the hands of humans, operating the city in an autonomous manner. This paper explores, in theory and practice, how the development of AI intersects with the development of the city. The contribution of the paper is threefold. First, the paper advances a theoretical framework to understand AI specifically in urban contexts. It develops the concept of urban artificial intelligence, capturing the main manifestations of AI in cities. Second, the paper examines the case of Masdar City, an Emirati urban experiment, to show how the genesis of urban artificial intelligences is part of a long-standing process of technological development and a politico-economic agenda which together are enabling the transition from automation to autonomy. Third, it proposes a research agenda to investigate what the paper terms the autonomous city.","3546":"We introduce four principles for explainable arti\ufb01cial intelligence (AI) that comprise fundamental properties for explainable AI systems. We propose that explainable AI systems deliver accompanying evidence or reasons for outcomes and processes; provide explanations that are understandable to individual users; provide explanations that correctly re\ufb02ect the system\u2019s process for generating the output; and that a system only operates under conditions for which it was designed and when it reaches suf\ufb01cient con\ufb01dence in its output. We have termed these four principles as explanation, meaningful, explanation accuracy, and knowledge limits, respectively. Through signi\ufb01cant stakeholder engagement, these four principles were developed to encompass the multidisciplinary nature of explainable AI, including the \ufb01elds of computer science, engineering, and psychology. Because one-size-\ufb01ts-all explanations do not exist, different users will require different types of explanations. We present \ufb01ve categories of explanation and summarize theories of explainable AI. We give an overview of the algorithms in the \ufb01eld that cover the major classes of explainable algorithms. As a baseline comparison, we assess how well explanations provided by people follow our four principles. This assessment provides insights to the challenges of designing explainable AI systems.","3547":null,"3548":null,"3549":"\u201cTechnological intelligence\u201d is the capacity to appreciate and adapt technological advancements, and \u201cartificial intelligence\u201d is the key to achieve persuasive operational transformations in majority of contemporary organizational set-ups. Implicitly, artificial intelligence (the philosophies of machines to think, behave and perform either same or similar to humans) has knocked the doors of business organizations as an imperative activity. Artificial intelligence, as a discipline, initiated by scientist John McCarthy and formally publicized at Dartmouth Conference in 1956, now occupies a central stage for many organizations. Implementation of artificial intelligence provides competitive edge to an organization with a definite augmentation in its social and corporate status. Mere application of a concept will not furnish real output until and unless its performance is reviewed systematically. Technological changes are dynamic and advancing at a rapid rate. Subsequently, it becomes highly crucial to understand that where have the people reached with respect to artificial intelligence research. The present article aims to review significant work by eminent researchers towards artificial intelligence in the form of top contributing universities, authors, keywords, funding sources, journals and citation statistics.,As rightly remarked by past researchers that reviewing is learning from experience, research team has reviewed (by applying systematic literature review through bibliometric analysis) the concept of artificial intelligence in this article. A sum of 1,854 articles extracted from Scopus database for the year 2018\u20132019 (31st of May) with selected keywords (artificial intelligence, genetic algorithms, agent-based systems, expert systems, big data analytics and operations management) along with certain filters (subject\u2013business, management and accounting; language-English; document\u2013article, article in press, review articles and source-journals).,Results obtained from cluster analysis focus on predominant themes for present as well as future researchers in the area of artificial intelligence. Emerged clusters include Cluster 1: Artificial Intelligence and Optimization; Cluster 2: Industrial Engineering\/Research and Automation; Cluster 3: Operational Performance and Machine Learning; Cluster 4: Sustainable Supply Chains and Sustainable Development; Cluster 5: Technology Adoption and Green Supply Chain Management and Cluster 6: Internet of Things and Reverse Logistics.,The result of review of selected studies is in itself a unique contribution and a food for thought for operations managers and policy makers.","3550":null,"3551":"Artificial intelligence education (AIEd) is defined in the field of education as the utilization of artificial intelligence. There are currently many AIEd\u2010driven applications in schools and universities. This paper applies an artificial intelligence module combined with the knowledge recommendation to the system and develops an online English teaching system in comparison with the common teaching auxiliary system. The method of English teaching is useful in investigating the potential internal connections between evaluation outcomes and various factors. This article develops deep learning\u2010assisted online intelligent English teaching system that utilizes to create a modern tool platform to help students improve their English language teaching efficiency in line with their mastery of knowledge and personality. The decision tree algorithm and neural networks have been used and to generate an English teaching assessment implementation model based on decision tree technologies. It provides valuable data from extensive information, summarizes rules and data, and helps teachers to improve their education and the English scores of students. This system reflects the thinking of the artificial intelligence expert system. Test application demonstrates that the system can help students improve their learning efficiency and will make learning content more relevant. Besides, the system provides an example model with similar methods and has a referential definition.","3552":null,"3553":null,"3554":null,"3555":null,"3556":null,"3557":null,"3558":null,"3559":"Artificial intelligence has been advancing in fields including anesthesiology. This scoping review of the intersection of artificial intelligence and anesthesia research identified and summarized six themes of applications of artificial intelligence in anesthesiology: (1) depth of anesthesia monitoring, (2) control of anesthesia, (3) event and risk prediction, (4) ultrasound guidance, (5) pain management, and (6) operating room logistics. Based on papers identified in the review, several topics within artificial intelligence were described and summarized: (1) machine learning (including supervised, unsupervised, and reinforcement learning), (2) techniques in artificial intelligence (e.g., classical machine learning, neural networks and deep learning, Bayesian methods), and (3) major applied fields in artificial intelligence. The implications of artificial intelligence for the practicing anesthesiologist are discussed as are its limitations and the role of clinicians in further developing artificial intelligence for use in clinical care. Artificial intelligence has the potential to impact the practice of anesthesiology in aspects ranging from perioperative support to critical care delivery to outpatient pain management. This scoping review of artificial intelligence in anesthesiology summarizes six areas of research: (1) depth of anesthesia monitoring, (2) control of anesthesia, (3) event\/risk prediction, (4) ultrasound guidance, (5) pain management, and (6) operating room logistics. Supplemental Digital Content is available in the text.","3560":"As the core driving force of the new round of informatization development and industrial revolution, the disruptive achievements of artificial intelligence (AI) are rapidly and comprehensively infi...","3561":null,"3562":"Industry 4.0 concepts and technologies ensure the ongoing development of micro- and macro-economic entities by focusing on the principles of interconnectivity, digitalization, and automation. In this context, artificial intelligence is seen as one of the major enablers for Smart Logistics and Smart Production initiatives. This paper systematically analyzes the scientific literature on artificial intelligence, machine learning, and deep learning in the context of Smart Logistics management in industrial enterprises. Furthermore, based on the results of the systematic literature review, the authors present a conceptual framework, which provides fruitful implications based on recent research findings and insights to be used for directing and starting future research initiatives in the field of artificial intelligence (AI), machine learning (ML), and deep learning (DL) in Smart Logistics.","3563":"Abstract Over the past ten years, we have worked in a collaboration between educators and computer scientists at the University of Illinois to imagine futures for education in the context of what is loosely called \u201cartificial intelligence.\u201d Unhappy with the first generation of digital learning environments, our agenda has been to design alternatives and research their implementation. Our starting point has been to ask, what is the nature of machine intelligence, and what are its limits and potentials in education? This paper offers some tentative answers, first conceptually, and then practically in an overview of the results of a number of experimental implementations documented in greater detail elsewhere. Our key finding is that artificial intelligence\u2014in the context of the practices of electronic computing developing over the past three quarters of a century\u2014will never in any sense \u201ctake over\u201d the role of teacher, because how it works and what it does are so profoundly different from human intelligence. However, within the limits that we describe in this paper, it offers the potential to transform education in ways that\u2014counterintuitively perhaps\u2014make education more human, not less.","3564":null,"3565":"ABSTRACT Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI\u2019s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.","3566":null,"3567":"The power system worldwide is going through a revolutionary transformation due to the integration with various distributed components, including advanced metering infrastructure, communication infrastructure, distributed energy resources, and electric vehicles, to improve the reliability, energy efficiency, management, and security of the future power system. These components are becoming more tightly integrated with IoT. They are expected to generate a vast amount of data to support various applications in the smart grid, such as distributed energy management, generation forecasting, grid health monitoring, fault detection, home energy management, etc. With these new components and information, artificial intelligence techniques can be applied to automate and further improve the performance of the smart grid. In this paper, we provide a comprehensive review of the state-of-the-art artificial intelligence techniques to support various applications in a distributed smart grid. In particular, we discuss how artificial techniques are applied to support the integration of renewable energy resources, the integration of energy storage systems, demand response, management of the grid and home energy, and security. As the smart grid involves various actors, such as energy produces, markets, and consumers, we also discuss how artificial intelligence and market liberalization can potentially help to increase the overall social welfare of the grid. Finally, we provide further research challenges for large-scale integration and orchestration of automated distributed devices to realize a truly smart grid.","3568":null,"3569":null,"3570":"Background The advancement of health care information technology and the emergence of artificial intelligence has yielded tools to improve the quality of various health care processes. Few studies have investigated employee perceptions of artificial intelligence implementation in Saudi Arabia and the Arabian world. In addition, limited studies investigated the effect of employee knowledge and job title on the perception of artificial intelligence implementation in the workplace. Objective The aim of this study was to explore health care employee perceptions and attitudes toward the implementation of artificial intelligence technologies in health care institutions in Saudi Arabia. Methods An online questionnaire was published, and responses were collected from 250 employees, including doctors, nurses, and technicians at 4 of the largest hospitals in Riyadh, Saudi Arabia. Results The results of this study showed that 3.11 of 4 respondents feared artificial intelligence would replace employees and had a general lack of knowledge regarding artificial intelligence. In addition, most respondents were unaware of the advantages and most common challenges to artificial intelligence applications in the health sector, indicating a need for training. The results also showed that technicians were the most frequently impacted by artificial intelligence applications due to the nature of their jobs, which do not require much direct human interaction. Conclusions The Saudi health care sector presents an advantageous market potential that should be attractive to researchers and developers of artificial intelligence solutions.","3571":null,"3572":null,"3573":"Artificial intelligence arising from the use of machine learning is rapidly being developed and deployed by governments to enhance operations, public services, and compliance and security activities. This article reviews how artificial intelligence is being used in public sector for automated decision making, for chatbots to provide information and advice, and for public safety and security. It then outlines four public administration challenges to deploying artificial intelligence in public administration: accuracy, bias and discrimination; legality, due process and administrative justice; responsibility, accountability, transparency and explainability; and power, compliance and control. The article outlines technological and governance innovations that are being developed to address these challenges.","3574":"Artificial intelligence and, in particular, machine learning methods have gained notable attention in the tribological community due to their ability to predict tribologically relevant parameters such as, for instance, the coefficient of friction or the oil film thickness. This perspective aims at highlighting some of the recent advances achieved by implementing artificial intelligence, specifically artificial neutral networks, towards tribological research. The presentation and discussion of successful case studies using these approaches in a tribological context clearly demonstrates their ability to accurately and efficiently predict these tribological characteristics. Regarding future research directions and trends, we emphasis on the extended use of artificial intelligence and machine learning concepts in the field of tribology including the characterization of the resulting surface topography and the design of lubricated systems.","3575":"SUMMARY: Artificial intelligence technology is a rapidly expanding field with many applications in acute stroke imaging, including ischemic and hemorrhage subtypes. Early identification of acute stroke is critical for initiating prompt intervention to reduce morbidity and mortality. Artificial intelligence can help with various aspects of the stroke treatment paradigm, including infarct or hemorrhage detection, segmentation, classification, large vessel occlusion detection, Alberta Stroke Program Early CT Score grading, and prognostication. In particular, emerging artificial intelligence techniques such as convolutional neural networks show promise in performing these imaging-based tasks efficiently and accurately. The purpose of this review is twofold: first, to describe AI methods and available public and commercial platforms in stroke imaging, and second, to summarize the literature of current artificial intelligence\u2013driven applications for acute stroke triage, surveillance, and prediction.","3576":"This conceptual paper addresses the issues of transparency as linked to artificial intelligence (AI) from socio-legal and computer scientific perspectives. Firstly, we discuss the conceptual distinction between transparency in AI and algorithmic transparency, and argue for the wider concept \u2018in AI\u2019, as a partly contested albeit useful notion in relation to transparency. Secondly, we show that transparency as a general concept is multifaceted, and of widespread theoretical use in multiple disciplines over time, particularly since the 1990s. Still, it has had a resurgence in contemporary notions of AI governance, such as in the multitude of recently published ethics guidelines on AI. Thirdly, we discuss and show the relevance of the fact that transparency expresses a conceptual metaphor of more general significance, linked to knowing, bringing positive connotations that may have normative effects to regulatory debates. Finally, we draw a possible categorisation of aspects related to transparency in AI, or what we interchangeably call AI transparency, and argue for the need of developing a multidisciplinary understanding, in order to contribute to the governance of AI as applied on markets and in society. (Less)","3577":"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.","3578":null,"3579":"Artificial intelligence (AI) and people\u2019s interactions with it\u2014through virtual agents, socialbots, and language-generation software\u2014do not fit neatly into paradigms of communication theory that have long focused on human\u2013human communication. To address this disconnect between communication theory and emerging technology, this article provides a starting point for articulating the differences between communicative AI and previous technologies and introduces a theoretical basis for navigating these conditions in the form of scholarship within human\u2013machine communication (HMC). Drawing on an HMC framework, we outline a research agenda built around three key aspects of communicative AI technologies: (1) the functional dimensions through which people make sense of these devices and applications as communicators, (2) the relational dynamics through which people associate with these technologies and, in turn, relate to themselves and others, and (3) the metaphysical implications called up by blurring ontological boundaries surrounding what constitutes human, machine, and communication.","3580":null,"3581":null,"3582":null,"3583":null,"3584":null,"3585":"Since 2010, substantial progress has been made in artificial intelligence (AI) and its application to medicine. AI is explored in gastroenterology for endoscopic analysis of lesions, in detection of cancer, and to facilitate the analysis of inflammatory lesions or gastrointestinal bleeding during wireless capsule endoscopy. AI is also tested to assess liver fibrosis and to differentiate patients with pancreatic cancer from those with pancreatitis. AI might also be used to establish prognoses of patients or predict their response to treatments, based on multiple factors. We review the ways in which AI may help physicians make a diagnosis or establish a prognosis and discuss its limitations, knowing that further randomized controlled studies will be required before the approval of AI techniques by the health authorities.","3586":"The article critically examines the possibilities of using steadily developing artificial intelligence systems in the public sector of foreign countries and Russia. It is noted that despite the prospects of obtaining significant gains, there are a number of technical, economic and socio-ethical limitations associated with the introduction of artificial intelligence, taking into account its features as a general purpose technology. The increasing value of professional judgment, which allows using the results of artificial intelligence, is emphasized. Based on the principles of working with artificial intelligence developed in world practice, as well as the peculiarities of the domestic institutional structure and trust in it by the citizens, a conclusion is made about the need for a cautious approach to the use of artificial intelligence technologies in applications of the Russian public sector. Such practices can not only cause considerable harm to specific individuals in the process of current functioning of domestic institutions, but also hinder their transformation.","3587":"Artificial intelligence (AI) is one of the core drivers of industrial development and a critical factor in promoting the integration of emerging technologies, such as graphic processing unit, Inter...","3588":null,"3589":"\nPurpose\nConsidering the increasing impact of Artificial Intelligence (AI) on financial technology (FinTech), the purpose of this paper is to propose a research framework to better understand robo-advisor adoption by a wide range of potential customers. It also predicts that personal and sociodemographic variables (familiarity with robots, age, gender and country) moderate the main relationships.\n\n\nDesign\/methodology\/approach\nData from a web survey of 765 North American, British and Portuguese potential users of robo-advisor services confirm the validity of the measurement scales and provide the input for structural equation modeling and multisample analyses of the hypotheses.\n\n\nFindings\nConsumers\u2019 attitudes toward robo-advisors, together with mass media and interpersonal subjective norms, are found to be the key determinants of adoption. The influences of perceived usefulness and attitude are slightly higher for users with a higher level of familiarity with robots; in turn, subjective norms are significantly more relevant for users with a lower familiarity and for customers from Anglo-Saxon countries.\n\n\nPractical implications\nBanks and other firms in the finance industry should design robo-advisors to be used by a wide spectrum of consumers. Marketing tactics applied should consider the customer\u2019s level of familiarity with robots.\n\n\nOriginality\/value\nThis research identifies the key drivers of robo-advisor adoption and the moderating effect of personal and sociodemographic variables. It contributes to understanding consumers\u2019 perceptions regarding the introduction of AI in FinTech.\n","3590":null,"3591":"Technological innovations such as artificial intelligence and robotics may be of potential use in telemedicine and in building capacity to respond to future pandemics beyond the current COVID-19 era. Our international consortium of interdisciplinary experts in clinical medicine, health policy, and telemedicine have identified gaps in uptake and implementation of telemedicine or telehealth across geographics and medical specialties. This paper discusses various artificial intelligence and robotics-assisted telemedicine or telehealth applications during COVID-19 and presents an alternative artificial intelligence assisted telemedicine framework to accelerate the rapid deployment of telemedicine and improve access to quality and cost-effective healthcare. We postulate that the artificial intelligence assisted telemedicine framework would be indispensable in creating futuristic and resilient health systems that can support communities amidst pandemics.","3592":"Abstract Lake et al. offer a timely critique on the recent accomplishments in artificial intelligence from the vantage point of human intelligence and provide insightful suggestions about research directions for building more human-like intelligence. Because we agree with most of the points they raised, here we offer a few points that are complementary.","3593":"Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI). Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.","3594":"This article explores the role of artificial intelligence (AI) in aiding personalized engagement marketing\u2014an approach to create, communicate, and deliver personalized offerings to customers. It proposes that consumers are ready for a new journey in which AI is a tool for endless options and information that are narrowed and curated in a personalized way. It also provides predictions for managers regarding the AI-driven environment on branding and customer management practices in both developed and developing countries.","3595":"How does organizational decision-making change with the advent of artificial intelligence (AI)-based decision-making algorithms? This article identifies the idiosyncrasies of human and AI-based decision making along five key contingency factors: specificity of the decision search space, interpretability of the decision-making process and outcome, size of the alternative set, decision-making speed, and replicability. Based on a comparison of human and AI-based decision making along these dimensions, the article builds a novel framework outlining how both modes of decision making may be combined to optimally benefit the quality of organizational decision making. The framework presents three structural categories in which decisions of organizational members can be combined with AI-based decisions: full human to AI delegation; hybrid\u2014human-to-AI and AI-to-human\u2014sequential decision making; and aggregated human\u2013AI decision making.","3596":"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","3597":"Artificial Intelligence is a booming technological domain capable of altering every aspect of our social interactions. In education, AI has begun producing new teaching and learning solutions that are now undergoing testing in different contexts. This working paper, written for education policymakers, anticipates the extent to which AI affects the education sector to allow for informed and appropriate policy responses. This paper gathers examples of the introduction of AI in education worldwide, particularly in developing countries, discussions in the context of the 2019 Mobile Learning Week and beyond, as part of the multiple ways to accomplish Sustainable Development Goal 4, which strives for equitable, quality education for all.","3598":"Recent advances in artificial intelligence are primarily driven by machine learning, a prediction technology. Prediction is useful because it is an input into decision-making. In order to appreciate the impact of artificial intelligence on jobs, it is important to understand the relative roles of prediction and decision tasks. We describe and provide examples of how artificial intelligence will affect labor, emphasizing differences between when the automation of prediction leads to automating decisions versus enhancing decision-making by humans.","3599":null,"3600":null,"3601":"Artificial intelligence can optimize cancer drug discovery, development, and administration Artificial intelligence (AI) approaches have the potential to affect several facets of cancer therapy. These include drug discovery and development and how these drugs are clinically validated and ultimately administered at the point of care, among others. Currently, these processes are expensive and time-consuming. Moreover, therapies often result in variable treatment outcomes between patients. The convergence of AI and cancer therapy has resulted in multiple solutions to address these challenges. AI platforms ranging from machine learning to neural networks can accelerate drug discovery, harness biomarkers to accurately match patients to clinical trials, and truly personalize cancer therapy using only a patient's own data. These advances are indicators that practice-changing cancer therapy empowered by AI may be on the horizon.","3602":"PURPOSE OF REVIEW\nDiabetic retinopathy is the most common specific complication of diabetes mellitus. Traditional care for patients with diabetes and diabetic retinopathy is fragmented, uncoordinated and delivered in a piecemeal nature, often in the most expensive and high-resource tertiary settings. Transformative new models incorporating digital technology are needed to address these gaps in clinical care.\n\n\nRECENT FINDINGS\nArtificial intelligence and telehealth may improve access, financial sustainability and coverage of diabetic retinopathy screening programs. They enable risk stratifying patients based on individual risk of vision-threatening diabetic retinopathy including diabetic macular edema (DME), and predicting which patients with DME best respond to antivascular endothelial growth factor therapy.\n\n\nSUMMARY\nProgress in artificial intelligence and tele-ophthalmology for diabetic retinopathy screening, including artificial intelligence applications in 'real-world settings' and cost-effectiveness studies are summarized. Furthermore, the initial research on the use of artificial intelligence models for diabetic retinopathy risk stratification and management of DME are outlined along with potential future directions. Finally, the need for artificial intelligence adoption within ophthalmology in response to coronavirus disease 2019 is discussed. Digital health solutions such as artificial intelligence and telehealth can facilitate the integration of community, primary and specialist eye care services, optimize the flow of patients within healthcare networks, and improve the efficiency of diabetic retinopathy management.","3603":"Neuro-symbolic and statistical relational artificial intelligence both integrate frameworks for learning with logical reasoning. This survey identifies several parallels across seven different dimensions between these two fields. These cannot only be used to characterize and position neuro-symbolic artificial intelligence approaches but also to identify a number of directions for further research.","3604":"The role of artificial intelligence has created a massive impact on all sectors of the world. Shortly, it will be more rapidly expanding to mark its significance for shaping the world. The present study cites some of the scientific studies highlighting the different roles of artificial intelligence to advance the applicative properties in the current world. The present report envisions the importance and needs of artificial intelligence in the modern world and society, shaping the future world.","3605":null,"3606":"Recent advances in edge computing and caching have significant impacts on the developments of vehicular networks. Nevertheless, the heterogeneous requirements of on-vehicle applications and the time variability on popularity of contents bring great challenges for edge servers to efficiently utilize their resources. Moreover, the high mobility of smart vehicles adds substantial complexity in jointly optimizing edge computing and caching. Artificial intelligence (AI) can greatly enhance the cognition and intelligence of vehicular networks and thus assist in optimally allocating resources for problems with diverse, time-variant, and complex features. In this article, we propose a new architecture that can dynamically orchestrate edge computing and caching resources to improve system utility by making full use of AI-based algorithms. Then we formulate a joint edge computing and caching scheme to maximize system utility and develop a novel resource management scheme by exploiting deep reinforcement learning. Numerical results demonstrate the effectiveness of the proposed scheme.","3607":"Artificial intelligence (AI) is the ability of a machine or computer system to simulate and perform tasks that would normally require human intelligence, such as logical reasoning, learning, and problem solving. Artificial intelligence is based on the use of machine learning algorithms and technologies to give machines the ability to apply certain cognitive abilities and perform tasks on their own autonomously or semi-autonomously. Artificial intelligence is distinguished by its degree of cognitive capacity or by its degree of autonomy. By capacity it can be weak or limited, general or superlative. Due to its autonomy, it can be reactive, deliberative, cognitive, or totally autonomous. As artificial intelligence improves, many processes are becoming more efficient and tasks that seem complicated today will be performed more quickly and accurately.","3608":null,"3609":"Pathology is the cornerstone of cancer care. The need for accuracy in histopathologic diagnosis of cancer is increasing as personalized cancer therapy requires accurate biomarker assessment. The appearance of digital image analysis holds promise to improve both the volume and precision of histomorphological evaluation. Recently, machine learning, and particularly deep learning, has enabled rapid advances in computational pathology. The integration of machine learning into routine care will be a milestone for the healthcare sector in the next decade, and histopathology is right at the centre of this revolution. Examples of potential high\u2010value machine learning applications include both model\u2010based assessment of routine diagnostic features in pathology, and the ability to extract and identify novel features that provide insights into a disease. Recent groundbreaking results have demonstrated that applications of machine learning methods in pathology significantly improves metastases detection in lymph nodes, Ki67 scoring in breast cancer, Gleason grading in prostate cancer and tumour\u2010infiltrating lymphocyte (TIL) scoring in melanoma. Furthermore, deep learning models have also been demonstrated to be able to predict status of some molecular markers in lung, prostate, gastric and colorectal cancer based on standard HE slides. Moreover, prognostic (survival outcomes) deep neural network models based on digitized HE slides have been demonstrated in several diseases, including lung cancer, melanoma and glioma. In this review, we aim to present and summarize the latest developments in digital image analysis and in the application of artificial intelligence in diagnostic pathology.","3610":null,"3611":"The DARPA's Explainable Artificial Intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. This talk will summarize the XAI program and present highlights from these Phase 1 evaluations.","3612":"The increasing application of Artificial Intelligence (AI) in health and medicine has attracted a great deal of research interest in recent decades. This study aims to provide a global and historical picture of research concerning AI in health and medicine. A total of 27,451 papers that were published between 1977 and 2018 (84.6% were dated 2008\u20132018) were retrieved from the Web of Science platform. The descriptive analysis examined the publication volume, and authors and countries collaboration. A global network of authors\u2019 keywords and content analysis of related scientific literature highlighted major techniques, including Robotic, Machine learning, Artificial neural network, Artificial intelligence, Natural language process, and their most frequent applications in Clinical Prediction and Treatment. The number of cancer-related publications was the highest, followed by Heart Diseases and Stroke, Vision impairment, Alzheimer\u2019s, and Depression. Moreover, the shortage in the research of AI application to some high burden diseases suggests future directions in AI research. This study offers a first and comprehensive picture of the global efforts directed towards this increasingly important and prolific field of research and suggests the development of global and national protocols and regulations on the justification and adaptation of medical AI products.","3613":"Radiomics is a relatively new word for the field of radiology, meaning the extraction of a high number of quantitative features from medical images. Artificial intelligence (AI) is broadly a set of advanced computational algorithms that basically learn the patterns in the data provided to make predictions on unseen data sets. Radiomics can be coupled with AI because of its better capability of handling a massive amount of data compared with the traditional statistical methods. Together, the primary purpose of these fields is to extract and analyze as much and meaningful hidden quantitative data as possible to be used in decision support. Nowadays, both radiomics and AI have been getting attention for their remarkable success in various radiological tasks, which has been met with anxiety by most of the radiologists due to the fear of replacement by intelligent machines. Considering ever-developing advances in computational power and availability of large data sets, the marriage of humans and machines in future clinical practice seems inevitable. Therefore, regardless of their feelings, the radiologists should be familiar with these concepts. Our goal in this paper was three-fold: first, to familiarize radiologists with the radiomics and AI; second, to encourage the radiologists to get involved in these ever-developing fields; and, third, to provide a set of recommendations for good practice in design and assessment of future works.","3614":"Health care is evolving and with it the need to reform medical education. As the practice of medicine enters the age of artificial intelligence (AI), the use of data to improve clinical decision making will grow, pushing the need for skillful medicine-machine interaction. As the rate of medical knowledge grows, technologies such as AI are needed to enable health care professionals to effectively use this knowledge to practice medicine. Medical professionals need to be adequately trained in this new technology, its advantages to improve cost, quality, and access to health care, and its shortfalls such as transparency and liability. AI needs to be seamlessly integrated across different aspects of the curriculum. In this paper, we have addressed the state of medical education at present and have recommended a framework on how to evolve the medical education curriculum to include AI.","3615":null,"3616":"Owing to the rich emerging multimedia applications and services in the past decade, super large amount of multimedia data has been produced for the purpose of advanced research in multimedia. Furthermore, multimedia research has made great progress on image\/video content analysis, multimedia search and recommendation, multimedia streaming, multimedia content delivery etc. At the same time, Artificial Intelligence (AI) has undergone a \u201cnew\u201d wave of development since being officially regarded as an academic discipline in 1950s, which should give credits to the extreme success of deep learning. Thus, one question naturally arises: What happens when multimedia meets Artificial Intelligence? To answer this question, we introduce the concept of Multimedia Intelligence through investigating the mutual-influence between multimedia and Artificial Intelligence. We explore the mutual influences between multimedia and Artificial Intelligence from two aspects: i) multimedia drives Artificial Intelligence to experience a paradigm shift towards more explainability and ii) Artificial Intelligence in turn injects new ways of thinking for multimedia research. As such, these two aspects form a loop in which multimedia and Artificial Intelligence interactively enhance each other. In this paper, we discuss what and how efforts have been done in literature and share our insights on research directions that deserve further study to produce potentially profound impact on multimedia intelligence.","3617":"This report presents a broad look at the American public\u2019s attitudes toward artificial intelligence (AI) and AI governance, based on findings from a nationally representative survey of 2,000 American adults. As the study of the public opinion toward AI is relatively new, we aimed for breadth over depth, with our questions touching on: workplace automation; attitudes regarding international cooperation; the public\u2019s trust in various actors to develop and regulate AI; views about the importance and likely impact of different AI governance challenges; and historical and cross-national trends in public opinion regarding AI. Our results provide preliminary insights into the character of US public opinion regarding AI.","3618":null,"3619":"Abstract The term Artificial Intelligence (AI) was coined by John McCarthy in 1956 during a conference held on this subject. However, the possibility of machines being able to simulate human behavior and actually think was raised earlier by Alan Turing who developed the Turing test in order to differentiate humans from machines. Since then, computational power has grown to the point of instant calculations and the ability evaluate new data, according to previously assessed data, in real time. Today, AI is integrated into our daily lives in many forms, such as personal assistants (Siri, Alexa, Google assistant etc.), automated mass transportation, aviation and computer gaming. More recently, AI has also begun to be incorporated into medicine to improve patient care by speeding up processes and achieving greater accuracy, opening the path to providing better healthcare overall. Radiological images, pathology slides, and patients\u2019 electronic medical records (EMR) are being evaluated by machine learning, aiding in the process of diagnosis and treatment of patients and augmenting physicians\u2019 capabilities. Herein we describe the current status of AI in medicine, the way it is used in the different disciplines and future trends.","3620":"A continuous journey towards an appropriate governance framework for AI As part of its European strategy for Artificial Intelligence (AI), and as a response to the increasing ethical questions raised by this technology, the European Commission established an independent High-Level Expert Group on Artificial Intelligence (AI HLEG) in June 2018. The group was tasked to draft two deliver-ables: AI Ethics Guidelines and Policy and Investment Recommendations. Nine months later, its first deliverable was published, putting forward a comprehensive framework to achieve \u201c Trustworthy AI \u201d by offering ethical guidance to AI practitioners. This paper dives into the work carried out by the group, focusing in particular on its AI Ethics Guidelines. First, this paper clarifies the context that led to the creation of the AI HLEG and its mandate (I.). Subsequently, it elaborates on the Guidelines \u2019 aim and purpose (II.), and analyses the Guidelines \u2019 drafting process (III.). Particular focus is given to the questions surrounding the respective role played by ethics and law in the AI governance landscape (IV.), as well as some of the challenges that had to be overcome throughout the process (V.). Finally, this paper places the Guidelines in an international context, and sets out the next steps (VI.) ahead on the journey towards an appropriate governance framework for AI (VII.).","3621":"Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated decision-making systems. We address the topic of transparency in artificial intelligence by integrating legal, social, and ethical aspects. We first investigate the ratio legis of the transparency requirement in the General Data Protection Regulation and its ethical underpinnings, showing its focus on the provision of information and explanation. We then discuss the pitfalls with respect to this requirement by focusing on the significance of contextual and performative factors in the implementation of transparency. We show that human\u2013computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies due to the impact of a wide range of contextual factors, including performative aspects. We conclude by integrating the information- and explanation-based approach to transparency with the critical contextual approach, proposing that transparency as required by the General Data Protection Regulation in itself may be insufficient to achieve the positive goals associated with transparency. Instead, we propose to understand transparency relationally, where information provision is conceptualized as communication between technology providers and users, and where assessments of trustworthiness based on contextual factors mediate the value of transparency communications. This relational concept of transparency points to future research directions for the study of transparency in artificial intelligence systems and should be taken into account in policymaking.","3622":null,"3623":"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model\u2019s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.","3624":"Artificial intelligence (AI) is gaining high visibility in the realm of health care innovation. Broadly defined, AI is a field of computer science that aims to mimic human intelligence with computer systems.1 This mimicry is accomplished through iterative, complex pattern matching, generally at a speed and scale that exceed human capability. Proponents suggest, often enthusiastically, that AI will revolutionize health care for patients and populations. However, key questions must be answered to translate its promise into action.","3625":null,"3626":"Importance\nSurgeons make complex, high-stakes decisions under time constraints and uncertainty, with significant effect on patient outcomes. This review describes the weaknesses of traditional clinical decision-support systems and proposes that artificial intelligence should be used to augment surgical decision-making.\n\n\nObservations\nSurgical decision-making is dominated by hypothetical-deductive reasoning, individual judgment, and heuristics. These factors can lead to bias, error, and preventable harm. Traditional predictive analytics and clinical decision-support systems are intended to augment surgical decision-making, but their clinical utility is compromised by time-consuming manual data management and suboptimal accuracy. These challenges can be overcome by automated artificial intelligence models fed by livestreaming electronic health record data with mobile device outputs. This approach would require data standardization, advances in model interpretability, careful implementation and monitoring, attention to ethical challenges involving algorithm bias and accountability for errors, and preservation of bedside assessment and human intuition in the decision-making process.\n\n\nConclusions and Relevance\nIntegration of artificial intelligence with surgical decision-making has the potential to transform care by augmenting the decision to operate, informed consent process, identification and mitigation of modifiable risk factors, decisions regarding postoperative management, and shared decisions regarding resource use.","3627":null,"3628":"Evolutionary fuzzy systems are one of the greatest advances within the area of computational intelligence. They consist of evolutionary algorithms applied to the design of fuzzy systems. Thanks to this hybridization, superb abilities are provided to fuzzy modeling in many different data science scenarios. This contribution is intended to comprise a position paper developing a comprehensive analysis of the evolutionary fuzzy systems research field. To this end, the \"4 W\" questions are posed and addressed with the aim of understanding the current context of this topic and its significance. Specifically, it will be pointed out why evolutionary fuzzy systems are important from an explainable point of view, when they began, what they are used for, and where the attention of researchers should be directed to in the near future in this area. They must play an important role for the emerging area of eXplainable Artificial Intelligence (XAI) learning from data.","3629":null,"3630":"\nPurpose\nThe purpose of this paper is to explain the technological phenomenon artificial intelligence (AI) and how it can contribute to knowledge-based marketing in B2B. Specifically, this paper describes the foundational building blocks of any artificial intelligence system and their interrelationships. This paper also discusses the implications of the different building blocks with respect to market knowledge in B2B marketing and outlines avenues for future research.\n\n\nDesign\/methodology\/approach\nThe paper is conceptual and proposes a framework to explicate the phenomenon AI and its building blocks. It further provides a structured discussion of how AI can contribute to different types of market knowledge critical for B2B marketing: customer knowledge, user knowledge and external market knowledge.\n\n\nFindings\nThe paper explains AI from an input\u2013processes\u2013output lens and explicates the six foundational building blocks of any AI system. It also discussed how the combination of the building blocks transforms data into information and knowledge.\n\n\nPractical implications\nAimed at general marketing executives, rather than AI specialists, this paper explains the phenomenon artificial intelligence, how it works and its relevance for the knowledge-based marketing in B2B firms. The paper highlights illustrative use cases to show how AI can impact B2B marketing functions.\n\n\nOriginality\/value\nThe study conceptualizes the technological phenomenon artificial intelligence from a knowledge management perspective and contributes to the literature on knowledge management in the era of big data. It addresses calls for more scholarly research on AI and B2B marketing.\n","3631":"Algorithmic decision-making, enabled by machine learning, is ubiquitous, powerful, often opaque, sometimes invisible, and, most importantly, consequential. Machine learning is embedded in many information tools and systems, central to numerous research methods, and pervasive in the applications of everyday life. Safiya Noble emphasizes the critical nature of artificial intelligence (AI) by observing that it will become \u201ca major human rights issue in the twenty-first century.\u201d1 As with nearly all aspects of contemporary life, AI is having a profound influence on research libraries, scholarly communication, and key functions of the academy.","3632":null,"3633":"The Novel Materials Discovery (NOMAD) Laboratory is a user-driven platform for sharing and exploiting computational materials science data. It accounts for the various aspects of data being a crucial raw material and most relevant to accelerate materials research and engineering. NOMAD, with the NOMAD Repository, and its code-independent and normalized form, the NOMAD Archive, comprises the worldwide largest data collection of this field. Based on its findable accessible, interoperable, reusable data infrastructure, various services are offered, comprising advanced visualization, the NOMAD Encyclopedia, and artificial-intelligence tools. The latter are realized in the NOMAD Analytics Toolkit. Prerequisite for all this is the NOMAD metadata, a unique and thorough description of the data, that are produced by all important computer codes of the community. Uploaded data are tagged by a persistent identifier, and users can also request a digital object identifier to make data citable. Developments and advancements of parsers and metadata are organized jointly with users and code developers. In this work, we review the NOMAD concept and implementation, highlight its orthogonality to and synergistic interplay with other data collections, and provide an outlook regarding ongoing and future developments.","3634":"Artificial intelligence (AI) using deep-learning (DL) has emerged as a breakthrough computer technology. By the era of big data, the accumulation of an enormous number of digital images and medical records drove the need for the utilization of AI to efficiently deal with these data, which have become fundamental resources for a machine to learn by itself. Among several DL models, the convolutional neural network showed outstanding performance in image analysis. In the field of gastroenterology, physicians handle large amounts of clinical data and various kinds of image devices such as endoscopy and ultrasound. AI has been applied in gastroenterology in terms of diagnosis, prognosis, and image analysis. However, potential inherent selection bias cannot be excluded in the form of retrospective study. Because overfitting and spectrum bias (class imbalance) have the possibility of overestimating the accuracy, external validation using unused datasets for model development, collected in a way that minimizes the spectrum bias, is mandatory. For robust verification, prospective studies with adequate inclusion\/exclusion criteria, which represent the target populations, are needed. DL has its own lack of interpretability. Because interpretability is important in that it can provide safety measures, help to detect bias, and create social acceptance, further investigations should be performed.","3635":"In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.","3636":"Artificial intelligence (AI) and machine learning (ML) techniques are revolutionizing several industrial and research fields like computer vision, autonomous driving, natural language processing, and speech recognition. These novel tools are already having a major impact in radiology, diagnostics, and many other fields in which the availability of automated solution may benefit the accuracy and repeatability of the execution of critical tasks. In this narrative review, we first present a brief description of the various techniques that are being developed nowadays, with special focus on those used in spine research. Then, we describe the applications of AI and ML to problems related to the spine which have been published so far, including the localization of vertebrae and discs in radiological images, image segmentation, computer\u2010aided diagnosis, prediction of clinical outcomes and complications, decision support systems, content\u2010based image retrieval, biomechanics, and motion analysis. Finally, we briefly discuss major ethical issues related to the use of AI in healthcare, namely, accountability, risk of biased decisions as well as data privacy and security, which are nowadays being debated in the scientific community and by regulatory agencies.","3637":"Background Applications of artificial intelligence (AI) in health care have garnered much attention in recent years, but the implementation issues posed by AI have not been substantially addressed. Objective In this paper, we have focused on machine learning (ML) as a form of AI and have provided a framework for thinking about use cases of ML in health care. We have structured our discussion of challenges in the implementation of ML in comparison with other technologies using the framework of Nonadoption, Abandonment, and Challenges to the Scale-Up, Spread, and Sustainability of Health and Care Technologies (NASSS). Methods After providing an overview of AI technology, we describe use cases of ML as falling into the categories of decision support and automation. We suggest these use cases apply to clinical, operational, and epidemiological tasks and that the primary function of ML in health care in the near term will be decision support. We then outline unique implementation issues posed by ML initiatives in the categories addressed by the NASSS framework, specifically including meaningful decision support, explainability, privacy, consent, algorithmic bias, security, scalability, the role of corporations, and the changing nature of health care work. Results Ultimately, we suggest that the future of ML in health care remains positive but uncertain, as support from patients, the public, and a wide range of health care stakeholders is necessary to enable its meaningful implementation. Conclusions If the implementation science community is to facilitate the adoption of ML in ways that stand to generate widespread benefits, the issues raised in this paper will require substantial attention in the coming years.","3638":"Recognizing the rapid advances in sales digitization and artificial intelligence technologies, we develop concepts, priorities, and questions to help guide future research and practice in the field of personal selling and sales management. Our analysis reveals that the influence of sales digitalization technologies, which include digitization and artificial intelligence, is likely to be more significant and more far reaching than previous sales technologies. To organize our analysis of this influence, we discuss the opportunities and threats that sales digitalization technologies pose for (a) the sales profession in terms of its contribution to creating value for customers, organizations, and society and (b) sales professionals, in terms of both employees in organizations and individuals as self, seeking growth, fulfillment, and status in the functions they serve and roles they live. We summarize our discussion by detailing specific research priorities and questions that warrant further study and development by researchers and practitioners alike.","3639":"SARS-COV-2 has roused the scientific community with a call to action to combat the growing pandemic. At the time of this writing, there are as yet no novel antiviral agents or approved vaccines available for deployment as a frontline defense. Understanding the pathobiology of COVID-19 could aid scientists in their discovery of potent antivirals by elucidating unexplored viral pathways. One method for accomplishing this is the leveraging of computational methods to discover new candidate drugs and vaccines in silico. In the last decade, machine learning-based models, trained on specific biomolecules, have offered inexpensive and rapid implementation methods for the discovery of effective viral therapies. Given a target biomolecule, these models are capable of predicting inhibitor candidates in a structural-based manner. If enough data are presented to a model, it can aid the search for a drug or vaccine candidate by identifying patterns within the data. In this review, we focus on the recent advances of COVID-19 drug and vaccine development using artificial intelligence and the potential of intelligent training for the discovery of COVID-19 therapeutics. To facilitate applications of deep learning for SARS-COV-2, we highlight multiple molecular targets of COVID-19, inhibition of which may increase patient survival. Moreover, we present CoronaDB-AI, a dataset of compounds, peptides, and epitopes discovered either in silico or in vitro that can be potentially used for training models in order to extract COVID-19 treatment. The information and datasets provided in this review can be used to train deep learning-based models and accelerate the discovery of effective viral therapies.","3640":"Recent progress in Artificial Intelligence (AI) techniques, the large-scale availability of high-quality data, as well as advances in both hardware and software to efficiently process these data, a...","3641":"We praise Jordan for bringing much needed clarity about the current status of Artificial Intelligence (AI)\u2014what it currently is and what it is not\u2014as well as explaining the current challenges lying ahead and outlining what is missing and remains to be done. Jordan makes several claims supported by a list of talking points that we hope will reach a wide audience; ideally, that audience will include academic, university, and governmental leaders, at a time where significant resources are being allocated to AI for research and education.","3642":"Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI \u2018black-box\u2019 algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.","3643":"Artificial intelligence (AI) is a broad transdisciplinary field with roots in logic, statistics, cognitive psychology, decision theory, neuroscience, linguistics, cybernetics, and computer engineering. The modern field of AI began at a small summer workshop at Dartmouth College in 1956. Since then, AI applications made possible by machine learning (ML), an AI subdiscipline, include Internet searches, e-commerce sites, goods and services recommender systems, image and speech recognition, sensor technologies, robotic devices, and cognitive decision support systems (DSSs). As more applications are integrated into everyday life, AI is predicted to have a globally transformative influence on economic and social structures similar to the effect that other general-purpose technologies, such as steam engines, railroads, electricity, electronics, and the Internet, have had. Novel AI applications in the workplace of the future raise important issues for occupational safety and health. This commentary reviews the origins of AI, use of ML methods, and emerging AI applications embedded in physical objects like sensor technologies, robotic devices, or operationalized in intelligent DSSs. Selected implications on the future of work arising from the use of AI applications, including job displacement from automation and management of human-machine interactions, are also reviewed. Engaging in strategic foresight about AI workplace applications will shift occupational research and practice from a reactive posture to a proactive one. Understanding the possibilities and challenges of AI for the future of work will help mitigate\u00a0the unfavorable effects of AI on worker safety, health, and well-being.","3644":null,"3645":"Objective: The aim of this review was to summarize major topics in artificial intelligence (AI), including their applications and limitations in surgery. This paper reviews the key capabilities of AI to help surgeons understand and critically evaluate new AI applications and to contribute to new developments. Summary Background Data: AI is composed of various subfields that each provide potential solutions to clinical problems. Each of the core subfields of AI reviewed in this piece has also been used in other industries such as the autonomous car, social networks, and deep learning computers. Methods: A review of AI papers across computer science, statistics, and medical sources was conducted to identify key concepts and techniques within AI that are driving innovation across industries, including surgery. Limitations and challenges of working with AI were also reviewed. Results: Four main subfields of AI were defined: (1) machine learning, (2) artificial neural networks, (3) natural language processing, and (4) computer vision. Their current and future applications to surgical practice were introduced, including big data analytics and clinical decision support systems. The implications of AI for surgeons and the role of surgeons in advancing the technology to optimize clinical effectiveness were discussed. Conclusions: Surgeons are well positioned to help integrate AI into modern practice. Surgeons should partner with data scientists to capture data across phases of care and to provide clinical context, for AI has the potential to revolutionize the way surgery is taught and practiced with the promise of a future optimized for the highest quality patient care.","3646":"The use of artificial intelligence in medicine is currently an issue of great interest, especially with regard to the diagnostic or predictive analysis of medical images. Adoption of an artificial intelligence tool in clinical practice requires careful confirmation of its clinical utility. Herein, the authors explain key methodology points involved in a clinical evaluation of artificial intelligence technology for use in medicine, especially high-dimensional or overparameterized diagnostic or predictive models in which artificial deep neural networks are used, mainly from the standpoints of clinical epidemiology and biostatistics. First, statistical methods for assessing the discrimination and calibration performances of a diagnostic or predictive model are summarized. Next, the effects of disease manifestation spectrum and disease prevalence on the performance results are explained, followed by a discussion of the difference between evaluating the performance with use of internal and external datasets, the importance of using an adequate external dataset obtained from a well-defined clinical cohort to avoid overestimating the clinical performance as a result of overfitting in high-dimensional or overparameterized classification model and spectrum bias, and the essentials for achieving a more robust clinical evaluation. Finally, the authors review the role of clinical trials and observational outcome studies for ultimate clinical verification of diagnostic or predictive artificial intelligence tools through patient outcomes, beyond performance metrics, and how to design such studies. \u00a9 RSNA, 2018.","3647":null,"3648":null,"3649":null,"3650":"The ethical issues related to the possible future creation of machines with general intellectual capabilities far outstripping those of humans are quite distinct from any ethical problems arising in current automation and information systems. Such superintelligence would not be just another technological development; it would be the most important invention ever made, and would lead to explosive progress in all scientific and technological fields, as the superintelligence would conduct research with superhuman efficiency. To the extent that ethics is a cognitive pursuit, a superintelligence could also easily surpass humans in the quality of its moral thinking. However, it would be up to the designers of the superintelligence to specify its original motivations. Since the superintelligence may become unstoppably powerful because of its intellectual superiority and the technologies it could develop, it is crucial that it be provided with human-friendly motivations. This paper surveys some of the unique ethical issues in creating superintelligence, and discusses what motivations we ought to give a superintelligence, and introduces some cost-benefit considerations relating to whether the development of superintelligent machines ought to be accelerated or retarded.","3651":"This paper aims to move the debate forward regarding the potential for artificial intelligence (AI) and autonomous robotic surgery with a particular focus on ethics, regulation and legal aspects (such as civil law, international law, tort law, liability, medical malpractice, privacy and product\/device legislation, among other aspects).","3652":"Abstract This article systematically analyzes the problem of defining \u201cartificial intelligence.\u201d It starts by pointing out that a definition influences the path of the research, then establishes four criteria of a good working definition of a notion: being similar to its common usage, drawing a sharp boundary, leading to fruitful research, and as simple as possible. According to these criteria, the representative definitions in the field are analyzed. A new definition is proposed, according to it intelligence means \u201cadaptation with insufficient knowledge and resources.\u201d The implications of this definition are discussed, and it is compared with the other definitions. It is claimed that this definition sheds light on the solution of many existing problems and sets a sound foundation for the field.","3653":"The booming field of artificial intelligence (AI) is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade. Just because algorithms are based on code doesn9t mean experiments are easily replicated. Far from it. Unpublished codes and a sensitivity to training conditions have made it difficult for AI researchers to reproduce many key results. That is leading to a new conscientiousness about research methods and publication protocols. Last week, at a meeting of the Association for the Advancement of Artificial Intelligence in New Orleans, Louisiana, reproducibility was on the agenda, with some teams diagnosing the problem\u2014and one laying out tools to mitigate it.","3654":null,"3655":"With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.","3656":null,"3657":null,"3658":"Combining deep learning with brain-like innate structures may guide network models toward human-like learning When the mathematician Alan Turing posed the question \u201cCan machines think?\u201d in the first line of his seminal 1950 paper that ushered in the quest for artificial intelligence (AI) (1), the only known systems carrying out complex computations were biological nervous systems. It is not surprising, therefore, that scientists in the nascent field of AI turned to brain circuits as a source for guidance. One path that was taken since the early attempts to perform intelligent computation by brain-like circuits (2), and which led recently to remarkable successes, can be described as a highly reductionist approach to model cortical circuitry. In its basic current form, known as a \u201cdeep network\u201d (or deep net) architecture, this brain-inspired model is built from successive layers of neuron-like elements, connected by adjustable weights, called \u201csynapses\u201d after their biological counterparts (3). The application of deep nets and related methods to AI systems has been transformative. They proved superior to previously known methods in central areas of AI research, including computer vision, speech recognition and production, and playing complex games. Practical applications are already in broad use, in areas such as computer vision and speech and text translation, and large-scale efforts are under way in many other areas. Here, I discuss how additional aspects of brain circuitry could supply cues for guiding network models toward broader aspects of cognition and general AI.","3659":"We summarize a framework for the study of the implications of automation and AI on the demand for labor, wages, and employment. Our task-based framework emphasizes the displacement effect that automation creates as machines and AI replace labor in tasks that it used to perform. This displacement effect tends to reduce the demand for labor and wages. But it is counteracted by a productivity effect, resulting from the cost savings generated by automation, which increase the demand for labor in non-automated tasks. The productivity effect is complemented by additional capital accumulation and the deepening of automation (improvements of existing machinery), both of which further increase the demand for labor. These countervailing effects are incomplete. Even when they are strong, automation in- creases output per worker more than wages and reduce the share of labor in national income. The more powerful countervailing force against automation is the creation of new labor-intensive tasks, which reinstates labor in new activities and tends to in- crease the labor share to counterbalance the impact of automation. Our framework also highlights the constraints and imperfections that slow down the adjustment of the economy and the labor market to automation and weaken the resulting produc- tivity gains from this transformation: a mismatch between the skill requirements of new technologies, and the possibility that automation is being introduced at an excessive rate, possibly at the expense of other productivity-enhancing technologies.","3660":"Recently, the advancement in communications, intelligent transportation systems, and computational systems has opened up new opportunities for intelligent traffic safety, comfort, and efficiency solutions. Artificial intelligence (AI) has been widely used to optimize traditional data-driven approaches in different areas of the scientific research. Vehicle-to-everything (V2X) system together with AI can acquire the information from diverse sources, can expand the driver\u2019s perception, and can predict to avoid potential accidents, thus enhancing the comfort, safety, and efficiency of the driving. This paper presents a comprehensive survey of the research works that have utilized AI to address various research challenges in V2X systems. We have summarized the contribution of these research works and categorized them according to the application domains. Finally, we present open problems and research challenges that need to be addressed for realizing the full potential of AI to advance V2X systems.","3661":"Artificial Intelligence (AI) is evolving rapidly in healthcare, and various AI applications have been developed to solve some of the most pressing problems that health organizations currently face. It is crucial for health leaders to understand the state of AI technologies and the ways that such technologies can be used to improve the efficiency, safety, and access of health services, achieving value-based care. This article provides a guide to understand the fundamentals of AI technologies (ie, machine learning, natural language processing, and AI voice assistants) as well as their proper use in healthcare. It also provides practical recommendations to help decision-makers develop an AI strategy that can support their digital healthcare transformation.","3662":"Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.","3663":"This essay highlights the increasing use of artificial intelligence (AI) in governance and society and explores the relationship between AI, discretion, and bureaucracy. AI is an advanced information communication technology tool (ICT) that changes both the nature of human discretion within a bureaucracy and the structure of bureaucracies. To better understand this relationship, AI, discretion, and bureaucracy are explored in some detail. It is argued that discretion and decision-making are strongly influenced by intelligence, and that improvements in intelligence, such as those that can be found within the field of AI, can help improve the overall quality of administration. Furthermore, the characteristics, strengths, and weaknesses of both human discretion and AI are explored. Once these characteristics are laid out, a further exploration of the role AI may play in bureaucracies and bureaucratic structure is presented, followed by a specific focus on systems-level bureaucracies. In addition, it is argued that task distribution and task characteristics play a large role, along with the organizational and legal context, in whether a task favors human discretion or the use of AI. Complexity and uncertainty are presented as the major defining characteristics for categorizing tasks. Finally, a discussion is provided about the important cautions and concerns of utilizing AI in governance, in particular, with respect to existential risk and administrative evil.","3664":"Artificial intelligence (AI) and nanotechnology are two fields that are instrumental in realizing the goal of precision medicine\u2014tailoring the best treatment for each cancer patient. Recent conversion between these two fields is enabling better patient data acquisition and improved design of nanomaterials for precision cancer medicine. Diagnostic nanomaterials are used to assemble a patient\u2010specific disease profile, which is then leveraged, through a set of therapeutic nanotechnologies, to improve the treatment outcome. However, high intratumor and interpatient heterogeneities make the rational design of diagnostic and therapeutic platforms, and analysis of their output, extremely difficult. Integration of AI approaches can bridge this gap, using pattern analysis and classification algorithms for improved diagnostic and therapeutic accuracy. Nanomedicine design also benefits from the application of AI, by optimizing material properties according to predicted interactions with the target drug, biological fluids, immune system, vasculature, and cell membranes, all affecting therapeutic efficacy. Here, fundamental concepts in AI are described and the contributions and promise of nanotechnology coupled with AI to the future of precision cancer medicine are reviewed.","3665":null,"3666":"This chapter proposes a cost-effective and scalable approach to obtain information on the current living standards and development in rural areas across India. The model utilizes a CNN to analyze satellite images of an area and predict its land type and level of development. A decision tree classifies a region as rural or urban based on the analysis. A summary describing the area is generated from inferences made on the recorded statistics. The CNN is able to predict the land and development distribution with an accuracy of 95.1%. The decision tree predicts rural areas with a precision of 99.6% and recall of 88.9%. The statistics obtained for a dataset of more than 1000 villages in India are cross-validated against the Census of India 2011 data. The proposed technique is in contrast to traditional door-to-door surveying methods as the information retrieved is relevant and obtained without human intervention. Hence, it can aid efforts in tracking poverty at a finer level and provide insight on improving the economic livelihood in rural areas.","3667":"Artificial intelligence (AI) is a technical term often referring to artifacts used to detect contexts for human actions, or sometimes also for machines able to effect actions in response to detected contexts. Our capacity to build such artifacts has been increasing, and with it the impact they have on our society. This does not alter the fundamental roots or motivations of law, regulation, or diplomacy, which rest on persuading humans to behave in a way that provides sustainable security for humans. It does however alter nearly every other aspect of human social behaviour, including making accountability and responsibility potentially easier to trace. This chapter reviews the nature and implications of AI with particular attention to how they impinge on possible applications to and of law.","3668":null,"3669":"Objective: Accurate diagnosis and prognosis are essential in lung cancer treatment selection and planning. With the rapid advance of medical imaging technology, whole slide imaging (WSI) in pathology is becoming a routine clinical procedure. An interplay of needs and challenges exists for computer-aided diagnosis based on accurate and efficient analysis of pathology images. Recently, artificial intelligence, especially deep learning, has shown great potential in pathology image analysis tasks such as tumor region identification, prognosis prediction, tumor microenvironment characterization, and metastasis detection. Materials and Methods: In this review, we aim to provide an overview of current and potential applications for AI methods in pathology image analysis, with an emphasis on lung cancer. Results: We outlined the current challenges and opportunities in lung cancer pathology image analysis, discussed the recent deep learning developments that could potentially impact digital pathology in lung cancer, and summarized the existing applications of deep learning algorithms in lung cancer diagnosis and prognosis. Discussion and Conclusion: With the advance of technology, digital pathology could have great potential impacts in lung cancer patient care. We point out some promising future directions for lung cancer pathology image analysis, including multi-task learning, transfer learning, and model interpretation.","3670":"Health care applications of artificial intelligence (AI) have recently emerged. Artificial intelligence approaches, such as deep learning, rely on vast amounts of data and complex model structures ...","3671":null,"3672":null,"3673":null,"3674":"Due to the exponential growth of computational algorithms, artificial intelligence (AI) methods are poised to improve the precision of diagnostic and therapeutic methods in medicine. The field of radiomics in neuro-oncology has been and will likely continue to be at the forefront of this revolution. A variety of AI methods applied to conventional and advanced neuro-oncology MRI data can already delineate infiltrating margins of diffuse gliomas, differentiate pseudoprogression from true progression, and predict recurrence and survival better than methods used in daily clinical practice. Radiogenomics will also advance our understanding of cancer biology, allowing noninvasive sampling of the molecular environment with high spatial resolution and providing a systems-level understanding of underlying heterogeneous cellular and molecular processes. By providing in vivo markers of spatial and molecular heterogeneity, these AI-based radiomic and radiogenomic tools have the potential to stratify patients into more precise initial diagnostic and therapeutic pathways and enable better dynamic treatment monitoring in this era of personalized medicine. Although substantial challenges remain, radiologic practice is set to change considerably as AI technology is further developed and validated for clinical use.","3675":"A recurrent concern about machine learning algorithms is that they operate as \u201cblack boxes,\u201d making it difficult to identify how and why the algorithms reach particular decisions, recommendations, or predictions. Yet judges will confront machine learning algorithms with increasing frequency, including in criminal, administrative, and tort cases. This Essay argues that judges should demand explanations for these algorithmic outcomes. One way to address the \u201cblack box\u201d problem is to design systems that explain how the algorithms reach their conclusions or predictions. If and as judges demand these explanations, they will play a seminal role in shaping the nature and form of \u201cexplainable artificial intelligence\u201d (or \u201cxAI\u201d). Using the tools of the common law, courts can develop what xAI should mean in different legal contexts. \n \nThere are advantages to having courts to play this role: Judicial reasoning that builds from the bottom up, using case-by-case consideration of the facts to produce nuanced decisions, is a pragmatic way to develop rules for xAI. Further, courts are likely to stimulate the production of different forms of xAI that are responsive to distinct legal settings and audiences. More generally, we should favor the greater involvement of public actors in shaping xAI, which to date has largely been left in private hands.","3676":"The application of Artificial Intelligence (AI) has been evident in the agricultural sector recently. The sector faces numerous challenges in order to maximize its yield including improper soil treatment, disease and pest infestation, big data requirements, low output, and knowledge gap between farmers and technology. The main concept of AI in agriculture is its flexibility, high performance, accuracy, and cost-effectiveness. This paper presents a review of the applications of AI in soil management, crop management, weed management and disease management. A special focus is laid on the strength and limitations of the application and the way in utilizing expert systems for higher productivity.","3677":"Artificial intelligence (AI), particularly deep learning algorithms, is gaining extensive attention for its excellent performance in image-recognition tasks. They can automatically make a quantitative assessment of complex medical image characteristics and achieve an increased accuracy for diagnosis with higher efficiency. AI is widely used and getting increasingly popular in the medical imaging of the liver, including radiology, ultrasound, and nuclear medicine. AI can assist physicians to make more accurate and reproductive imaging diagnosis and also reduce the physicians\u2019 workload. This article illustrates basic technical knowledge about AI, including traditional machine learning and deep learning algorithms, especially convolutional neural networks, and their clinical application in the medical imaging of liver diseases, such as detecting and evaluating focal liver lesions, facilitating treatment, and predicting liver treatment response. We conclude that machine-assisted medical services will be a promising solution for future liver medical care. Lastly, we discuss the challenges and future directions of clinical application of deep learning techniques.","3678":null,"3679":"A remarkable time of human promise has been ushered in by the convergence of the ever-expanding availability of big data, the soaring speed and stretch of cloud computing platforms, and the advancement of increasingly sophisticated machine learning algorithms. Innovations in AI are already leaving a mark on government by improving the provision of essential social goods and services from healthcare, education, and transportation to food supply, energy, and environmental management. These bounties are likely just the start. The prospect that progress in AI will help government to confront some of its most urgent challenges is exciting, but legitimate worries abound. As with any new and rapidly evolving technology, a steep learning curve means that mistakes and miscalculations will be made and that both unanticipated and harmful impacts will occur. \nThis guide, written for department and delivery leads in the UK public sector and adopted by the British Government in its publication, 'Using AI in the Public Sector,' identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. It stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems. It also highlights the need for algorithmically supported outcomes to be interpretable by their users and made understandable to decision subjects in clear, non-technical, and accessible ways. Finally, it builds out a vision of human-centred and context-sensitive implementation that gives a central role to communication, evidence-based reasoning, situational awareness, and moral justifiability.","3680":null,"3681":null,"3682":"OBJECTIVE\nThe purpose of this article is to highlight best practices for writing and reviewing articles on artificial intelligence for medical image analysis.\n\n\nCONCLUSION\nArtificial intelligence is in the early phases of application to medical imaging, and patient safety demands a commitment to sound methods and avoidance of rhetorical and overly optimistic claims. Adherence to best practices should elevate the quality of articles submitted to and published by clinical journals.","3683":"Purpose\nTo evaluate the use of artificial intelligence (AI) to shorten digital breast tomosynthesis (DBT) reading time while maintaining or improving accuracy.\n\n\nMaterials and Methods\nA deep learning AI system was developed to identify suspicious soft-tissue and calcified lesions in DBT images. A reader study compared the performance of 24 radiologists (13 of whom were breast subspecialists) reading 260 DBT examinations (including 65 cancer cases) both with and without AI. Readings occurred in two sessions separated by at least 4 weeks. Area under the receiver operating characteristic curve (AUC), reading time, sensitivity, specificity, and recall rate were evaluated with statistical methods for multireader, multicase studies.\n\n\nResults\nRadiologist performance for the detection of malignant lesions, measured by mean AUC, increased 0.057 with the use of AI (95% confidence interval [CI]: 0.028, 0.087; P < .01), from 0.795 without AI to 0.852 with AI. Reading time decreased 52.7% (95% CI: 41.8%, 61.5%; P < .01), from 64.1 seconds without to 30.4 seconds with AI. Sensitivity increased from 77.0% without AI to 85.0% with AI (8.0%; 95% CI: 2.6%, 13.4%; P < .01), specificity increased from 62.7% without to 69.6% with AI (6.9%; 95% CI: 3.0%, 10.8%; noninferiority P < .01), and recall rate for noncancers decreased from 38.0% without to 30.9% with AI (7.2%; 95% CI: 3.1%, 11.2%; noninferiority P < .01).\n\n\nConclusion\nThe concurrent use of an accurate DBT AI system was found to improve cancer detection efficacy in a reader study that demonstrated increases in AUC, sensitivity, and specificity and a reduction in recall rate and reading time.\u00a9 RSNA, 2019See also the commentary by Hsu and Hoyt in this issue.","3684":"\"Prepare Yourselves, Robots Will Soon Replace Doctors in Healthcare,\" screamed the headline in a 2017 Forbes magazine article. Media coverage like that makes it easy to see why artificial intelligence (AI) sounds like scary science fiction to some physicians.","3685":"We developed a novel early childhood artificial intelligence (AI) platform, PopBots, where preschool children train and interact with social robots to learn three AI concepts: knowledge-based systems, supervised machine learning, and generative AI. We evaluated how much children learned by using AI assessments we developed for each activity. The median score on the cumulative assessment was 70% and children understood knowledge-based systems the best. Then, we analyzed the impact of the activities on children's perceptions of robots. Younger children came to see robots as toys that were smarter than them, but their older counterparts saw them more as people that were not as smart as them. Children who performed worse on the AI assessments believed that robots were like toys that were not as smart as them, however children who did better on the assessments saw robots as people who were smarter than them. We believe early AI education can empower children to understand the AI devices that are increasingly in their lives.","3686":null,"3687":null,"3688":"Artificial intelligence has experienced major developments in recent years and represents an emerging technology that will revolutionize the ways in which human beings live. This technology is already being introduced in the field of higher education, although many teachers are unaware of its scope and, above all, of what it consists of. Therefore, the purpose of this paper was to analyse the scientific production on artificial intelligence in higher education indexed in Web of Science and Scopus databases during 2007\u20132017. A bespoke methodology of bibliometric studies was used in the most relevant databases in social science. The sample was composed of 132 papers in total. From the results obtained, it was observed that there is a worldwide interest in the topic and that the literature on this subject is just at an incipient stage. We conclude that, although artificial intelligence is a reality, the scientific production about its application in higher education has not been consolidated.","3689":"Key Points Question How accurate is an artificial intelligence\u2013based melanoma detection algorithm, which analyzes dermoscopic images taken by smartphone and digital single-lens reflex cameras, compared with clinical assessment and histopathological diagnosis? Findings In this diagnostic study, 1550 images of suspicious and benign skin lesions were analyzed by an artificial intelligence algorithm. When compared with histopathological diagnosis, the algorithm achieved an area under the receiver operator characteristic curve of 91.8%. At 100% sensitivity, the algorithm achieved a specificity of 64.8%, while clinicians achieved a specificity of 69.9%. Meaning As the burden of skin cancer increases, artificial intelligence technology could play a role in identifying lesions with a high likelihood of melanoma.","3690":"Treatment planning is an essential step of the radiotherapy workflow. It has become more sophisticated over the past couple of decades with the help of computer science, enabling planners to design highly complex radiotherapy plans to minimize the normal tissue damage while persevering sufficient tumor control. As a result, treatment planning has become more labor intensive, requiring hours or even days of planner effort to optimize an individual patient case in a trial-and-error fashion. More recently, artificial intelligence has been utilized to automate and improve various aspects of medical science. For radiotherapy treatment planning, many algorithms have been developed to better support planners. These algorithms focus on automating the planning process and\/or optimizing dosimetric trade-offs, and they have already made great impact on improving treatment planning efficiency and plan quality consistency. In this review, the smart planning tools in current clinical use are summarized in 3 main categories: automated rule implementation and reasoning, modeling of prior knowledge in clinical practice, and multicriteria optimization. Novel artificial intelligence\u2013based treatment planning applications, such as deep learning\u2013based algorithms and emerging research directions, are also reviewed. Finally, the challenges of artificial intelligence\u2013based treatment planning are discussed for future works.","3691":"The article specifies the organizational capabilities of application of artificial intelligence technologies in the model of sustainable development of the organization. Also, the article provided the theoretical and methodological background of the organizational changes and development, as well as determined the possibilities of application of artificial intelligence technologies in the functionality of the organization. It was proposed to use the methodological approach to application of neural networks in maintaining of the intelligent management of organizational development. There was developed the combinatorial model of artificial intelligence for decision making about the organizational development.","3692":null,"3693":"Abstract Artificial intelligence (AI) is a growing phenomenon, and will soon facilitate wide-scale changes in many professions, including medical education. In order for medical educators to be properly prepared for AI, they will need to have at least a fundamental knowledge of AI in relation to learning and teaching, and the extent to which it will impact on medical education. This Guide begins by introducing the broad concepts of AI by using fairly well-known examples to illustrate AI\u2019s implications within the context of education. It then considers the impact of AI on medicine and the implications of this impact for educators trying to educate future doctors. Drawing on these strands, it then identifies AI\u2019s direct impact on the methodology and content of medical education, in an attempt to prepare medical educators for the changing demands and opportunities that are about to face them because of AI.","3694":null,"3695":null,"3696":null,"3697":"In recent years, artificial intelligence (AI) has become an emerging trend in different fields: science, business, medicine, automotive and education. AI has also reached marketing. The aim of the paper is to research how deeply AI is applied in marketing and what implications there are for marketing practitioners. Authors stated two research questions - which areas of AI are used in marketing and what implications AI delivers for marketing managers. To answer those questions, the authors conducted research on secondary data with AI examples used for marketing purpose. The analysis of gathered examples shows that AI is widely introduced into the marketing field, though the applications are at the operational level. This may be the effect of careful implementation of the new technology, still at the level of experimenting with it. The uncertainty of the outcome of AI implementation may affect the caution in putting these innovations into practice as well. Gathered examples proved that AI influences all aspects of marketing mix impacting both consumer value delivery as well as the marketing organization and management. The paper delivers implications for business, especially ideas about implementing AI into marketing, designing innovations and the ideas on how to incorporate new skills into marketing team required by the new technology.","3698":"An argument that?despite dramatic advances in the field?artificial intelligence is nowhere near developing systems that are genuinely intelligent.In this provocative book, Brian Cantwell Smith argues that artificial intelligence is nowhere near developing systems that are genuinely intelligent. Second wave AI, machine learning, even visions of third-wave AI: none will lead to human-level intelligence and judgment, which have been honed over millennia. Recent advances in AI may be of epochal significance, but human intelligence is of a different order than even the most powerful calculative ability enabled by new computational capacities. Smith calls this AI ability ?reckoning,? and argues that it does not lead to full human judgment?dispassionate, deliberative thought grounded in ethical commitment and responsible action.Taking judgment as the ultimate goal of intelligence, Smith examines the history of AI from its first-wave origins (?good old-fashioned AI,? or GOFAI) to such celebrated second-wave approaches as machine learning, paying particular attention to recent advances that have led to excitement, anxiety, and debate. He considers each AI technology's underlying assumptions, the conceptions of intelligence targeted at each stage, and the successes achieved so far. Smith unpacks the notion of intelligence itself?what sort humans have, and what sort AI aims at. Smith worries that, impressed by AI's reckoning prowess, we will shift our expectations of human intelligence. What we should do, he argues, is learn to use AI for the reckoning tasks at which it excels while we strengthen our commitment to judgment, ethics, and the world.","3699":null,"3700":"Artificial intelligence (AI) is becoming a part of our daily lives at a fast pace, offering myriad benefits for society. At the same time, there is concern about the unpredictability and uncontrollability of AI. In response, legislators and scholars call for more transparency and explainability of AI. This article considers what it would mean to require transparency of AI. It advocates looking beyond the opaque concept of AI, focusing on the concrete risks and biases of its underlying technology: machine-learning algorithms. The article discusses the biases that algorithms may produce through the input data, the testing of the algorithm and the decision model. Any transparency requirement for algorithms should result in explanations of these biases that are both understandable for the prospective recipients, and technically feasible for producers. Before asking how much transparency the law should require from algorithms, we should therefore consider if the explanation that programmers could offer is useful in specific legal contexts.","3701":null,"3702":null,"3703":"Histopathology has undergone major changes firstly with the introduction of Immunohistochemistry, and latterly with Genomic Medicine. We\u00a0argue that a third revolution is underway: Artificial Intelligence (AI). Coming on the back of Digital Pathology (DP), the introduction of AI has the potential to both challenge traditional practice and provide a totally new realm for pathology diagnostics. Hereby we stress the importance of certified pathologists having learned from the experience of previous revolutions and be willing to accept such disruptive technologies, ready to innovate and actively engage in the creation, application and validation of technologies and oversee the safe introduction of AI into diagnostic practice. This article is protected by copyright. All rights reserved.","3704":"Commercial applications of artificial intelligence and machine learning have made remarkable progress recently, particularly in areas such as image recognition, natural speech processing, language translation, textual analysis, and self-learning. Progress had historically languished in these areas, such that these skills had come to seem ineffably bound to intelligence. However, these commercial advances have performed best at single-task applications in which imperfect outputs and occasional frank errors can be tolerated.The practice of anesthesiology is different. It embodies a requirement for high reliability, and a pressured cycle of interpretation, physical action, and response rather than any single cognitive act. This review covers the basics of what is meant by artificial intelligence and machine learning for the practicing anesthesiologist, describing how decision-making behaviors can emerge from simple equations. Relevant clinical questions are introduced to illustrate how machine learning might help solve them-perhaps bringing anesthesiology into an era of machine-assisted discovery.","3705":null,"3706":"Computer code based on continual problem re-solving beats human professional poker players at a two-player variant of poker. Artificial intelligence masters poker Computers can beat humans at games as complex as chess or go. In these and similar games, both players have access to the same information, as displayed on the board. Although computers have the ultimate poker face, it has been tricky to teach them to be good at poker, where players cannot see their opponents' cards. Morav\u010d\u00edk et al. built a code dubbed DeepStack that managed to beat professional poker players at a two-player poker variant called heads-up no-limit Texas hold'em. Instead of devising its strategy beforehand, DeepStack recalculated it at each step, taking into account the current state of the game. The principles behind DeepStack may enable advances in solving real-world problems that involve information asymmetry. Science, this issue p. 508 Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold\u2019em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.","3707":null,"3708":null,"3709":"Artificial intelligence (AI) has experienced rapid growth over the past few years, moving from the experimental to the implementation phase in various fields, including medicine. Advances in learning algorithms and theories, the availability of large datasets and improvements in computing power have contributed to breakthroughs in current AI applications. Machine learning (ML), a subset of AI, allows computers to detect patterns from large complex datasets automatically and uses these patterns to make predictions. AI is proving to be increasingly applicable to healthcare, and multiple machine learning techniques have been used to improve the performance of assisted reproductive technology (ART). Despite various challenges, the integration of AI and reproductive medicine is bound to give an essential direction to medical development in the future. In this review, we discuss the basic aspects of AI and machine learning, and we address the applications, potential limitations and challenges of AI. We also highlight the prospects and future directions in the context of reproductive medicine.","3710":null,"3711":"Much has been written recently about artificial intelligence (AI) and law. But what is AI, and what is its relation to the practice and administration of law? This article addresses those questions by providing a high-level overview of AI and its use within law. The discussion aims to be nuanced but also understandable to those without a technical background. To that end, I first discuss AI generally. I then turn to AI and how it is being used by lawyers in the practice of law, people and companies who are governed by the law, and government officials who administer the law. A key motivation in writing this article is to provide a realistic, demystified view of AI that is rooted in the actual capabilities of the technology. This is meant to contrast with discussions about AI and law that are decidedly futurist in nature. That body of work speculates about the effects of AI developments that do not currently exist and which may, or may not, ever come about. Although those futurist conversations have their place, it is important to acknowledge that they involve significant, sometimes unsupported, assumptions about where the technology is headed. That speculative discussion often distracts from the important, but perhaps less exotic, law and policy issues actually raised by AI technology today.","3712":"Artificial intelligence is playing an increasingly important role in many fields of medicine, assisting physicians in most steps of patient management. In nephrology, artificial intelligence can already be used to improve clinical care, hemodialysis prescriptions, and follow-up of transplant recipients. However, many nephrologists are still unfamiliar with the basic principles of medical artificial intelligence. This review seeks to provide an overview of medical artificial intelligence relevant to the practicing nephrologist, in all fields of nephrology. We define the core concepts of artificial intelligence and machine learning and cover the basics of the functioning of neural networks and deep learning. We also discuss the most recent clinical applications of artificial intelligence in nephrology and medicine; as an example, we describe how artificial intelligence can predict the occurrence of progressive immunoglobulin A nephropathy. Finally, we consider the future of artificial intelligence in clinical nephrology and its impact on medical practice, and conclude with a discussion of the ethical issues that the use of artificial intelligence raises in terms of clinical decision making, physician-patient relationship, patient privacy, and data collection.","3713":"The advancements in the technologies, the revolution in the business procedures and the entailment to modify the operation in the warehousing as the result of the accumulating orders along with the complications involved in it, and the shortage in the management skills has paved way for the emergence of the smart ware housing. More over as the warehousing takes a vital role in the supply chain and prevails as the key feature in the logistics, smart ware housing is very much necessitated to enhance the organization management and success. The application of the artificial intelligence in the warehousing operations enhances the potentials of the warehousing functioning in the logistics, management and the co-ordination. The application of the artificial intelligence in the warehousing to make it a smart environment for the automated logistics is proposed in the\npaper. The paper concentrates on the automated storage and the retrieval using the internet of things, artificial intelligence and the cloud computing to have an any time access of the stock available in the warehouse.","3714":"Despite the great media attention for artificial intelligence (AI), for many health care professionals the term and the functioning of AI remain a \u201cblack box,\u201d leading to exaggerated expectations on the one hand and unfounded fears on the other. In this review, we provide a conceptual classification and a brief summary of the technical fundamentals of AI. Possible applications are discussed on the basis of a typical work flow in medical imaging, grouped by planning, scanning, interpretation, and reporting. The main limitations of current AI techniques, such as issues with interpretability or the need for large amounts of annotated data, are briefly addressed. Finally, we highlight the possible impact of AI on the nuclear medicine profession, the associated challenges and, last but not least, the opportunities.","3715":null,"3716":"We live in an age of paradox. Systems using artificial intelligence match or surpass human level performance in more and more domains, leveraging rapid advances in other technologies and driving soaring stock prices. Yet measured productivity growth has declined by half over the past decade, and real income has stagnated since the late 1990s for a majority of Americans. We describe four potential explanations for this clash of expectations and statistics: false hopes, mismeasurement, redistribution, and implementation lags. While a case can be made for each, we argue that lags have likely been the biggest contributor to the paradox. The most impressive capabilities of AI, particularly those based on machine learning, have not yet diffused widely. More importantly, like other general purpose technologies, their full effects won\u2019t be realized until waves of complementary innovations are developed and implemented. The required adjustment costs, organizational changes, and new skills can be modeled as a kind of intangible capital. A portion of the value of this intangible capital is already reflected in the market value of firms. However, going forward, national statistics could fail to measure the full benefits of the new technologies and some may even have the wrong sign.","3717":"Artificial intelligence (AI) is gaining extensive attention for its excellent performance in image-recognition tasks and increasingly applied in breast ultrasound. AI can conduct a quantitative assessment by recognizing imaging information automatically and make more accurate and reproductive imaging diagnosis. Breast cancer is the most commonly diagnosed cancer in women, severely threatening women\u2019s health, the early screening of which is closely related to the prognosis of patients. Therefore, utilization of AI in breast cancer screening and detection is of great significance, which can not only save time for radiologists, but also make up for experience and skill deficiency on some beginners. This article illustrates the basic technical knowledge regarding AI in breast ultrasound, including early machine learning algorithms and deep learning algorithms, and their application in the differential diagnosis of benign and malignant masses. At last, we talk about the future perspectives of AI in breast ultrasound.","3718":null,"3719":null,"3720":null,"3721":null,"3722":"The key to the explosion of the Internet of Things and the ability to collect, analyze, and provide big data in the cloud is edge computing, which is a new computing paradigm in which data is processed from edges. Edge Computing has been attracting attention as one of the top 10 strategic technology trends in the past two years and has innovative potential. It provides shorter response times, lower bandwidth costs, and more robust data safety and privacy protection than cloud computing. In particular, artificial intelligence technologies are rapidly incorporating edge computing. In this paper, we introduce the concepts, backgrounds, and pros and cons of edge computing, explain how it operates and its structure hierarchically with artificial intelligence concepts, list examples of its applications in various fields, and finally suggest some improvements and discuss the challenges of its application in three representative technological fields. We intend to clarify various analyses and opinions regarding edge computing and artificial intelligence.","3723":"Summary Objectives: This paper provides a discussion about the potential scope of applicability of Artificial Intelligence methods within the telehealth domain. These methods are focussed on clinical needs and provide some insight to current directions, based on reports of recent advances. Methods: Examples of telehealth innovations involving Artificial Intelligence to support or supplement remote health care delivery were identified from recent literature by the authors, on the basis of expert knowledge. Observations from the examples were synthesized to yield an overview of contemporary directions for the perceived role of Artificial Intelligence in telehealth. Results: Two major focus areas for related contemporary directions were established. These were first, quality improvement for existing clinical practice and service delivery, and second, the development and support of new models of care. Case studies from each focus area have been chosen for illustration purposes. Conclusion: Examples of the role of Artificial Intelligence in delivery of health care remotely include use of tele-assessment, tele-diagnosis, tele-interactions, and tele-monitoring. Further developments of underlying algorithms and validation of methods will be required for wider adoption. Certain key social and ethical considerations also need consideration more generally in the health system, as Artificial-Intelligence-enabled-telehealth becomes more commonplace.","3724":null,"3725":"The following organisations are named on the report: Future of Humanity Institute, University of Oxford, Centre for the Study of Existential Risk, University of Cambridge, Center for a New American Security, Electronic Frontier Foundation, OpenAI. The Future of Life Institute is acknowledged as a funder.","3726":"Artificial intelligence (AI) and its sister ambient intelligence (AmI) have in recent years become one of the main contributors to the progress of digital society and human civilization [...]","3727":"The use of grammars in design and analysis has been set back by the lack of automated ways to induce them from arbitrarily structured datasets. Machine translation methods provide a construct for inducing grammars from coded data which have been extended to be used for design through pre-coded design data. This work introduces a four-step process for inducing grammars from un-coded structured datasets which can constitute a wide variety of data types, including many used in the design. The method includes: (1) extracting objects from the data, (2) forming structures from objects, (3) expanding structures into rules based on frequency, and (4) finding rule similarities that lead to consolidation or abstraction. To evaluate this method, grammars are induced from generated data, architectural layouts and threedimensional design models to demonstrate that this method offers usable grammars automatically which are functionally similar to grammars produced by hand.","3728":"Over the next decade, one issue which will dominate sociotechnical studies in health informatics is the extent to which the promise of artificial intelligence in health care will be realized, along with the social and ethical issues which accompany it. A useful thought experiment is the application of the Turing test to user-facing artificial intelligence systems in health care (such as chatbots or conversational agents). In this paper I argue that many medical decisions require value judgements and the doctor-patient relationship requires empathy and understanding to arrive at a shared decision, often handling large areas of uncertainty and balancing competing risks. Arguably, medicine requires wisdom more than intelligence, artificial or otherwise. Artificial intelligence therefore needs to supplement rather than replace medical professionals, and identifying the complementary positioning of artificial intelligence in medical consultation is a key challenge for the future. In health care, artificial intelligence needs to pass the implementation game, not the imitation game.","3729":null,"3730":"Quantum information technologies, and intelligent learning systems, are both emergent technologies that will likely have a transforming impact on our society. The respective underlying fields of research -- quantum information (QI) versus machine learning (ML) and artificial intelligence (AI) -- have their own specific challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question to what extent these fields can learn and benefit from each other. QML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently, we have witnessed breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups in ML, critical in our \"big data\" world. Conversely, ML already permeates cutting-edge technologies, and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been demonstrated for interactive learning, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments, and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement, researchers have also broached the fundamental issue of quantum generalizations of ML\/AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is described by quantum mechanics. In this review, we describe the main ideas, recent developments, and progress in a broad spectrum of research investigating machine learning and artificial intelligence in the quantum domain.","3731":"Smart home and artificial intelligence technologies are developing rapidly, and various smart home products associated with artificial intelligence (AI) improved the quality of living for occupants. Although some studies discussed the application of artificial intelligence in smart homes, few publications fully considered the integration of literature and products. In this paper, we aim to answer the research questions of \u201cwhat is the trend of smart home technology and products\u201d and \u201cwhat is the relationship between literature and products in smart homes with AI\u201d. Literature reviews and product reviews are given to define the functions and roles of artificial intelligence in smart homes. We determined the application status of artificial intelligence in smart home products and how it is utilized in our house so that we could understand how artificial intelligence is used to make smart homes. Furthermore, our results revealed that there is a delay between literature and products, and smart home intelligent interactions will become more and more popular.","3732":null,"3733":"Purpose of review The use of computers has become increasingly relevant to medical decision-making, and artificial intelligence methods have recently demonstrated significant advances in medicine. We therefore provide an overview of current artificial intelligence methods and their applications, to help the practicing ophthalmologist understand their potential impact on glaucoma care. Recent findings Techniques used in artificial intelligence can successfully analyze and categorize data from visual fields, optic nerve structure [e.g., optical coherence tomography (OCT) and fundus photography], ocular biomechanical properties, and a combination thereof to identify disease severity, determine disease progression, and\/or recommend referral for specialized care. Algorithms have become increasingly complex in recent years, utilizing both supervised and unsupervised methods of artificial intelligence. Impressive performance of these algorithms on previously unseen data has been reported, often outperforming standard global indices and expert observers. However, there remains no clearly defined gold standard for determining the presence and severity of glaucoma, which undermines the training of these algorithms. To improve upon existing methodologies, future work must employ more robust definitions of disease, optimize data inputs for artificial intelligence analysis, and improve methods of extracting knowledge from learned results. Summary Artificial intelligence has the potential to revolutionize the screening, diagnosis, and classification of glaucoma, both through the automated processing of large data sets, and by earlier detection of new disease patterns. In addition, artificial intelligence holds promise for fundamentally changing research aimed at understanding the development, progression, and treatment of glaucoma, by identifying novel risk factors and by evaluating the importance of existing ones.","3734":"With the continuous expansion of the application scope of computer network technology, various malicious attacks that exist in the Internet range have caused serious harm to computer users and network resources. This paper attempts to apply artificial intelligence (AI) to computer network technology and research on the application of AI in computing network technology. Designing an intrusion detection model based on improved back propagation (BP) neural network. By studying the attack principle, analyzing the characteristics of the attack method, extracting feature data, establishing feature sets, and using the agent technology as the supporting technology, the simulation experiment is used to prove the improvement effect of the system in terms of false alarm rate, convergence speed, and false negative rate, the rate reached 86.7%. The results show that this fast algorithm reduces the training time of the network, reduces the network size, improves the classification performance, and improves the intrusion detection rate.","3735":null,"3736":null,"3737":null,"3738":"Based on research into the applications of artificial intelligence (AI) technology in the manufacturing industry in recent years, we analyze the rapid development of core technologies in the new era of \u2018Internet plus AI\u2019, which is triggering a great change in the models, means, and ecosystems of the manufacturing industry, as well as in the development of AI. We then propose new models, means, and forms of intelligent manufacturing, intelligent manufacturing system architecture, and intelligent manufacturing technology system, based on the integration of AI technology with information communications, manufacturing, and related product technology. Moreover, from the perspectives of intelligent manufacturing application technology, industry, and application demonstration, the current development in intelligent manufacturing is discussed. Finally, suggestions for the application of AI in intelligent manufacturing in China are presented.","3739":null,"3740":null,"3741":null,"3742":"Humankind has the ability of learning new things automatically due to the capacities with which we were born. \nWe simply need to have experiences, read, study\u2026 live. For these processes, we are capable of acquiring new \nabilities or modifying those we already have. Another ability we possess is the faculty of thinking, imagine, \ncreate our own ideas, and dream. Nevertheless, what occurs when we extrapolate this to machines? Machines \ncan learn. We can teach them. In the last years, considerable advances have been done and we have seen cars \nthat can recognise pedestrians or other cars, systems that distinguish animals, and even, how some artificial \nintelligences have been able to dream, paint, and compose music by themselves. Despite this, the doubt is \nthe following: Can machines think? Or, in other words, could a machine which is talking to a person and is \nsituated in another room make them believe they are talking with another human? This is a doubt that has \nbeen present since Alan Mathison Turing contemplated it and it has not been resolved yet. In this article, we \nwill show the beginnings of what is known as Artificial Intelligence and some branches of it such as Machine \nLearning, Computer Vision, Fuzzy Logic, and Natural Language Processing. We will talk about each of them, \ntheir concepts, how they work, and the related work on the Internet of Things fields.","3743":"From the Publisher: \nVirtually all the literature on artificial intelligence is expressed in the jargon of commuter science, crowded with complex matrix algebra and differential equations. Unlike many other books on computer intelligence, this one demonstrates that most ideas behind intelligent systems are simple and straightforward. The book has evolved from lectures given to students with little knowledge of calculus, and the reader needs no prerequisites associated with knowledge of any programming language. The methods used in the book have been extensively tested through several courses given by the author. \n \nThe book provides an introduction to the field of computer intelligence, covering \n \nrule-based expert systems, \nfuzzy expert systems, \nframe-based expert systems, \nartificail neural networks, \nevolutionary computation, \nhybrid intelligent systems, \nknowledge engineering, \ndata mining. \n \n \nIn a university setting the book can be used as an introductory course within computer science, information systems or engineering departments. The book is also suitable as a self-study guide for non-computer science professionals, giving access to the state of the art in knowledge-based systems and computational intelligence. Everyone who faces challenging problems and cannot solve them using traditional approaches can benefit","3744":"Our essay discusses an AI process developed for making art (AICAN), and the issues AI creativity raises for understanding art and artists in the 21st century. Backed by our training in computer science (Elgammal) and art history (Mazzone), we argue for the consideration of AICAN\u2019s works as art, relate AICAN works to the contemporary art context, and urge a reconsideration of how we might define human and machine creativity. Our work in developing AI processes for art making, style analysis, and detecting large-scale style patterns in art history has led us to carefully consider the history and dynamics of human art-making and to examine how those patterns can be modeled and taught to the machine. We advocate for a connection between machine creativity and art broadly defined as parallel to but not in conflict with human artists and their emotional and social intentions of art making. Rather, we urge a partnership between human and machine creativity when called for, seeing in this collaboration a means to maximize both partners\u2019 creative strengths.","3745":null,"3746":null,"3747":"5G cellular networks are assumed to be the key enabler and infrastructure provider in the ICT industry, by offering a variety of services with diverse requirements. The standardization of 5G cellular networks is being expedited, which also implies more of the candidate technologies will be adopted. Therefore, it is worthwhile to provide insight into the candidate techniques as a whole and examine the design philosophy behind them. In this article, we try to highlight one of the most fundamental features among the revolutionary techniques in the 5G era, i.e., there emerges initial intelligence in nearly every important aspect of cellular networks, including radio resource management, mobility management, service provisioning management, and so on. However, faced with ever-increasingly complicated configuration issues and blossoming new service requirements, it is still insufficient for 5G cellular networks if it lacks complete AI functionalities. Hence, we further introduce fundamental concepts in AI and discuss the relationship between AI and the candidate techniques in 5G cellular networks. Specifically, we highlight the opportunities and challenges to exploit AI to achieve intelligent 5G networks, and demonstrate the effectiveness of AI to manage and orchestrate cellular network resources. We envision that AI-empowered 5G cellular networks will make the acclaimed ICT enabler a reality.","3748":"Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty. (JEL D21, D43, D83, L12, L13)","3749":null,"3750":null,"3751":"Machine learning (ML) is the ability of computers to learn from data without being programmed explicitly for that purpose, and to apply the acquired knowledge to unknown cases. The application of ML in medicine will increase exponentially in the years to come. Doctors should have some basic knowledge of ML. Only then will they be able to use ML optimally and to recognise the limits and difficulties of ML.","3752":null,"3753":"The current economy's efficiency may be greatly improved by artificial intelligence. We distinguish between automation-oriented applications like robotics and the potential for recent developments in \"deep learning\" to serve as a general-purpose method of invention, finding strong evidence of a \"shift\" in the importance of application-oriented learning research since 2009. However, it may have an even larger impact by serving as a new general-purpose \"method of invention\" that can reshape the nature of the innovation process and the organization of R&D. We suggest that this will likely result in a significant shift away from routine, labor-intensive research and toward research that makes use of the interaction between improved prediction algorithms and passively generated large datasets. In addition, strong incentives for specific businesses to acquire and control crucial large datasets and application-specific algorithms will likely usher in a period of racing as a result of the potential commercial rewards of mastering this method of research. We suggest that in the future, policies that promote transparency and the sharing of core datasets between public and private actors may be essential for boosting research productivity and encouraging innovation-oriented competition.","3754":"Distributional semantics based on neural approaches is a cornerstone of Natural Language Processing, with surprising connections to human meaning representation as well. Recent Transformer-based Language Models have proven capable of producing contextual word representations that reliably convey sense-speci\ufb01c information, simply as a product of self-supervision. Prior work has shown that these contextual representations can be used to accurately represent large sense inventories as sense embeddings, to the extent that a distance-based solution to Word Sense Disambiguation (WSD) tasks outperforms models trained speci\ufb01cally for the task. Still, there remains much to understand on how to use these Neural Language Models (NLMs) to produce sense embeddings that can better harness each NLM\u2019s meaning representation abilities. In this work we introduce a more principled approach to leverage information from all layers of NLMs, informed by a probing analysis on 14 NLM variants. We also emphasize the versatility of these sense embeddings in contrast to task-speci\ufb01c models, applying them on several sense-related tasks, besides WSD, while demonstrating improved performance using our proposed approach over prior work focused on sense embeddings. Finally, we discuss unexpected \ufb01ndings regarding layer and model performance variations, and potential applications for downstream tasks.","3755":"In light of the recent success of artificial intelligence (AI) in computer vision applications, many researchers and physicians expect that AI would be able to assist in many tasks in digital pathology. Although opportunities are both manifest and tangible, there are clearly many challenges that need to be overcome in order to exploit the AI potentials in computational pathology. In this paper, we strive to provide a realistic account of all challenges and opportunities of adopting AI algorithms in digital pathology from both engineering and pathology perspectives.","3756":null,"3757":null,"3758":null,"3759":null,"3760":"Background Artificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis. Objective The objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges. Methods A review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review. Results We propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results. Conclusions We obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients\u2019 quality of life.","3761":null,"3762":"This paper is the introduction to the special issue entitled: \u2018Governing artificial intelligence: ethical, legal and technical opportunities and challenges'. Artificial intelligence (AI) increasingly permeates every aspect of our society, from the critical, like urban infrastructure, law enforcement, banking, healthcare and humanitarian aid, to the mundane like dating. AI, including embodied AI in robotics and techniques like machine learning, can improve economic, social welfare and the exercise of human rights. Owing to the proliferation of AI in high-risk areas, the pressure is mounting to design and govern AI to be accountable, fair and transparent. How can this be achieved and through which frameworks? This is one of the central questions addressed in this special issue, in which eight authors present in-depth analyses of the ethical, legal-regulatory and technical challenges posed by developing governance regimes for AI systems. It also gives a brief overview of recent developments in AI governance, how much of the agenda for defining AI regulation, ethical frameworks and technical approaches is set, as well as providing some concrete suggestions to further the debate on AI governance. This article is part of the theme issue \u2018Governing artificial intelligence: ethical, legal, and technical opportunities and challenges\u2019.","3763":"Artificial Intelligence and machine learning have the potential to be the catalyst for transformation of health systems to improve efficiency and effectiveness, create headroom for universal health coverage and improve outcomes","3764":"This paper explores the question of ethical governance for robotics and artificial intelligence (AI) systems. We outline a roadmap\u2014which links a number of elements, including ethics, standards, regulation, responsible research and innovation, and public engagement\u2014as a framework to guide ethical governance in robotics and AI. We argue that ethical governance is essential to building public trust in robotics and AI, and conclude by proposing five pillars of good ethical governance. This article is part of the theme issue \u2018Governing artificial intelligence: ethical, legal, and technical opportunities and challenges\u2019.","3765":null,"3766":"\n \n Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20% and 30% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with in- creased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.\n \n","3767":null,"3768":"The field of artificial intelligence (AI) has shown an upward trend of growth in the 21st century (from 2000 to 2015). The evolution in AI has advanced the development of human society in our own time, with dramatic revolutions shaped by both theories and techniques. However, the multidisciplinary and fast-growing features make AI a field in which it is difficult to be well understood. In this paper, we study the evolution of AI at the beginning of the 21st century using publication metadata extracted from 9 top-tier journals and 12 top-tier conferences of this discipline. We find that the area is in the sustainable development and its impact continues to grow. From the perspective of reference behavior, the decrease in self-references indicates that the AI is becoming more and more open-minded. The influential papers\/researchers\/institutions we identified outline landmarks in the development of this field. Last but not least, we explore the inner structure in terms of topics\u2019 evolution over time. We have quantified the temporal trends at the topic level and discovered the inner connection among these topics. These findings provide deep insights into the current scientific innovations, as well as shedding light on funding policies.","3769":"Generative artificial intelligence offers a fresh view on molecular design. We present the first\u2010time prospective application of a deep learning model for designing new druglike compounds with desired activities. For this purpose, we trained a recurrent neural network to capture the constitution of a large set of known bioactive compounds represented as SMILES strings. By transfer learning, this general model was fine\u2010tuned on recognizing retinoid X and peroxisome proliferator\u2010activated receptor agonists. We synthesized five top\u2010ranking compounds designed by the generative model. Four of the compounds revealed nanomolar to low\u2010micromolar receptor modulatory activity in cell\u2010based assays. Apparently, the computational model intrinsically captured relevant chemical and biological knowledge without the need for explicit rules. The results of this study advocate generative artificial intelligence for prospective de novo molecular design, and demonstrate the potential of these methods for future medicinal chemistry.","3770":null,"3771":null,"3772":"Abstract Nanophotonics has been an active research field over the past two decades, triggered by the rising interests in exploring new physics and technologies with light at the nanoscale. As the demands of performance and integration level keep increasing, the design and optimization of nanophotonic devices become computationally expensive and time-inefficient. Advanced computational methods and artificial intelligence, especially its subfield of machine learning, have led to revolutionary development in many applications, such as web searches, computer vision, and speech\/image recognition. The complex models and algorithms help to exploit the enormous parameter space in a highly efficient way. In this review, we summarize the recent advances on the emerging field where nanophotonics and machine learning blend. We provide an overview of different computational methods, with the focus on deep learning, for the nanophotonic inverse design. The implementation of deep neural networks with photonic platforms is also discussed. This review aims at sketching an illustration of the nanophotonic design with machine learning and giving a perspective on the future tasks.","3773":"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.","3774":"Artificial intelligence (AI) research within medicine is growing rapidly. In 2016, healthcare AI projects attracted more investment than AI projects within any other sector of the global economy.1 However, among the excitement, there is equal scepticism, with some urging caution at inflated expectations.2 This article takes a close look at current trends in medical AI and the future possibilities for general practice.\n\nInforming clinical decision making through insights from past data is the essence of evidence-based medicine. Traditionally, statistical methods have approached this task by characterising patterns within data as mathematical equations, for example, linear regression suggests a \u2018line of best fit\u2019. Through \u2018machine learning\u2019 (ML), AI provides techniques that uncover complex associations which cannot easily be reduced to an equation. For example, neural networks represent data through vast numbers of interconnected neurones in a similar fashion to the human brain. This allows ML systems to approach complex problem solving just as a clinician might \u2014 by carefully weighing evidence to reach reasoned conclusions. However, unlike a single clinician, these systems can simultaneously observe and rapidly process an almost limitless number of inputs. For example, an AI-driven smartphone app now capably handles the task of triaging 1.2 million people in North London to Accident & Emergency (A&E).3 Furthermore, these systems are able to learn from each incremental case and can be exposed, within minutes, to more cases than a clinician could see in many lifetimes. This is why an AI-driven application is able to \u2026","3775":"Summary In recent years, there has been massive progress in artificial intelligence (AI) with the development of deep neural networks, natural language processing, computer vision and robotics. These techniques are now actively being applied in healthcare with many of the health service activities currently being delivered by clinicians and administrators predicted to be taken over by AI in the coming years. However, there has also been exceptional hype about the abilities of AI with a mistaken notion that AI will replace human clinicians altogether. These perspectives are inaccurate, and if a balanced perspective of the limitations and promise of AI is taken, one can gauge which parts of the health system AI can be integrated to make a meaningful impact. The four main areas where AI would have the most influence would be: patient administration, clinical decision support, patient monitoring and healthcare interventions. This health system where AI plays a central role could be termed an AI-enabled or AI-augmented health system. In this article, we discuss how this system can be developed based on a realistic assessment of current AI technologies and predicted developments.","3776":null,"3777":"Background It is expected that artificial intelligence (AI) will be used extensively in the medical field in the future. Objective The purpose of this study is to investigate the awareness of AI among Korean doctors and to assess physicians\u2019 attitudes toward the medical application of AI. Methods We conducted an online survey composed of 11 closed-ended questions using Google Forms. The survey consisted of questions regarding the recognition of and attitudes toward AI, the development direction of AI in medicine, and the possible risks of using AI in the medical field. Results A total of 669 participants completed the survey. Only 40 (5.9%) answered that they had good familiarity with AI. However, most participants considered AI useful in the medical field (558\/669, 83.4% agreement). The advantage of using AI was seen as the ability to analyze vast amounts of high-quality, clinically relevant data in real time. Respondents agreed that the area of medicine in which AI would be most useful is disease diagnosis (558\/669, 83.4% agreement). One possible problem cited by the participants was that AI would not be able to assist in unexpected situations owing to inadequate information (196\/669, 29.3%). Less than half of the participants(294\/669, 43.9%) agreed that AI is diagnostically superior to human doctors. Only 237 (35.4%) answered that they agreed that AI could replace them in their jobs. Conclusions This study suggests that Korean doctors and medical students have favorable attitudes toward AI in the medical field. The majority of physicians surveyed believed that AI will not replace their roles in the future.","3778":"Artificial intelligence is increasingly influencing the opinions and behaviour of people in everyday life. However, the over-representation of men in the design of these technologies could quietly undo decades of advances in gender equality. Over centuries, humans developed critical theory to inform decisions and avoid basing them solely on personal experience. However, machine intelligence learns primarily from observing data that it is presented with. While a machine\u2019s ability to process large volumes of data may address this in part, if that data is laden with stereotypical concepts of gender, the resulting application of the technology will perpetuate this bias. While some recent studies sought to remove bias from learned algorithms they largely ignore decades of research on how gender ideology is embedded in language. Awareness of this re-search and incorporating it into approaches to machine learning from text would help prevent the generation of biased algorithms. Leading thinkers in the emerging eld addressing bias in artificial intelligence are also primarily female, suggesting that those who are potentially affected by bias are more likely to see, understand and attempt to resolve it. Gender balance in machine learning is therefore crucial to prevent algorithms from perpetuating gender ideologies that disadvantage women.","3779":"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: \u00e2\u20ac\u00a2 With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. \u00e2\u20ac\u00a2 With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.","3780":null,"3781":"ABSTRACT Analytics have been employed by companies for several decades, but now many firms are interested in building their capabilities for artificial intelligence (AI). Many AI systems, however, are based on statistics and other forms of analytics. Companies can get a \u201crunning start\u201d on AI by building upon their analytical competencies. The focus of this article is how to transition from analytics to AI. Three eras of analytical focus are detailed, with AI portrayed as a fourth era. The types of AI methods that are and are not based on analytics are described. AI applications that build on analytical strengths are discussed. Approaches to assessing analytical capabilities that relate to AI, and the development of an organizational plan and strategy for AI, are also described in brief.","3782":"BACKGROUND Machine learning (ML) is a domain of artificial intelligence that allows computer algorithms to learn from experience without being explicitly programmed. OBJECTIVE To summarize neurosurgical applications of ML where it has been compared to clinical expertise, here referred to as \u201cnatural intelligence.\u201d METHODS A systematic search was performed in the PubMed and Embase databases as of August 2016 to review all studies comparing the performance of various ML approaches with that of clinical experts in neurosurgical literature. RESULTS Twenty\u2010three studies were identified that used ML algorithms for diagnosis, presurgical planning, or outcome prediction in neurosurgical patients. Compared to clinical experts, ML models demonstrated a median absolute improvement in accuracy and area under the receiver operating curve of 13% (interquartile range 4\u201021%) and 0.14 (interquartile range 0.07\u20100.21), respectively. In 29 (58%) of the 50 outcome measures for which a P\u2010value was provided or calculated, ML models outperformed clinical experts (P < .05). In 18 of 50 (36%), no difference was seen between ML and expert performance (P > .05), while in 3 of 50 (6%) clinical experts outperformed ML models (P < .05). All 4 studies that compared clinicians assisted by ML models vs clinicians alone demonstrated a better performance in the first group. CONCLUSION We conclude that ML models have the potential to augment the decision\u2010making capacity of clinicians in neurosurgical applications; however, significant hurdles remain associated with creating, validating, and deploying ML models in the clinical setting. Shifting from the preconceptions of a human\u2010vs\u2010machine to a human\u2010and\u2010machine paradigm could be essential to overcome these hurdles.","3783":null,"3784":"As in other domains, artificial intelligence is becoming increasingly important in medicine. In particular, deep learning-based pattern recognition methods can advance the field of pathology by incorporating clinical, radiologic, and genomic data to accurately diagnose diseases and predict patient prognoses. In this review, we present an overview of artificial intelligence, the brief history of artificial intelligence in the medical domain, recent advances in artificial intelligence applied to pathology, and future prospects of pathology driven by artificial intelligence.","3785":"Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al1 reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler2 pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to \u201ctrust a \u2018black box\u2019\u201d). Sixteen months later, the Food and Drug Administration (FDA)3 permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist. This issue of JAMA contains 2 Viewpoints on deep learning in health care. Hinton4 explains the technology underlying AI and deep learning, using clinical examples. AI is the general term for imitating human intelligence with computer systems. Early AI systems represented human reasoning with symbolic logic. As computer processing and storage became more powerful, researchers developed machine-learning techniques to imitate the way the human brain learns. The first machine learning continued to rely on human experts to label the data the system trained on (eg, the diagnosis) and to identify the significant features (eg, findings). Machine learning weighted the features from the data. With continued advances in computational power and with larger data sets, researchers began to develop deep learning techniques. The first deep learning algorithms were \u201csupervised\u201d in that human experts continued to label the training data, and the deep learning algorithms learned the features and weights directly from the data. The retinopathy screening algorithms are an example of supervised deep learning. Hinton4 describes continuing development of new deep learning techniques, including ones that are completely unsupervised. He also points out that it is not feasible to see the features learned by deep learning to explain how the system reaches a conclusion. Naylor5 identifies 7 factors driving adoption of AI and deep learning in health care: (1) the strengths of digital imaging over human interpretation; (2) the digitization of health-related records and data sharing; (3) the adaptability of deep learning to analysis of heterogeneous data sets; (4) the capacity of deep learning for hypothesis generation in research; (5) the promise of deep learning to streamline clinical workflows and empower patients; (6) the rapid-diffusion open-source and proprietary deep learning programs; and (7) of the adequacy of today\u2019s basic deep learning technology to deliver improved performance as data sets get larger. Factors 3, 4, and 6 are specific to deep learning; the other factors apply to other AI techniques as well. Artificial intelligence is a family of technical techniques in the same way the radiologic imaging tool kit includes flat images, computed tomography scans, and functional imaging such as magnetic resonance imaging. Advances in computational technology, computer science, informatics, and statistics improve existing techniques and make new techniques possible. The addition of deep learning to the AI family of techniques represents an advance similar in magnitude to the addition of the computed tomography scanner to the radiology tool kit. Each AI technique has strengths and weaknesses. Symbolic logic is self-explaining but difficult to scale.6 For example, knowledge engineers extract the logic by interviewing or observing human experts. Statistical techniques such as supervised deep learning scale, but are subject to bias in the training data, and the reasoning cannot be explained. Since deep learning systems are trained on data from the past, they are not prepared to reason in the way humans do about conditions that have not been seen before. In the future, unsupervised deep learning may reduce this gap between human intelligence and AI. The potential applications of AI in health care present a range of computational difficulty. Narrow tasks, in which the context is predefined, are relatively easy. Imageprocessing tasks such as recognizing the border of an organ to suggest where to cut off a scan, or highlighting a suspicious area in an image for the radiologist or pathologist, are examples of narrow tasks. Image analysis and diagnostic prediction tasks such as the diabetic retinopathy example are broader and harder, but doable with today\u2019s technology. Very broad data analysis and pattern prediction tasks such as analyzing heterogeneous data sets from diverse sources to suggest novel associations are feasible today because the purpose is limited to hypothesis generation. Thinking in the way humans do\u2014reasoning, for example, from a few observations to suggest a novel scientific framework as Einstein did with the theory of relativity\u2014is beyond technology on the horizon. Clinicians should view the output of AI programs or devices as statistical predictions. They should maintain an index of suspicion that the prediction may be wrong, just as they Viewpoint pages 1099 and 1101 Opinion","3786":"Colonoscopy is considered the gold standard for decreasing colorectal cancer incidence and mortality (1). To maximize the benefits and cost-effectiveness of colonoscopy, resection of all colorectal polyps except diminutive (5 mm), nonneoplastic rectosigmoid polyps is recommended (2). To allow a diagnose-and-leave strategy for these nonneoplastic (or hyperplastic) polyps, the American Society of Gastrointestinal Endoscopy in its second Preservation and Incorporation of Valuable Endoscopic Innovations (PIVI-2) declaration proposed a negative predictive value (NPV) threshold of 90% or greater for real-time optical diagnosis using advanced endoscopic imaging for detecting diminutive, neoplastic rectosigmoid lesions (such as adenomas) (2). A meta-analysis conducted by the society (3) showed that optical diagnosis of diminutive polyps with narrow-band imaging (NBI [Olympus]; application of narrow-spectrum optical filters at the push of the endoscope's button) in expert hands exceeded the 90% NPV threshold at 93% (95% CI, 91% to 96%), although nonexperts do not perform as well (NPV, 87% [CI, 83% to 91%]). Therefore, a major problem with optical diagnosis is its standardization. To overcome this limitation, computer-aided diagnosis (CAD) powered by artificial intelligence is attracting attention as an option to improve optical diagnosis (4, 5). Several CAD systems designed for use with magnifying NBI showed 90% or greater NPV for diagnosing diminutive adenomas (610). Most of these studies, however, were conducted in experimental, retrospective settings, in which accuracy tends to be greater than in the clinical setting owing to selection bias. Hence, the role of CAD in a colonoscopy practice has not been established. This prospective study assessed the feasibility and accuracy of CAD in assessing diminutive polyps during colonoscopy in a large group of patients. We primarily aimed to clarify whether CAD designed for use with endocytoscopes (520 ultramagnifying endoscopes [Olympus]) could exceed the 90% or greater NPV threshold for diagnosing diminutive rectosigmoid adenomas sufficiently to adopt the diagnose-and-leave strategy of PIVI-2. Methods Study Design and Sites This study was a single-group, open-label, prospective study. Patient enrollment and colonoscopy procedures were conducted at a single center (Showa University Northern Yokohama Hospital). The CAD system was trained by using endoscopic data acquired from 5 centers (National Cancer Center Hospital, National Cancer Center Hospital East, Shizuoka Cancer Center, Tokyo Medical and Dental University, and Showa University Northern Yokohama Hospital). Patients Consecutive patients scheduled to undergo routine colonoscopy were recruited to the study between June and December 2017. Only those aged 18 years or older, not receiving anticoagulant therapy, and able to give informed consent were eligible to participate. Exclusion criteria included a history of inflammatory bowel disease, chemotherapy, or radiation therapy for colorectal cancer. Endoscopy Procedures The study participants underwent colonoscopy with an endocytoscope under conscious sedation (diazepam, 5 to 10 mg intravenously). Endoscopists were not required to meet any qualification level. While withdrawing the colonoscope from the cecum, endoscopists predicted the pathology of the detected polyps with on-site use of CAD (EB-01 prototype [Cybernet Systems]), whose output (neoplastic or nonneoplastic) was displayed in real time on the endoscopy monitor. The endoscopists used CAD first with the NBI mode (CAD-NBI) followed by the stained mode (CAD-stained) and were encouraged to capture 10 or more endocytoscopic images of each polyp (for details, see the sections Endocytoscope and Real-Time Working Artificial Intelligence System). The polypsserving as subjects of the analysiswere then resected and evaluated pathologically. The ability of CAD to differentiate neoplastic from nonneoplastic polyps was assessed, with pathologic findings as the gold standard. Registration of Case Report Forms Immediately after each colonoscopy, endoscopists completed case report forms by inputting the size, location, morphology, and resection technique for each detected polyp, together with the insertion time and any complications. The pathologic prediction output by CAD for each polyp was automatically recorded in a comma-separated value file in the computer connected to the endoscopy unit. Two research assistants were engaged in inputting these data into the database. One assistant input data from the endoscopypathology case reports, while the other, who was blinded to these reports, input the data from the comma-separated value file. Endocytoscope We used a prototype endocytoscope (CF-Y-0058; the same model is commercially available as CF-H290ECI [Olympus]) to acquire images for CAD analysis. This endoscope has a contact light microscopy system (magnification, 520; focusing depth, 35 m; field of view, 570500 m) integrated into the distal tip of a colonoscope, enabling in vivo microvascular evaluation with the NBI mode and cellular visualization after staining with 1.0% methylene blue (stained mode) simply by pulling a hand-operated lever (Appendix Figure 1). The endocytoscope also functions as a normal, high-definition endoscope that allows the same insertion, observation, and treatment as a traditional colonoscope. Appendix Figure 1. Endocytoscopic images. A diminutive adenoma (A to C) and hyperplastic polyp (D to F) are seen in the rectum on white light (A and D) and endocytoscopic images (magnification, 520) in NBI (B and E) and stained mode (C and F). Endocytoscopic images of the adenoma reveal networking microvessels in NBI mode and slit-like lumens (arrowheads) with slightly swollen nuclei (arrows) in stained mode. In contrast, endocytoscopic images of the hyperplastic polyp reveal obscure microvessels in NBI mode and serrated lumens (arrowheads) and small intracellular granules (arrows) in stained mode. NBI = narrow-band imaging. Real-Time Working Artificial Intelligence System The algorithm of the CAD model (Figure 1) was developed on the basis of our previous study (8, 9). The computer with the installed CAD system was connected directly to an endoscopy unit (Evis Lucera Elite CV 290 [Olympus]), allowing fully automated, real-time diagnosis (Supplement Video). The CAD algorithm has 3 steps: feature extraction, classification, and pathologic prediction. Figure 1. Real-time CAD for use with an endocytoscope (CF-Y0058-I\/CF-H290ECI [Olympus]) and its algorithm. The computer on which CAD is installed is connected directly to an endoscopy unit (Evis Lucera Elite CV 290 [Olympus]), allowing fully automated diagnosis. Endoscopists can initiate CAD output simply by pushing the capture button on the endoscope. The algorithm for CAD in both NBI and stained modes comprises 3 steps: texture analysis, classification, and pathologic prediction. These analyses take 0.4 seconds, immediately outputting the pathologic prediction of the target polyp as either neoplastic or nonneoplastic, along with the probability of the diagnosis (0% to 100%). CAD= computer-aided diagnosis; NBI= narrow-band imaging. During feature extraction, 312 variables were drawn from the acquired endocytoscopic image by analyzing textures that were characterized by contrast differences of each adjacent pixel. Arrangements of vessels and cellular images thus were quantified indirectly. After extraction, a support vector machine classified the imaged polyp as neoplastic or nonneoplastic on the basis of the 312 extracted features. Our support vector machine, a machine-learning method for optimal separation of complex objects (11), was trained by using endocytoscopic images provided by the aforementioned 5 academic centers. During the study period, we updated the CAD system 5 times by periodically adding training images for machine learningstarting with 28152 images (1 June 2017) and ending with 61925 (5 December 2017). The predicted pathologic results were finally displayed as neoplastic or nonneoplastic, along with the probability of each prediction (0% to 100%) as estimated by the support vector machine. Endoscopists were encouraged to capture several images of each polyp, so the final CAD result was based on majority rule (that is, if 6 outputs classified the polyp as neoplastic and 2 as nonneoplastic, the final diagnosis was neoplastic). If the quality of the captured image was not appropriate for automated diagnosis, CAD automatically recognized it as nonanalyzable, showing Not a good sample on the monitor. Outcome Measures The primary end point was whether CAD-stained analysis produced a 90% or greater NPV for diagnosing diminutive rectosigmoid adenomas. We selected the performance of CAD-stained analysis as the primary outcome measure, because a retrospective study showed that its diagnostic ability reached the PIVI-2 threshold (9), whereas we had no substantial corresponding data for CAD-NBI (8). Secondary outcome measures included the ability of CAD-NBI to distinguish diminutive rectosigmoid polyps and the technical success rate of performing CAD. We also recorded the cecal intubation rate, insertion time, and complications related to insertion to verify the basic function of the endocytoscope. This study had 2 additional co-primary end points: whether trainees' image acquisition rates were higher when they performed endocytoscopy with NBI versus magnified endoscopy with NBI for diminutive polyps and whether CAD-stained analysis had a positive predictive value greater than 90% for diagnosing invasive colorectal lesions 20 mm or larger. We plan to report these end points in separate publications. Pathology Pathologists blinded to the optical diagnosis classified the specimens according to the World Health Organization Classification of Tumours, 4th edition (12). We excluded sessile serrated adenomas and polyps (SSA\/Ps) and pathologically heterogeneous lesio","3787":"This paper examines the potential impact of artificial intelligence (A.I.) on economic growth. We model A.I. as the latest form of automation, a broader process dating back more than 200 years. Electricity, internal combustion engines, and semiconductors facilitated automation in the last century, but A.I. now seems poised to automate many tasks once thought to be out of reach, from driving cars to making medical recommendations and beyond. How will this affect economic growth and the division of income between labor and capital? What about the potential emergence of \u201csingularities\u201d and \u201csuperintelligence,\u201d concepts that animate many discussions in the machine intelligence community? How will the linkages between A.I. and growth be mediated by firm-level considerations, including organization and market structure? The goal throughout is to refine a set of critical questions about A.I. and economic growth and to contribute to shaping an agenda for the field. One theme that emerges is based on Baumol\u2019s \u201ccost disease\u201d insight: growth may be constrained not by what we are good at but rather by what is essential and yet hard to improve.","3788":"There is a wide range of interdisciplinary intersections between cyber security and artificial intelligence (AI). On one hand, AI technologies, such as deep learning, can be introduced into cyber security to construct smart models for implementing malware classification and intrusion detection and threating intelligence sensing. On the other hand, AI models will face various cyber threats, which will disturb their sample, learning, and decisions. Thus, AI models need specific cyber security defense and protection technologies to combat adversarial machine learning, preserve privacy in machine learning, secure federated learning, etc. Based on the above two aspects, we review the intersection of AI and cyber security. First, we summarize existing research efforts in terms of combating cyber attacks using AI, including adopting traditional machine learning methods and existing deep learning solutions. Then, we analyze the counterattacks from which AI itself may suffer, dissect their characteristics, and classify the corresponding defense methods. Finally, from the aspects of constructing encrypted neural network and realizing a secure federated deep learning, we expatiate the existing research on how to build a secure AI system.","3789":"Background and purpose \u2014 Recent advances in artificial intelligence (deep learning) have shown remarkable performance in classifying non-medical images, and the technology is believed to be the next technological revolution. So far it has never been applied in an orthopedic setting, and in this study we sought to determine the feasibility of using deep learning for skeletal radiographs. Methods \u2014 We extracted 256,000 wrist, hand, and ankle radiographs from Danderyd\u2019s Hospital and identified 4 classes: fracture, laterality, body part, and exam view. We then selected 5 openly available deep learning networks that were adapted for these images. The most accurate network was benchmarked against a gold standard for fractures. We furthermore compared the network\u2019s performance with 2 senior orthopedic surgeons who reviewed images at the same resolution as the network. Results \u2014 All networks exhibited an accuracy of at least 90% when identifying laterality, body part, and exam view. The final accuracy for fractures was estimated at 83% for the best performing network. The network performed similarly to senior orthopedic surgeons when presented with images at the same resolution as the network. The 2 reviewer Cohen\u2019s kappa under these conditions was 0.76. Interpretation \u2014 This study supports the use for orthopedic radiographs of artificial intelligence, which can perform at a human level. While current implementation lacks important features that surgeons require, e.g. risk of dislocation, classifications, measurements, and combining multiple exam views, these problems have technical solutions that are waiting to be implemented for orthopedics.","3790":"Artificial intelligence and automation are topics dominating global discussions on the future of professional employment, societal change, and economic performance. In this paper, we describe fundamental concepts underlying AI and Big Data and their significance to public health. We highlight issues involved and describe the potential impacts and challenges to medical professionals and diagnosticians. The possible benefits of advanced data analytics and machine learning are described in the context of recently reported research. Problems are identified and discussed with respect to ethical issues and the future roles of professionals and specialists in the age of artificial intelligence.","3791":"Functioning of the Internet is persistently transforming from the Internet of computers (IoC) to the \u2018Internet of things (IoT)\u2019. Furthermore, massively interconnected systems, also known as cyber-physical systems (CPSs), are emerging from the assimilation of many facets like infrastructure, embedded devices, smart objects, humans, and physical environments. What the authors are heading to is a huge \u2018Internet of Everything in a Smart Cyber Physical Earth\u2019. IoT and CPS conjugated with \u2018data science\u2019 may emerge as the next \u2018smart revolution\u2019. The concern that arises then is to handle the huge data generated with the much weaker existing computation power. The research in data science and artificial intelligence (AI) has been striving to give an answer to this problem. Thus, IoT with AI can become a huge breakthrough. This is not just about saving money, smart things, reducing human effort, or any trending hype. This is much more than that \u2013 easing human life. There are, however, some serious issues like the security concerns and ethical issues which will go on plaguing IoT. The big picture is not how fascinating IoT with AI seems, but how the common people perceive it \u2013 a boon, a burden, or a threat.","3792":null,"3793":"Artificial Intelligence (AI) has recently been developed into a sizzling topic in the area of medical care industry. The biopharmaceutical industries are making efforts to approach AI to enhance drug discovery process, reduce research and development expenses, diminish failure rates in clinical trials and ultimately generate superior medicines. The accessibility of immense statistics in life sciences and a speedy development in machine learning algorithms led to an evolution of AI-based start-up companies focused on drug discovery over the recent years [1]. Numerous remarkable AIbiopharmaceutical alliance were declared in 2016-2017 that include Pfizer and IBM Watson, Sanofi Genzyme and Recursion Pharmaceuticals, AstraZeneca, Abbvie, Merck, Novartis, GSK and Exscientia, etc.","3794":"Artificial Intelligence (AI) plays a pivotal role in drug discovery. In particular artificial neural networks such as deep neural networks or recurrent networks drive this area. Numerous applications in property or activity predictions like physicochemical and ADMET properties have recently appeared and underpin the strength of this technology in quantitative structure-property relationships (QSPR) or quantitative structure-activity relationships (QSAR). Artificial intelligence in de novo design drives the generation of meaningful new biologically active molecules towards desired properties. Several examples establish the strength of artificial intelligence in this field. Combination with synthesis planning and ease of synthesis is feasible and more and more automated drug discovery by computers is expected in the near future.","3795":"Autonomous systems will play an essential role in many applications across diverse domains including space, marine, air, field, road, and service robotics. They will assist us in our daily routines and perform dangerous, dirty, and dull tasks. However, enabling robotic systems to perform autonomously in complex, real-world scenarios over extended time periods (i.e., weeks, months, or years) poses many challenges. Some of these have been investigated by subdisciplines of Artificial Intelligence (AI) including navigation and mapping, perception, knowledge representation and reasoning, planning, interaction, and learning. The different subdisciplines have developed techniques that, when re-integrated within an autonomous system, can enable robots to operate effectively in complex, long-term scenarios. In this letter, we survey and discuss AI techniques as \u201cenablers\u201d for long-term robot autonomy, current progress in integrating these techniques within long-running robotic systems, and the future challenges and opportunities for AI in long-term autonomy.","3796":"Based on recent developments in the field of artificial intelligence (AI), we examine what type of human labor will be a substitute versus a complement to emerging technologies. We argue that these recent developments reduce the costs of providing a particular set of tasks \u2013 prediction tasks. Prediction about uncertain states of the world is an input into decision-making. We show that prediction allows riskier decisions to be taken and this is its impact on observed productivity although it could also increase the variance of outcomes as well. We consider the role of human judgment in decision-making as prediction technology improves. Judgment is exercised when the objective function for a particular set of decisions cannot be described (i.e., coded). However, we demonstrate that better prediction impacts the returns to different types of judgment in opposite ways. Hence, not all human judgment will be a complement to AI. Finally, we show that humans will delegate some decisions to machines even when the decision would be superior with human input.","3797":"Summary There are many ethical questions relating the issue of developing an intelligent system. There is strong and increasing pressure to raise capabilities of the artificial intelligence at least to the human levelled intelligence as the ultimate goal. This essay describes possible paths of development of the artificial intelligence. It is discussed how this changes will affect our society and challenges that humanity will have to face. Principles, guideways and modern viewpoints are presented and confirmed with the statements of the renowned scientists and experts in the field of the artificial intelligence ethics.","3798":"\nPurpose\nThis paper aims to review the applications of artificial intelligence (AI) in the hiring process and its practical implications. This paper highlights the strategic shift in recruitment industry caused due to the adoption of AI in the recruitment process.\n\n\nDesign\/methodology\/approach\nThis paper is prepared by independent academicians who have synthesized their views by a review of the latest reports, articles, research papers and other relevant literature.\n\n\nFindings\nThis paper describes the impact of developments in the field of AI on the hiring process and the recruitment industry. The application of AI for managing the recruitment process is leading to efficiency as well as qualitative gains for both clients and candidates.\n\n\nPractical implications\nThis paper offers strategic insights into automation of the recruitment process and presents practical ideas for implementation of AI in the recruitment industry. It also discusses the strategic implications of the usage of AI in the recruitment industry.\n\n\nOriginality\/value\nThis article describes the role of technological advancements in AI and its application for creating value for the recruitment industry as well as the clients. It saves the valuable reading time of practitioners and researchers by highlighting the AI applications in the recruitment industry in a concise and simple format.\n","3799":null,"3800":"Machine learning (ML) and artificial intelligence (AI) have been around for many years. However, in the last 5 years, remarkable progress has been made using multilayered neural networks in diverse areas such as image recognition, speech recognition, and machine translation. AI is a general purpose technology that is likely to impact many industries. In this chapter I consider how machine learning availability might affect the industrial organization of both firms that provide AI services and industries that adopt AI technology. My intent is not to provide an extensive overview of this rapidly-evolving area, but instead to provide a short summary of some of the forces at work and to describe some possible areas for future research.","3801":null,"3802":"Interest in artificial intelligence (AI) research has grown rapidly over the past few years, in part thanks to the numerous successes of modern machine learning techniques such as deep learning, the availability of large datasets and improvements in computing power. AI is proving to be increasingly applicable to healthcare and there is a growing list of tasks where algorithms have matched or surpassed physician performance. Despite the successes there remain significant concerns and challenges surrounding algorithm opacity, trust and patient data security. Notwithstanding these challenges, AI technologies will likely become increasingly integrated into emergency medicine in the coming years. This perspective presents an overview of current AI research relevant to emergency medicine.","3803":"Recent advances in quantitative phase imaging (QPI) and artificial intelligence (AI) have opened up the possibility of an exciting frontier. The fast and label-free nature of QPI enables the rapid generation of large-scale and uniform-quality imaging data in two, three, and four dimensions. Subsequently, the AI-assisted interrogation of QPI data using data-driven machine learning techniques results in a variety of biomedical applications. Also, machine learning enhances QPI itself. Herein, we review the synergy between QPI and machine learning with a particular focus on deep learning. Furthermore, we provide practical guidelines and perspectives for further development.","3804":"Echocardiography plays a crucial role in the diagnosis and management of cardiovascular disease. However, interpretation remains largely reliant on the subjective expertise of the operator. As a result inter-operator variability and experience can lead to incorrect diagnoses. Artificial intelligence (AI) technologies provide new possibilities for echocardiography to generate accurate, consistent and automated interpretation of echocardiograms, thus potentially reducing the risk of human error. In this review, we discuss a subfield of AI relevant to image interpretation, called machine learning, and its potential to enhance the diagnostic performance of echocardiography. We discuss recent applications of these methods and future directions for AI-assisted interpretation of echocardiograms. The research suggests it is feasible to apply machine learning models to provide rapid, highly accurate and consistent assessment of echocardiograms, comparable to clinicians. These algorithms are capable of accurately quantifying a wide range of features, such as the severity of valvular heart disease or the ischaemic burden in patients with coronary artery disease. However, the applications and their use are still in their infancy within the field of echocardiography. Research to refine methods and validate their use for automation, quantification and diagnosis are in progress. Widespread adoption of robust AI tools in clinical echocardiography practice should follow and have the potential to deliver significant benefits for patient outcome.","3805":null,"3806":null,"3807":"AI technology has a long history which is actively and constantly changing and growing. It focuses on intelligent agents, which contain devices that perceive the environment and based on which takes actions in order to maximize goal success chances. In this paper, we will explain the modern AI basics and various representative applications of AI. In the context of the modern digitalized world, AI is the property of machines, computer programs, and systems to perform the intellectual and creative functions of a person, independently find ways to solve problems, be able to draw conclusions and make decisions. Most artificial intelligence systems have the ability to learn, which allows people to improve their performance over time. The recent research on AI tools, including machine learning, deep learning and predictive analysis intended toward increasing the planning, learning, reasoning, thinking and action taking ability. Based on which, the proposed research intends towards exploring on how the human intelligence differs from the artificial intelligence. Moreover, we critically analyze what AI of today is capable of doing, why it still cannot reach human intelligence and what are the open challenges existing in front of AI to reach and outperform human level of intelligence. Furthermore, it will explore the future predictions for artificial intelligence and based on which potential solution will be recommended to solve it within next decades.","3808":null,"3809":"What began as a quest for artificial general intelligence branched into several pursuits, including intelligent assistants developed by tech companies and task-oriented chatbots that deliver more information or services in specific domains. Progress quickened with the spread of low-latency networking, then accelerated dramatically a few years ago. In 2016, task-focused chatbots became a centerpiece of machine intelligence, promising interfaces that are more engaging than robotic answering systems and that can accommodate our increasingly phone-based information needs. Hundreds of thousands were built. Creating successful non-trivial chatbots proved more difficult than anticipated. Some developers now design for human-chatbot (humbot) teams, with people handling difficult queries. This paper describes the conversational agent space, difficulties in meeting user expectations, potential new design approaches, uses of human-bot hybrids, and implications for the ultimate goal of creating software with general intelligence.","3810":"ABSTRACT: This paper provides an overview of the emergence of artificial intelligence in accounting and auditing. We discuss the current capabilities of cognitive technologies and the implications these technologies will have on human auditors and the audit process itself. We also provide industry examples of artificial intelligence implementation by Big 4 accounting firms. Finally, we address some potential biases associated with the creation and use of artificial intelligence and discuss implications for future research.","3811":"On a clear January morning in Florida, a Tesla enthusiast and network entrepreneur was driving his new Tesla Model S on US Highway 27A, returning from a family trip. He had posted dozens of widely circulated YouTube tutorial videos on his vehicle and clearly understood many of the technical details of his car. That day, he let the vehicle run autonomously on Autopilot mode for 37 min, before it crashed into the trailer of a truck turning left. The Autopilot did not identify the white side of the trailer as a potential hazard, and the driver was killed, leaving his family and his high-tech business behind.1 This tragedy is not a metaphor for artificial intelligence (AI) applications but an example of a long-recognised challenge in AI: the Frame Problem.2 Although rarely appreciated in the scholarly and lay descriptions of the stunning recent successes of AI in medical applications, the Frame Problem and related AI challenges will have unintended harmful effects to the care of patients if not directly addressed.\n\nWith the recent advancement in machine learning algorithms, many medical tasks previously thought to require human expertise have been replicated by AI systems at or above the level of accuracy in human experts. These important demonstrations range from evaluating fundus retinography3 and histopathology4 to reading chest radiographs5 and assessment of skin lesions.6 These studies have encompassed very large numbers of patient cases and have been extensively benchmarked against clinicians. However, all these studies are retrospective in that they involve a collection of labelled cases against which the AI systems are trained and another collection against which they are tested or validated. So far, they have not entered into routine prospective use in the clinic where the Frame Problem will manifest itself most pathologically.\n\nThe Frame \u2026","3812":"Inequality is one of the main challenges posed by the proliferation of artificial intelligence (AI) and other forms of worker-replacing technological progress. This paper provides a taxonomy of the associated economic issues: First, we discuss the general conditions under which new technologies such as AI may lead to a Pareto improvement. Secondly, we delineate the two main channels through which inequality is affected \u2013 the surplus arising to innovators and redistributions arising from factor price changes. Third, we provide several simple economic models to describe how policy can counter these effects, even in the case of a \u201csingularity\u201d where machines come to dominate human labor. Under plausible conditions, non-distortionary taxation can be levied to compensate those who otherwise might lose. Fourth, we describe the two main channels through which technological progress may lead to technological unemployment \u2013 via efficiency wage effects and as a transitional phenomenon. Lastly, we speculate on how technologies to create super-human levels of intelligence may affect inequality and on how to save humanity from the Malthusian destiny that may ensue.","3813":null,"3814":"Artificial intelligence (AI) enables machines to provide unparalleled value in a myriad of industries and applications. In recent years, researchers have harnessed artificial intelligence to analyze large-volume, unstructured medical data and perform clinical tasks, such as the identification of diabetic retinopathy or the diagnosis of cutaneous malignancies. Applications of artificial intelligence techniques, specifically machine learning and more recently deep learning, are beginning to emerge in gastrointestinal endoscopy. The most promising of these efforts have been in computer-aided detection and computer-aided diagnosis of colorectal polyps, with recent systems demonstrating high sensitivity and accuracy even when compared to expert human endoscopists. AI has also been utilized to identify gastrointestinal bleeding, to detect areas of inflammation, and even to diagnose certain gastrointestinal infections. Future work in the field should concentrate on creating seamless integration of AI systems with current endoscopy platforms and electronic medical records, developing training modules to teach clinicians how to use AI tools, and determining the best means for regulation and approval of new AI technology.","3815":"Artificial intelligence based applications are emerging in a broad range of expert domains. News about AI based solutions in medicine, industrial production processes, logistics, mobility and digital marketing trigger discussions and a lot of speculation. The market research industry seems hesitant and at the same time eager to embrace this new technology. In this article the author provides a definition of artificial intelligence and its different forms: narrow AI, hybrid AI and strong AI. He concludes his reflection on the question whether it\u2019s feasible to develop AI based marketing insights solutions with the recommendation: it\u2019s time to embrace AI.","3816":"Artificial intelligence (AI) has a huge impact on our personal lives and also on our democratic society as a whole. While AI offers vast opportunities for the benefit of people, its potential to embed and perpetuate bias and discrimination remains one of the most pressing challenges deriving from its increasing use. This new study, which was prepared by Prof. Frederik Zuiderveen Borgesius for the Anti-discrimination Department of the Council of Europe, elaborates on the risks of discrimination caused by algorithmic decision-making and other types of artificial intelligence (AI).","3817":"The applications of Artificial Intelligence lie all around us and affect all aspects of our lives. The results of Artificial Intelligence have been invaluable to biologists, psychologists, and linguists in helping to understand the processes of memory, learning, and language from a fresh angle. As a concept, Artificial Intelligence has fuelled and sharpened the philosophical debates concerning the nature of the mind, intelligence, and the uniqueness of human beings. Artificial Intelligence: A Very Short Introduction considers the history of Artificial Intelligence, its successes, its limitations, and its future goals. It also reviews the philosophical and technological challenges raised by Artificial Intelligence, considering whether programs could ever be really intelligent, creative, or even conscious.","3818":null,"3819":null,"3820":null,"3821":null,"3822":"Ali Rahimi, a researcher in artificial intelligence (AI) at Google in San Francisco, California, has charged that machine learning algorithms, in which computers learn through trial and error, have become a form of \"alchemy.\" Researchers, he says, do not know why some algorithms work and others don9t, nor do they have rigorous criteria for choosing one AI architecture over another. Now, in a paper presented on 30 April at the International Conference on Learning Representations in Vancouver, Canada, Rahimi and his collaborators document examples of what they see as the alchemy problem and offer prescriptions for bolstering AI9s rigor. The issue is distinct from AI9s reproducibility problem, in which researchers can9t replicate each other9s results because of inconsistent experimental and publication practices. It also differs from the \"black box\" or \"interpretability\" problem in machine learning: the difficulty of explaining how a particular AI has come to its conclusions.","3823":"Artificial intelligence has seen a number of breakthroughs in recent years, with games often serving as significant milestones. A common feature of games with these successes is that they involve information symmetry among the players, where all players have identical information. This property of perfect information, though, is far more common in games than in real-world problems. Poker is the quintessential game of imperfect information, and it has been a longstanding challenge problem in artificial intelligence. In this paper we introduce DeepStack, a new algorithm for imperfect information settings such as poker. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition about arbitrary poker situations that is automatically learned from selfplay games using deep learning. In a study involving dozens of participants and 44,000 hands of poker, DeepStack becomes the first computer program to beat professional poker players in heads-up no-limit Texas hold\u2019em. Furthermore, we show this approach dramatically reduces worst-case exploitability compared to the abstraction paradigm that has been favored for over a decade.","3824":"Talk of artificial intelligence is everywhere. People marvel at the capacity of machines to translate any language and master any game. Others condemn the use of secret algorithms to sentence criminal defendants or recoil at the prospect of machines gunning for blue, pink, and white-collar jobs. Some worry aloud that artificial intelligence will be humankind\u2019s \u201cfinal invention.\u201d\u00a0 This essay, prepared in connection with UC Davis Law Review's 50th anniversary symposium, explains why AI is suddenly on everyone's mind and provides a roadmap to the major policy questions AI raises. The essay is designed to help policymakers, investors, technologists, scholars, and students understand the contemporary policy environment around AI at least well enough to initiate their own exploration. Topics covered include:\u00a0justice and equity, use of force, safety and certification, privacy (including data parity) and taxation and displacement of labor. In addition to these topics, the essay will touch briefly on a selection of broader systemic questions: institutional configuration and expertise, investment and procurement, removing hurdles to accountability and correcting mental models of AI.","3825":"Artificial intelligence is a general term that means to accomplish a task mainly by a computer, with the least human beings participation, and it is widely accepted as the invention of robots. With the development of this new technology, artificial intelligence has been one of the most influential information technology revolutions. We searched these English-language studies relative to ophthalmology published on PubMed and Springer databases. The application of artificial intelligence in ophthalmology mainly concentrates on the diseases with a high incidence, such as diabetic retinopathy, age-related macular degeneration, glaucoma, retinopathy of prematurity, age-related or congenital cataract and few with retinal vein occlusion. According to the above studies, we conclude that the sensitivity of detection and accuracy for proliferative diabetic retinopathy ranged from 75% to 91.7%, for non-proliferative diabetic retinopathy ranged from 75% to 94.7%, for age-related macular degeneration it ranged from 75% to 100%, for retinopathy of prematurity ranged over 95%, for retinal vein occlusion just one study reported ranged over 97%, for glaucoma ranged 63.7% to 93.1%, and for cataract it achieved a more than 70% similarity against clinical grading.","3826":"Artificial Intelligence principles define social and ethical considerations to develop future AI. They come from research institutes, government organizations and industries. All versions of AI principles are with different considerations covering different perspectives and making different emphasis. None of them can be considered as complete and can cover the rest AI principle proposals. Here we introduce LAIP, an effort and platform for linking and analyzing different Artificial Intelligence Principles. We want to explicitly establish the common topics and links among AI Principles proposed by different organizations and investigate on their uniqueness. Based on these efforts, for the long-term future of AI, instead of directly adopting any of the AI principles, we argue for the necessity of incorporating various AI Principles into a comprehensive framework and focusing on how they can interact and complete each other.","3827":null,"3828":null,"3829":"By virtue of their wide applications in personal electronic devices and industrial monitoring, pressure sensors are attractive candidates for promoting the advancement of science and technology in modern society. Flexible pressure sensors based on organic materials, which combine unique advantages of flexibility and low-cost, have emerged as a highly active field due to their promising applications in artificial intelligence systems and wearable health care devices. In this review, we focus on the fundamentals of flexible pressure sensors, and subsequently on several critical concepts for the exploration of functional materials and optimization of sensing devices toward practical applications. Perspectives on self-powered, transparent and implantable pressure sensing devices are also examined to highlight the development directions in this exciting research field.","3830":null,"3831":"Artificial intelligence (AI) is intrinsically data-driven. It calls for the application of statistical concepts through human-machine collaboration during the generation of data, the development of algorithms, and the evaluation of results. This paper discusses how such human-machine collaboration can be approached through the statistical concepts of population, question of interest, representativeness of training data, and scrutiny of results (PQRS). The PQRS workflow provides a conceptual framework for integrating statistical ideas with human input into AI products and researches. These ideas include experimental design principles of randomization and local control as well as the principle of stability to gain reproducibility and interpretability of algorithms and data results. We discuss the use of these principles in the contexts of self-driving cars, automated medical diagnoses, and examples from the authors\u2019 collaborative research.","3832":"New York Times Best Seller How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technologyand theres nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor whos helped mainstream research on how to keep AI beneficial. How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give todays kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle? What sort of future do you want? This book empowers you to join what may be the most important conversation of our time. It doesnt shy away from the full range of viewpoints or from the most controversial issuesfrom superintelligence to meaning, consciousness and the ultimate physical limits on life in the cosmos.","3833":"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together \u2013 each targeting a straightforward prediction task \u2013 to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","3834":"Innovative educational technologies have revolutionized the methods of teaching and learning. Recently, with advancements of artificial intelligence, higher education has begun to adopt new technologies. This conceptual review paper aims to investigate the emergence of using artificial intelligence in teaching and learning in education. It examines the educational consequences of emergent technologies on how institutions teach and the way students learn. This study intends to predict the role of artificial intelligence in the future nature of education in a world. The effective application of artificial intelligence methods is considered as a means of improving the quality of teaching and learning. However, the challenges of integrating artificial intelligence in educational institutions is addressed. Moreover, the challenges faced by students in adopting artificial intelligence in terms of students\u2019 support, teaching, learning, and administration are discussed.\u00a0 This paper presents a concise overview of the most recent studies to showcase the application of artificial intelligence in educational contexts. The implications and directions for further research are suggested.\u00a0","3835":null,"3836":"With the rapid development of information technology and the needs of economic society, artificial intelligence has ushered in the golden age. The application of artificial intelligence technology in the accounting field is an inevitable trend, which will bring tremendous changes and development to the accounting industry. This paper takes the application of artificial intelligence in the accounting industry as the research object, analyzes the impact of artificial intelligence on the development of accounting industry, and puts forward relevant suggestions for its existing problems.","3837":null,"3838":"The potential role of artificial intelligence in improving organisations\u2019 performance and productivity has been promoted regularly and vociferously since the 1960s. Artificial intelligence is today reborn out of big business, similar to the occurrences surrounding big data in the 1990s, and expectations are high regarding AI\u2019s potential role in businesses. This article discusses different aspects of knowledge work that tend to be ignored in the debate about whether or not artificial intelligence systems are a threat to jobs. A great deal of knowledge work concerns highly complex problem solving and must be understood in contextual, social and relational terms. These aspects have no generic nor universal rules and solutions and, thus, cannot be easily replaced by artificial intelligence or programmed into computer systems, nor are they constructed based on models of the rational brain. In this respect, this article draws on philosopher Herbert Dreyfus\u2019 thesis regarding artificial intelligence.","3839":"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.","3840":null,"3841":"Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool \u23bcChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students\u2019 learning.","3842":"Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.","3843":"Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.","3844":"As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.","3845":null,"3846":"Correspondence to Dr Nash Anderson; nash. anderson@ gmail. com \u00a9 Author(s) (or their employer(s)) 2023. Reuse permitted under CC BYNC. No commercial reuse. See rights and permissions. Published by BMJ. INTRODUCTION Researching a topic and generating an academic paper is a nuanced skill. It can take months or years to produce and publish one, if it is ever published at all. What if there were a way to make this happen instantly? Artificial intelligence (AI) may hold a flame to quickly analyse a research topic and generate an academic paper. There are many forms of AI; this editorial discusses natural language modelbased AI, such as ChatGPT, and their potential ability to generate academic papers. Natural language modelbased AI, in particular ChatGPT, is generating new content and a lot of controversies. This AI software is innovative. It generates, de novo, content that has a natural conversational flow. It can quickly answer questions and write poems, fan fiction and children\u2019s books. ChatGPT has even passed the United States Medical Licensing Examination theory section with no additional training and\/or years of studying medicine. Languagebased AI has already entered the scientific community. Nature reported that four manuscripts in preprint credit ChatGPT as an author. Also, an article reported that AI had been used to generate an academic paper. In this editorial, we discuss the pros and cons of AI for manuscript generation in sports and exercise medicine (SEM), generate an academic paper using AI and bypass AIgeneration detection, and discuss potential concerns regarding natural language modelbased AI. We aim to get insights on how AI, in particular ChatGPT and similar language modelbased AI, will impact the future of manuscript generation in SEM. To achieve such purpose, we ought to consider what is an academic paper, whether AI should write academic papers, what the issues are, what our stance should be on AIgenerated texts and how we deal with them.","3847":"AI lifecycle,","3848":null,"3849":"Metaverse as the latest buzzword has attracted great attention from both industry and academia. Metaverse seamlessly integrates the real world with the virtual world and allows avatars to carry out rich activities including creation, display, entertainment, social networking, and trading. Thus, it is promising to build an exciting digital world and to transform a better physical world through the exploration of the metaverse. In this survey, we dive into the metaverse by discussing how Blockchain and Artificial Intelligence (AI) fuse with it through investigating the state-of-the-art studies across the metaverse components, digital currencies, AI applications in the virtual world, and blockchain-empowered technologies. Further exploitation and interdisciplinary research on the fusion of AI and Blockchain towards metaverse will definitely require collaboration from both academia and industries. We wish that our survey can help researchers, engineers, and educators build an open, fair, and rational future metaverse.","3850":null,"3851":"In recent years, AI research has demonstrated enormous potential for the benefit of humanity and society. While often better than its human counterparts in classification and pattern recognition tasks, however, AI still struggles with complex tasks that require commonsense reasoning such as natural language understanding. In this context, the key limitations of current AI models are: dependency, reproducibility, trustworthiness, interpretability, and explainability. In this work, we propose a commonsense-based neurosymbolic framework that aims to overcome these issues in the context of sentiment analysis. In particular, we employ unsupervised and reproducible subsymbolic techniques such as auto-regressive language models and kernel methods to build trustworthy symbolic representations that convert natural language to a sort of protolanguage and, hence, extract polarity from text in a completely interpretable and explainable manner.","3852":null,"3853":"Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.","3854":"We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.","3855":"AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades\u2014compounding events causing negative, downstream effects from data issues\u2014triggered by conventional AI\/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.","3856":"ATLAS OF AI: Power, Politics, and the Planetary Costs of Artificial Intelligence by Kate Crawford. New Haven, CT: Yale University Press, 2021. 336 pages. Hardcover; $28.00. ISBN: 9780300209570. *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence is Kate Crawford's analysis of the state of the AI industry. A central idea of her book is the importance of redefining Artificial Intelligence (AI). She states, \"I've argued that there is much at stake in how we define AI, what its boundaries are, and who determines them: it shapes what can be seen and contested\" (p. 217). *My own definition of AI goes something like this: I\u00ac\u2020imagine a future where I'm sitting in a cafe drinking coffee with my friends, but in this future, one of my friends is a robot, who like me is trying to make a living in this world. A future where humans and robots live in harmony. Crawford views this definition as mythological: \"These mythologies are particularly strong in the field of artificial intelligence, where the belief that human intelligence can be formalized and reproduced by machines has been axiomatic since the mid-twentieth century\" (p.\u00ac\u20205). I do not know if my definition of artificial intelligence can come true, but I am enjoying the process of building, experimenting, and dreaming. *In her book, she asks me to consider that I may be unknowingly participating, as she states, in \"a material product of colonialism, with its patterns of extraction, conflict, and environmental destruction\" (p. 38). The book's subtitle illuminates the purpose of the book: specifically, the power, politics, and planetary costs of usurping artificial intelligence. Of course, this is not exactly Crawford's subtitle, and this is where I both agree and disagree with her. The book's subtitle is actually Power, Politics, and the Planetary Costs of Artificial Intelligence. In my opinion, AI is more the canary in the coal mine. We can use the canary to detect the poisonous gases, but we cannot blame the canary for the poisonous gas. It risks missing the point. Is AI itself to be feared? Should we no longer teach or learn AI? Or is this more about how we discern responsible use and direction for AI technology? *There is another author who speaks to similar issues. In Weapons of Math Destruction, Cathy O'Neil states it this way, \"If we had been clear-headed, we all would have taken a step back at this point to figure out how math had been misused ... But instead ... new mathematical techniques were hotter than ever ... A computer program could speed through thousands of resumes or loan applications in a second or two and sort them into neat lists, with the most promising candidates on top\" (p. 13). *Both Crawford and O'Neil point to human flaws that often lead to well-intentioned software developers creating code that results in unfair and discriminatory decisions. AI models encode unintended human biases that may not evaluate candidates as fairly as we would expect, yet there is a widespread notion that we can trust the algorithm. For example, the last time you registered an account on a website, did you click the checkbox confirming that \"yes, I read the disclaimer\" even though you did not? When we click \"yes\" we are accepting this disclaimer and placing trust in the software. Business owners place trust in software when they use it to make predictions. Engineers place trust in their algorithms when they write software without rigorous testing protocols. I\u00ac\u2020am just as guilty. *Crawford suggests that AI is often used in ways that are harmful. In the Atlas of AI we are given a tour of how technology is damaging our world: strip mining, labor injustice, the misuse of personal data, issues of state and power, to name a few of the concerns Crawford raises. The reality is that AI is built upon existing infrastructure. For example, Facebook, Instagram, YouTube, Amazon, TikTok have been collecting our information for profit even before AI became important to them. The data centers, CPU houses, and worldwide network infrastructure were already in place to meet consumer demand and geopolitics. But it is true that AI brings new technologies to the table, such as automated face recognition and decision tools to compare prospective employment applicants with diverse databases and employee monitoring tools that can make automatic recommendations. Governments, militaries, and intelligence agencies have taken notice. As invasion of privacy and social justice concerns emerge, Crawford calls us to consider these issues carefully. *Reading Crawford's words pricked my conscience, convicting me to reconsider my erroneous ways. For big tech to exist, to supply what we demand, it needs resources. She walks us through the many resources the technology industry needs to provide what we want, and AI is the \"new kid on the block.\" This book is not about AI, per se; it is instead about the side effects of poor business\/research practices, opportunist behavior, power politics, and how these behaviors not only exploit our planet but also unjustly affect marginalized people. The AI industry is simply a new example of this reality: data mining, low wages to lower costs, foreign workers with fewer rights, strip mining, relying on coal and oil for electricity (although some tech companies have made strides to improve sustainability). This sounds more like a parable about the sins of the tech industry than a critique about the dangers of AI. *Could the machine learning community, like the inventors of dynamite who wanted to simply help railroads excavate tunnels, be unintentionally causing harm? Should we, as a community, be on the lookout for these potential harms? Do we have a moral responsibility? Maybe the technology sector needs to look more inwardly to ensure that process efficiency and cost savings are not elevated as most important. *I did not agree with everything that Crawford classified as AI, but I do agree that as a community we are responsible for our actions. If there are injustices, then this should be important to us. In particular, as people of faith, we should heed the call of Micah 6:8 to act justly in this world, and this includes how we use AI. *Reviewed by Joseph Vybihal, Professor of Computer Science, McGill University, Montreal, PQ H3A 0G4.","3857":null,"3858":"Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.","3859":"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.","3860":"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.","3861":"Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI\u2019s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?","3862":"Artificial intelligence (AI) is a modern approach based on computer science that develops programs and algorithms to make devices intelligent and efficient for performing tasks that usually require skilled human intelligence. AI involves various subsets, including machine learning (ML), deep learning (DL), conventional neural networks, fuzzy logic, and speech recognition, with unique capabilities and functionalities that can improve the performances of modern medical sciences. Such intelligent systems simplify human intervention in clinical diagnosis, medical imaging, and decision-making ability. In the same era, the Internet of Medical Things (IoMT) emerges as a next-generation bio-analytical tool that combines network-linked biomedical devices with a software application for advancing human health. In this review, we discuss the importance of AI in improving the capabilities of IoMT and point-of-care (POC) devices used in advanced healthcare sectors such as cardiac measurement, cancer diagnosis, and diabetes management. The role of AI in supporting advanced robotic surgeries developed for advanced biomedical applications is also discussed in this article. The position and importance of AI in improving the functionality, detection accuracy, decision-making ability of IoMT devices, and evaluation of associated risks assessment is discussed carefully and critically in this review. This review also encompasses the technological and engineering challenges and prospects for AI-based cloud-integrated personalized IoMT devices for designing efficient POC biomedical systems suitable for next-generation intelligent healthcare.","3863":"A growing number of artificial intelligence (AI)-based clinical decision support systems are showing promising performance in preclinical, in silico, evaluation, but few have yet demonstrated real benefit to patient care. Early stage clinical evaluation is important to assess an AI system\u2019s actual clinical performance at small scale, ensure its safety, evaluate the human factors surrounding its use, and pave the way to further large scale trials. However, the reporting of these early studies remains inadequate. The present statement provides a multistakeholder, consensus-based reporting guideline for the Developmental and Exploratory Clinical Investigations of DEcision support systems driven by Artificial Intelligence (DECIDE-AI). We conducted a two round, modified Delphi process to collect and analyse expert opinion on the reporting of early clinical evaluation of AI systems. Experts were recruited from 20 predefined stakeholder categories. The final composition and wording of the guideline was determined at a virtual consensus meeting. The checklist and the Explanation & Elaboration (E&E) sections were refined based on feedback from a qualitative evaluation process. 123 experts participated in the first round of Delphi, 138 in the second, 16 in the consensus meeting, and 16 in the qualitative evaluation. The DECIDE-AI reporting guideline comprises 17 AI specific reporting items (made of 28 subitems) and 10 generic reporting items, with an E&E paragraph provided for each. Through consultation and consensus with a range of stakeholders, we have developed a guideline comprising key items that should be reported in early stage clinical studies of AI-based decision support systems in healthcare. By providing an actionable checklist of minimal reporting items, the DECIDE-AI guideline will facilitate the appraisal of these studies and replicability of their findings.","3864":"The recent upsurge of diversified mobile applications, especially those supported by Artificial Intelligence (AI), is spurring heated discussions on the future evolution of wireless communications. While 5G is being deployed around the world, efforts from industry and academia have started to look beyond 5G and conceptualize 6G. We envision 6G to undergo an unprecedented transformation that will make it substantially different from the previous generations of wireless cellular systems. In particular, 6G will go beyond mobile Internet and will be required to support ubiquitous AI services from the core to the end devices of the network. Meanwhile, AI will play a critical role in designing and optimizing 6G architectures, protocols, and operations. In this article, we discuss potential technologies for 6G to enable mobile AI applications, as well as AI-enabled methodologies for 6G network design and optimization. Key trends in the evolution to 6G will also be discussed.","3865":"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into \u201cblack box\u201d approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.","3866":"Machine learning has been described as an effective measure in avoiding most cyberattacks. The development of AI has therefore promoted increased security for most computer attacks. Phishing attacks are risky and can be prevented through AI-based solutions. This factor suggests the need for increased awareness of cybersecurity through AI. Developing awareness for most people will prevent these types of attacks. The research paper describes how the awareness of AI-based cybersecurity could ensure a reduction of phishing attacks. The paper, therefore, showcases the effectiveness of AI-based cybersecurity awareness training and how it may influence cyber-attacks.","3867":null,"3868":"The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a dynamic task graph computation model that supports both the task-parallel and the actor programming models. To meet the performance requirements of AI applications, we propose an architecture that logically centralizes the system's control state using a sharded storage system and a novel bottom-up distributed scheduler. In our experiments, we demonstrate sub-millisecond remote task latencies and linear throughput scaling beyond 1.8 million tasks per second. We empirically validate that Ray speeds up challenging benchmarks and serves as both a natural and performant fit for an emerging class of reinforcement learning applications and algorithms.","3869":"Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.","3870":null,"3871":"Technologies that accelerate the delivery of reliable battery-based energy storage will not only contribute to decarbonization such as transportation electrification, smart grid, but also strengthen the battery supply chain. As battery inevitably ages with time, losing its capacity to store charge and deliver it efficiently. This directly affects battery safety and efficiency, making related health management necessary. Recent advancements in automation science and engineering raised interest in AI-based solutions to prolong battery lifetime from both manufacturing and management perspectives. This paper aims at presenting a critical review of the state-of-the-art AI-based manufacturing and management strategies towards long lifetime battery. First, AI-based battery manufacturing and smart battery to benefit battery health are showcased. Then the most adopted AI solutions for battery life diagnostic including state-of-health estimation and ageing prediction are reviewed with a discussion of their advantages and drawbacks. Efforts through designing suitable AI solutions to enhance battery longevity are also presented. Finally, the main challenges involved and potential strategies in this field are suggested. This work will inform insights into the feasible, advanced AI for the health-conscious manufacturing, control and optimization of battery on different technology readiness levels.","3872":"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034\/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","3873":null,"3874":"Purpose: Develop AI-based automated CT image analysis tools for detection, quantification, and tracking of Coronavirus; demonstrate they can differentiate coronavirus patients from non-patients. Materials and Methods: Multiple international datasets, including from Chinese disease-infected areas were included. We present a system that utilizes robust 2D and 3D deep learning models, modifying and adapting existing AI models and combining them with clinical understanding. We conducted multiple retrospective experiments to analyze the performance of the system in the detection of suspected COVID-19 thoracic CT features and to evaluate evolution of the disease in each patient over time using a 3D volume review, generating a Corona score. The study includes a testing set of 157 international patients (China and U.S). Results: Classification results for Coronavirus vs Non-coronavirus cases per thoracic CT studies were 0.996 AUC (95%CI: 0.989-1.00) ; on datasets of Chinese control and infected patients. Possible working point: 98.2% sensitivity, 92.2% specificity. For time analysis of Coronavirus patients, the system output enables quantitative measurements for smaller opacities (volume, diameter) and visualization of the larger opacities in a slice-based heat map or a 3D volume display. Our suggested Corona score measures the progression of disease over time. Conclusion: This initial study, which is currently being expanded to a larger population, demonstrated that rapidly developed AI-based image analysis can achieve high accuracy in detection of Coronavirus as well as quantification and tracking of disease burden.","3875":"Our physics-inspired algorithm for symbolic regression is able to discover complex physics equations from mere tables of numbers. A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.","3876":"ABSTRACT Artificial Intelligence (AI) has been argued to offer a myriad of improvements in how we work and live. The notion of AI comprises a wide-ranging set of technologies that allow individuals and organizations to integrate and analyze data and use that insight to improve or automate decision-making. While most attention has been placed on the positive aspects companies realize by the adoption by the adoption and use of AI, there is a growing concern around the negative and unintended consequences of such technologies. In this special issue we have made a call for research papers that help us explore the dark side of AI use. By adopting a dark side lens, we aimed to expand our understanding of how AI should be implemented in practice, and how to minimize or avoid negative outcomes. In this editorial, we build on the notion of responsible AI, to highlight the different ways in which AI can potentially produce unintended consequences, as well as to suggest alternative paths future IS research can follow to improve our knowledge about how to mitigate such occurrences. We further expand on dark side theorizing in order to uncover hidden assumptions of current literature as well as to propose other prominent themes that can guide future IS research on AI adoption and use.","3877":null,"3878":"We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at this http URL AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.","3879":null,"3880":"The latest 5G mobile networks have enabled many exciting Internet of Things (IoT) applications that employ unmanned aerial vehicles (UAVs\/drones). The success of most UAV-based IoT applications is heavily dependent on artificial intelligence (AI) technologies, for instance, computer vision and path planning. These AI methods must process data and provide decisions while ensuring low latency and low energy consumption. However, the existing cloud-based AI paradigm finds it difficult to meet these strict UAV requirements. Edge AI, which runs AI on-device or on edge servers close to users, can be suitable for improving UAV-based IoT services. This article provides a comprehensive analysis of the impact of edge AI on key UAV technical aspects (i.e., autonomous navigation, formation control, power management, security and privacy, computer vision, and communication) and applications (i.e., delivery systems, civil infrastructure inspection, precision agriculture, search and rescue (SAR) operations, acting as aerial wireless base stations (BSs), and drone light shows). As guidance for researchers and practitioners, this article also explores UAV-based edge AI implementation challenges, lessons learned, and future research directions.","3881":null,"3882":"Abstract As AI-enhanced technologies become common in a variety of domains, there is an increasing need to define and examine the trust that users have in such technologies. Given the progress in the development of AI, a correspondingly sophisticated understanding of trust in the technology is required. This paper addresses this need by explaining the role of trust in the intention to use AI technologies. Study 1 examined the role of trust in the use of AI voice assistants based on survey responses from college students. A path analysis confirmed that trust had a significant effect the on intention to use AI, which operated through perceived usefulness and participants\u2019 attitude toward voice assistants. In Study 2, using data from a representative sample of the U.S. population, different dimensions of trust were examined using exploratory factor analysis, which yielded two dimensions: human-like trust and functionality trust. The results of the path analyses from Study 1 were replicated in Study 2, confirming the indirect effect of trust and the effects of perceived usefulness, ease of use, and attitude on intention to use. Further, both dimensions of trust shared a similar pattern of effects within the model, with functionality-related trust exhibiting a greater total impact on usage intention than human-like trust. Overall, the role of trust in the acceptance of AI technologies was significant across both studies. This research contributes to the advancement and application of the TAM in AI-related applications and offers a multidimensional measure of trust that can be utilized in the future study of trustworthy AI.","3883":null,"3884":null,"3885":"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these \"AI principles,\" there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.\r\n\r\nTo that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this \u201cnormative core,\u201d our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.","3886":"Artificial intelligence (AI) has the potential to revolutionize the drug discovery process, offering improved efficiency, accuracy, and speed. However, the successful application of AI is dependent on the availability of high-quality data, the addressing of ethical concerns, and the recognition of the limitations of AI-based approaches. In this article, the benefits, challenges, and drawbacks of AI in this field are reviewed, and possible strategies and approaches for overcoming the present obstacles are proposed. The use of data augmentation, explainable AI, and the integration of AI with traditional experimental methods, as well as the potential advantages of AI in pharmaceutical research, are also discussed. Overall, this review highlights the potential of AI in drug discovery and provides insights into the challenges and opportunities for realizing its potential in this field. Note from the human authors: This article was created to test the ability of ChatGPT, a chatbot based on the GPT-3.5 language model, in terms of assisting human authors in writing review articles. The text generated by the AI following our instructions (see Supporting Information) was used as a starting point, and its ability to automatically generate content was evaluated. After conducting a thorough review, the human authors practically rewrote the manuscript, striving to maintain a balance between the original proposal and the scientific criteria. The advantages and limitations of using AI for this purpose are discussed in the last section.","3887":null,"3888":"The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or\"price tag\"of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.","3889":null,"3890":"This article summarizes the author's Robert S. Englemore Memorial Lecture presented at the Thirty-Fourth AAAI Conference on Artificial Intelligence on February 10, 2020. It explores recurring themes in the history of AI, real and imagined dangers from AI, and the future of the field.","3891":"AI now masters six-player poker Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold'em poker. However, poker games usually include six players\u2014a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold'em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker. Science, this issue p. 885; see also p. 864 An AI dubbed Pluribus performs significantly better than human professionals in six-player no-limit Texas hold\u2019em poker. In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold\u2019em poker, the most popular form of poker played by humans.","3892":null,"3893":null,"3894":"Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to operationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts, such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners' needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We highlight aspects of organizational culture that may impact the efficacy of AI fairness checklists, and suggest future design directions.","3895":"This article develops a strategic framework for using artificial intelligence (AI) to engage customers for different service benefits. This framework lays out guidelines of how to use different AIs to engage customers based on considerations of nature of service task, service offering, service strategy, and service process. AI develops from mechanical, to thinking, and to feeling. As AI advances to a higher intelligence level, more human service employees and human intelligence (HI) at the intelligence levels lower than that level should be used less. Thus, at the current level of AI development, mechanical service should be performed mostly by mechanical AI, thinking service by both thinking AI and HI, and feeling service mostly by HI. Mechanical AI should be used for standardization when service is routine and transactional, for cost leadership, and mostly at the service delivery stage. Thinking AI should be used for personalization when service is data-rich and utilitarian, for quality leadership, and mostly at the service creation stage. Feeling AI should be used for relationalization when service is relational and high touch, for relationship leadership, and mostly at the service interaction stage. We illustrate various AI applications for the three major AI benefits, providing managerial guidelines for service providers to leverage the advantages of AI as well as future research implications for service researchers to investigate AI in service from modeling, consumer, and policy perspectives.","3896":null,"3897":"What explains the dramatic progress from 20th-century to 21st-century AI, and how can the remaining limitations of current AI be overcome? The widely accepted narrative attributes this progress to massive increases in the quantity of computational and data resources available to support statistical learning in deep artificial neural networks. We show that an additional crucial factor is the development of a new type of computation. Neurocompositional computing adopts two principles that must be simultaneously respected to enable human-level cognition: the principles of Compositionality and Continuity. These have seemed irreconcilable until the recent mathematical discovery that compositionality can be realized not only through discrete methods of symbolic computing, but also through novel forms of continuous neural computing. The revolutionary recent progress in AI has resulted from the use of limited forms of neurocompositional computing. New, deeper forms of neurocompositional computing create AI systems that are more robust, accurate, and comprehensible.","3898":"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.","3899":"One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.","3900":"Artificial Intelligence (AI) plays an increasingly important role in improving HCI and user experience. Yet many challenges persist in designing and innovating valuable human-AI interactions. For example, AI systems can make unpredictable errors, and these errors damage UX and even lead to undesired societal impact. However, HCI routinely grapples with complex technologies and mitigates their unintended consequences. What makes AI different? What makes human-AI interaction appear particularly difficult to design? This paper investigates these questions. We synthesize prior research, our own design and research experience, and our observations when teaching human-AI interaction. We identify two sources of AI's distinctive design challenges: 1) uncertainty surrounding AI's capabilities, 2) AI's output complexity, spanning from simple to adaptive complex. We identify four levels of AI systems. On each level, designers encounter a different subset of the design challenges. We demonstrate how these findings reveal new insights for designers, researchers, and design tool makers in productively addressing the challenges of human-AI interaction going forward.","3901":"Recently, artificial intelligence (AI) and blockchain have become two of the most trending and disruptive technologies. Blockchain technology has the ability to automate payment in cryptocurrency and to provide access to a shared ledger of data, transactions, and logs in a decentralized, secure, and trusted manner. Also with smart contracts, blockchain has the ability to govern interactions among participants with no intermediary or a trusted third party. AI, on the other hand, offers intelligence and decision-making capabilities for machines similar to humans. In this paper, we present a detailed survey on blockchain applications for AI. We review the literature, tabulate, and summarize the emerging blockchain applications, platforms, and protocols specifically targeting AI area. We also identify and discuss open research challenges of utilizing blockchain technologies for AI.","3902":null,"3903":"Artificial Intelligence (AI) is already having a major impact on society. As a result, many organizations have launched a wide range of initiatives to establish ethical principles for the adoption of socially beneficial AI. Unfortunately, the sheer volume of proposed principles threatens to overwhelm and confuse. How might this problem of \u2018principle proliferation\u2019 be solved? In this paper, we report the results of a fine-grained analysis of several of the highest-profile sets of ethical principles for AI. We assess whether these principles converge upon a set of agreed-upon principles, or diverge, with significant disagreement over what constitutes \u2018ethical AI.\u2019 Our analysis finds a high degree of overlap among the sets of principles we analyze. We then identify an overarching framework consisting of five core principles for ethical AI. Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. On the basis of our comparative analysis, we argue that a new principle is needed in addition: explicability, understood as incorporating both the epistemological sense of intelligibility (as an answer to the question \u2018how does it work?\u2019) and in the ethical sense of accountability (as an answer to the question: \u2018who is responsible for the way it works?\u2019). In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.KeywordsAccountability; Autonomy; Artificial Intelligence; Beneficence; Ethics; Explicability; Fairness; Intelligibility; Justice; Non-maleficence.","3904":null,"3905":"As AI-powered systems increasingly mediate consequential decision-making, their explainability is critical for end-users to take informed and accountable actions. Explanations in human-human interactions are socially-situated. AI systems are often socio-organizationally embedded. However, Explainable AI (XAI) approaches have been predominantly algorithm-centered. We take a developmental step towards socially-situated XAI by introducing and exploring Social Transparency (ST), a sociotechnically informed perspective that incorporates the socio-organizational context into explaining AI-mediated decision-making. To explore ST conceptually, we conducted interviews with 29 AI users and practitioners grounded in a speculative design scenario. We suggested constitutive design elements of ST and developed a conceptual framework to unpack ST\u2019s effect and implications at the technical, decision-making, and organizational level. The framework showcases how ST can potentially calibrate trust in AI, improve decision-making, facilitate organizational collective actions, and cultivate holistic explainability. Our work contributes to the discourse of Human-Centered XAI by expanding the design space of XAI.","3906":null,"3907":null,"3908":null,"3909":null,"3910":null,"3911":null,"3912":"There is a tendency across different subfields in AI to valorize a small collection of influential benchmarks. These benchmarks operate as stand-ins for a range of anointed common problems that are frequently framed as foundational milestones on the path towards flexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore the limits of such benchmarks in order to reveal the construct validity issues in their framing as the functionally\"general\"broad measures of progress they are set up to be.","3913":"Libratus versus humans Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold'em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent. Science, this issue p. 418 An artificial intelligence program called Libratus played 120,000 hands of a two-player variant of poker and beat four leading human professionals. No-limit Texas hold\u2019em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold\u2019em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.","3914":null,"3915":"Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that \"All models are wrong but some are useful.\" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a \"do it yourself kit\" for explanations, allowing a practitioner to directly answer \"what if questions\" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.","3916":"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (\n https:\/\/github.com\/ibm\/aif360 \n). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.","3917":"Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.","3918":"Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.","3919":"This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.","3920":"Recently, along with the rapid development of mobile communication technology, edge computing theory and techniques have been attracting more and more attention from global researchers and engineers, which can significantly bridge the capacity of cloud and requirement of devices by the network edges, and thus can accelerate content delivery and improve the quality of mobile services. In order to bring more intelligence to edge systems, compared to traditional optimization methodology, and driven by the current deep learning techniques, we propose to integrate the Deep Reinforcement Learning techniques and Federated Learning framework with mobile edge systems, for optimizing mobile edge computing, caching and communication. And thus, we design the \"In-Edge AI\" framework in order to intelligently utilize the collaboration among devices and edge nodes to exchange the learning parameters for a better training and inference of the models, and thus to carry out dynamic system-level optimization and application-level enhancement while reducing the unnecessary system communication load. \"In-Edge AI\" is evaluated and proved to have near-optimal performance but relatively low overhead of learning, while the system is cognitive and adaptive to mobile communication systems. Finally, we discuss several related challenges and opportunities for unveili","3921":null,"3922":null,"3923":"The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.","3924":"We identify three specific areas of focus to advance human-centered AI:\u2022 Designers and systems must understand the context of use and sense changes over time: Successful AI Engineering depends on the team\u2019s ability to identify and articulate the desired system outcome and understand human and contextual factors affecting the outcome. The system itself must be able to learn when shifts in context have occurred. What are the best ways to maintain clarity around operational intent and mechanisms for adapting and evolving systems based on dynamic contexts and user needs?\u2022 Development of tools, processes, and practices to scope and facilitate human-machine teaming: Implementation of AI systems entails high levels of interdependence between human and machine. Adoption of AI systems requires the primary users to interact with and understand systems, gaining appropriate levels of trust. Every AI system needs to be designed to recognize boundaries and unfamiliar scenarios, and to provide transparency regarding its limitations.\u2022 Methods, mechanisms, and mindsets to engage in critical oversight: AI systems learn through data and observations, rather than being explicitly programmed for a deterministic outcome. Critical and reflective oversight by organizations, teams, and individuals that create and use AI systems is needed to uphold ethical principles and proactively consider the risks of bias, misuse, abuse, and unintended consequences through design, development, and ongoing deployment.For each area, we identify ongoing work as well and challenges and opportunities in developing and deploying AI systems with confidence.","3925":"Although AI is transforming the world, there are serious concerns about its ability to behave and make decisions responsibly. Many ethical regulations, principles, and frameworks for responsible AI have been issued recently. However, they are high level and difficult to put into practice. On the other hand, most AI researchers focus on algorithmic solutions, while the responsible AI challenges actually crosscut the entire engineering lifecycle and components of AI systems. To close the gap in operationalizing responsible AI, this paper aims to develop a roadmap on software engineering for responsible AI. The roadmap focuses on (i) establishing multi-level governance for responsible AI systems, (ii) setting up the development processes incorporating process-oriented practices for responsible AI systems, and (iii) building responsible-AI-by-design into AI systems through system-level architectural style, patterns and techniques. CCS CONCEPTS \u2022 Software and its engineering;","3926":"\n The need for interpretable and accountable intelligent systems grows along with the prevalence of\n artificial intelligence\n (\n AI\n ) applications used in everyday life.\n Explainable AI\n (\n XAI\n ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.\n","3927":"Pairing prediction and robotic synthesis Progress in automated synthesis of organic compounds has been proceeding along parallel tracks. One goal is algorithmic prediction of viable routes to a desired compound; the other is implementation of a known reaction sequence on a platform that needs little to no human intervention. Coley et al. now report preliminary integration of these two protocols. They paired a retrosynthesis prediction algorithm with a robotically reconfigurable flow apparatus. Human intervention was still required to supplement the predictor with practical considerations such as solvent choice and precise stoichiometry, although predictions should improve as accessible data accumulate for training. Science, this issue p. eaax1566 An automated synthesis platform conducts reactions on the basis of a human-devised workflow informed by a retrosynthesis algorithm. INTRODUCTION The ability to synthesize complex organic molecules is essential to the discovery and manufacture of functional compounds, including small-molecule medicines. Despite advances in laboratory automation, the identification and development of synthetic routes remain a manual process and experimental synthesis platforms must be manually configured to suit the type of chemistry to be performed, requiring time and effort investment from expert chemists. The ideal automated synthesis platform would be capable of planning its own synthetic routes and executing them under conditions that facilitate scale-up to production goals. Individual elements of the chemical development process (design, route development, experimental configuration, and execution) have been streamlined in previous studies, but none has presented a path toward integration of computer-aided synthesis planning (CASP), expert refined chemical recipe generation, and robotically executed chemical synthesis. RATIONALE We describe an approach toward automated, scalable synthesis that combines techniques in artificial intelligence (AI) for planning and robotics for execution. Millions of previously published reactions inform the computational design of synthetic routes; expert-refined chemical recipe files (CRFs) are run on a robotic flow chemistry platform for scalable, reproducible synthesis. This development strategy augments a chemist\u2019s ability to approach target-oriented flow synthesis while substantially reducing the necessary information gathering and manual effort. RESULTS We developed an open source software suite for CASP trained on millions of reactions from the Reaxys database and the U.S. Patent and Trademark Office. The software was designed to generalize known chemical reactions to new substrates by learning to apply retrosynthetic transformations, to identify suitable reaction conditions, and to evaluate whether reactions are likely to be successful when attempted experimentally. Suggested routes partially populate CRFs, which require additional details from chemist users to define residence times, stoichiometries, and concentrations that are compatible with continuous flow. To execute these syntheses, a robotic arm assembles modular process units (reactors and separators) into a continuous flow path according to the desired process configuration defined in the CRF. The robot also connects reagent lines and computer-controlled pumps to reactor inlets through a fluidic switchboard. When that is completed, the system primes the lines and starts the synthesis. After a specified synthesis time, the system flushes the lines with a cleaning solvent, and the robotic arm disconnects reagent lines and removes process modules to their appropriate storage locations. This paradigm of flow chemistry development was demonstrated for a suite of 15 medicinally relevant small molecules. In order of increasing complexity, we investigated the synthesis of aspirin and secnidazole run back to back; lidocaine and diazepam run back to back to use a common feedstock; (S)-warfarin and safinamide to demonstrate the planning program\u2019s stereochemical awareness; and two compound libraries: a family of five ACE inhibitors including quinapril and a family of four nonsteroidal anti-inflammatory drugs including celecoxib. These targets required a total of eight particular retrosynthetic routes and nine specific process configurations. CONCLUSION The software and platform herein represent a milestone on the path toward fully autonomous chemical synthesis, where routes still require human input and process development. Over time, the results generated by this and similar automated experimental platforms may reduce our reliance on historical reaction data, particularly in combination with smaller-scale flow-screening platforms. Increased availability of reaction data will further enable robotically realized syntheses based on AI recommendations, relieving expert chemists of manual tasks so that they may focus on new ideas. Planning and execution. A robotically reconfigurable flow chemistry platform performs multistep chemical syntheses planned in part by AI. The synthesis of complex organic molecules requires several stages, from ideation to execution, that require time and effort investment from expert chemists. Here, we report a step toward a paradigm of chemical synthesis that relieves chemists from routine tasks, combining artificial intelligence\u2013driven synthesis planning and a robotically controlled experimental platform. Synthetic routes are proposed through generalization of millions of published chemical reactions and validated in silico to maximize their likelihood of success. Additional implementation details are determined by expert chemists and recorded in reusable recipe files, which are executed by a modular continuous-flow platform that is automatically reconfigured by a robotic arm to set up the required unit operations and carry out the reaction. This strategy for computer-augmented chemical synthesis is demonstrated for 15 drug or drug-like substances.","3928":"Digital twinning is one of the top ten technology trends in the last couple of years, due to its high applicability in the industrial sector. The integration of big data analytics and artificial intelligence\/machine learning (AI-ML) techniques with digital twinning, further enriches its significance and research potential with new opportunities and unique challenges. To date, a number of scientific models have been designed and implemented related to this evolving topic. However, there is no systematic review of digital twinning, particularly focusing on the role of AI-ML and big data, to guide the academia and industry towards future developments. Therefore, this article emphasizes the role of big data and AI-ML in the creation of digital twins (DTs) or DT-based systems for various industrial applications, by highlighting the current state-of-the-art deployments. We performed a systematic review on top of multidisciplinary electronic bibliographic databases, in addition to existing patents in the field. Also, we identified development-tools that can facilitate various levels of the digital twinning. Further, we designed a big data driven and AI-enriched reference architecture that leads developers to a complete DT-enabled system. Finally, we highlighted the research potential of AI-ML for digital twinning by unveiling challenges and current opportunities.","3929":null,"3930":null,"3931":null,"3932":"Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.","3933":"Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI\/ML in the medical domain, and specifically help to facilitate transparency and trust.","3934":null,"3935":"Technology advances are making tech more...human. This changes everything you thought you knew about innovation and strategy. In their groundbreaking book Human + Machine, Accenture technology leaders Paul Daugherty and H. James Wilson showed how leading organizations use the power of humanmachine collaboration to transform their processes and their bottom lines. Now, as AI continues to rapidly impact both life and work, those companies and other pioneers across industries are tipping the balance even more strikingly toward the human with technology-led strategy that is reshaping the very nature of innovation. In Radically Human, Daugherty and Wilson show this profound shift, fast-forwarded by the pandemic, toward more human\u2014and more humane\u2014technology. Artificial intelligence is becoming less artificial and more intelligent. Instead of data-hungry approaches to AI, innovators are pursuing dataefficient approaches that enable machines to learn as humans do. Instead of replacing workers with machines, they are unleashing human expertise to create human-centered AI. In place of lumbering legacy IT systems, they are building cloud-first IT architectures able to continuously adapt to a world of billions of connected devices. And they are pursuing strategies that will take their place alongside classic winning business formulas like disruptive innovation. These against-the-grain approaches to the basic building blocks of business\u2014Intelligence, Data, Experience, Architecture, and Strategy (IDEAS)\u2014are transforming competition. Industrial giants and startups alike are drawing on this radically human IDEAS framework to create new business models, optimize postpandemic approaches to work and talent, rebuild trust with their stakeholders, and show the way toward a sustainable future. With compelling insights and fresh examples from a variety of industries, Radically Human will forever change the way you think about, practice, and win with innovation. Artificial intelligence threatens to disrupt the professions as it has manufacturing. Frank Pasquale argues that law and policy can avert this outcome and promote better ones: instead of replacing humans, technology can make our labor more valuable. Through regulation, we can ensure that AI promotes inclusive prosperity. Two statistics professors describe how intelligent machines are changing the world and use stories, rather than equations, to explain the mathematical language they use and provide a better grasp on concepts in data and probability. AI is radically transforming business. Are you ready? Look around you. Artificial intelligence is no longer just a futuristic notion. It's here right now--in software that senses what we need, supply chains that \"think\" in real time, and robots that respond to changes in their environment. Twenty-first-century pioneer companies are already using AI to innovate and grow fast. The bottom line is this: Businesses that understand how to harness AI can surge ahead. Those that","3936":"The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people\u2019s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.","3937":"AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2\/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.","3938":"As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI models and AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our survey highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other's work and produce generalizable scientific knowledge. We also hope this survey will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.","3939":"Welcome to the fourth edition of the AI Index Report. This year we significantly expanded the amount of data available in the report, worked with a broader set of external organizations to calibrate our data, and deepened our connections with the Stanford Institute for Human-Centered Artificial Intelligence (HAI). The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Its mission is to provide unbiased, rigorously vetted, and globally sourced data for policymakers, researchers, executives, journalists, and the general public to develop intuitions about the complex field of AI. The report aims to be the most credible and authoritative source for data and insights about AI in the world.","3940":"There has been an emerging paradigm shift from the era of \u201cinternet AI\u201d to \u201cembodied AI,\u201d where AI algorithms and agents no longer learn from datasets of images, videos or text curated primarily from the internet. Instead, they learn through interactions with their environments from an egocentric perception similar to humans. Consequently, there has been substantial growth in the demand for embodied AI simulators to support various embodied AI research tasks. This growing interest in embodied AI is beneficial to the greater pursuit of Artificial General Intelligence (AGI), but there has not been a contemporary and comprehensive survey of this field. This paper aims to provide an encyclopedic survey for the field of embodied AI, from its simulators to its research. By evaluating nine current embodied AI simulators with our proposed seven features, this paper aims to understand the simulators in their provision for use in embodied AI research and their limitations. Lastly, this paper surveys the three main research tasks in embodied AI \u2013 visual exploration, visual navigation and embodied question answering (QA), covering the state-of-the-art approaches, evaluation metrics and datasets. Finally, with the new insights revealed through surveying the field, the paper will provide suggestions for simulator-for-task selections and recommendations for the future directions of the field.","3941":"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.","3942":"This study provided a content analysis of studies aiming to disclose how artificial intelligence (AI) has been applied to the education sector and explore the potential research trends and challenges of AI in education. A total of 100 papers including 63 empirical papers (74 studies) and 37 analytic papers were selected from the education and educational research category of Social Sciences Citation Index database from 2010 to 2020. The content analysis showed that the research questions could be classified into development layer (classification, matching, recommendation, and deep learning), application layer (feedback, reasoning, and adaptive learning), and integration layer (affection computing, role-playing, immersive learning, and gamification). Moreover, four research trends, including Internet of Things, swarm intelligence, deep learning, and neuroscience, as well as an assessment of AI in education, were suggested for further investigation. However, we also proposed the challenges in education may be caused by AI with regard to inappropriate use of AI techniques, changing roles of teachers and students, as well as social and ethical issues. The results provide insights into an overview of the AI used for education domain, which helps to strengthen the theoretical foundation of AI in education and provides a promising channel for educators and AI engineers to carry out further collaborative research.","3943":"Technologies related to artificial intelligence (AI) have a strong impact on the changes of research and creative practices in visual arts. The growing number of research initiatives and creative applications that emerge in the intersection of AI and art motivates us to examine and discuss the creative and explorative potentials of AI technologies in the context of art. This article provides an integrated review of two facets of AI and art: (1) AI is used for art analysis and employed on digitized artwork collections, or (2) AI is used for creative purposes and generating novel artworks. In the context of AI-related research for art understanding, we present a comprehensive overview of artwork datasets and recent works that address a variety of tasks such as classification, object detection, similarity retrieval, multimodal representations, and computational aesthetics, among others. In relation to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and consolidate related works that deal with those topics in detail. Finally, we provide a concise outlook on the future progression and potential impact of AI technologies on our understanding and creation of art.","3944":"In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone\u2019s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.","3945":"Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.","3946":"Background Artificial intelligence (AI) is increasingly being used in healthcare. Here, AI-based chatbot systems can act as automated conversational agents, capable of promoting health, providing education, and potentially prompting behaviour change. Exploring the motivation to use health chatbots is required to predict uptake; however, few studies to date have explored their acceptability. This research aimed to explore participants\u2019 willingness to engage with AI-led health chatbots. Methods The study incorporated semi-structured interviews (N-29) which informed the development of an online survey (N-216) advertised via social media. Interviews were recorded, transcribed verbatim and analysed thematically. A survey of 24 items explored demographic and attitudinal variables, including acceptability and perceived utility. The quantitative data were analysed using binary regressions with a single categorical predictor. Results Three broad themes: \u2018Understanding of chatbots\u2019, \u2018AI hesitancy\u2019 and \u2018Motivations for health chatbots\u2019 were identified, outlining concerns about accuracy, cyber-security, and the inability of AI-led services to empathise. The survey showed moderate acceptability (67%), correlated negatively with perceived poorer IT skills OR\u2009=\u20090.32 [CI95%:0.13\u20130.78] and dislike for talking to computers OR\u2009=\u20090.77 [CI95%:0.60\u20130.99] as well as positively correlated with perceived utility OR\u2009=\u20095.10 [CI95%:3.08\u20138.43], positive attitude OR\u2009=\u20092.71 [CI95%:1.77\u20134.16] and perceived trustworthiness OR\u2009=\u20091.92 [CI95%:1.13\u20133.25]. Conclusion Most internet users would be receptive to using health chatbots, although hesitancy regarding this technology is likely to compromise engagement. Intervention designers focusing on AI-led health chatbots need to employ user-centred and theory-based approaches addressing patients\u2019 concerns and optimising user experience in order to achieve the best uptake and utilisation. Patients\u2019 perspectives, motivation and capabilities need to be taken into account when developing and assessing the effectiveness of health chatbots.","3947":"AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such AI-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate? We argue \"not necessarily\" --- predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the team's expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration.","3948":null,"3949":"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.","3950":"Artificial intelligence (AI) has achieved remarkable breakthroughs in a wide range of fields, ranging from speech processing, image classification to drug discovery. This is driven by the explosive growth of data, advances in machine learning (especially deep learning), and the easy access to powerful computing resources. Particularly, the wide scale deployment of edge devices (e.g., IoT devices) generates an unprecedented scale of data, which provides the opportunity to derive accurate models and develop various intelligent applications at the network edge. However, such enormous data cannot all be sent to the cloud for processing, due to the varying channel quality, traffic congestion and\/or privacy concerns, and the enormous energy consumption. By pushing inference and training processes of AI models to edge nodes, edge AI has emerged as a promising alternative. AI at the edge requires close cooperation among edge devices, such as smart phones and smart vehicles, and edge servers at the wireless access points and base stations, which however result in heavy communication overheads. In this paper, we present a comprehensive survey of the recent developments in various techniques for overcoming these communication challenges. Specifically, we first identify key communication challenges in edge AI systems. We then introduce communication-efficient techniques, from both algorithmic and system perspectives for training and inference tasks at the network edge. Potential future research directions are also highlighted.","3951":"Digital twin (DT) and artificial intelligence (AI) technologies have grown rapidly in recent years and are considered by both academia and industry to be key enablers for Industry 4.0. As a digital replica of a physical entity, the basis of DT is the infrastructure and data, the core is the algorithm and model, and the application is the software and service. The grounding of DT and AI in industrial sectors is even more dependent on the systematic and in-depth integration of domain-specific expertise. This survey comprehensively reviews over 300 manuscripts on AI-driven DT technologies of Industry 4.0 used over the past five years and summarizes their general developments and the current state of AI-integration in the fields of smart manufacturing and advanced robotics. These cover conventional sophisticated metal machining and industrial automation as well as emerging techniques, such as 3D printing and human\u2013robot interaction\/cooperation. Furthermore, advantages of AI-driven DTs in the context of sustainable development are elaborated. Practical challenges and development prospects of AI-driven DTs are discussed with a respective focus on different levels. A route for AI-integration in multiscale\/fidelity DTs with multiscale\/fidelity data sources in Industry 4.0 is outlined.","3952":"How to integrate artificial intelligence (AI) technologies in the functioning and structures of our society has become a concern of contemporary politics and public debates. In this paper, we investigate national AI strategies as a peculiar form of co-shaping this development, a hybrid of policy and discourse that offers imaginaries, allocates resources, and sets rules. Conceptually, the paper is informed by sociotechnical imaginaries, the sociology of expectations, myths, and the sublime. Empirically we analyze AI policy documents of four key players in the field, namely China, the United States, France, and Germany. The results show that the narrative construction of AI strategies is strikingly similar: they all establish AI as an inevitable and massively disrupting technological development by building on rhetorical devices such as a grand legacy and international competition. Having established this inevitable, yet uncertain, AI future, national leaders proclaim leadership intervention and articulate opportunities and distinct national pathways. While this narrative construction is quite uniform, the respective AI imaginaries are remarkably different, reflecting the vast cultural, political, and economic differences of the countries under study. As governments endow these imaginary pathways with massive resources and investments, they contribute to coproducing the installment of these futures and, thus, yield a performative lock-in function.","3953":null,"3954":"This book explores the intertwining domains of artificial intelligence (AI) and ethics\u2014two highly divergent fields which at first seem to have nothing to do with one another. AI is a collection of computational methods for studying human knowledge, learning, and behavior, including by building agents able to know, learn, and behave. Ethics is a body of human knowledge\u2014far from completely understood\u2014that helps agents (humans today, but perhaps eventually robots and other AIs) decide how they and others should behave. Despite these differences, however, the rapid development in AI technology today has led to a growing number of ethical issues in a multitude of fields, ranging from disciplines as far-reaching as international human rights law to issues as intimate as personal identity and sexuality. In fact, the number and variety of topics in this volume illustrate the width, diversity of content, and at times exasperating vagueness of the boundaries of \u201cAI Ethics\u201d as a domain of inquiry. Within this discourse, the book points to the capacity of sociotechnical systems that utilize data-driven algorithms to classify, to make decisions, and to control complex systems. Given the wide-reaching and often intimate impact these AI systems have on daily human lives, this volume attempts to address the increasingly complicated relations between humanity and artificial intelligence. It considers not only how humanity must conduct themselves toward AI but also how AI must behave toward humanity.","3955":null,"3956":"With the global rollout of fifth generation (5G) networks, it is necessary to look beyond 5G and envision 6G networks. 6G networks are expected to have space-air-ground integrated networks, advanced network virtualization, and ubiquitous intelligence. This article presents an artificial intelligence (AI)-native network slicing architecture for 6G networks to enable the synergy of AI and network slicing, thereby facilitating intelligent network management and supporting emerging AI services. AI-based solutions are first discussed across the network slicing life cycle to intelligently manage network slices (i.e., AI for slicing). Then network slicing solutions are studied to support emerging AI services by constructing AI instances and performing efficient resource management (i.e., slicing for AI). Finally, a case study is presented, followed by a discussion of open research issues that are essential for AI-native network slicing in 6G networks.","3957":"The very first infected novel coronavirus case (COVID-19) was found in Hubei, China in Dec. 2019. The COVID-19 pandemic has spread over 214 countries and areas in the world, and has significantly affected every aspect of our daily lives. At the time of writing this article, the numbers of infected cases and deaths still increase significantly and have no sign of a well-controlled situation, e.g., as of 13 July 2020, from a total number of around 13.1 million positive cases, 571,527 deaths were reported in the world. Motivated by recent advances and applications of artificial intelligence (AI) and big data in various areas, this paper aims at emphasizing their importance in responding to the COVID-19 outbreak and preventing the severe effects of the COVID-19 pandemic. We firstly present an overview of AI and big data, then identify the applications aimed at fighting against COVID-19, next highlight challenges and issues associated with state-of-the-art solutions, and finally come up with recommendations for the communications to effectively control the COVID-19 situation. It is expected that this paper provides researchers and communities with new insights into the ways AI and big data improve the COVID-19 situation, and drives further studies in stopping the COVID-19 outbreak.","3958":null,"3959":null,"3960":"ABSTRACT The data-centric revolution generally celebrates the proliferation of business analytics and AI in exploiting firm\u2019s potential and success. However, there is a lack of research on how the unintended consequences of AI integrated business analytics (AI-BA) influence a firm\u2019s overall competitive advantage. In this backdrop, this study aims to identify how factors, such as AI-BA opacity, suboptimal business decisions and perceived risk are responsible for a firm\u2019s operational inefficiency and competitive disadvantage. Drawing on the resource-based view, dynamic capability view, and contingency theory, the proposed research model captures the components and effects of an AI-BA opacity on a firm\u2019s risk environment and negative performance. The data were gathered from 355 operational, mid-level and senior managers from various service sectors across all different size organisations in India. The results indicated that lack of governance, poor data quality, and inefficient training of key employees led to an AI-BA opacity. It then triggers suboptimal business decisions and higher perceived risk resulting in operational inefficiency. The findings show that operational inefficiency significantly contributes to negative sales growth and employees\u2019 dissatisfaction, which result in a competitive disadvantage for a firm. The findings also highlight the significant moderating effect of contingency plan in the nomological chain.","3961":"Visual recognition ecosystems (e.g. ImageNet, Pascal, COCO) have undeniably played a prevailing role in the evolution of modern computer vision. We argue that interactive and embodied visual AI has reached a stage of development similar to visual recognition prior to the advent of these ecosystems. Recently, various synthetic environments have been introduced to facilitate research in embodied AI. Notwithstanding this progress, the crucial question of how well models trained in simulation generalize to reality has remained largely unanswered. The creation of a comparable ecosystem for simulation-to-real embodied AI presents many challenges: (1) the inherently interactive nature of the problem, (2) the need for tight alignments between real and simulated worlds, (3) the difficulty of replicating physical conditions for repeatable experiments, (4) and the associated cost. In this paper, we introduce RoboTHOR to democratize research in interactive and embodied visual AI. RoboTHOR offers a framework of simulated environments paired with physical counterparts to systematically explore and overcome the challenges of simulation-to-real transfer, and a platform where researchers across the globe can remotely test their embodied models in the physical world. As a first benchmark, our experiments show there exists a significant gap between the performance of models trained in simulation when they are tested in both simulations and their carefully constructed physical analogs. We hope that RoboTHOR will spur the next stage of evolution in embodied computer vision.","3962":null,"3963":"\n Advances in personalization algorithms and other applications of machine learning have vastly enhanced the ease and convenience of our media and communication experiences, but they have also raised significant concerns about privacy, transparency of technologies and human control over their operations. Going forth, reconciling such tensions between machine agency and human agency will be important in the era of artificial intelligence (AI), as machines get more agentic and media experiences become increasingly determined by algorithms. Theory and research should be geared toward a deeper understanding of the human experience of algorithms in general and the psychology of Human\u2013AI interaction (HAII) in particular. This article proposes some directions by applying the dual-process framework of the Theory of Interactive Media Effects (TIME) for studying the symbolic and enabling effects of the affordances of AI-driven media on user perceptions and experiences.","3964":"This study aims to investigate the customers\u2019 behavioral intention and actual usage (AUE) of artificial intelligence (AI)-powered chatbots for hospitality and tourism in India by extending the technology adoption model (TAM) with context-specific variables.,To understand the customers\u2019 behavioral intention and AUE of AI-powered chatbots for tourism, the mixed-method design was used whereby qualitative and quantitative techniques were combined. A total of 36 senior managers and executives from the travel agencies were interviewed and the analysis of interview data was done using NVivo 8.0 software. A total of 1,480 customers were surveyed and the partial least squares structural equation modeling technique was used for data analysis.,As per the results, the predictors of chatbot adoption intention (AIN) are perceived ease of use, perceived usefulness, perceived trust (PTR), perceived intelligence (PNT) and anthropomorphism (ANM). Technological anxiety (TXN) does not influence the chatbot AIN. Stickiness to traditional human travel agents negatively moderates the relation of AIN and AUE of chatbots in tourism and provides deeper insights into manager\u2019s commitment to providing travel planning services using AI-based chatbots.,This research presents unique practical insights to the practitioners, managers and executives in the tourism industry, system designers and developers of AI-based chatbot technologies to understand the antecedents of chatbot adoption by travelers. TXN is a vital concern for the customers; so, designers and developers should ensure that chatbots are easily accessible, have a user-friendly interface, be more human-like and communicate in various native languages with the customers.,This study contributes theoretically by extending the TAM to provide better explanatory power with human\u2013robot interaction context-specific constructs \u2013 PTR, PNT, ANM and TXN \u2013 to examine the customers\u2019 chatbot AIN. This is the first step in the direction to empirically test and validate a theoretical model for chatbots\u2019 adoption and usage, which is a disruptive technology in the hospitality and tourism sector in an emerging economy such as India.","3965":"Although rapid advances in machine learning have made it increasingly applicable to expert decision-making, the delivery of accurate algorithmic predictions alone is insufficient for effective human-AI collaboration. In this work, we investigate the key types of information medical experts desire when they are first introduced to a diagnostic AI assistant. In a qualitative lab study, we interviewed 21 pathologists before, during, and after being presented deep neural network (DNN) predictions for prostate cancer diagnosis, to learn the types of information that they desired about the AI assistant. Our findings reveal that, far beyond understanding the local, case-specific reasoning behind any model decision, clinicians desired upfront information about basic, global properties of the model, such as its known strengths and limitations, its subjective point-of-view, and its overall design objective--what it's designed to be optimized for. Participants compared these information needs to the collaborative mental models they develop of their medical colleagues when seeking a second opinion: the medical perspectives and standards that those colleagues embody, and the compatibility of those perspectives with their own diagnostic patterns. These findings broaden and enrich discussions surrounding AI transparency for collaborative decision-making, providing a richer understanding of what experts find important in their introduction to AI assistants before integrating them into routine practice.","3966":null,"3967":"PurposeThe automation of services is rapidly growing, led by sectors such as banking and financial investment. The growing number of investments managed by artificial intelligence (AI) suggests that this technology-based service will become increasingly popular. This study examines how customers' technology readiness and service awareness affect their intention to use analytical AI investment services.Design\/methodology\/approachThe automation of services is rapidly growing, led by sectors such as banking and financial investment. The growing number of investments managed by AI suggests that this technology-based service will become increasingly popular. This study examines how customers' technology readiness and service awareness affect their intention to use analytical AI investment services.FindingsThe results indicated that customers' technological optimism increases, and insecurity decreases, their intention to use robo-advisors. Surprisingly, feelings of technological discomfort positively influenced robo-advisor adoption. This interesting finding challenges previous insights into technology adoption and value co-creation as analytical AI puts customers into a very passive role and reduces barriers to technology adoption. The research also analyzes how consumers become aware of robo-advisors, and how this influences their acceptance.Originality\/valueThis is the first study to analyze the role of customers' technology readiness in the adoption of analytical AI. The authors link the findings to previous technology adoption and automated services' literature and provide specific managerial implications and avenues for further research.","3968":"The development of AI applications is a multidisciplinary effort, involving multiple roles collaborating with the AI developers, an umbrella term we use to include data scientists and other AI-adjacent roles on the same team. During these collaborations, there is a knowledge mismatch between AI developers, who are skilled in data science, and external stakeholders who are typically not. This difference leads to communication gaps, and the onus falls on AI developers to explain data science concepts to their collaborators. In this paper, we report on a study including analyses of both interviews with AI developers and artifacts they produced for communication. Using the analytic lens of shared mental models, we report on the types of communication gaps that AI developers face, how AI developers communicate across disciplinary and organizational boundaries, and how they simultaneously manage issues regarding trust and expectations.","3969":"In the current era, people and society have grown increasingly reliant on artificial intelligence (AI) technologies. AI has the potential to drive us towards a future in which all of humanity flourishes. It also comes with substantial risks for oppression and calamity. Discussions about whether we should (re)trust AI have repeatedly emerged in recent years and in many quarters, including industry, academia, healthcare, services, and so on. Technologists and AI researchers have a responsibility to develop trustworthy AI systems. They have responded with great effort to design more responsible AI algorithms. However, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI\u2019s indifferent behavior. In this survey, we provide a systematic framework of Socially Responsible AI Algorithms that aims to examine the subjects of AI indifference and the need for socially responsible AI algorithms, define the objectives, and introduce the means by which we may achieve these objectives. We further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention\/mitigation. \nThis article appears in the special track on AI & Society.","3970":"Various tools and practices have been developed to support practitioners in identifying, assessing, and mitigating fairness-related harms caused by AI systems. However, prior research has highlighted gaps between the intended design of these tools and practices and their use within particular contexts, including gaps caused by the role that organizational factors play in shaping fairness work. In this paper, we investigate these gaps for one such practice: disaggregated evaluations of AI systems, intended to uncover performance disparities between demographic groups. By conducting semi-structured interviews and structured workshops with thirty-three AI practitioners from ten teams at three technology companies, we identify practitioners' processes, challenges, and needs for support when designing disaggregated evaluations. We find that practitioners face challenges when choosing performance metrics, identifying the most relevant direct stakeholders and demographic groups on which to focus, and collecting datasets with which to conduct disaggregated evaluations. More generally, we identify impacts on fairness work stemming from a lack of engagement with direct stakeholders or domain experts, business imperatives that prioritize customers over marginalized groups, and the drive to deploy AI systems at scale.","3971":"The convergence of artificial intelligence (AI) and precision medicine promises to revolutionize health care. Precision medicine methods identify phenotypes of patients with less\u2010common responses to treatment or unique healthcare needs. AI leverages sophisticated computation and inference to generate insights, enables the system to reason and learn, and empowers clinician decision making through augmented intelligence. Recent literature suggests that translational research exploring this convergence will help solve the most difficult challenges facing precision medicine, especially those in which nongenomic and genomic determinants, combined with information from patient symptoms, clinical history, and lifestyles, will facilitate personalized diagnosis and prognostication.","3972":"AI seems like the perfect response to the growing challenges of content moderation on social media platforms: the immense scale of the data, the relentlessness of the violations, and the need for human judgments without wanting humans to have to make them. The push toward automated content moderation is often justified as a necessary response to the scale: the enormity of social media platforms like Facebook and YouTube stands as the reason why AI approaches are desirable, even inevitable. But even if we could effectively automate content moderation, it is not clear that we should.","3973":"PurposeThe main purpose of our study is to analyze the influence of Artificial Intelligence (AI) on firm performance, notably by building on the business value of AI-based transformation projects. This study was conducted using a four-step sequential approach: (1) analysis of AI and AI concepts\/technologies; (2) in-depth exploration of case studies from a great number of industrial sectors; (3) data collection from the databases (websites) of AI-based solution providers; and (4) a review of AI literature to identify their impact on the performance of organizations while highlighting the business value of AI-enabled projects transformation within organizations.Design\/methodology\/approachThis study has called on the theory of IT capabilities to seize the influence of AI business value on firm performance (at the organizational and process levels). The research process (responding to the research question, making discussions, interpretations and comparisons, and formulating recommendations) was based on a review of 500 case studies from IBM, AWS, Cloudera, Nvidia, Conversica, Universal Robots websites, etc. Studying the influence of AI on the performance of organizations, and more specifically, of the business value of such organizations\u2019 AI-enabled transformation projects, required us to make an archival data analysis following the three steps, namely the conceptual phase, the refinement and development phase, and the assessment phase.FindingsAI covers a wide range of technologies, including machine translation, chatbots and self-learning algorithms, all of which can allow individuals to better understand their environment and act accordingly. Organizations have been adopting AI technological innovations with a view to adapting to or disrupting their ecosystem while developing and optimizing their strategic and competitive advantages. AI fully expresses its potential through its ability to optimize existing processes and improve automation, information and transformation effects, but also to detect, predict and interact with humans. Thus, the results of our study have highlighted such AI benefits in organizations, and more specifically, its ability to improve on performance at both the organizational (financial, marketing and administrative) and process levels. By building on these AI attributes, organizations can, therefore, enhance the business value of their transformed projects. The same results also showed that organizations achieve performance through AI capabilities only when they use their features\/technologies to reconfigure their processes.Research limitations\/implicationsAI obviously influences the way businesses are done today. Therefore, practitioners and researchers need to consider AI as a valuable support or even a pilot for a new business model. For the purpose of our study, we adopted a research framework geared toward a more inclusive and comprehensive approach so as to better account for the intangible benefits of AI within organizations. In terms of interest, this study nurtures a scientific interest, which aims at proposing a model for analyzing the influence of AI on the performance of organizations, and at the same time, filling the associated gap in the literature. As for the managerial interest, our study aims to provide managers with elements to be reconfigured or added in order to take advantage of the full benefits of AI, and therefore improve organizations\u2019 performance, the profitability of their investments in AI transformation projects, and some competitive advantage. This study also allows managers to consider AI not as a single technology but as a set\/combination of several different configurations of IT in the various company\u2019s business areas because multiple key elements must be brought together to ensure the success of AI: data, talent mix, domain knowledge, key decisions, external partnerships and scalable infrastructure.Originality\/valueThis article analyses case studies on the reuse of secondary data from AI deployment reports in organizations. The transformation of projects based on the use of AI focuses mainly on business process innovations and indirectly on those occurring at the organizational level. Thus, 500 case studies are being examined to provide significant and tangible evidence about the business value of AI-based projects and the impact of AI on firm performance. More specifically, this article, through these case studies, exposes the influence of AI at both the organizational and process performance levels, while considering it not as a single technology but as a set\/combination of the several different configurations of IT in various industries.","3974":null,"3975":"The coronavirus disease 2019 (COVID-19) breaking out in late December 2019 is gradually being controlled in China, but it is still spreading rapidly in many other countries and regions worldwide. It is urgent to conduct prediction research on the development and spread of the epidemic. In this article, a hybrid artificial-intelligence (AI) model is proposed for COVID-19 prediction. First, as traditional epidemic models treat all individuals with coronavirus as having the same infection rate, an improved susceptible\u2013infected (ISI) model is proposed to estimate the variety of the infection rates for analyzing the transmission laws and development trend. Second, considering the effects of prevention and control measures and the increase of the public\u2019s prevention awareness, the natural language processing (NLP) module and the long short-term memory (LSTM) network are embedded into the ISI model to build the hybrid AI model for COVID-19 prediction. The experimental results on the epidemic data of several typical provinces and cities in China show that individuals with coronavirus have a higher infection rate within the third to eighth days after they were infected, which is more in line with the actual transmission laws of the epidemic. Moreover, compared with the traditional epidemic models, the proposed hybrid AI model can significantly reduce the errors of the prediction results and obtain the mean absolute percentage errors (MAPEs) with 0.52%, 0.38%, 0.05%, and 0.86% for the next six days in Wuhan, Beijing, Shanghai, and countrywide, respectively.","3976":null,"3977":"In this experience report, we describe an AI summer workshop designed to prepare middle school students to become informed citizens and critical consumers of AI technology and to develop their foundational knowledge and skills to support future endeavors as AI-empowered workers. The workshop featured the 30-hour \"Developing AI Literacy\" or DAILy curriculum that is grounded in literature on child development, ethics education, and career development. The participants in the workshop were students between the ages of 10 and 14; 87% were from underrepresented groups in STEM and Computing. In this paper we describe the online curriculum, its implementation during synchronous online workshop sessions in summer of 2020, and preliminary findings on student outcomes. We reflect on the successes and lessons we learned in terms of supporting students' engagement and conceptual learning of AI, shifting attitudes toward AI, and fostering conceptions of future selves as AI-enabled workers. We conclude with discussions of the affordances and barriers to bringing AI education to students from underrepresented groups in STEM and Computing.","3978":"In many contexts, lying \u2013 the use of verbal falsehoods to deceive \u2013 is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI \u201clies\u201d (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: 1. identify clear truthfulness standards; 2. create institutions that can judge adherence to those standards; and 3. develop AI systems that are robustly truthful. Our initial proposals for these areas include: 1. a standard of avoiding \u201cnegligent falsehoods\u201d (a generalisation of lies that is easier to assess); 2. institutions to evaluate AI systems before and after real-world deployment; 3. explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set. ar X iv :2 11 0. 06 67 4v 1 [ cs .C Y ] 1 3 O ct 2 02 1","3979":"Artificial intelligence (AI) is one of the most disruptive technologies of our time. Interest in the use of AI for urban innovation continues to grow. Particularly, the rise of smart cities\u2014urban locations that are enabled by community, technology, and policy to deliver productivity, innovation, livability, wellbeing, sustainability, accessibility, good governance, and good planning\u2014has increased the demand for AI-enabled innovations. There is, nevertheless, no scholarly work that provides a comprehensive review on the topic. This paper generates insights into how AI can contribute to the development of smarter cities. A systematic review of the literature is selected as the methodologic approach. Results are categorized under the main smart city development dimensions, i.e., economy, society, environment, and governance. The findings of the systematic review containing 93 articles disclose that: (a) AI in the context of smart cities is an emerging field of research and practice. (b) The central focus of the literature is on AI technologies, algorithms, and their current and prospective applications. (c) AI applications in the context of smart cities mainly concentrate on business efficiency, data analytics, education, energy, environmental sustainability, health, land use, security, transport, and urban management areas. (d) There is limited scholarly research investigating the risks of wider AI utilization. (e) Upcoming disruptions of AI in cities and societies have not been adequately examined. Current and potential contributions of AI to the development of smarter cities are outlined in this paper to inform scholars of prospective areas for further research.","3980":"The ubiquity of AI in society means the time is ripe to consider what educated 21st century digital citizens should know about this subject. In May 2018, the Association for the Advancement of Artificial Intelligence (AAAI) and the Computer Science Teachers Association (CSTA) formed a joint working group to develop national guidelines for teaching AI to K-12 students. Inspired by CSTA's national standards for K-12 computing education, the AI for K-12 guidelines will define what students in each grade band should know about artificial intelligence, machine learning, and robotics. The AI for K-12 working group is also creating an online resource directory where teachers can find AI- related videos, demos, software, and activity descriptions they can incorporate into their lesson plans. This blue sky talk invites the AI research community to reflect on the big ideas in AI that every K-12 student should know, and how we should communicate with the public about advances in AI and their future impact on society. It is a call to action for more AI researchers to become AI educators, creating resources that help teachers and students understand our work.","3981":"Artificial intelligence (AI) technology has been increasingly used in the implementation of advanced Clinical Decision Support Systems (CDSS). Research demonstrated the potential usefulness of AI-powered CDSS (AI-CDSS) in clinical decision making scenarios. However, post-adoption user perception and experience remain understudied, especially in developing countries. Through observations and interviews with 22 clinicians from 6 rural clinics in China, this paper reports the various tensions between the design of an AI-CDSS system (\u201cBrilliant Doctor\u201d) and the rural clinical context, such as the misalignment with local context and workflow, the technical limitations and usability barriers, as well as issues related to transparency and trustworthiness of AI-CDSS. Despite these tensions, all participants expressed positive attitudes toward the future of AI-CDSS, especially acting as \u201ca doctor\u2019s AI assistant\u201d to realize a Human-AI Collaboration future in clinical settings. Finally we draw on our findings to discuss implications for designing AI-CDSS interventions for rural clinical contexts in developing countries.","3982":"(EC), recently published ethics guidelines for what it terms \u201c trustworthy \u201d AI. These guidelines are aimed at a variety of stakeholders, especially guiding practitioners toward more ethical and more robust applications of AI. In line with efforts of the EC, AI ethics scholarship focuses increasingly on converting abstract principles into actionable recommendations. However, the interpretation, relevance, and implementation of trustworthy AI depend on the domain and the context in which the AI system is used. The main contribution of this paper is to demonstrate how to use the general AI HLEG trustworthy AI guidelines in practice in the healthcare domain. To this end, we present a best practice of assessing the use of machine learning as a supportive tool to recognize cardiac arrest in emergency calls. The AI system under assessment is currently in use in the city of Copenhagen in Denmark. The assessment is accomplished by an independent team composed of philosophers, policy makers, social scientists, technical, legal, and medical experts. By leveraging an interdisciplinary team, we aim to expose the complex trade-offs and the necessity for such thorough human review when tackling socio-technical applications of AI in healthcare. For the assessment, we use a process to assess trustworthy AI, called 1 Z-Inspection \u00ae to identify speci \ufb01 c challenges and potential ethical trade-offs when we consider AI in practice.","3983":"\n We define Artificial Intelligence-Mediated Communication (AI-MC) as interpersonal communication in which an intelligent agent operates on behalf of a communicator by modifying, augmenting, or generating messages to accomplish communication goals. The recent advent of AI-MC raises new questions about how technology may shape human communication and requires re-evaluation \u2013 and potentially expansion \u2013 of many of Computer-Mediated Communication\u2019s (CMC) key theories, frameworks, and findings. A research agenda around AI-MC should consider the design of these technologies and the psychological, linguistic, relational, policy and ethical implications of introducing AI into human\u2013human communication. This article aims to articulate such an agenda.","3984":"We describe a framework for research and evaluation in Embodied AI. Our proposal is based on a canonical task: Rearrangement. A standard task can focus the development of new techniques and serve as a source of trained models that can be transferred to other settings. In the rearrangement task, the goal is to bring a given physical environment into a specified state. The goal state can be specified by object poses, by images, by a description in language, or by letting the agent experience the environment in the goal state. We characterize rearrangement scenarios along different axes and describe metrics for benchmarking rearrangement performance. To facilitate research and exploration, we present experimental testbeds of rearrangement scenarios in four different simulation environments. We anticipate that other datasets will be released and new simulation platforms will be built to support training of rearrangement agents and their deployment on physical systems.","3985":"As artificial intelligence and machine learning algorithms make further inroads into society, calls are increasing from multiple stakeholders for these algorithms to explain their outputs. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, present different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360 (this http URL), an open-source software toolkit featuring eight diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. We also discuss enhancements to bring research innovations closer to consumers of explanations, ranging from simplified, more accessible versions of algorithms, to tutorials and an interactive web demo to introduce AI explainability to different audiences and application domains. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.","3986":"Integrating ethics into artificial intelligence education and development.","3987":"The foreseen complexity in operating and managing 5G and beyond networks has propelled the trend toward closed-loop automation of network and service management operations. To this end, the ETSI Zero-touch network and Service Management (ZSM) framework is envisaged as a next-generation management system that aims to have all operational processes and tasks executed automatically, ideally with 100 percent automation. Artificial Intelligence (AI) is envisioned as a key enabler of self-managing capabilities, resulting in lower operational costs, accelerated time-tovalue and reduced risk of human error. Nevertheless, the growing enthusiasm for leveraging AI in a ZSM system should not overlook the potential limitations and risks of using AI techniques. The current paper aims to introduce the ZSM concept and point out the AI-based limitations and risks that need to be addressed in order to make ZSM a reality.","3988":"As the efficacy of artificial intelligence (AI) in improving aspects of healthcare delivery is increasingly becoming evident, it becomes likely that AI will be incorporated in routine clinical care in the near future. This promise has led to growing focus and investment in AI medical applications both from governmental organizations and technological companies. However, concern has been expressed about the ethical and regulatory aspects of the application of AI in health care. These concerns include the possibility of biases, lack of transparency with certain AI algorithms, privacy concerns with the data used for training AI models, and safety and liability issues with AI application in clinical environments. While there has been extensive discussion about the ethics of AI in health care, there has been little dialogue or recommendations as to how to practically address these concerns in health care. In this article, we propose a governance model that aims to not only address the ethical and regulatory issues that arise out of the application of AI in health care, but also stimulate further discussion about governance of AI in health care.","3989":"The purpose of this article is to determine the future proportion and variants of usage of human intellect and artificial intelligence (AI) in entrepreneurship of industry 4.0 that fits social entrepreneurship the most. It could be convergence (simultaneous utilization during the same entrepreneurial processes with the emphasis on unique features by the terms of the competition) or divergence (usage during different business processes by the terms of labor division).,The authors determine the influence of usage of human capital and AI on the efficiency of social entrepreneurship. The authors identify the perspective directions of usage of AI in social entrepreneurship and evaluate the readiness and interest in the implementation of these directions of concerned parties. The authors also model the optimal proportions and the variant of usage of human intellect and AI in social entrepreneurship in the conditions of Industry 4.0 in the future (until 2030).,It is found that social entrepreneurship will use the opportunities of Industry 4.0 for optimization of its activities until 2030, but will refuse from full automatization, using human intellect and AI at the same time.,The most perspective directions of application of AI at social companies are a collection of social goods and services, marketing studies and promotion of social goods and services. Neither convergence nor divergence of human and artificial intellectual capital does not fully conform to the interests of concerned parties. The most preferable (optimal) variant of usage of human intellect and AI in social entrepreneurship in the Industry 4.0 is human intelligent decision support.","3990":"Decisions made by human-AI teams (e.g., AI-advised humans) are increasingly common in high-stakes domains such as healthcare, criminal justice, and finance. Achieving high team performance depends on more than just the accuracy of the AI system: Since the human and the AI may have different expertise, the highest team performance is often reached when they both know how and when to complement one another. We focus on a factor that is crucial to supporting such complementary: the human\u2019s mental model of the AI capabilities, specifically the AI system\u2019s error boundary (i.e. knowing \u201cWhen does the AI err?\u201d). Awareness of this lets the human decide when to accept or override the AI\u2019s recommendation. We highlight two key properties of an AI\u2019s error boundary, parsimony and stochasticity, and a property of the task, dimensionality. We show experimentally how these properties affect humans\u2019 mental models of AI capabilities and the resulting team performance. We connect our evaluations to related work and propose goals, beyond accuracy, that merit consideration during model selection and optimization to improve overall human-AI team performance.","3991":null,"3992":"PurposeInnovative firms have rapidly developed artificial intelligence (AI) capabilities into their service ecosystems, essentially changing perceptions of what is service quality and service delivery in their respective industries. Nonetheless, the issues surrounding AI services remain relatively unknown. The purpose for this paper is to offer a digital servitization framework for understanding how AI services impact value perceptions, consumer engagement and firm performance measures. The authors use the financial service ecosystem to explore this topic.Design\/methodology\/approachThe authors explore relevant literature on digital servitization, service-dominant logic and AI\/disruptive innovation. Next, a conceptual framework, organized by AI-Service Exchange Antecedents, Context of AI Usage and Digital Servitization Consequences, is developed. The authors conceptualize consequences for consumers and firms.FindingsThe main findings suggest that the linkages between consumers, financial institutions and fintech companies with AI usage in a service ecosystem should be identified;how value is created among multiple SD Logic-AI network actors should be analyzed;and the effects of AI-consumer interactions (lower-level and higher levels of engagement) on firm performance measures should be explored.Research limitations\/implicationsThe conceptual framework identifies gaps in the literature and suggests research questions for future studies.Practical implicationsThis paper may assist practitioners with the development of AI-enabled banking activities that involve direct consumer engagement.Originality\/valueTo the authors\u2019 best knowledge, this research agenda is the first comprehensive framework for understanding value co-creation in the context of AI in financial services, linking antecedents, usage and consequences.","3993":"The prevalence of artificial intelligence (AI) has considerably affected management and society. This paper aims to explore its potential impact on hospitality industry employees, bringing enlightenment to both employees and managers.,Data were collected from a survey of 432 employees who worked in full-service hotels in China. Structural equation modeling (SEM) was used to analyze the data.,Results presented a positive relationship between AI awareness and job burnout. No significant direct relationship was found between AI awareness and career competencies. Organizational commitment mediated the relationship between AI awareness and career competencies, as well as the relationship between AI awareness and job burnout.,This study contributes to human resource management in the hospitality industry to theoretical and practical aspects. Theoretically, it enriched both career theory and fit theory. Practically, this study reminds managers to pay attention to the adverse effect of AI on human capital. It also enlightens the manager to think of the positive effects that AI may bring. Managers should provide proper support to overcome AI\u2019s threat to human resources.,Practically, this study reminds managers to pay attention to the adverse effect of AI on human capital. It also enlightens the manager to think of the positive effects that AI may bring. Managers should provide proper support to overcome AI\u2019s threat to human resources.,The study aims to analyze the impact of AI from a career perspective. It provided theoretical support and evidence for hotel managers for the effects of AI awareness on hotel employees. The study conveys a potential topic of concern that the hospitality industry may face in the future.","3994":null,"3995":"Abstract The rise of digital data and computing power have contributed to significant advancements in artificial intelligence (AI), leading to the use of classification and prediction models in health care to enhance clinical decision-making for diagnosis, treatment and prognosis. However, such advances are limited by the lack of reporting standards for the data used to develop those models, the model architecture, and the model evaluation and validation processes. Here, we present MINIMAR (MINimum Information for Medical AI Reporting), a proposal describing the minimum information necessary to understand intended predictions, target populations, and hidden biases, and the ability to generalize these emerging technologies. We call for a standard to accurately and responsibly report on AI in health care. This will facilitate the design and implementation of these models and promote the development and use of associated clinical decision support tools, as well as manage concerns regarding accuracy and bias.","3996":"Explainability of AI systems is critical for users to take informed actions and hold systems accountable. While\"opening the opaque box\"is important, understanding who opens the box can govern if the Human-AI interaction is effective. In this paper, we conduct a mixed-methods study of how two different groups of whos--people with and without a background in AI--perceive different types of AI explanations. These groups were chosen to look at how disparities in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively share what the perceptions are along five dimensions: confidence, intelligence, understandability, second chance, and friendliness. Qualitatively, we highlight how the AI background influences each group's interpretations and elucidate why the differences might exist through the lenses of appropriation and cognitive heuristics. We find that (1) both groups had unwarranted faith in numbers, to different extents and for different reasons, (2) each group found explanatory values in different explanations that went beyond the usage we designed them for, and (3) each group had different requirements of what counts as humanlike explanations. Using our findings, we discuss potential negative consequences such as harmful manipulation of user trust and propose design interventions to mitigate them. By bringing conscious awareness to how and why AI backgrounds shape perceptions of potential creators and consumers in XAI, our work takes a formative step in advancing a pluralistic Human-centered Explainable AI discourse.","3997":"This is an integrative review that address the question, \"What makes for a good explanation?\" with reference to AI systems. Pertinent literatures are vast. Thus, this review is necessarily selective. That said, most of the key concepts and issues are expressed in this Report. The Report encapsulates the history of computer science efforts to create systems that explain and instruct (intelligent tutoring systems and expert systems). The Report expresses the explainability issues and challenges in modern AI, and presents capsule views of the leading psychological theories of explanation. Certain articles stand out by virtue of their particular relevance to XAI, and their methods, results, and key points are highlighted. It is recommended that AI\/XAI researchers be encouraged to include in their research reports fuller details on their empirical or experimental methods, in the fashion of experimental psychology research reports: details on Participants, Instructions, Procedures, Tasks, Dependent Variables (operational definitions of the measures and metrics), Independent Variables (conditions), and Control Conditions.","3998":"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.","3999":null,"4000":"Abstract While AI has benefited humans, it may also harm humans if not appropriately developed. The priority of current HCI work should focus on transiting from conventional human interaction with non-AI computing systems to interaction with AI systems. We conducted a high-level literature review and a holistic analysis of current work in developing AI systems from an HCI perspective. Our review and analysis highlight the new changes introduced by AI technology and the new challenges that HCI professionals face when applying the human-centered AI (HCAI) approach in the development of AI systems. We also identified seven main issues in human interaction with AI systems, which HCI professionals did not encounter when developing non-AI computing systems. To further enable the implementation of the HCAI approach, we identified new HCI opportunities tied to specific HCAI-driven design goals to guide HCI professionals addressing these new issues. Finally, our assessment of current HCI methods shows the limitations of these methods in support of developing HCAI systems. We propose the alternative methods that can help overcome these limitations and effectively help HCI professionals apply the HCAI approach to the development of AI systems. We also offer strategic recommendation for HCI professionals to effectively influence the development of AI systems with the HCAI approach, eventually developing HCAI systems.","4001":null,"4002":"The capability of AI is currently expanding beyond mechanical and repetitive to analytical and thinking. A \u201cFeeling Economy\u201d is emerging, in which AI performs many of the analytical and thinking tasks, and human workers gravitate more toward interpersonal and empathetic tasks. Although these people-focused tasks have always been important to jobs, they are now becoming more important to an unprecedented degree. To manage more effectively in the Feeling Economy, managers must adapt the nature of jobs to compensate for the fact that many of the analytical and thinking tasks are increasingly being performed by AI, and, thus, human workers must place increased emphasis on the empathetic and emotional dimensions of their work.","4003":"\n Artificial intelligence (AI) is set to influence every aspect of our lives, not least the way production is organised. AI, as a technology platform, can automate tasks previously performed by labour or create new tasks and activities in which humans can be productively employed. Recent technological change has been biased towards automation, with insufficient focus on creating new tasks where labour can be productively employed. The consequences of this choice have been stagnating labour demand, declining labour share in national income, rising inequality and lowering productivity growth. The current tendency is to develop AI in the direction of further automation, but this might mean missing out on the promise of the \u2018right\u2019 kind of AI, with better economic and social outcomes.","4004":"The beginning of 2020 has seen the emergence of coronavirus outbreak caused by a novel virus called SARS-CoV-2. The sudden explosion and uncontrolled worldwide spread of COVID-19 show the limitations of existing healthcare systems in timely handling public health emergencies. In such contexts, innovative technologies such as blockchain and Artificial Intelligence (AI) have emerged as promising solutions for fighting coronavirus epidemic. In particular, blockchain can combat pandemics by enabling early detection of outbreaks, ensuring the ordering of medical data, and ensuring reliable medical supply chain during the outbreak tracing. Moreover, AI provides intelligent solutions for identifying symptoms caused by coronavirus for treatments and supporting drug manufacturing. Therefore, we present an extensive survey on the use of blockchain and AI for combating COVID-19 epidemics. First, we introduce a new conceptual architecture which integrates blockchain and AI for fighting COVID-19. Then, we survey the latest research efforts on the use of blockchain and AI for fighting COVID-19 in various applications. The newly emerging projects and use cases enabled by these technologies to deal with coronavirus pandemic are also presented. A case study is also provided using federated AI for COVID-19 detection. Finally, we point out challenges and future directions that motivate more research efforts to deal with future coronavirus-like epidemics.","4005":null,"4006":null,"4007":"We study the impact of AI on labor markets, using establishment level data on vacancies with detailed occupational information comprising the near-universe of online vacancies in the US from 2010 onwards. We classify establishments as \u201cAI exposed\u201d when their workers engage in tasks that are compatible with current AI capabilities. We document rapid growth in AI related vacancies over 2010-2018 that is not limited to the Professional and Business Services and Information Technology sectors and is significantly greater in AI-exposed establishments. AI-exposed establishments are differentially eliminating vacancy postings that list a range of previously-posted skills while simultaneously posting skill requirements that were not previously listed. Establishment-level estimates suggest that AI-exposed establishments are reducing hiring in non-AI positions even as they expand AI hiring. However, we find no discernible impact of AI exposure on employment or wages at the occupation or industry level, implying that AI is currently substituting for humans in a subset of tasks but it is not yet having detectable aggregate labor market consequences.","4008":"While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https:\/\/github.com\/HumanCompatibleAI\/overcooked_ai.","4009":null,"4010":"ABSTRACT Against a background of global competition to seize the opportunities promised by Artificial Intelligence (AI), many countries and regions are explicitly taking part in a \u2018race to AI\u2019. Yet the increased visibility of the technology\u2019s risks has led to ever-louder calls for regulators to look beyond the benefits, and also secure appropriate regulation to ensure AI that is \u2018trustworthy\u2019 \u2013 i.e. legal, ethical and robust. Besides minimising risks, such regulation could facilitate AI\u2019s uptake, boost legal certainty, and hence also contribute to advancing countries\u2019 position in the race. Consequently, this paper argues that the \u2018race to AI\u2019 also brings forth a \u2018race to AI regulation\u2019. After discussing the regulatory toolbox for AI and some of the challenges that regulators face when making use thereof, this paper assesses to which extent regulatory competition for AI \u2013 or its counterpart, regulatory convergence \u2013 is a possibility, a reality and a desirability.","4011":null,"4012":"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. \nWe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and\/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","4013":"This study sought to investigate the impact of AI on digital financial inclusion. Digital financial inclusion is becoming central in the debate on how to ensure that people who are at the lower levels of the pyramid become financially active. Fintech companies are using AI and its various applications to ensure that the goal of digital financial inclusion is realized that is to ensure that low-income earners, the poor, women, youths, small businesses participate in the mainstream financial market. This study used conceptual and documentary analysis of peer-reviewed journals, reports and other authoritative documents on AI and digital financial inclusion to assess the impact of AI on digital financial inclusion. The present study discovered that AI has a strong influence on digital financial inclusion in areas related to risk detection, measurement and management, addressing the problem of information asymmetry, availing customer support and helpdesk through chatbots and fraud detection and cybersecurity. Therefore, it is recommended that financial institutions and non-financial institutions and governments across the world adopt and scale up the use of AI tools and applications as they present benefits in the quest to ensure that the vulnerable groups of people who are not financially active do participate in the formal financial market with minimum challenges and maximum benefits.","4014":"While generative deep neural networks (DNNs) have demonstrated their capacity for creating novel musical compositions, less attention has been paid to the challenges and potential of co-creating with these musical AIs, especially for novices. In a needfinding study with a widely used, interactive musical AI, we found that the AI can overwhelm users with the amount of musical content it generates, and frustrate them with its non-deterministic output. To better match co-creation needs, we developed AI-steering tools, consisting of Voice Lanes that restrict content generation to particular voices; Example-Based Sliders to control the similarity of generated content to an existing example; Semantic Sliders to nudge music generation in high-level directions (happy\/sad, conventional\/surprising); and Multiple Alternatives of generated content to audition and choose from. In a summative study (N=21), we discovered the tools not only increased users' trust, control, comprehension, and sense of collaboration with the AI, but also contributed to a greater sense of self-efficacy and ownership of the composition relative to the AI.","4015":"Artificial intelligence (AI)-based technology has achieved many great things, such as facial recognition, medical diagnosis, and self-driving cars. AI promises enormous benefits for economic growth, social development, as well as human well-being and safety improvement. However, the low-level of explainability, data biases, data security, data privacy, and ethical problems of AI-based technology pose significant risks for users, developers, humanity, and societies. As AI advances, one critical issue is how to address the ethical and moral challenges associated with AI. Even though the concept of \u201cmachine ethics\u201d was proposed around 2006, AI ethics is still in the infancy stage. AI ethics is the field related to the study of ethical issues in AI. To address AI ethics, one needs to consider the ethics of AI and how to build ethical AI. Ethics of AI studies the ethical principles, rules, guidelines, policies, and regulations that are related to AI. Ethical AI is an AI that performs and behaves ethically. One must recognize and understand the potential ethical and moral issues that may be caused by AI to formulate the necessary ethical principles, rules, guidelines, policies, and regulations for AI (i.e., Ethics of AI). With the appropriate ethics of AI, one can then build AI that exhibits ethical behavior (i.e., Ethical AI). This paper will discuss AI ethics by looking at the ethics of AI and ethical AI. What are the perceived ethical and moral issues with AI? What are the general and common ethical principles, rules, guidelines, policies, and regulations that can resolve or at least attenuate these ethical and moral issues with AI? What are some of the necessary features and characteristics of an ethical AI? How to adhere to the ethics of AI to build ethical AI?","4016":null,"4017":"The ongoing debate between symbolic and connectionist AI attends to some of the most fundamental issues in the field. In this column, I briefly review the evolution of the unfolding discussion. I also point out that there is a lot more to intelligence than the symbolic and connectionist views of AI.","4018":"In 2018, a landmark challenge in artificial intelligence (AI) took place, namely, the Explainable Machine Learning Challenge. The goal of the competition was to create a complicated black box model for the dataset and explain how it worked. One team did not follow the rules. Instead of sending in a black box, they created a model that was fully interpretable. This leads to the question of whether the real world of machine learning is similar to the Explainable Machine Learning Challenge, where black box models are used even when they are not needed. We discuss this team\u2019s thought processes during the competition and their implications, which reach far beyond the competition itself.Keywords: interpretability, explainability, machine learning, finance","4019":null,"4020":"Tackling real-world socio-economic challenges requires designing and testing economic policies. However, this is hard in practice, due to a lack of appropriate (micro-level) economic data and limited opportunity to experiment. In this work, we train social planners that discover tax policies in dynamic economies that can effectively trade-off economic equality and productivity. We propose a two-level deep reinforcement learning approach to learn dynamic tax policies, based on economic simulations in which both agents and a government learn and adapt. Our data-driven approach does not make use of economic modeling assumptions, and learns from observational data alone. We make four main contributions. First, we present an economic simulation environment that features competitive pressures and market dynamics. We validate the simulation by showing that baseline tax systems perform in a way that is consistent with economic theory, including in regard to learned agent behaviors and specializations. Second, we show that AI-driven tax policies improve the trade-off between equality and productivity by 16% over baseline policies, including the prominent Saez tax framework. Third, we showcase several emergent features: AI-driven tax policies are qualitatively different from baselines, setting a higher top tax rate and higher net subsidies for low incomes. Moreover, AI-driven tax policies perform strongly in the face of emergent tax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are also effective when used in experiments with human participants. In experiments conducted on MTurk, an AI tax policy provides an equality-productivity trade-off that is similar to that provided by the Saez framework along with higher inverse-income weighted social welfare.","4021":"An ethical framework will help to harness the potential of AI while keeping humans in control Artificial intelligence (AI) is not just a new technology that requires regulation. It is a powerful force that is reshaping daily practices, personal and professional interactions, and environments. For the well-being of humanity it is crucial that this power is used as a force of good. Ethics plays a key role in this process by ensuring that regulations of AI harness its potential while mitigating its risks.","4022":null,"4023":"Behavior Trees (BTs) provide a way to structure the behavior of an artificial agent such as a robot or a non-player character in a computer game.\u00a0 Traditional design methods, such as finite state m ...","4024":null,"4025":"Abstract Online misinformation has become a constant; only the way actors create and distribute that information is changing. Advances in artificial intelligence (AI) such as GPT-2 mean that actors can now synthetically generate text in ways that mimic the style and substance of human-created news stories. We carried out three original experiments to study whether these AI-generated texts are credible and can influence opinions on foreign policy. The first evaluated human perceptions of AI-generated text relative to an original story. The second investigated the interaction between partisanship and AI-generated news. The third examined the distributions of perceived credibility across different AI model sizes. We find that individuals are largely incapable of distinguishing between AI- and human-generated text; partisanship affects the perceived credibility of the story; and exposure to the text does little to change individuals\u2019 policy views. The findings have important implications in understanding AI in online misinformation campaigns.","4026":"AI systems are being deployed to support human decision making in high-stakes domains such as healthcare and criminal justice. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI\u2019s inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI\u2019s predictive performance, they may also lead to behavioral changes that are at odds with the user\u2019s prior experiences and confidence in the AI\u2019s inferences. We show that updates that increase AI performance may actually hurt team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes classification tasks show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance\/compatibility tradeoff across different datasets, enabling more compatible yet accurate updates.","4027":"We illustrate the emergent spectrum of human\u2013AI hybrids in digital platforms and discuss some implications for IS research by using one classof digital platforms: digital labor platforms. Recognizing the service orientation and the expanding role of AI in digital platforms, we definedigital labor platforms3 as online environments where digital services are sourced and delivered in exchange for compensation,4 with constituenttasks for the services determined, executed, and coordinated by human and AI agents. Work done on these platforms is, by definition, digitaland can thus be modularized into tasks which require a range of cognitive skills for execution and coordination, providing a rich context toillustrate human\u2013AI hybrids and some key issues for next-generation digital platforms","4028":"ABSTRACT As government and public administration lag behind the rapid development of AI in their efforts to provide adequate governance, they need respective concepts to keep pace with this dynamic progress. The literature provides few answers to the question of how government and public administration should respond to the great challenges associated with AI and use regulation to prevent harm. This study analyzes AI challenges and former AI regulation approaches. Based on this analysis and regulation theory, an integrated AI governance framework is developed that compiles key aspects of AI governance and provides a guide for the regulatory process of AI and its application. The article concludes with theoretical implications and recommendations for public officers.","4029":null,"4030":". The current hype of Arti\ufb01cial Intelligence (AI) mostly refers to the success of machine learning and its sub-domain of deep learning. However, AI is also about other areas, such as Knowledge Representation and Reasoning, or Distributed AI, i.e., areas that need to be combined to reach the level of intelligence initially envisioned in the 1950s. Explainable AI (XAI) now refers to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. This paper reviews XAI not only from a Machine Learning perspective, but also from the other AI research areas, such as AI Planning or Constraint Satisfaction and Search. We expose the XAI challenges of AI \ufb01elds, their existing approaches, limitations and opportunities for Knowledge Graphs and their underlying technologies","4031":"AI technologies have been incorporated into many end-user applications. However, expectations of the capabilities of such systems vary among people. Furthermore, bloated expectations have been identified as negatively affecting perception and acceptance of such systems. Although the intelligibility of ML algorithms has been well studied, there has been little work on methods for setting appropriate expectations before the initial use of an AI-based system. In this work, we use a Scheduling Assistant - an AI system for automated meeting request detection in free-text email - to study the impact of several methods of expectation setting. We explore two versions of this system with the same 50% level of accuracy of the AI component but each designed with a different focus on the types of errors to avoid (avoiding False Positives vs. False Negatives). We show that such different focus can lead to vastly different subjective perceptions of accuracy and acceptance. Further, we design expectation adjustment techniques that prepare users for AI imperfections and result in a significant increase in acceptance.","4032":"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.","4033":"Conversational agents are exploding in popularity. However, much work remains in the area of social conversation as well as free-form conversation over a broad range of domains and topics. To advance the state of the art in conversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar university competition where sixteen selected university teams were challenged to build conversational agents, known as socialbots, to converse coherently and engagingly with humans on popular topics such as Sports, Politics, Entertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers the academic community a unique opportunity to perform research with a live system used by millions of users. The competition provided university teams with real user conversational data at scale, along with the user-provided ratings and feedback augmented with annotations by the Alexa team. This enabled teams to effectively iterate and make improvements throughout the competition while being evaluated in real-time through live user interactions. To build their socialbots, university teams combined state-of-the-art techniques with novel strategies in the areas of Natural Language Understanding, Context Modeling, Dialog Management, Response Generation, and Knowledge Acquisition. To support the efforts of participating teams, the Alexa Prize team made significant scientific and engineering investments to build and improve Conversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice User Experience, and tools for traffic management and scalability. This paper outlines the advances created by the university teams as well as the Alexa Prize team to achieve the common goal of solving the problem of Conversational AI.","4034":"The performance of mobile AI accelerators has been evolving rapidly in the past two years, nearly doubling with each new generation of SoCs. The current 4th generation of mobile NPUs is already approaching the results of CUDA-compatible Nvidia graphics cards presented not long ago, which together with the increased capabilities of mobile deep learning frameworks makes it possible to run complex and deep AI models on mobile devices. In this paper, we evaluate the performance and compare the results of all chipsets from Qualcomm, HiSilicon, Samsung, MediaTek and Unisoc that are providing hardware acceleration for AI inference. We also discuss the recent changes in the Android ML pipeline and provide an overview of the deployment of deep learning models on mobile devices. All numerical results provided in this paper can be found and are regularly updated on the official project website: http:\/\/ai-benchmark.com.","4035":"Many artificial intelligence (AI) edge devices use nonvolatile memory (NVM) to store the weights for the neural network (trained off-line on an AI server), and require low-energy and fast I\/O accesses. The deep neural networks (DNN) used by AI processors [1,2] commonly require p-layers of a convolutional neural network (CNN) and q-layers of a fully-connected network (FCN). Current DNN processors that use a conventional (von-Neumann) memory structure are limited by high access latencies, I\/O energy consumption, and hardware costs. Large working data sets result in heavy accesses across the memory hierarchy, moreover large amounts of intermediate data are also generated due to the large number of multiply-and-accumulate (MAC) operations for both CNN and FCN. Even when binary-based DNN [3] are used, the required CNN and FCN operations result in a major memory I\/O bottleneck for AI edge devices.","4036":"The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.","4037":"Ethics has powerful teeth, but these are barely being used in the ethics of AI today \u2013 it is no wonder the ethics of AI is then blamed for having no teeth. This article argues that \u2018ethics\u2019 in the current AI ethics field is largely ineffective, trapped in an \u2018ethical principles\u2019 approach and as such particularly prone to manipulation, especially by industry actors. Using ethics as a substitute for law risks its abuse and misuse. This significantly limits what ethics can achieve and is a great loss to the AI field and its impacts on individuals and society. This article discusses these risks and then highlights the teeth of ethics and the essential value they can \u2013 and should \u2013 bring to AI ethics now.","4038":"Recent advancements in artificial intelligence (AI) technologies have induced tremendous growth in innovation and automation. Although these AI technologies offer significant benefits, they can be used maliciously. Highly targeted and evasive attacks in benign carrier applications, such as DeepLocker, have demonstrated the intentional use of AI for harmful purposes. Threat actors are constantly changing and improving their attack strategy with particular emphasis on the application of AI-driven techniques in the attack process, called AI-based cyber attack, which can be used in conjunction with conventional attack techniques to cause greater damage. Despite several studies on AI and security, researchers have not summarized AI-based cyber attacks enough to be able to understand the adversary\u2019s actions and to develop proper defenses against such attacks. This study aims to explore existing studies of AI-based cyber attacks and to map them onto a proposed framework, providing insight into new threats. Our framework includes the classification of several aspects of malicious uses of AI during the cyber attack life cycle and provides a basis for their detection to predict future threats. We also explain how to apply this framework to analyze AI-based cyber attacks in a hypothetical scenario of a critical smart grid infrastructure.","4039":"The hype surrounding AI has given rise to all kinds of speculations about the future of AI and indeed the future of what it is to be human. One popular idea, which is not only repeated often in the media and in the public discourse about AI but is also entertained by influential tech people who develop AI technology such as Elon Musk and Ray Kurzweil, is that of superintelligence and, more generally, the idea that machines will take over, will master us rather than the other way around. For some, this is a dream; for many, a nightmare. And for some, it is both at the same time.","4040":null,"4041":"We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algo- rithmic mechanisms; interpretable systems where users can mathemat- ically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV\/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.","4042":null,"4043":"With new technologies related to the development of computers, graphics, and hardware, the virtual world has become a reality. As COVID-19 spreads around the world, the demand for virtual reality increases, and the industry represented by the Metaverse is developing. In the Metaverse, a virtual world that transcends reality, artificial intelligence and blockchain technology are being combined. This chapter explains how artificial intelligence and blockchain can affect the Metaverse.","4044":"Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be \"for\" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.","4045":null,"4046":"Recent work has explored how complementary strengths of humans and artificial intelligence (AI) systems might be productively combined. However, successful forms of human-AI partnership have rarely been demonstrated in real-world settings. We present the iterative design and evaluation of Lumilo, smart glasses that help teachers help their students in AI-supported classrooms by presenting real-time analytics about students' learning, metacognition, and behavior. Results from a field study conducted in K-12 classrooms indicate that students learn more when teachers and AI tutors work together during class. We discuss implications of this research for the design of human-AI partnerships. We argue for more participatory approaches to research and design in this area, in which practitioners and other stakeholders are deeply, meaningfully involved throughout the process. Furthermore, we advocate for theory-building and for principled approaches to the study of human-AI decision-making in real-world contexts.","4047":null,"4048":"The field of artificial intelligence (AI) has evolved considerably in the last 60 years. While there are now many AI applications that have been deployed in high-income country contexts, use in resource-poor settings remains relatively nascent. With a few notable exceptions, there are limited examples of AI being used in such settings. However, there are signs that this is changing. Several high-profile meetings have been convened in recent years to discuss the development and deployment of AI applications to reduce poverty and deliver a broad range of critical public services. We provide a general overview of AI and how it can be used to improve health outcomes in resource-poor settings. We also describe some of the current ethical debates around patient safety and privacy. Despite current challenges, AI holds tremendous promise for transforming the provision of healthcare services in resource-poor settings. Many health system hurdles in such settings could be overcome with the use of AI and other complementary emerging technologies. Further research and investments in the development of AI tools tailored to resource-poor settings will accelerate realising of the full potential of AI for improving global health.","4049":null,"4050":"Artificial Intelligence (AI) intent is to facilitate human limits. It is getting a standpoint on human administrations, filled by the growing availability of restorative clinical data and quick progression of insightful strategies. Motivated by the need to highlight the need for employing AI in battling the COVID-19 Crisis, this survey summarizes the current state of AI applications in clinical administrations while battling COVID-19. Furthermore, we highlight the application of Big Data while understanding this virus. We also overview various intelligence techniques and methods that can be applied to various types of medical information-based pandemic. We classify the existing AI techniques in clinical data analysis, including neural systems, classical SVM, and edge significant learning. Also, an emphasis has been made on regions that utilize AI-oriented cloud computing in combating various similar viruses to COVID-19. This survey study is an attempt to benefit medical practitioners and medical researchers in overpowering their faced difficulties while handling COVID-19 big data. The investigated techniques put forth advances in medical data analysis with an exactness of up to 90%. We further end up with a detailed discussion about how AI implementation can be a huge advantage in combating various similar viruses.","4051":"ABSTRACT Artificial intelligence (AI)-powered chatbots are changing the nature of service interfaces from being human-driven to technology-dominant. As a result, customers are expected to resolve issues themselves before reaching out to customer service representatives, ultimately becoming a central element of service production as co-creators of value. However, AI-powered interactions can also fail, potentially leading to anger, confusion, and customer dissatisfaction. We draw on the value co-creation literature to investigate the process of co-destruction in AI-powered service interactions. We adopt an exploratory approach based on in-depth interviews with 27 customers who have interacted with AI-powered chatbots in customer service settings. We find five antecedents of failed interactions between customers and chatbots: authenticity issues, cognition challenges, affective issues, functionality issues, and integration conflicts. We observe that although customers do accept part of the responsibility for co-destruction, they largely attribute the problems they experience to resource misintegration by service providers. Our findings contribute a better understanding of value co-destruction in AI-powered service settings and provide a richer conceptualization of the link between customer resource loss, attributions of resource loss, and subsequent customer coping strategies. Our findings also offer service managers insights into how to avoid and mitigate value co-destruction in AI service settings.","4052":"Artificial intelligence (AI) is becoming increasingly present in radiology and health care. This expansion is driven by the principal AI strengths: automation, accuracy, and objectivity. However, as radiology AI matures to become fully integrated into the daily radiology routine, it needs to go beyond replicating static models, toward discovering new knowledge from the data and environments around it. Continuous learning AI presents the next substantial step in this direction and brings a new set of opportunities and challenges. Herein, the authors discuss the main concepts and requirements for implementing continuous AI in radiology and illustrate them with examples from emerging applications.","4053":"Recent years have seen a reemergence of interest in artificial intelligence (AI) among both managers and academics. Driven by technological advances and public interest, AI is considered by some as an unprecedented revolutionary technology with the potential to transform humanity. But, at this stage, managers are left with little empirical advice on how to prepare and use AI in their firm\u2019s operations. Based on case studies and the results of two global surveys among senior managers across industries, this article shows that AI is typically implemented and used with other advanced digital technologies in firms\u2019 digital transformation projects. The digital transformation projects in which AI is deployed are mostly in support of firms\u2019 existing businesses, thereby demystifying some of the transformative claims made about AI. This article then presents a framework for successfully implementing AI in the context of digital transformation, offering specific guidance in the areas of data, intelligence, being grounded, integrated, teaming, agility, and leadership.","4054":"Human resource managers are adopting AI technology for conducting various tasks of human resource management, starting from manpower planning till employee exit. AI technology is prominently used for talent acquisition in organizations. This research investigates the adoption of AI technology for talent acquisition.,This study employs Technology-Organization-Environment (TOE) and Task-Technology-Fit (TTF) framework and proposes a model to explore the adoption of AI technology for talent acquisition. The survey was conducted among the 562 human resource managers and talent acquisition managers with a structured questionnaire. The analysis of data was completed using PLS-SEM.,This research reveals that cost-effectiveness, relative advantage, top management support, HR readiness, competitive pressure and support from AI vendors positively affect AI technology adoption for talent acquisition. Security and privacy issues negatively influence the adoption of AI technology. It is found that task and technology characteristics influence the task technology fit of AI technology for talent acquisition. Adoption and task technology fit of AI technology influence the actual usage of AI technology for talent acquisition. It is revealed that stickiness to traditional talent acquisition methods negatively moderates the association between adoption and actual usage of AI technology for talent acquisition. The proposed model was empirically validated and revealed the predictors of adoption and actual usage of AI technology for talent acquisition.,This paper provides the predictors of the adoption of AI technology for talent acquisition, which is emerging extensively in the human resource domain. It provides vital insights to the human resource managers to benchmark AI technology required for talent acquisition. Marketers can develop their marketing plan considering the factors of adoption. It would help designers to understand the factors of adoption and design the AI technology algorithms and applications for talent acquisition. It contributes to advance the literature of technology adoption by interweaving it with the human resource domain literature on talent acquisition.,This research uniquely validates the model for the adoption of AI technology for talent acquisition using the TOE and TTF framework. It reveals the factors influencing the adoption and actual usage of AI technology for talent acquisition.","4055":"The ubiquity of systems using artificial intelligence or \"AI\" has brought increasing attention to how those systems should be regulated. The choice of how to regulate AI systems will require care. AI systems have the potential to synthesize large amounts of data, allowing for greater levels of personalization and precision than ever before|applications range from clinical decision support to autonomous driving and predictive policing. That said, common sense reasoning [McCarthy, 1960] remains one of the holy grails of AI, and there exist legitimate concerns about the intentional and unintentional negative consequences of AI systems [Bostrom, 2003, Amodei et al., 2016, Sculley et al., 2014]. \n \nThere are many ways to hold AI systems accountable. In this work, we focus on one: explanation. Questions about a legal right to explanation from AI systems was recently debated in the EU General Data Protection Regulation [Goodman and Flaxman, 2016, Wachter et al., 2017], and thus thinking carefully about when and how explanation from AI systems might improve accountability is timely. Good choices about when to demand explanation can help prevent negative consequences from AI systems, while poor choices may not only fail to hold AI systems accountable but also hamper the development of much-needed beneficial AI systems. \n \nBelow, we briefly review current societal, moral, and legal norms around explanation, and then focus on the different contexts under which explanation is currently required under the law. We find that there exists great variation around when explanation is demanded, but there also exists important consistencies: when demanding explanation from humans, what we typically want to know is how and whether certain input factors affected the final decision or outcome. \n \nThese consistencies allow us to list the technical considerations that must be considered if we desired AI systems that could provide kinds of explanations that are currently required of humans under the law. Contrary to popular wisdom of AI systems as indecipherable black boxes, we find that this level of explanation should often be technically feasible but may sometimes be practically onerous|there are certain aspects of explanation that may be simple for humans to provide but challenging for AI systems, and vice versa. As an interdisciplinary team of legal scholars, computer scientists, and cognitive scientists, we recommend that for the present, AI systems can and should be held to a similar standard of explanation as humans currently are; in the future we may wish to hold an AI to a different standard.","4056":null,"4057":"In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.","4058":"Abstract Brand endorsers can contribute to a brand\u2019s success or failure (in the case of endorser transgressions). Recent advancements in technology have produced new, nonhuman alternatives to traditional celebrity endorsers. These new endorsers rely on artificial intelligence (AI) to interact with and influence consumers. Two studies demonstrate that AI influencers can produce positive brand benefits similar to those produced by human celebrity endorsers. Moreover, just like their human counterparts, AI influencers can also commit transgressions that result in degradation of the endorsed brand. Importantly, though, AI influencers differ from human celebrity endorsers in that consumers are less likely to view them as unique entities (as tested in a pilot study). Thus, consumers are more likely to perceive a transgression committed by an AI influencer as behavior applicable to all AI influencers, but they are less likely to view celebrity endorser behaviors as interchangeable. As such, after an AI influencer has committed a transgression, replacing the AI influencer with a celebrity endorser attenuates negative brand perceptions, an effect which cannot be realized if the replacement is another AI influencer.","4059":"Modeling dynamic geophysical phenomena is at the core of Earth and environmental studies. The geoscientific community relying mainly on physical representations may want to consider much deeper adoption of artificial intelligence (AI) instruments in the context of AI's global success and emergence of big Earth data. A new perspective of using hybrid physics\u2010AI approaches is a grand vision, but actualizing such approaches remains an open question in geoscience. This study develops a general approach to improving AI geoscientific awareness, wherein physical approaches such as temporal dynamic geoscientific models are included as special recurrent neural layers in a deep learning architecture. The illustrative case of runoff modeling across the conterminous United States demonstrates that the physics\u2010aware DL model has enhanced prediction accuracy, robust transferability, and good intelligence for inferring unobserved processes. This study represents a firm step toward realizing the vision of tackling Earth system challenges by physics\u2010AI integration.","4060":"The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.","4061":"Artificial Intelligent (AI) and Machine Learning (ML) algorithms are coming out of research labs into the real-world applications, and recent research has focused a lot on Human-AI Interaction (HAI) and Explainable AI (XAI). However, Interaction is not the same as Collaboration. Collaboration involves mutual goal understanding, preemptive task co-management and shared progress tracking. Most of human activities today are done collaboratively, thus, to integrate AI into the already-complicated human workflow, it is critical to bring the Computer-Supported Cooperative Work (CSCW) perspective into the root of the algorithmic research and plan for a Human-AI Collaboration future of work. In this panel we ask: Can this future for trusted human-AI collaboration be realized? If so, what will it take? This panel will bring together HCI experts who work on human collaboration and AI applications in various application contexts, from industry and academia and from both the U.S. and China. Panelists will engage the audience through discussion of their shared and diverging visions, and through suggestions for opportunities and challenges for the future of human-AI collaboration.","4062":"Artificial Intelligence (AI) is an enabling technology that when integrated into healthcare applications and smart wearable devices such as Fitbits etc. can predict the occurrence of health conditions in users by capturing and analysing their health data. The integration of AI and smart wearable devices has a range of potential applications in the area of smart healthcare but there is a challenge in the black box operation of decisions made by AI models which have resulted in a lack of accountability and trust in the decisions made. Explainable AI (XAI) is a domain in which techniques are developed to explain predictions made by AI systems. In this paper, XAI is discussed as a technique that can used in the analysis and diagnosis of health data by AI-based systems and a proposed approach presented with the aim of achieving accountability. transparency, result tracing, and model improvement in the domain of healthcare.","4063":"In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user. The user may ignore this advice or take it into consideration to modify their decision. With the increasing prevalence of such human-AI interactions, it is important to understand how users react to AI advice. In this paper, we recruited over 1100 crowdworkers to characterize how humans use AI suggestions relative to equivalent suggestions from a group of peer humans across several experimental settings. We find that participants' beliefs about how human versus AI performance on a given task affects whether they heed the advice. When participants do heed the advice, they use it similarly for human and AI suggestions. Based on these results, we propose a two-stage, \"activation-integration\" model for human behavior and use it to characterize the factors that affect human-AI interactions.","4064":"The accuracy and reliability of machine learning algorithms are an important concern for suppliers of artificial intelligence (AI) services, but considerations beyond accuracy, such as safety, security, and provenance, are also critical elements to engender consumers' trust in a service. In this paper, we propose a supplier's declaration of conformity (SDoC) for AI services to help increase trust in AI services. An SDoC is a transparent, standardized, but often not legally required, document used in many industries and sectors to describe the lineage of a product along with the safety and performance testing it has undergone. We envision an SDoC for AI services to contain purpose, performance, safety, security, and provenance information to be completed and voluntarily released by AI service providers for examination by consumers. Importantly, it conveys product-level rather than component-level functional testing. We suggest a set of declaration items tailored to AI and provide examples for two fictitious AI services.","4065":"ABSTRACT Organizations are rapidly deploying artificial intelligence (AI) systems to manage their workers. However, AI has been found at times to be unfair to workers. Unfairness toward workers has been associated with decreased worker effort and increased worker turnover. To avoid such problems, AI systems must be designed to support fairness and redress instances of unfairness. Despite the attention related to AI unfairness, there has not been a theoretical and systematic approach to developing a design agenda. This paper addresses the issue in three ways. First, we introduce the organizational justice theory, three different fairness types (distributive, procedural, interactional), and the frameworks for redressing instances of unfairness (retributive justice, restorative justice). Second, we review the design literature that specifically focuses on issues of AI fairness in organizations. Third, we propose a design agenda for AI fairness in organizations that applies each of the fairness types to organizational scenarios. Then, the paper concludes with implications for future research.","4066":"Abstract This book provides a comprehensive introduction to Conversational AI. While the idea of interacting with a computer using voice or text goes back a long way, it is only in recent years tha...","4067":"There has been an exponential growth in the application of AI in health and in pathology. This is resulting in the innovation of deep learning technologies that are specifically aimed at cellular imaging and practical applications that could transform diagnostic pathology. This paper reviews the different approaches to deep learning in pathology, the public grand challenges that have driven this innovation and a range of emerging applications in pathology. The translation of AI into clinical practice will require applications to be embedded seamlessly within digital pathology workflows, driving an integrated approach to diagnostics and providing pathologists with new tools that accelerate workflow and improve diagnostic consistency and reduce errors. The clearance of digital pathology for primary diagnosis in the US by some manufacturers provides the platform on which to deliver practical AI. AI and computational pathology will continue to mature as researchers, clinicians, industry, regulatory organizations and patient advocacy groups work together to innovate and deliver new technologies to health care providers: technologies which are better, faster, cheaper, more precise, and safe.","4068":"The new generation of artificial intelligence (AI), called AI 2.0, has recently become a research focus. Data\u2010driven AI 2.0 will accelerate the development of smart energy and electric power system (Smart EEPS). In AI 2.0, machine learning (ML) forms a typical representative algorithm category used to achieve predictions and judgments by analyzing and learning from massive amounts of historical and synthetic data to help people make optimal decisions. ML has preliminarily been applied to the Smart Grid (SG) and Energy Internet (EI) fields, which are important Smart EEPS representatives. AI 2.0, especially ML, is undergoing a critical period of rapid development worldwide and will play an essential role in Smart EEPS. In this context, this study, combined with the emerging SG and EI technologies, takes the typical representative of AI 2.0\u2014ML\u2014as the research objective and reviews its research status in the operation, optimization, control, dispatching, and management of SG and EI. The paper focuses on introducing and summarizing the mainstream uses of seven representative ML methods, including reinforcement learning, deep learning, transfer learning, parallel learning, hybrid learning, adversarial learning, and ensemble learning, in the SG and EI fields. In this survey, we begin with an introduction to these seven types of ML methods and then systematically review their applications in Smart EEPS. Finally, we discuss ML development under the big data thinking and offer a prospect for the future development of AI 2.0 and ML in Smart EEPS. We conduct this survey intended to arouse the interest and excitement of experts and scholars in the EEPS industry and to look ahead to efforts that jointly promote the rapid development of AI 2.0 in the Smart EEPS field.","4069":null,"4070":"\nPurpose\nThe purpose of the present article is to highlight the role of Artificial Intelligence (AI) and Robotics in the tourism industry. The various technologies being integrated to improve the service and customer experience in tourism. The expected changes and challenges in tourism in the future are focused in this paper.\n\n\nDesign\/methodology\/approach\nA systematic study on the emerging technologies of AI and Robotics applied in the tourism sector is presented in the form of a viewpoint.\n\n\nFindings\nAI certainly enhances tourism experiential services however cannot surpass the human touch which is an essential determinant of experiential tourism. AI acts as an effective complementary dimension to the future of tourism. With the emergence of artificial travel intelligence, it is simpler to make travel arrangements. AI offers travel services that are automated, customized and insightful. AI allows travelers to learn about their behaviors, interests to inclinations and provide a personalized experience. Gone are the days to consult a travel agent, meet him physically and indulge in an endless chain of troubling phone calls to inquire about travel arrangements.\n\n\nPractical implications\nTourism marketing to see a positive and improved change that will enhance the tourists\u2019 overall experience due to the application of AI and Robotics. New emerging technologies like chatbots, virtual reality, language translators, etc. can be effectively applied in Travel, Tourism & Hospitality industry.\n\n\nOriginality\/value\nThe present viewpoint discusses the application and role of AI and Robotics with the help of relevant industry examples and theory. The present paper highlights the different technologies being used and will be used in the future.\n","4071":"Artificial Intelligence systems are spreading to multiple applications and they are used by a more diverse audience. With this change of the use scenario, AI users will increasingly require explanations. The first part of this paper makes a review of the state of the art of Explainable AI and highlights how the current research is not paying enough attention to whom the explanations are targeted. In the second part of the paper, it is suggested a new explainability pipeline, where users are classified in three main groups (developers or AI researchers, domain experts and lay users). Inspired by the cooperative principles of conversations, it is discussed how creating different explanations for each of the targeted groups can overcome some of the difficulties related to creating good explanations and evaluating them.","4072":"The recent development of data-driven AI promises to automate medical diagnosis; however, most AI functions as 'black boxes' to physicians with limited computational knowledge. Using medical imaging as a point of departure, we conducted three iterations of design activities to formulate CheXplain \u0097 a system that enables physicians to explore and understand AI-enabled chest X-ray analysis: (i) a paired survey between referring physicians and radiologists reveals whether, when, and what kinds of explanations are needed; (ii) a low-fidelity prototype co-designed with three physicians formulates eight key features; and (iii) a high-fidelity prototype evaluated by another six physicians provides detailed summative insights on how each feature enables the exploration and understanding of AI. We summarize by discussing recommendations for future work to design and implement explainable medical AI systems that encompass four recurring themes: motivation, constraint, explanation, and justification.","4073":null,"4074":null,"4075":"Artificial intelligence (AI) is a rapidly growing technological phenomenon that all industries wish to exploit to benefit from efficiency gains and cost reductions. At the macrolevel, AI appears to be capable of replacing humans by undertaking intelligent tasks that were once limited to the human mind. However, another school of thought suggests that instead of being a replacement for the human mind, AI can be used for intelligence augmentation (IA). Accordingly, our research seeks to address these different views, their implications, and potential risks in an age of increased artificial awareness. We show that the ultimate goal of humankind is to achieve IA through the exploitation of AI. Moreover, we articulate the urgent need for ethical frameworks that define how AI should be used to trigger the next level of IA.","4076":"As more and more forms of AI become prevalent, it becomes increasingly important to understand how people develop mental models of these systems. In this work we study people's mental models of AI in a cooperative word guessing game. We run think-aloud studies in which people play the game with an AI agent; through thematic analysis we identify features of the mental models developed by participants. In a large-scale study we have participants play the game with the AI agent online and use a post-game survey to probe their mental model. We find that those who win more often have better estimates of the AI agent's abilities. We present three components for modeling AI systems, propose that understanding the underlying technology is insufficient for developing appropriate conceptual models (analysis of behavior is also necessary), and suggest future work for studying the revision of mental models over time.","4077":"ABSTRACT An increase in demand for online education has led to the creation of a new technology, machine teachers, or artificial intelligence (AI) teaching assistants. In fact, AI teaching assistants have already been implemented in a small number of courses in the United States. However, little is known about how students will perceive AI teaching assistants. Thus, the present study investigated students\u2019 perceptions about AI teaching assistants in higher education by use of an online survey. Primary findings indicate that perceived usefulness of an AI teaching assistant and perceived ease of communication with an AI teaching assistant are key to understanding an eventual adoption of AI teaching assistant-based education. These findings provide support for AI teaching assistant adoption. Based on the present study\u2019s findings, more research is needed to better understand the nuances associated with the learning experience one may have from an AI teaching assistant.","4078":"Abstract AI research is growing rapidly raising various ethical issues related to safety, risks, and other effects widely discussed in the literature. We believe that in order to adequately address those issues and engage in a productive normative discussion it is necessary to examine key concepts and categories. One such category is anthropomorphism. It is a well-known fact that AI\u2019s functionalities and innovations are often anthropomorphized (i.e., described and conceived as characterized by human traits). The general public\u2019s anthropomorphic attitudes and some of their ethical consequences (particularly in the context of social robots and their interaction with humans) have been widely discussed in the literature. However, how anthropomorphism permeates AI research itself (i.e., in the very language of computer scientists, designers, and programmers), and what the epistemological and ethical consequences of this might be have received less attention. In this paper we explore this issue. We first set the methodological\/theoretical stage, making a distinction between a normative and a conceptual approach to the issues. Next, after a brief analysis of anthropomorphism and its manifestations in the public, we explore its presence within AI research with a particular focus on brain-inspired AI. Finally, on the basis of our analysis, we identify some potential epistemological and ethical consequences of the use of anthropomorphic language and discourse within the AI research community, thus reinforcing the need of complementing the practical with a conceptual analysis.","4079":"Artificial intelligence (AI) is expected to revolutionise the practice of medicine. Recent advancements in the field of deep learning have demonstrated success in variety of clinical tasks: detecting diabetic retinopathy from images, predicting hospital readmissions, aiding in the discovery of new drugs, etc. AI\u2019s progress in medicine, however, has led to concerns regarding the potential effects of this technology on relationships of trust in clinical practice. In this paper, I will argue that there is merit to these concerns, since AI systems can be relied on, and are capable of reliability, but cannot be trusted, and are not capable of trustworthiness. Insofar as patients are required to rely on AI systems for their medical decision-making, there is potential for this to produce a deficit of trust in relationships in clinical practice.","4080":null,"4081":"This study examines tourists\u2019 attitudes toward the use of artificially intelligent (AI) devices in either relatively more utilitarian or hedonic tourism services (airline and hospitality services, respectively). Findings suggest that tourists\u2019 acceptance of the use of AI devices in both service contexts is influenced by social influence, hedonic motivation, anthropomorphism, performance and effort expectancy, and emotions toward the artificially intelligent devices. Findings further suggest that social influence is a stronger determinant in hospitality services compared to airline services. Tourists have higher performance expectancy from AI devices used to provide airline services compared to hospitality services. Tourists\u2019 willingness to accept the use of AI devices for delivering hospitality services is lower than airline services. These results suggest that while the utilization of AI devices for delivering functional services is acceptable, the use of AI devices in the delivery of hedonic services may backfire.","4082":null,"4083":"The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible \u201cglass-box\u201d approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.","4084":"Summary Introduction: Artificial intelligence (AI) technologies continue to attract interest from a broad range of disciplines in recent years, including health. The increase in computer hardware and software applications in medicine, as well as digitization of health-related data together fuel progress in the development and use of AI in medicine. This progress provides new opportunities and challenges, as well as directions for the future of AI in health. Objective: The goals of this survey are to review the current state of AI in health, along with opportunities, challenges, and practical implications. This review highlights recent developments over the past five years and directions for the future. Methods: Publications over the past five years reporting the use of AI in health in clinical and biomedical informatics journals, as well as computer science conferences, were selected according to Google Scholar citations. Publications were then categorized into five different classes, according to the type of data analyzed. Results: The major data types identified were multi-omics, clinical, behavioral, environmental and pharmaceutical research and development (R&D) data. The current state of AI related to each data type is described, followed by associated challenges and practical implications that have emerged over the last several years. Opportunities and future directions based on these advances are discussed. Conclusion: Technologies have enabled the development of AI-assisted approaches to healthcare. However, there remain challenges. Work is currently underway to address multi-modal data integration, balancing quantitative algorithm performance and qualitative model interpretability, protection of model security, federated learning, and model bias.","4085":"Meredith Whittaker, AI Now Institute at NYU Meryl Alper, Northeastern University; Cynthia L. Bennett, University of Washington; Sara Hendren, Olin College; Liz Kaziunas, AI Now Institute at NYU; Mara Mills, New York University; Meredith Ringel Morris, Microsoft Research; Joy Rankin, AI Now Institute at NYU; Emily\u200b Rogers\u200b, New York University, Marcel Salas, New York University; Sarah Myers West, AI Now Institute at NYU","4086":"Even as public pressure mounts for technology companies to consider societal impacts of products, industries and governments in the AI race are demanding technical talent. To meet this demand, universities clamor to add technical artificial intelligence (AI) and machine learning (ML) courses into computing curriculum-but how are societal and ethical considerations part of this landscape? We explore two pathways for ethics content in AI education: (1) standalone AI ethics courses, and (2) integrating ethics into technical AI courses. For both pathways, we ask: What is being taught? As we train computer scientists who will build and deploy AI tools, how are we training them to consider the consequences of their work? In this exploratory work, we qualitatively analyzed 31 standalone AI ethics classes from 22 U.S. universities and 20 AI\/ML technical courses from 12 U.S. universities to understand which ethics-related topics instructors include in courses. We identify and categorize topics in AI ethics education, share notable practices, and note omissions. Our analysis will help AI educators identify what topics should be taught and create scaffolding for developing future AI ethics education.","4087":null,"4088":"Recent years have seen rapid deployment of mobile computing and Internet of Things (IoT) networks, which can be mostly attributed to the increasing communication and sensing capabilities of wireless systems. Big data analysis, pervasive computing, and eventually artificial intelligence (AI) are envisaged to be deployed on top of the IoT and create a new world featured by data-driven AI. In this context, a novel paradigm of merging AI and wireless communications, called Wireless AI that pushes AI frontiers to the network edge, is widely regarded as a key enabler for future intelligent network evolution. To this end, we present a comprehensive survey of the latest studies in wireless AI from the data-driven perspective. Specifically, we first propose a novel Wireless AI architecture that covers five key data-driven AI themes in wireless networks, including Sensing AI, Network Device AI, Access AI, User Device AI and Data-provenance AI. Then, for each data-driven AI theme, we present an overview on the use of AI approaches to solve the emerging data-related problems and show how AI can empower wireless network functionalities. Particularly, compared to the other related survey papers, we provide an in-depth discussion on the Wireless AI applications in various data-driven domains wherein AI proves extremely useful for wireless network design and optimization. Finally, research challenges and future visions are also discussed to spur further research in this promising area.","4089":"Accuracy is an important concern for suppliers of artificial intelligence (AI) services, but considerations beyond accuracy, such as safety (which includes fairness and explainability), security, and provenance, are also critical elements to engender consumers\u2019 trust in a service. Many industries use transparent, standardized, but often not legally required documents called supplier's declarations of conformity (SDoCs) to describe the lineage of a product along with the safety and performance testing it has undergone. SDoCs may be considered multidimensional fact sheets that capture and quantify various aspects of the product and its development to make it worthy of consumers\u2019 trust. In this article, inspired by this practice, we propose FactSheets to help increase trust in AI services. We envision such documents to contain purpose, performance, safety, security, and provenance information to be completed by AI service providers for examination by consumers. We suggest a comprehensive set of declaration items tailored to AI in the Appendix of this article.","4090":"Summary Objectives: This paper draws attention to: i) key considerations for evaluating artificial intelligence (AI) enabled clinical decision support; and ii) challenges and practical implications of AI design, development, selection, use, and ongoing surveillance. Method: A narrative review of existing research and evaluation approaches along with expert perspectives drawn from the International Medical Informatics Association (IMIA) Working Group on Technology Assessment and Quality Development in Health Informatics and the European Federation for Medical Informatics (EFMI) Working Group for Assessment of Health Information Systems. Results: There is a rich history and tradition of evaluating AI in healthcare. While evaluators can learn from past efforts, and build on best practice evaluation frameworks and methodologies, questions remain about how to evaluate the safety and effectiveness of AI that dynamically harness vast amounts of genomic, biomarker, phenotype, electronic record, and care delivery data from across health systems. This paper first provides a historical perspective about the evaluation of AI in healthcare. It then examines key challenges of evaluating AI-enabled clinical decision support during design, development, selection, use, and ongoing surveillance. Practical aspects of evaluating AI in healthcare, including approaches to evaluation and indicators to monitor AI are also discussed. Conclusion: Commitment to rigorous initial and ongoing evaluation will be critical to ensuring the safe and effective integration of AI in complex sociotechnical settings. Specific enhancements that are required for the new generation of AI-enabled clinical decision support will emerge through practical application.","4091":"AI and blockchain are among the most disruptive technologies and will fundamentally reshape how we live, work, and interact. The authors summarize existing efforts and discuss the promising future of their integration, seeking to answer the question: What can smart, decentralized, and secure systems do for our society?","4092":"The purpose of this paper is to review literature about the applications of artificial intelligence (AI) in strategic situations and identify the research that is needed in the area of applying AI to strategic marketing decisions.,The approach was to carry out a literature review and to consult with marketing experts who were invited to contribute to the paper.,There is little research into applying AI to strategic marketing decision-making. This research is needed, as the frontier of AI application to decision-making is moving in many management areas from operational to strategic. Given the competitive nature of such decisions and the insights from applying AI to defence and similar areas, it is time to focus on applying AI to strategic marketing decisions.,The application of AI to strategic marketing decision-making is known to be taking place, but as it is commercially sensitive, data is not available to the authors.,There are strong implications for all businesses, particularly large businesses in competitive industries, where failure to deploy AI in the face of competition from firms, who have deployed AI to improve their decision-making could be dangerous.,The public sector is a very important marketing decision maker. Although in most cases it does not operate competitively, it must make decisions about making different services available to different citizens and identify the risks of not providing services to certain citizens; so, this paper is relevant to the public sector.,To the best of the authors\u2019 knowledge, this is one of the first papers to probe deployment of AI in strategic marketing decision-making.","4093":"This paper was written to support the G20 artificial intelligence (AI) dialogue. With the rise of artificial intelligence (AI), education faces two challenges: reaping the benefits of AI to improve education processes, both in the classroom and at the system level; and preparing students for new skillsets for increasingly automated economies and societies. AI applications are often still nascent, but there are many examples of promising uses that foreshadow how AI might transform education. With regard to the classroom, this paper highlights how AI can accelerate personalised learning, the support of students with special needs. At the system level, promising uses include predictive analysis to reduce dropout, and assessing new skillsets. A new demand for complex skills that are less easy to automate (e.g. higher cognitive skills like creativity and critical thinking) is also the consequence of AI and digitalisation. Reaching the full potential of AI requires that stakeholders trust not only the technology, but also its use by humans. This raises new policy challenges around \u201ctrustworthy AI\u201d, encompassing the privacy and security of data, but also possible wrongful uses of data leading to biases against individuals or groups.","4094":"This paper on artificial intelligence in education (AIEd) has two aims. The first: to explain to a non-specialist, interested, reader what AIEd is: its goals, how it is built, and how it works. The second: to set out the argument for what AIEd can offer teaching and learning, both now and in the future, with an eye towards improving learning and life outcomes for all. Computer systems that are artificially intelligent interact with the world using capabilities (such as speech recognition) and intelligent behaviours (such as using available information to take the most sensible actions toward a stated goal) that we would think of as essentially human. At the heart of artificial intelligence in education is the scientific goal to make knowledge, which is often left implicit, computationally precise and explicit. In other words, in addition to being the engine behind much \u2018smart\u2019 ed tech, AIEd is also designed to be a powerful tool to open up what is sometimes called the \u2018black box of learning,\u2019 giving us more fine-grained understandings of how learning actually happens. Although some might find the concept of AIEd alienating, the algorithms and models that underpin ed tech powered by AIEd form the basis of an essentially human endeavor. Using AIEd, teachers will be able to offer learners educational experiences that are more personalised, flexible, inclusive and engaging. Crucially, we do not see a future in which AIEd replaces teachers. What we do see is a future in which the extraordinary expertise of teachers is better leveraged and augmented through the thoughtful deployment of well designed AIEd. We have available, right now, AIEd tools that could support student learning at a scale previously unimaginable by providing one-on-one tutoring to every student, in every subject. Existing technologies also have the capacity to provide intelligent support to learners working in a group, and to create authentic virtual learning environments where students have the right support, at the right time, to tackle real-life problems and puzzles. In the near future, we expect that teaching and learning will increasingly be supported by the thoughtful application of AIEd tools. For example, by lifelong learning companions powered by AI that can accompany and support individual learners throughout their studies - in and beyond school - and new forms of assessment that measure learning while it is taking place, shaping the learning experience in real time. If we are ultimately successful, we predict that AIEd will help us address some of the most intractable problems in education, including achievement gaps and teacher retention. AIEd will also help us respond to the most significant social challenge that AI has already brought - the steady replacement of jobs and occupations with clever algorithms and robots. It is our view that this provides a new innovation imperative in education, which can be expressed simply: as humans live and work alongside increasingly smart machines, our education systems will need to achieve at levels that none have managed to date. True progress will require the development of an AIEd infrastructure. This will not, however, be a single monolithic AIEd system. Instead, it will resemble the marketplace that has developed for smartphone apps: hundreds and then thousands of individual AIEd components, developed in collaboration with educators, conformed to uniform international data standards, and shared with researchers and developers worldwide. These standards will also enable system-level data collation and analysis that will help us to learn much more about learning itself \u2013 and how to improve it. Moving forward, we will need to pay close attention to three powerful forces as we map the future of artificial intelligence in education, namely pedagogy, technology, and system change. Paying attention to the pedagogy will mean that the design of new edtech should always start with what we know about learning. It also means that the system for funding this work must be simultaneously opened up and refocused, moving away from isolated pockets of R&D and toward collaborative enterprises that prioritise areas known to make a real difference to teaching and learning. Paying attention to the technology will mean creating smarter demand for commercial grade AIEd products that work. It also means the development of a robust, component-based AIEd infrastructure, similar to the smartphone app marketplace, where researchers and developers can access standardised components that have been developed in collaboration with educators. Paying attention to system change will mean involving teachers, students, and parents in co-designing new tools, so that AIEd will appropriately address the inherent \u201cmessiness\u201d of real classroom, university, and workplace learning environments. It also means the development of data standards that promote the safe and ethical use of data. Said succinctly, we need intelligent technologies that embody what we know about great teaching and learning, embodied in enticing consumer grade products, which are then used effectively in real-life settings that combine the best of human and machine. We do not underestimate the new-thinking, inevitable wrong-turns, and effort required to realise these recommendations. However, if we are to properly unleash the intelligence of AIEd, we must do things differently - via new collaborations, sensible funding, and (always) a keen eye on the pedagogy. The potential prize is too great to act otherwise.","4095":"In this paper, we propose a novel accurate method for dead-reckoning of wheeled vehicles based only on an Inertial Measurement Unit (IMU). In the context of intelligent vehicles, robust and accurate dead-reckoning based on the IMU may prove useful to correlate feeds from imaging sensors, to safely navigate through obstructions, or for safe emergency stops in the extreme case of exteroceptive sensors failure. The key components of the method are the Kalman filter and the use of deep neural networks to dynamically adapt the noise parameters of the filter. The method is tested on the KITTI odometry dataset, and our dead-reckoning inertial method based only on the IMU accurately estimates 3D position, velocity, orientation of the vehicle and self-calibrates the IMU biases. We achieve on average a 1.10% translational error and the algorithm competes with top-ranked methods which, by contrast, use LiDAR or stereo vision.","4096":"We observed how 102 children (7-12 years old), from four different countries (U.S.A, Germany, Denmark, and Sweden), imagine smart devices and toys of the future and how they perceive current AI technologies. Children outside of U.S.A were overall more critical of these technologies and less exposed to them. The way children collaborated and communicated while describing their AI perception and expectations were influenced both by their social-economical and cultural background. Children in low and medium SES schools and centers were better are collaborating compared to high SES children, but had a harder time advancing because they had less experience with coding and interacting with these technologies. Children in high SES schools and centers had troubles collaborating initially but displayed a stronger understanding of AI concepts. Based on our initial findings we propose a series of guidelines for designing future hands-on learning activities with smart toys and AI devices for K8 students.","4097":"Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the ``manual AI approach.'' This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.","4098":null,"4099":"ABSTRACT Introduction: Various factors are driving interest in the application of artificial intelligence (AI) for breast cancer (BC) detection, but it is unclear whether the evidence warrants large-scale use in population-based screening. Areas covered: We performed a scoping review, a structured evidence synthesis describing a broad research field, to summarize knowledge on AI evaluated for BC detection and to assess AI\u2019s readiness for adoption in BC screening. Studies were predominantly small retrospective studies based on highly selected image datasets that contained a high proportion of cancers (median BC proportion in datasets 26.5%), and used heterogeneous techniques to develop AI models; the range of estimated AUC (area under ROC curve) for AI models was 69.2\u201397.8% (median AUC 88.2%). We identified various methodologic limitations including use of non-representative imaging data for model training, limited validation in external datasets, potential bias in training data, and few comparative data for AI versus radiologists\u2019 interpretation of mammography screening. Expert opinion: Although contemporary AI models have reported generally good accuracy for BC detection, methodological concerns, and evidence gaps exist that limit translation into clinical BC screening settings. These should be addressed in parallel to advancing AI techniques to render AI transferable to large-scale population-based screening.","4100":"Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI are far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability. In addition, model explainability is a prerequisite for building trust and adoption of AI systems in high stakes domains requiring reliability and safety such as healthcare and automated transportation, and critical industrial applications with significant economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling. As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale. The challenges for the research community include (i) defining model explainability, (ii) formulating explainability tasks for understanding model behavior and developing solutions for these tasks, and finally (iii) designing measures for evaluating the performance of models in explainability tasks. In this tutorial, we will present an overview of model interpretability and explainability in AI, key regulations\/laws, and techniques\/tools for providing explainability as part of AI\/ML systems. Then, we will focus on the application of explainability techniques in industry, wherein we present practical challenges\/ guidelines for using explainability techniques effectively and lessons learned from deploying explainable models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as search and recommendation systems, sales, lending, and fraud detection. Finally, based on our experiences in industry, we will identify open problems and research directions for the data mining\/machine learning community.","4101":"Interdisciplinary research from the learning sciences has helped us understand a great deal about the way that humans learn, and as a result we now have an improved understanding about how best to teach and train people. This same body of research must now be used to better inform the development of Artificial Intelligence (AI) technologies for use in education and training. In this paper, we use three case studies to illustrate how learning sciences research can inform the judicious analysis, of rich, varied and multimodal data, so that it can be used to help us scaffold students and support teachers. Based on this increased understanding of how best to inform the analysis of data through the application of learning sciences research, we are better placed to design AI algorithms that can analyse rich educational data at speed. Such AI algorithms and technology can then help us to leverage faster, more nuanced and individualised scaffolding for learners. However, most commercial AI developers know little about learning sciences research, indeed they often know little about learning or teaching. We therefore argue that in order to ensure that AI technologies for use in education and training embody such judicious analysis and learn in a learning sciences informed manner, we must develop inter\u2010stakeholder partnerships between AI developers, educators and researchers. Here, we exemplify our approach to such partnerships through the EDUCATE Educational Technology (EdTech) programme. Practitioner NotesWhat is already known about this topic? The progress of AI Technology and learning analytics lags behind the adoption of these approaches and technologies in other fields such as medicine or finance.Data are central to the empirical work conducted in the learning sciences and to the development of machine learning Artificial Intelligence (AI).Education is full of doubts about the value that any technology can bring to the teaching and learning process.What this paper adds? We argue that the learning sciences have an important role to play in the design of educational AI, through their provision of theories that can be operationalised and advanced.Through case studies, we illustrate that the analysis of data appropriately informed by interdisciplinary learning sciences research can be used to power AI educational technology.We provide a framework for inter\u2010stakeholder, interdisciplinary partnerships that can help educators better understand AI, and AI developers better understand education.Implications for practice and\/or policy? AI is here to stay and that it will have an increasing impact on the design of technology for use in education and training.Data, which is the power behind machine learning AI, can enable analysis that can vastly increase our understanding of when and how the teaching and learning process is progressing positively.Inter\u2010stakeholder, interdisciplinary partnerships must be used to make sure that AI provides some of the educational benefits its application in other areas promise us. [ABSTRACT FROM AUTHOR]","4102":null,"4103":"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.","4104":null,"4105":null,"4106":null,"4107":null,"4108":null,"4109":null,"4110":null,"4111":"Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI\/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users\u2019 needs, and we identify key open challenges.","4112":null,"4113":null,"4114":"We are entering an era of AI-Mediated Communication (AI-MC) where interpersonal communication is not only mediated by technology, but is optimized, augmented, or generated by artificial intelligence. Our study takes a first look at the potential impact of AI-MC on online self-presentation. In three experiments we test whether people find Airbnb hosts less trustworthy if they believe their profiles have been written by AI. We observe a new phenomenon that we term the Replicant Effect: Only when participants thought they saw a mixed set of AI- and human-written profiles, they mistrusted hosts whose profiles were labeled as or suspected to be written by AI. Our findings have implications for the design of systems that involve AI technologies in online self-presentation and chart a direction for future work that may upend or augment key aspects of Computer-Mediated Communication theory.","4115":null,"4116":null,"4117":null,"4118":"Machine learning advances have afforded an increase in algorithms capable of creating art, music, stories, games, and more. However, it is not yet well-understood how machine learning algorithms might best collaborate with people to support creative expression. To investigate how practicing designers perceive the role of AI in the creative process, we developed a game level design tool for Super Mario Bros.-style games with a built-in AI level designer. In this paper we discuss our design of the Morai Maker intelligent tool through two mixed-methods studies with a total of over one-hundred participants. Our findings are as follows: (1) level designers vary in their desired interactions with, and role of, the AI, (2) the AI prompted the level designers to alter their design practices, and (3) the level designers perceived the AI as having potential value in their design practice, varying based on their desired role for the AI.","4119":null,"4120":null,"4121":"With the increasing commoditization of computer vision, speech recognition and machine translation systems and the widespread deployment of learning-based back-end technologies such as digital advertising and intelligent infrastructures, AI (Artificial Intelligence) has moved from research labs to production. These changes have been made possible by unprecedented levels of data and computation, by methodological advances in machine learning, by innovations in systems software and architectures, and by the broad accessibility of these technologies. \nThe next generation of AI systems promises to accelerate these developments and increasingly impact our lives via frequent interactions and making (often mission-critical) decisions on our behalf, often in highly personalized contexts. Realizing this promise, however, raises daunting challenges. In particular, we need AI systems that make timely and safe decisions in unpredictable environments, that are robust against sophisticated adversaries, and that can process ever increasing amounts of data across organizations and individuals without compromising confidentiality. These challenges will be exacerbated by the end of the Moore's Law, which will constrain the amount of data these technologies can store and process. In this paper, we propose several open research directions in systems, architectures, and security that can address these challenges and help unlock AI's potential to improve lives and society.","4122":"Fair, accountable AI and robotics need precise regulation and better methods to certify, explain, and audit inscrutable systems. To create fair and accountable AI and robotics, we need precise regulation and better methods to certify, explain, and audit inscrutable systems.","4123":"The purpose of this article is to draw attention to an aspect of intelligence that has not yet received significant attention from the AI community, but that plays a crucial role in a technology\u2019s effectiveness in the world, namely teaming intelligence. We propose that Al will reach its full potential only if, as part of its intelligence, it also has enough teaming intelligence to work well with people. Although seemingly counterintuitive, the more intelligent the technological system, the greater the need for collaborative skills. This paper will argue why teaming intelligence is important to AI, provide a general structure for AI researchers to use in developing intelligent systems that team well, assess the current state of the art and, in doing so, suggest a path forward for future AI systems. This is not a call to develop a new capability, but rather, an approach to what AI capabilities should be built, and how, so as to imbue intelligent systems with teaming competence.","4124":"The artificial intelligence (AI) community has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI 'talent'. Both are crucial to the future of AI activism and worthy of sustained attention.","4125":"As capabilities of predictive algorithms improve, machine learning will become an important element of physician practice and patient care. Implementation of artificial intelligence (AI) raises complex legal questions regarding health care professionals' and technology manufacturers' liability, particularly if they cannot explain recommendations generated by AI technology. The limited literature on liability for innovation provides opportunities to consider possible implications of AI for medical malpractice and products liability and new legal solutions for addressing liability issues surrounding \"black-box\" medicine.","4126":"How AI is perceived by the public can have significant impact on how it is developed, deployed and regulated. Some commentators argue that perceptions are currently distorted or extreme. This paper discusses the results of a nationally representative survey of the UK population on their perceptions of AI. The survey solicited responses to eight common narratives about AI (four optimistic, four pessimistic), plus views on what AI is, how likely it is to impact in respondents' lifetimes, and whether they can influence it. 42% of respondents offered a plausible definition of AI, while 25% thought it meant robots. Of the narratives presented, those associated with automation were best known, followed by the idea that AI would become more powerful than humans. Overall results showed that the most common visions of the impact of AI elicit significant anxiety. Only two of the eight narratives elicited more excitement than concern (AI making life easier, and extending life). Respondents felt they had no control over AI's development, citing the power of corporations or government, or versions of technological determinism. Negotiating the deployment of AI will require contending with these anxieties.","4127":"AI-based systems are \u201cblack boxes,\u201d resulting in massive information asymmetries between the developers of such systems and consumers and policymakers. In order to bridge this information gap, this article proposes a conceptual framework for thinking about governance for AI.","4128":"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.","4129":"We document a dramatic increase in the demand for AI skills in online job postings over the period 2010-2019. The demand for AI skills is highest in IT occupations, followed by architecture\/engineering, life\/physical\/social sciences, and management. The sectors with the highest demand for AI are information, professional services, and finance. At the firm level, higher demand for AI skills is associated in the cross-section with larger market capitalization, higher cash holdings, and higher investments in R\\&D. We also document a large wage premium for job postings that require AI skills, as well as a wage premium for non-AI vacancies posted by firms with a high share of AI vacancies. Interestingly, managerial occupations have the highest wage premium for AI skills.","4130":"Since 2016, more than 80 AI ethics documents - including codes, principles, frameworks, and policy strategies - have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents' creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals.","4131":"We present AI2, an analyst-in-the-loop security system where Analyst Intuition (AI) is put together with state-of-the-art machine learning to build a complete end-to-end Artificially Intelligent solution (AI). The system presents four key features: a big data behavioral analytics platform, an outlier detection system, a mechanism to obtain feedback from security analysts, and a supervised learning module. We validate our system with a real-world data set consisting of 3.6 billion log lines and 70.2 million entities. The results show that the system is capable of learning to defend against unseen attacks. With respect to unsupervised outlier analysis, our system improves the detection rate in 2.92\u00d7 and reduces false positives by more than 5\u00d7.","4132":"Recent awareness of the impacts of bias in AI algorithms raises the risk for companies to deploy such algorithms, especially because the algorithms may not be explainable in the same way that non-AI algorithms are. Even with careful review of the algorithms and data sets, it may not be possible to delete all unwanted bias, particularly because AI systems learn from historical data, which encodes historical biases. In this paper, we propose a set of processes that companies can use to mitigate and manage three general classes of bias: those related to mapping the business intent into the AI implementation, those that arise due to the distribution of samples used for training, and those that are present in individual input samples. While there may be no simple or complete solution to this issue, best practices can be used to reduce the effects of bias on algorithmic outcomes.","4133":null,"4134":"Artificial intelligence (AI) has proven to be useful in many applications from automating cars to providing customer service responses. However, though many firms want to take advantage of AI to improve marketing, they lack a process by which to execute a Marketing AI project. This article discusses the use of AI to provide support for marketing decisions. Based on the established Cross-Industry Standard Process for Data Mining (CRISP-DM) framework, it creates a process for managers to use when executing a Marketing AI project and discusses issues that might arise. It explores how this framework was used to develop three cutting-edge Marketing AI applications.","4135":"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.","4136":null,"4137":"Artificial intelligence (AI) is increasingly being developed for use in medicine, including for diagnosis and in treatment decision making. The use of AI in medical treatment raises many ethical issues that are yet to be explored in depth by bioethicists. In this paper, I focus specifically on the relationship between the ethical ideal of shared decision making and AI systems that generate treatment recommendations, using the example of IBM\u2019s Watson for Oncology. I argue that use of this type of system creates both important risks and significant opportunities for promoting shared decision making. If value judgements are fixed and covert in AI systems, then we risk a shift back to more paternalistic medical care. However, if designed and used in an ethically informed way, AI could offer a potentially powerful way of supporting shared decision making. It could be used to incorporate explicit value reflection, promoting patient autonomy. In the context of medical treatment, we need value-flexible AI that can both respond to the values and treatment goals of individual patients and support clinicians to engage in shared decision making.","4138":"Recent rapid progress in machine learning (ML), particularly so\u00e2\u20ac called \u00e2\u20ac\u02dcdeep learning\u00e2\u20ac\u2122, has led to a resurgence in interest in explainability of artificial intelligence (AI) systems, reviving an area of research dating back to the 1970s. The aim of this article is to view current issues concerning ML\u00e2\u20ac based AI systems from the perspective of classical AI, showing that the fundamental problems are far from new, and arguing that elements of that earlier work offer routes to making progress towards explainable AI today.","4139":"Cutting through the hype, a practical guide to using artificial intelligence for business benefits and competitive advantage.In The AI Advantage, Thomas Davenport offers a guide to using artificial intelligence in business. He describes what technologies are available and how companies can use them for business benefits and competitive advantage. He cuts through the hype of the AI craze?remember when it seemed plausible that IBM's Watson could cure cancer??to explain how businesses can put artificial intelligence to work now, in the real world. His key recommendation: don't go for the ?moonshot? (curing cancer, or synthesizing all investment knowledge); look for the ?low-hanging fruit? to make your company more efficient.Davenport explains that the business value AI offers is solid rather than sexy or splashy. AI will improve products and processes and make decisions better informed?important but largely invisible tasks. AI technologies won't replace human workers but augment their capabilities, with smart machines to work alongside smart people. AI can automate structured and repetitive work; provide extensive analysis of data through machine learning (?analytics on steroids?), and engage with customers and employees via chatbots and intelligent agents. Companies should experiment with these technologies and develop their own expertise.Davenport describes the major AI technologies and explains how they are being used, reports on the AI work done by large commercial enterprises like Amazon and Google, and outlines strategies and steps to becoming a cognitive corporation. This book provides an invaluable guide to the real-world future of business AI.A book in the Management on the Cutting Edge series, published in cooperation with MIT Sloan Management Review.","4140":"AI Ethics is now a global topic of discussion in academic and policy circles. At least 63 public-private initiatives have produced statements describing high-level principles, values, and other tenets to guide the ethical development, deployment, and governance of AI. According to recent meta-analyses, AI Ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI Ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach in the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement.","4141":null,"4142":"\nPurpose\nThe purpose of this study is to explore the current use of artificial intelligence (AI) in the recruitment and selection of candidates. More specifically, this research investigates the level, rate and potential adoption areas for AI-tools across the hiring process.\n\n\nDesign\/methodology\/approach\nTo fulfill that purpose, a two-step approach was adopted. First, the literature was extensively reviewed to identify potential AI-application areas supporting the recruitment and selection (R&S) process. Second, primary research was carried out in the form of semi-structured thematic interviews with different types of R&S specialists including HR managers, consultants and academics to evaluate how much of the AI-applications areas identified in the literature review are being used in practice.\n\n\nFindings\nThis study presents a multitude of findings. First, it identifies 11 areas across the R&S Process where AI-applications can be applied. However, practitioners currently seem to rely mostly on three: chatbots, screening software and task automation tools. Second, most companies adopting these AI-tools tend to be larger, tech-focussed and\/or innovative firms. Finally, despite the exponential rate of AI-adoption, companies have yet to reach an inflection point as they currently show reluctance to invest in that technology for R&S.\n\n\nResearch limitations\/implications\nDue to the qualitative and exploratory nature behind the research, this study displays a significant amount of subjectivity, and therefore, lacks generalisability. Despite this limitation, this study opens the door to many opportunities for academic research, both qualitative and quantitative.\n\n\nOriginality\/value\nThis paper addresses the huge research gap surrounding AI in R&S, pertaining specifically to the scarcity and poor quality of the current academic literature. Furthermore, this research provides a comprehensive overview of the state of AI in R&S, which will be helpful for academics and practitioners looking to rapidly gain a holistic understanding of AI in R&S.\n","4143":"There is general consensus that it is important for artificial \nintelligence (AI) and machine learning systems to be explainable \nand\/or interpretable. However, there is no general \nconsensus over what is meant by \u2018explainable\u2019 and \u2018interpretable\u2019. \nIn this paper, we argue that this lack of consensus \nis due to there being several distinct stakeholder communities. \nWe note that, while the concerns of the individual \ncommunities are broadly compatible, they are not identical, \nwhich gives rise to different intents and requirements for explainability\/ \ninterpretability. We use the software engineering \ndistinction between validation and verification, and the epistemological \ndistinctions between knowns\/unknowns, to tease \napart the concerns of the stakeholder communities and highlight \nthe areas where their foci overlap or diverge. It is not \nthe purpose of the authors of this paper to \u2018take sides\u2019 \u2014 we \ncount ourselves as members, to varying degrees, of multiple \ncommunities \u2014 but rather to help disambiguate what stakeholders \nmean when they ask \u2018Why?\u2019 of an AI.","4144":"In recent years there has been increased attention on the possible impact of future robotics and AI systems. Prominent thinkers have publicly warned about the risk of a dystopian future when the complexity of these systems progresses further. These warnings stand in contrast to the current state-of-the-art of the robotics and AI-technology. This article reviews work considering both the future potential of robotics and AI systems, and ethical considerations that need to be taken in order to avoid a dystopian future. References to recent initiatives to outline ethical guidelines for both the design of systems and how they should operate are included.","4145":"\n \n The General Video Game AI framework and competition pose the problem of creating artificial intelligence that can play a wide, and in principle unlimited, range of games. Concretely, it tackles the problem of devising an algorithm that is able to play any game it is given, even if the game is not known a priori. This area of study can be seen as an approximation of General Artificial Intelligence, with very little room for game-dependent heuristics. This short paper summarizes the motivation, infrastructure, results and future plans of General Video Game AI, stressing the findings and first conclusions drawn after two editions of our competition, and outlining our future plans.\n \n","4146":"The emergence and growing interest in Artificial Intelligence (AI) will have increasing societal implications influencing, the responsibilities of decision-makers and policy analysts. Although an extensive body of literature relating to AI techniques already exists, this is not the case for AI adoption in organisations. This research-in-progress seeks to propose a research framework for AI adoption at firm level. To do so, two popular theories are considered: The Technology-Organisations-Environment (TOE) framework, and Diffusion of Innovation theory (DOI). This paper presents an in-depth interpretation of these theories for the adoption of AI technologies and proposes an AI adoption framework at firm level. A mixed methods research approach is proposed to test and validate the framework. Further work in this project will involve developing the research instrument for data collection via a survey targeted at firms.","4147":null,"4148":"This paper proposes a serverless platform for building and operating edge AI applications. We analyze edge AI use cases to illustrate the challenges in building and operating AI applications in edge cloud scenarios. By elevating concepts from AI lifecycle management into the established serverless model, we enable easy development of edge AI workflow functions. We take a deviceless approach, i.e., we treat edge resources transparently as cluster resources, but give developers fine-grained control over scheduling constraints. Furthermore, we demonstrate the limitations of current serverless function schedulers, and present the current state of our prototype.","4149":"Interpretability of the underlying AI representations is a key raison d'\\^{e}tre for Open Learner Modelling (OLM) -- a branch of Intelligent Tutoring Systems (ITS) research. OLMs provide tools for 'opening' up the AI models of learners' cognition and emotions for the purpose of supporting human learning and teaching. Over thirty years of research in ITS (also known as AI in Education) produced important work, which informs about how AI can be used in Education to best effects and, through the OLM research, what are the necessary considerations to make it interpretable and explainable for the benefit of learning. We argue that this work can provide a valuable starting point for a framework of interpretable AI, and as such is of relevance to the application of both knowledge-based and machine learning systems in other high-stakes contexts, beyond education.","4150":"Recent concern about harms of information technologies motivate consideration of regulatory action to forestall or constrain certain developments in the field of artificial intelligence (AI). However, definitional ambiguity hampers the possibility of conversation about this urgent topic of public concern. Legal and regulatory interventions require agreed-upon definitions, but consensus around a definition of AI has been elusive, especially in policy conversations. With an eye towards practical working definitions and a broader understanding of positions on these issues, we survey experts and review published policy documents to examine researcher and policy-maker conceptions of AI. We find that while AI researchers favor definitions of AI that emphasize technical functionality, policy-makers instead use definitions that compare systems to human thinking and behavior. We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies. As a result of this gap, ethical and regulatory efforts may overemphasize concern about future technologies at the expense of pressing issues with existing deployed technologies.","4151":"Humans and AI systems are usually portrayed as separate systems that we need to align in values and goals. However, there is a great deal of AI technology found in non-autonomous systems that are used as cognitive tools by humans. Under the extended mind thesis, the functional contributions of these tools become as essential to our cognition as our brains. But AI can take cognitive extension towards totally new capabilities, posing new philosophical, ethical and technical challenges. To analyse these challenges better, we define and place AI extenders in a continuum between fully-externalized systems, loosely coupled with humans, and fully internalized processes, with operations ultimately performed by the brain, making the tool redundant. We dissect the landscape of cognitive capabilities that can foreseeably be extended by AI and examine their ethical implications.We suggest that cognitive extenders using AI be treated as distinct from other cognitive enhancers by all relevant stakeholders, including developers, policy makers, and human users.","4152":null,"4153":"The time is ripe to consider what 21st-century digital citizens should know about artificial intelligence (AI). Efforts are under way in the USA, China, and many other countries to promote AI education in kindergarten through high school (K\u201312). The past year has seen the release of new curricula and online resources for the K\u201312 audience, and new professional development opportunities for K\u201312 teachers to learn the basics of AI. This column surveys the current state of K\u201312 AI education and introduces the work of the AI4K12 Initiative, which is developing national guidelines for AI education in the USA. \n\u00a0 \nA Note to the Reader \nThis is the inaugural column on AI education. It aims to inform the AAAI community of current and future developments in AI education.\u00a0We hope that the reader finds the columns to be informative and that they stimulate debate. It is our fond hope that this and subsequent columns inspire the reader to get involved in the broad field of AI education, by volunteering their expertise in their local school district, by providing level-headed input when discussing AI with family and friends or by lending their considerable expertise to various decision makers. We welcome your feedback, whether in the form of a response to an article or a suggestion for a future article. \n\u2013 Michael Wollowski, AI in Education Column Editor \n\u00a0","4154":"In this paper, we review recent emerging theoretical and technological advances of artificial intelligence (AI) in the big data settings. We conclude that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI, as follows: from shallow computation to deep neural reasoning; from merely data-driven model to data-driven with structured logic rules models; from task-oriented (domain-specific) intelligence (adherence to explicit instructions) to artificial general intelligence in a general context (the capability to learn from experience). Motivated by such endeavors, the next generation of AI, namely AI 2.0, is positioned to reinvent computing itself, to transform big data into structured knowledge, and to enable better decision-making for our society.","4155":"Artificial intelligence (AI) technologies will automate many jobs, but the effect on employment is not obvious. In manufacturing, technology has sharply reduced jobs in recent decades. But before that, for over a century, employment grew, even in industries experiencing rapid technological change. What changed? Demand was highly elastic at first and then became inelastic. The effect of artificial intelligence on jobs will similarly depend critically on the nature of demand. This paper presents a simple model of demand that accurately predicts the rise and fall of employment in the textile, steel and automotive industries. This model provides a useful framework for exploring how AI is likely to affect jobs over the next 10 or 20 years.","4156":"\n \n \nFrom its inception, artificial intelligence (AI) has had a rather ambivalent relationship to humans \u2014 swinging between their augmentation and their replacement. Now, as AI technologies enter our everyday lives at an ever-increasing pace, there is a greater need for AI systems to work synergistically with humans. To do this effectively, AI systems must pay more attention to aspects of intelligence that help humans work with each other \u2014 including social intelligence. I will discuss the research challenges in designing such human-aware AI systems, including modeling the mental states of humans-in-the-loop and recognizing their desires and intentions, providing proactive support, exhibiting explicable behavior, giving cogent explanations on demand, and engendering trust. I will survey the progress made so far on these challenges, and highlight some promising directions. I will also touch on the additional ethical quandaries that such systems pose. I will end by arguing that the quest for human-aware AI systems broadens the scope of AI enterprise; necessitates and facilitates true interdisciplinary collaborations; and can go a long way toward increasing public acceptance of AI technologies. \n \n \n","4157":null,"4158":"\u2013 Artificial intelligence (AI) is increasingly affecting our lives in smaller or greater ways. In order to ensure that systems will uphold human values, design methods are needed that incorporate ethical principles and address societal concerns. In this paper, we explore the impact of AI in the case of the expected effects on the European labor market, and propose the accountability, responsibility and transparency (ART) design principles for the development of AI systems that are sensitive to human values.","4159":"Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.","4160":null,"4161":"Hybrid intelligence systems combine machine and human intelligence to overcome the shortcomings of existing AI systems. This paper reviews recent research efforts towards developing hybrid systems focusing on reasoning methods for optimizing access to human intelligence and on gaining comprehensive understanding of humans as helpers of AI systems. It concludes by discussing short and long term research directions.","4162":null,"4163":null,"4164":"One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.","4165":null,"4166":"Artificial intelligence (AI) seeks to emulate human reasoning, but is still far from achieving such results for actionable sensing in complex situations. Instead of emulating human situation understanding, machines can amplify intelligence by accessing large amounts of data, filtering unimportant information, computing relevant context, and prioritizing results (for example, answers to human queries) to provide human\u2013machine shared context. Intelligence support can come from many contextual sources that augment data reasoning through physical, environmental, and social knowledge. We propose a decisions-to-data multimodal sensor and action through contextual agents (human or machine) that seek, combine, and make sense of relevant data. Decisions-to-data combines AI computational capabilities with human reasoning to manage data collections, perform data fusion, and assess complex situations (that is, context reasoning). Five areas of AI developments for context-based AI that cover decisions-to-data include: (1) situation modeling (data at rest), (2) measurement control (data in motion), (3) statistical algorithms (data in collect), (4) software computing (data in transit), and (5) human\u2013machine AI (data in use). A decisions-to-data example is presented of a command-guided swarm requiring contextual data analysis, systems-level design, and user interaction for effective and efficient multimodal sensing and action.","4167":"Background: Science is experiencing a reproducibility crisis. Artificial intelligence research is not an exception. Objective: To give practical and pragmatic recommendations for how to document AI research so that the results are reproducible. Method: Our analysis of the literature shows that AI publications fall short of providing enough documentation to facilitate reproducibility. Our suggested best practices are based on a framework for reproducibility and recommendations given for other disciplines. Results: We have made an author checklist based on our investigation and provided examples for how every item in the checklist can be documented. Conclusion: We encourage reviewers to use the suggested best practices and author checklist when reviewing submissions for AAAI publications and future AAAI conferences.","4168":"The more AI agents are deployed in scenarios with possibly unexpected situations, the more they need to be flexible, adaptive, and creative in achieving the goal we have given them. Thus, a certain level of freedom to choose the best path to the goal is inherent in making AI robust and flexible enough. At the same time, however, the pervasive deployment of AI in our life, whether AI is autonomous or collaborating with humans, raises several ethical challenges. AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues. These ethical principles should define the boundaries of AI\u2019s freedom and creativity. However, it is still a challenge to understand how to specify and reason with ethical boundaries in AI agents and how to combine them appropriately with subjective preferences and goal specifications. Some initial attempts employ either a data-driven examplebased approach for both, or a symbolic rule-based approach for both. We envision a modular approach where any AI technique can be used for any of these essential ingredients in decision making or decision support systems, paired with a contextual approach to define their combination and relative weight. In a world where neither humans nor AI systems work in isolation, but are tightly interconnected, e.g., the Internet of Things, we also envision a compositional approach to building ethically bounded AI, where the ethical properties of each component can be fruitfully exploited to derive those of the overall system. In this paper we define and motivate the notion of ethically-bounded AI, we describe two concrete examples, and we outline some outstanding challenges.","4169":"Artificial intelligence (AI), defined as intelligence exhibited by machines, has many applications in today's society, including robotics, mobile devices, smart transportation, healthcare service, and more. Recently, lots of AI investment in both big companies and startups have launched. Besides cloud-based solution, AI on the edge devices (Edge AI) takes the advantages of rapid response with low latency, high privacy, more robustness, and better efficient use of network bandwidth. To enable Edge AI, new embedded system technologies are desired, including machine learning, neural network acceleration and reduction, and heterogeneous run-time mechanism. This paper introduces challenges and technologies trend of Edge AI. In addition, it illustrates edge AI solutions from MediaTek, including the dedicated AI processing unit (APU) and NeuroPilot technology, which provides superior Edge AI ability in a wide range of applications.","4170":"No-limit Texas Hold'em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold'em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the first--and so far only--AI to defeat top human professionals in that game. Libratus's architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.","4171":"These recommendations reflect the views and research of the AI Now Institute at New York University. We thank the experts who contributed to the AI Now 2017 Symposium and Workshop for informing these perspectives, and our research team for helping shape the AI Now 2017 Report. Artificial intelligence (AI) technologies are in a phase of rapid development, and are being adopted widely. While the concept of artificial intelligence has existed for over sixty years, real-world applications have only accelerated in the last decade due to three concurrent developments: better algorithms, increases in networked computing power and the tech industry\u2019s ability to capture and store massive amounts of data. AI systems are already integrated in everyday technologies like smartphones and personal assistants, making predictions and determinations that help personalize experiences and advertise products. Beyond the familiar, these systems are also being introduced in critical areas like law, finance, policing and the workplace, where they are increasingly used to predict everything from our taste in music to our likelihood of committing a crime to our fitness for a job or an educational opportunity. AI companies promise that the technologies they create can automate the toil of repetitive work, identify subtle behavioral patterns and much more. However, the analysis and understanding of artificial intelligence should not be limited to its technical capabilities. The design and implementation of this next generation of computational tools presents deep normative and ethical challenges for our existing social, economic and political relationships and institutions, and these changes are already underway. Simply put, AI does not exist in a vacuum. We must also ask how broader phenomena like widening inequality, an intensification of concentrated geopolitical power and populist political movements will shape and be shaped by the development and application of AI technologies.","4172":"\n \n As AI continues to advance, human-AI teams are inevitable. However, progress in AI is routinely measured in isolation, without a human in the loop. It is crucial to benchmark progress in AI, not just in isolation, but also in terms of how it translates to helping humans perform certain tasks, i.e., the performance of human-AI teams. In this work, we design a cooperative game \u2014 GuessWhich \u2014 to measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the AI. The AI, which we call ALICE, is provided an image which is unseen by the human. Following a brief description of the image, the human questions ALICE about this secret image to identify it from a fixed pool of images. We measure performance of the human-ALICE team by the number of guesses it takes the human to correctly identify the secret image after a fixed number of dialog rounds with ALICE. We compare performance of the human-ALICE teams for two versions of ALICE. Our human studies suggest a counterintuitive trend \u2013 that while AI literature shows that one version outperforms the other when paired with an AI questioner bot, we find that this improvement in AI-AI performance does not translate to improved human-AI performance. This suggests a mismatch between benchmarking of AI in isolation and in the context of human-AI teams.\n \n","4173":"The rhetoric of the race for strategic advantage is increasingly being used with regard to the development of artificial intelligence (AI), sometimes in a military context, but also more broadly. This rhetoric also reflects real shifts in strategy, as industry research groups compete for a limited pool of talented researchers, and nation states such as China announce ambitious goals for global leadership in AI. This paper assesses the potential risks of the AI race narrative and of an actual competitive race to develop AI, such as incentivising corner-cutting on safe-ty and governance, or increasing the risk of conflict. It explores the role of the research community in respond-ing to these risks. And it briefly explores alternative ways in which the rush to develop powerful AI could be framed so as instead to foster collaboration and respon-sible progress.","4174":"A race for technological supremacy in AI could lead to serious negative consequences, especially whenever ethical and safety procedures are underestimated or even ignored, leading potentially to the rejection of AI in general. For all to enjoy the benefits provided by safe, ethical and trustworthy AI systems, it is crucial to incentivise participants with appropriate strategies that ensure mutually beneficial normative behaviour and safety-compliance from all parties involved. Little attention has been given to understanding the dynamics and emergent behaviours arising from this AI bidding war, and moreover, how to influence it to achieve certain desirable outcomes (e.g. AI for public good and participant compliance). To bridge this gap, this paper proposes a research agenda to develop theoretical models that capture key factors of the AI race, revealing which strategic behaviours may emerge and hypothetical scenarios therein. Strategies from incentive and agreement modelling are directly applicable to systematically analyse how different types of incentives (namely, positive vs. negative, peer vs. institutional, and their combinations) influence safety-compliant behaviours over time, and how such behaviours should be configured to ensure desired global outcomes, studying at the same time how these mechanisms influence AI development. This agenda will provide actionable policies, showing how they need to be employed and deployed in order to achieve compliance and thereby avoid disasters as well as loosing confidence and trust in AI in general.","4175":"Deep neural networks, or deep learning, as the field is also called, have the potential to revolutionize scientific discovery. But as these networks are applied to more and more disciplines, many scientists, whose very enterprise is founded on explanation, have been left with a nagging question: Why, model, why? This interpretability problem is galvanizing a new generation of researchers in both industry and academia. Just as the microscope revealed the cell, these researchers are crafting tools that will allow insight into how neural networks make decisions. Some tools probe the artificial intelligence (AI) without penetrating it; some are alternative algorithms that can compete with neural nets, but with more transparency; and some use still more deep learning to get inside the black box. Taken together, they add up to a new discipline. Some call it \"AI neuroscience.\"","4176":"Recently a number of well\u2010known public figures have expressed concern about the future development of artificial intelligence (AI), by noting that AI could get out of control and affect human beings and society in disastrous ways. Many of these cautionary notes are alarmist and unrealistic, and while there has been some pushback on these concerns, the deep flaws in the thinking that leads to them have not been called out. Much of the fear and trepidation is based on misunderstanding and confusion about what AI is and can ever be. In this work we identify 3 factors that contribute to this \u201cAI anxiety\u201d: an exclusive focus on AI programs that leaves humans out of the picture, confusion about autonomy in computational entities and in humans, and an inaccurate conception of technological development. With this analysis we argue that there are good reasons for anxiety about AI but not for the reasons typically given by AI alarmists.","4177":"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.","4178":null,"4179":"Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust. We contend that healthcare AI systems ought to be treated as assistive technologies that go beyond the usual functions of medical devices. As a result, we need to rethink regulation of healthcare AI systems to ensure they advance relevant values. We propose three distinct guidelines that can be universalized across federal regulatory boards to ensure that patient-doctor trust is not detrimentally affected by the deployment and widespread adoption of healthcare AI technologies.","4180":"Addressing ethical issues arising from AI research, and by extension from most areas of Data Science, is a core challenge in both the academic and industry worlds. The nature of research and the specific set of technical skills involved imply that AI and Data Science researchers are not equipped to identify and anticipate such issues arising, or to establish solutions at the time a specific research project is being designed. In this paper, we discuss the need for a methodology for ethical research design that involves a broader set of skills from the start of the project. We specifically identify, from the relevant literature, a set of requirements that we argue to be needed for such a methodology. We then explore two case studies where such ethical considerations have been explored in conjunction with the development of specific research projects, in order to validate those assumptions and generalise them into a set of principles guiding an \"Ethics by Design\" method for conducting AI and Data Science research.","4181":null,"4182":"Given the well-known limitations of the Turing Test, there is a need for objective tests to both focus attention on, and measure progress towards, the goals of AI. In this paper we argue that machine performance on standardized tests should be a key component of any new measure of AI, because attaining a high level of performance requires solving significant AI problems involving language understanding and world modeling - critical skills for any machine that lays claim to intelligence. In addition, standardized tests have all the basic requirements of a practical test: they are accessible, easily comprehensible, clearly measurable, and offer a graduated progression from simple tasks to those requiring deep understanding of the world. Here we propose this task as a challenge problem for the community, summarize our state-of-the-art results on math and science tests, and provide supporting datasets","4183":"In fall 2014, we launched a foundational course in artificial intelligence (CS7637: Knowledge-Based AI) as part of the Georgia Institute of Technology's Online Master of Science in Computer Science program. We incorporated principles and practices from the cognitive and learning sciences into the development of the online AI course. We also integrated AI techniques into the instruction of the course, including embedding 100 highly focused intelligent tutoring agents in the video lessons. By now, more than 2000 students have taken the course. Evaluations have indicated that OMSCS students enjoy the course compared to traditional courses, and more importantly, that online students have matched residential students' performance on the same assessments. In this article, we present the design, delivery, and evaluation of the course, focusing on the use of AI for teaching AI. We also discuss lessons we learned for scaling the teaching and learning of AI.","4184":"While there has been an explosion of impressive, data-driven AI applications in recent years, machines still largely lack a deeper understanding of the world to answer questions that go beyond information explicitly stated in text, and to explain and discuss those answers. To reach this next generation of AI applications, it is imperative to make faster progress in areas of knowledge, modeling, reasoning, and language. Standardized tests have often been proposed as a driver for such progress, with good reason: Many of the questions require sophisticated understanding of both language and the world, pushing the boundaries of AI, while other questions are easier, supporting incremental progress. In Project Aristo at the Allen Institute for AI, we are working on a specific version of this challenge, namely having the computer pass Elementary School Science and Math exams. Even at this level there is a rich variety of problems and question types, the most difficult requiring significant progress in AI. Here we propose this task as a challenge problem for the community, and are providing supporting datasets. Solutions to many of these problems would have a major impact on the field so we encourage you: Take the Aristo Challenge!","4185":"We report on a series of new platforms and events dealing with AI evaluation that may change the way in which AI systems are compared and their progress is measured. The introduction of a more diverse and challenging set of tasks in these platforms can feed AI research in the years to come, shaping the notion of success and the directions of the field. However, the playground of tasks and challenges presented there may misdirect the field without some meaningful structure and systematic guidelines for its organization and use. Anticipating this issue, we also report on several initiatives and workshops that are putting the focus on analyzing the similarity and dependencies between tasks, their difficulty, what capabilities they really measure and ultimately on elaborating new concepts and tools that can arrange tasks and benchmarks into a meaningful taxonomy.","4186":null,"4187":"From the enraged robots in the 1920 play R.U.R. to the homicidal computer H.A.L. in 2001: A Space Odyssey, science fiction writers have embraced the dark side of artificial intelligence (AI) ever since the concept entered our collective imagination. Sluggish progress in AI research, especially during the \u201cAI winter\u201d of the 1970s and 1980s, made such worries seem far-fetched. But recent breakthroughs in machine learning and vast improvements in computational power have brought a flood of research funding\u2014 and fresh concerns about where AI may lead us. One researcher now speaking up is Stuart Russell, a computer scientist at the University of California, Berkeley, who with Peter Norvig, director of research at Google, wrote the premier AI textbook, Artificial Intelligence: A Modern Approach, now in its third edition. Last year, Russell joined the Centre for the Study of Existential Risk at Cambridge University in the United Kingdom as an AI expert focusing on \u201crisks that could lead to human extinction.\u201d Among his chief concerns, which he aired at an April meeting in Geneva, Switzerland, run by the United Nations, is the danger of putting military drones and weaponry under the full control of AI systems. This interview has been edited for clarity and brevity.","4188":"This paper proposes a model for designing games around Artificial Intelligence (AI). AI-based games put AI in the foreground of the player experience rather than in a supporting role as is often the case in many commercial games. We analyze the use of AI in a number of existing games and identify design patterns for AI in games. We propose a generative ideation technique to combine a design pattern with an AI technique or capacity to make new AI-based games. Finally, we demonstrate this technique through two examples of AI-based game prototypes created using these patterns.","4189":"From the Publisher: \nThis text covers all the material needed to understand the principles behind the AI approach to robotics and to program an artificially intelligent robot for applications involving sensing, navigation, planning, and uncertainty. Robin Murphy is extremely effective at combining theoretical and practical rigor with a light narrative touch. In the overview, for example, she touches upon anthropomorphic robots from classic films and science fiction stories before delving into the nuts and bolts of organizing intelligence in robots. \nFollowing the overview, Murphy contrasts AI and engineering approaches and discusses what she calls the three paradigms of AI robotics: hierarchical, reactive, and hybrid deliberative\/reactive. Later chapters explore multiagent scenarios, navigation and path-planning for mobile robots, and the basics of computer vision and range sensing. Each chapter includes objectives, review questions, and exercises. Many chapters contain one or more case studies showing how the concepts were implemented on real robots. Murphy, who is well known for her classroom teaching, conveys the intellectual adventure of mastering complex theoretical and technical material.","4190":"This literature review covers AI techniques used for real-time strategy video games, focusing specifically on StarCraft. It finds that the main areas of current academic research are in tactical and strategic decision-making, plan recognition, and learning, and it outlines the research contributions in each of these areas. The paper then contrasts the use of game AI in academia and industry, finding the academic research heavily focused on creating game-winning agents, while the indus- try aims to maximise player enjoyment. It finds the industry adoption of academic research is low because it is either in- applicable or too time-consuming and risky to implement in a new game, which highlights an area for potential investi- gation: bridging the gap between academia and industry. Fi- nally, the areas of spatial reasoning, multi-scale AI, and co- operation are found to require future work, and standardised evaluation methods are proposed to produce comparable re- sults between studies.","4191":"Computational notebooks are documents that serve dual purposes: they serve as an archive format containing code, text, images and equations; but they can also be run like computer programs. This paper explores the use of these new computational notebooks to teach AI and introduces tools that we have developed \u2014 ICalico and Calysto \u2014 to facilitate that use. Not only do these new tools broaden the languages and contexts available to students exploring notebook-based AI computing, but they offer a new mode of teaching and learning for the AI classroom.","4192":"Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, different computational techniques related to Artificial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in Artificial Intelligence.","4193":"Predicting the development of artificial intelligence (AI) is a difficult project \u2013 but a vital one, according to some analysts. AI predictions are already abound: but are they reliable? This paper starts by proposing a decomposition schema for classifying them. Then it constructs a variety of theoretical tools for analysing, judging and improving them. These tools are demonstrated by careful analysis of five famous AI predictions: the initial Dartmouth conference, Dreyfus's criticism of AI, Searle's Chinese room paper, Kurzweil's predictions in the Age of Spiritual Machines, and Omohundro's \u2018AI drives\u2019 paper. These case studies illustrate several important principles, such as the general overconfidence of experts, the superiority of models over expert judgement and the need for greater uncertainty in all types of predictions. The general reliability of expert judgement in AI timeline predictions is shown to be poor, a result that fits in with previous studies of expert competence.","4194":"The possibility of creating thinking machines raises a host of ethical issues. These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves. The first section discusses issues that may arise in the near future of AI. The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence. The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status. In the fourth section, we consider how AIs might differ from humans in certain basic respects relevant to our ethical assessment of them. The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill. Bostrom, Nick, and Eliezer Yudkowsky. Forthcoming. \u201cThe Ethics of Artificial Intelligence.\u201d In Cambridge Handbook of Artificial Intelligence, edited by Keith Frankish and William Ramsey. New York: Cambridge University Press. This version contains minor changes. Nick Bostrom, Eliezer Yudkowsky 1. Ethics in Machine Learning and Other Domain-Specific AI Algorithms Imagine, in the near future, a bank using a machine learning algorithm to recommend mortgage applications for approval. A rejected applicant brings a lawsuit against the bank, alleging that the algorithm is discriminating racially against mortgage applicants. The bank replies that this is impossible, since the algorithm is deliberately blinded to the race of the applicants. Indeed, that was part of the bank\u2019s rationale for implementing the system. Even so, statistics show that the bank\u2019s approval rate for black applicants has been steadily dropping. Submitting ten apparently equally qualified genuine applicants (as determined by a separate panel of human judges) shows that the algorithm accepts white applicants and rejects black applicants. What could possibly be happening? Finding an answer may not be easy. If the machine learning algorithm is based on a complicated neural network, or a genetic algorithm produced by directed evolution, then it may prove nearly impossible to understand why, or even how, the algorithm is judging applicants based on their race. On the other hand, a machine learner based on decision trees or Bayesian networks is much more transparent to programmer inspection (Hastie, Tibshirani, and Friedman 2001), which may enable an auditor to discover that the AI algorithm uses the address information of applicants who were born or previously resided in predominantly poverty-stricken areas. AI algorithms play an increasingly large role in modern society, though usually not labeled \u201cAI.\u201d The scenario described above might be transpiring even as we write. It will become increasingly important to develop AI algorithms that are not just powerful and scalable, but also transparent to inspection\u2014to name one of many socially important properties. Some challenges of machine ethics are much like many other challenges involved in designing machines. Designing a robot arm to avoid crushing stray humans is no more morally fraught than designing a flame-retardant sofa. It involves new programming challenges, but no new ethical challenges. But when AI algorithms take on cognitive work with social dimensions-cognitive tasks previously performed by humans\u2014the AI algorithm inherits the social requirements. It would surely be frustrating to find that no bank in the world will approve your seemingly excellent loan application, and nobody knows why, and nobody can find out even in principle. (Maybe you have a first name strongly associated with deadbeats? Who knows?) Transparency is not the only desirable feature of AI. It is also important that AI algorithms taking over social functions be predictable to those they govern. To understand the importance of such predictability, consider an analogy. The legal principle of stare decisis binds judges to follow past precedent whenever possible. To an engineer, this","4195":"One of the enduring concerns of moral philosophy is deciding who or what is deserving of ethical consideration. Much recent attention has been devoted to the \"animal question\"--consideration of the moral status of nonhuman animals. In this book, David Gunkel takes up the \"machine question\": whether and to what extent intelligent and autonomous machines of our own making can be considered to have legitimate moral responsibilities and any legitimate claim to moral consideration. The machine question poses a fundamental challenge to moral thinking, questioning the traditional philosophical conceptualization of technology as a tool or instrument to be used by human agents. Gunkel begins by addressing the question of machine moral agency: whether a machine might be considered a legitimate moral agent that could be held responsible for decisions and actions. He then approaches the machine question from the other side, considering whether a machine might be a moral patient due legitimate moral consideration. Finally, Gunkel considers some recent innovations in moral philosophy and critical theory that complicate the machine question, deconstructing the binary agent--patient opposition itself. Technological advances may prompt us to wonder if the science fiction of computers and robots whose actions affect their human companions (think of HAL in 2001: A Space Odyssey) could become science fact. Gunkel's argument promises to influence future considerations of ethics, ourselves, and the other entities who inhabit this world.","4196":"This article focuses on contributions that AI can make to address long-term educational goals. It describes five challenges that would support: (1) mentors for every learner; (2) learning twenty-first century skills; (3) interaction data to support learning; (4) universal access to global classrooms; and (5) lifelong and life-wide learning. A vision and brief research agenda are described for each challenge along with goals that lead to access to global educational resources and the reuse and sharing of digital educational resources. Instructional systems with AI technology are described that currently support richer experiences for learners and supply researchers with new opportunities to analyze vast data sets of instructional behavior from big databases, containing elements of learning, affect, motivation, and social interaction. Personalized learning is described using computational tools that enhance student and group experience, reflection, and analysis, and supply data for development of novel theory development.","4197":"Computer simulation and predictive models are widely used in engineering, much less considered in life sciences. We present an initiative aimed to establish a dialogue within the community of scientists, regulators, industry representatives, offering a platform which combines the predictive capability of computer models, with some explanation tools, which may be convincing and helpful for human users to derive a conclusion. The resulting system covers a large set of toxicological endpoints.","4198":null,"4199":"Markov Decision Processes (MDPs) are widely popular in Artificial Intelligence for modeling sequential decision-making scenarios with probabilistic dynamics. They are the framework of choice when designing an intelligent agent that needs to act for long periods of time in an environment where its actions could have uncertain outcomes. MDPs are actively researched in two related subareas of AI, probabilistic planning and reinforcement learning. Probabilistic planning assumes known models for the agent's goals and domain dynamics, and focuses on determining how the agent should behave to achieve its objectives. On the other hand, reinforcement learning additionally learns these models based on the feedback the agent gets from the environment. This book provides a concise introduction to the use of MDPs for solving probabilistic planning problems, with an emphasis on the algorithmic perspective. It covers the whole spectrum of the field, from the basics to state-of-the-art optimal and approximation algorithms. We first describe the theoretical foundations of MDPs and the fundamental solution techniques for them. We then discuss modern optimal algorithms based on heuristic search and the use of structured representations. A major focus of the book is on the numerous approximation schemes for MDPs that have been developed in the AI literature. These include determinization-based approaches, sampling techniques, heuristic functions, dimensionality reduction, and hierarchical representations. Finally, we briefly introduce several extensions of the standard MDP classes that model and solve even more complex planning problems. Table of Contents: Introduction \/ MDPs \/ Fundamental Algorithms \/ Heuristic Search Algorithms \/ Symbolic Algorithms \/ Approximation Algorithms \/ Advanced Notes","4200":"Game AI Pro2: Collected Wisdom of Game AI Professionals presents cutting-edge tips, tricks, and techniques for artificial intelligence (AI) in games, drawn from developers of shipped commercial games as well as some of the best-known academics in the field. It contains knowledge, advice, hard-earned wisdom, and insights gathered from across the community of developers and researchers who have devoted themselves to game AI. In this book, 47 expert developers and researchers have come together to bring you their newest advances in game AI, along with twists on proven techniques that have shipped in some of the most successful commercial games of the last few years. The book provides a toolbox of proven techniques that can be applied to many common and not-so-common situations. It is written to be accessible to a broad range of readers. Beginners will find good general coverage of game AI techniques and a number of comprehensive overviews, while intermediate to expert professional game developers will find focused, deeply technical chapters on specific topics of interest to them. Covers a wide range of AI in games, with topics applicable to almost any game Touches on most, if not all, of the topics necessary to get started in game AI Provides real-life case studies of game AI in published commercial games Gives in-depth, technical solutions from some of the industrys best-known games Includes downloadable demos and\/or source code, available at http:\/\/www.gameaipro.com","4201":"We give a brief overview of the Mario AI Championship, a series of competitions based on an open source clone of the seminal platform game Super Mario Bros. The competition has four tracks. The gameplay and learning tracks resemble traditional reinforcement learning competitions, the Level generation track focuses on the generation of entertaining game levels, and the Turing Test track focuses on humanlike game-playing behavior. We also outline some lessons learned from the competition and its future. The article is written by the four organizers of the competition.","4202":"More than a decade after the early research efforts on the use of artificial intelligence (AI) in computer games and the establishment of a new AI domain the term ``game AI'' needs to be redefined. Traditionally, the tasks associated with game AI revolved around non player character (NPC) behavior at different levels of control, varying from navigation and pathfinding to decision making. Commercial-standard games developed over the last 15 years and current game productions, however, suggest that the traditional challenges of game AI have been well addressed via the use of sophisticated AI approaches, not necessarily following or inspired by advances in academic practices. The marginal penetration of traditional academic game AI methods in industrial productions has been mainly due to the lack of constructive communication between academia and industry in the early days of academic game AI, and the inability of academic game AI to propose methods that would significantly advance existing development processes or provide scalable solutions to real world problems. Recently, however, there has been a shift of research focus as the current plethora of AI uses in games is breaking the non-player character AI tradition. A number of those alternative AI uses have already shown a significant potential for the design of better games.\n This paper presents four key game AI research areas that are currently reshaping the research roadmap in the game AI field and evidently put the game AI term under a new perspective. These game AI flagship research areas include the computational modeling of player experience, the procedural generation of content, the mining of player data on massive-scale and the alternative AI research foci for enhancing NPC capabilities.","4203":"This paper describes the Mario AI benchmark, a game-based benchmark for reinforcement learning algorithms and game AI techniques developed by the authors. The benchmark is based on a public domain clone of Nintendo's classic platform game Super Mario Bros, and completely open source. During the last two years, the benchmark has been used in a number of competitions associated with international conferences, and researchers and students from around the world have contributed diverse solutions to try to beat the benchmark. The paper summarizes these contributions, gives an overview of the state of the art in Mario-playing AIs, and chronicles the development of the benchmark. This paper is intended as the definitive point of reference for those using the benchmark for research or teaching.","4204":null,"4205":null,"4206":"Traditionally focused on good old-fashioned AI and robotics, the Spanish AI community holds a vigorous computational intelligence substrate. Neuromorphic, evolutionary, or fuzzylike systems have been developed by many research groups in the Spanish computer sciences. It is no surprise, then, that these naturegrounded efforts start to emerge, enriching the AI catalogue of research projects and publications and, eventually, leading to new directions of basic or applied research. In this article, we review the contribution of Melomics in computational creativity.","4207":null,"4208":null,"4209":"The advent of Web 2.0 and social media content has stirred much excitement and created abundant opportunities for understanding the opinions of the general public and consumers toward social events, political movements, company strategies, marketing campaigns, and product preferences. Many new and exciting social, geopolitical, and business-related research questions can be answered by analyzing the thousands, even millions, of comments and responses expressed in various blogs (such as the blogosphere), forums (such as Yahoo Forums), social media and social network sites (including YouTube, Facebook, and Flikr), virtual worlds (such as Second Life), and tweets (Twitter). Opinion mining, a subdiscipline within data mining and computational linguistics, refers to the computational techniques for extracting, classifying, understanding, and assessing the opinions expressed in various online news sources, social media comments, and other user-generated content. Sentiment analysis is often used in opinion mining to identify sentiment, affect, subjectivity, and other emotional states in online text.","4210":"In less than 2 months, the artificial intelligence (AI) program ChatGPT has become a cultural sensation. It is freely accessible through a web portal created by the tool\u2019s developer, OpenAI. The program\u2014which automatically creates text based on written prompts\u2014is so popular that it\u2019s likely to be \u201cat capacity right now\u201d if you attempt to use it. When you do get through, ChatGPT provides endless entertainment. I asked it to rewrite the first scene of the classic American play Death of a Salesman, but to feature Princess Elsa from the animated movie Frozen as the main character instead of Willy Loman. The output was an amusing conversation in which Elsa\u2014who has come home from a tough day of selling\u2014is told by her son Happy, \u201cCome on, Mom. You\u2019re Elsa from Frozen. You have ice powers and you\u2019re a queen. You\u2019re unstoppable.\u201d Mash-ups like this are certainly fun, but there are serious implications for generative AI programs like ChatGPT in science and academia.","4211":null,"4212":"Classic approaches to game AI require either a high quality of domain knowledge, or a long time to generate effective AI behaviour. These two characteristics hamper the goal of establishing challenging game AI. In this paper, we put forward Monte-Carlo Tree Search as a novel, unified framework to game AI. In the framework, randomized explorations of the search space are used to predict the most promising game actions. We will demonstrate that Monte-Carlo Tree Search can be applied effectively to (1) classic board-games, (2) modern board-games, and (3) video games.","4213":"The paper contributes to the development of the theory of AI-Completeness by formalizing the notion of AIComplete and AI-Hard problems. The intended goal is to provide a classification of problems in the field of General Artificial Intelligence. We prove Turing Test to be an instance of an AI-Complete problem and further show numerous AI problems to be AI-Complete or AI-Hard via polynomial time reductions. Finally, the paper suggests some directions for future work on the theory of AI-Completeness. KeywordsAI-Complete, AI-Easy, AI-Hard, Human","4214":"Today, approximately 10 percent of the world's population is over the age of 60; by 2050 this proportion will have more than doubled. Moreover, the greatest rate of increase is amongst the \"oldest old,\" people aged 85 and over. While many older adults remain healthy and productive, overall this segment of the population is subject to physical and cognitive impairment at higher rates than younger people. This article surveys new technologies that incorporate artificial intelligence techniques to support older adults and help them cope with the changes of aging, in particular with cognitive decline.","4215":"The use of artificial intelligence in academia is a hot topic in the education field. ChatGPT is an AI tool that offers a range of benefits, including increased student engagement, collaboration, and accessibility. However, is also raises concerns regarding academic hon-esty and plagiarism. This paper examines the opportunities and challenges of using ChatGPT in higher education, and discusses the potential risks and rewards of these tools. The paper also considers the difficulties of detecting and preventing academic dishonesty, and suggests strategies that universities can adopt to ensure ethical and responsible use of these tools. These strategies include developing policies and procedures, providing training and support, and using various methods to detect and prevent cheating. The paper concludes that while the use of AI in higher education presents both opportunities and challenges, universities can effectively address these concerns by taking a proactive and ethical approach to the use of these tools.","4216":null,"4217":null,"4218":null,"4219":"The Level Generation Competition, part of the IEEE Computational Intelligence Society (CIS)-sponsored 2010 Mario AI Championship, was to our knowledge the world's first procedural content generation competition. Competitors participated by submitting level generators - software that generates new levels for a version of Super Mario Bros tailored to individual players' playing style. This paper presents the rules of the competition, the software used, the scoring procedure, the submitted level generators, and the results of the competition. We also discuss what can be learned from this competition, both about organizing procedural content generation competitions and about automatically generating levels for platform games. The paper is coauthored by the organizers of the competition (the first three authors) and the competitors.","4220":null,"4221":"Artificial intelligence (AI) is the science that allows\ncomputers to replicate human intelligence in areas such as\ndecision-making, text processing, visual perception. Artificial\nIntelligence is the broader field that contains several subfields\nsuch as machine learning, robotics, and computer vision.\nMachine Learning is a branch of Artificial Intelligence that\nallows a machine to learn and improve at a task over time. Deep\nLearning is a subset of machine learning that makes use of deep\nartificial neural networks for training. The paper proposed on\noutlier detection for multivariate high dimensional data for\nAutoencoder unsupervised model.","4222":"How well can AI models write law school exams without human assistance? To find out","4223":null,"4224":"\ufffd Although one of the fundamental goals of AI is to understand and develop intelligent systems that have all the capabilities of humans, there is little active research directly pursuing this goal. We propose that AI for interactive computer games is an emerging application area in which this goal of human-level AI can successfully be pursued. Interactive computer games have increasingly complex and realistic worlds and increasingly complex and intelligent computer-controlled characters. In this article, we further motivate our proposal of using interactive computer games for AI research, review previous research on AI and games, and present the different game genres and the roles that human-level AI could play within these genres. We then describe the research issues and AI techniques that are relevant to each of these roles. Our conclusion is that interactive computer games provide a rich environment for incremental research on human-level AI.","4225":"In this tutorial paper, we look into the evolution and prospect of network architecture and propose a novel conceptual architecture for the 6th generation (6G) networks. The proposed architecture has two key elements, i.e., holistic network virtualization and pervasive artificial intelligence (AI). The holistic network virtualization consists of network slicing and digital twin, from the aspects of service provision and service demand, respectively, to incorporate service-centric and user-centric networking. The pervasive network intelligence integrates AI into future networks from the perspectives of networking for AI and AI for networking, respectively. Building on holistic network virtualization and pervasive network intelligence, the proposed architecture can facilitate three types of interplay, i.e., the interplay between digital twin and network slicing paradigms, between model-driven and data-driven methods for network management, and between virtualization and AI, to maximize the flexibility, scalability, adaptivity, and intelligence for 6G networks. We also identify challenges and open issues related to the proposed architecture. By providing our vision, we aim to inspire further discussions and developments on the potential architecture of 6G.","4226":"For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https:\/\/github.com\/WooooDyy\/LLM-Agent-Paper-List.","4227":"When I was teaching at MIT in the 1960s, students from the Artificial Intelligence Laboratory would come to my Heidegger course and say in effect: \u2018\u2018You philosophers have been reflecting in your armchairs for over 2000 years and you still don\u2019t understand intelligence. We in the AI Lab have taken over and are succeeding where you philosophers have failed.\u2019\u2019 But in 1963, when I was invited to evaluate the work of Alan Newell and Herbert Simon on physical symbol systems, I found to my surprise that, far from replacing philosophy, these pioneering researchers had learned a lot, directly and indirectly, from us philosophers: e.g., Hobbes\u2019 claim that reasoning was calculating, Descartes\u2019 mental representations, Leibniz\u2019s idea of a \u2018universal characteristic\u2019 (a set of primitives in which all knowledge could be expressed), Kant\u2019s claim that concepts were rules, Frege\u2019s formalization of such rules, and Wittgenstein\u2019s postulation of logical atoms in his Tractatus. In short, without realizing it, AI researchers were hard at work turning rationalist philosophy into a research program. But I began to suspect that the insights formulated in existentialist armchairs, especially Heidegger\u2019s and Merleau-Ponty\u2019s, were bad news for those working in AI laboratories\u2014that, by combining representationalism, conceptualism, formalism,","4228":"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.","4229":"Humans have a remarkable capability to perform a wide variety of physical and mental tasks without any measurements and any computations. Familar examples are: parking a car, driving in city traffic, playing golf, cooking a meal, and summarizing a story. In performing such tasks, humans employ perceptions of time, direction, speed, shape, possibility, likelihood, truth, and other attributes of physical and mental objects. Reflecting the bounded ability of the human brain to resolve detail, perceptions are intrinsically imprecise. In more concrete terms, perceptions are f-granular, meaning that (a) the boundaries of perceived classes are unsharp; and (b) the values of attributes are granulated, with a granule being a clump of values (points, objects) drawn together by indistinguishability, similarity, proximity, and functionality. For example, the granules of age might be labeled very young, young, middle-age, old, very old, etc.","4230":"In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.","4231":"We provide an overview of more than two decades of work, mostly in AI, that studies computational complexity as a barrier against manipulation in elections.","4232":"Games are played by a wide variety of audiences. Different individuals will play with different gaming styles and employ different strategic approaches. This often involves interacting with nonplayer characters that are controlled by the game AI. From a developer's standpoint, it is important to design a game AI that is able to satisfy the variety of players that will interact with the game. Thus, an adaptive game AI that can scale the difficulty of the game according to the proficiency of the player has greater potential to customize a personalized and entertaining game experience compared to a static game AI. In particular, dynamic game difficulty scaling refers to the use of an adaptive game AI that performs game adaptations in real time during the game session. This paper presents two adaptive algorithms that use ideas from reinforcement learning and evolutionary computation to improve player satisfaction by scaling the difficulty of the game AI while the game is being played. The effects of varying the learning and mutation rates are examined and a general rule of thumb for the parameters is proposed. The proposed algorithms are demonstrated to be capable of matching its opponents in terms of mean scores and winning percentages. Both algorithms are able to generalize well to a variety of opponents.","4233":"The Alchemy package provides a series of algorithms for statistical relational learning and probabilistic logic inference, based on the Markov logic representation. If you are not already familiar with Markov logic, we recommend that you read the papers Markov Logic Networks [7], Discriminative Training of Markov Logic Networks [9], Learning the Structure of Markov Logic Networks [3], Memory-Efficient Inference in Relational Domains [10] and Sound and Efficient Inference with Probabilistic and Deterministic Dependencies [6] (mln.pdf, dtmln.pdf, lsmln.pdf, lazysat.pdf and mcsat.pdf in the papers\/ directory) before reading this manual. We welcome your feedback on any aspect of the Alchemy package. Please email us at alchemy@cs.washington.edu to let us know what you find easy or hard to use, what results you have obtained with Alchemy, the features you wish to have but are not currently provided, and any bugs that you encounter. Please cite Kok et al. (2005) [4] if you use the Alchemy system.","4234":"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.","4235":"Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner. WSD is considered an AI-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. We overview supervised, unsupervised, and knowledge-based approaches. The assessment of WSD systems is discussed in the context of the Senseval\/Semeval campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. Finally, applications, open problems, and future directions are discussed.","4236":"Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.","4237":"Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.","4238":null,"4239":"Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. \nThe following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between \"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.","4240":"Fog is an emergent architecture for computing, storage, control, and networking that distributes these services closer to end users along the cloud-to-things continuum. It covers both mobile and wireline scenarios, traverses across hardware and software, resides on network edge but also over access networks and among end users, and includes both data plane and control plane. As an architecture, it supports a growing variety of applications, including those in the Internet of Things (IoT), fifth-generation (5G) wireless systems, and embedded artificial intelligence (AI). This survey paper summarizes the opportunities and challenges of fog, focusing primarily in the networking context of IoT.","4241":null,"4242":"Data generated as a side effect of game play also solves computational problems and trains AI algorithms.","4243":null,"4244":"The Internet of Vehicles (IoV) plays a crucial role in providing diversified services because of its powerful capability of collecting real-time information. Generally, collected information is transmitted to a centralized resource-intensive cloud platform for service implementation. Edge Computing (EC) that deploys physical resources near road-side units is involved in IoV to support real-time services for vehicular users. Additionally, many measures are adopted to optimize the performance of EC-enabled IoV, but they hardly help make dynamic decisions according to real-time requests. Artificial Intelligence (AI) is capable of enhancing the learning capacity of edge devices and thus assists in allocating resources dynamically. Although extensive research has employed AI to optimize EC performance, summaries with relative concepts or prospects are quite few. To address this gap, we conduct an exhaustive survey about utilizing AI in edge service optimization in IoV. Firstly, we establish the general condition and relative concepts about IoV, EC, and AI. Secondly, we review the edge service frameworks for IoV and explore the use of AI in edge server placement and service offloading. Finally, we discuss a number of open issues in optimizing edge services with AI.","4245":"dent on automated intelligent systems; at the same time, these systems have become more and more complicated. Society\u2019s expectation regarding the capabilities and intelligence of such systems has also grown. We have become a more complicated society with more complicated problems. As the expectation of intelligent systems rises, we discover many more applications for AI. Additionally, as the difficulty level and computational requirements of such problems rise, there is a need to distribute the problem solving. Although the field of multiagent systems and distributed AI is relatively young, the importance and applicability of this technology for solving today\u2019s problems continues to grow. As the title indicates, Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence covers the design and development of multiagent and distributed AI systems. The purpose of this book is to provide a comprehensive overview of the field. It is an excellent collection of closely related papers that provides a wonderful introduction to multiagent systems and distributed AI. The book provides not only basic introductory information but also detailed discussions covering the important topics in the field, practical examples and applications, and a section dedicated to the relationship between multiagent systems and various other research areas. This book compiles the important concepts and methodologies required to develop a multiagent system in an understandable, and comprehensive, manner. Not only does the book focus on the known solutions and issues, it also discusses the open questions and dilemmas. The prologue begins by defining distributed AI as \u201cthe study, construc-","4246":"Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large datasets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.","4247":"This paper describes the 2009 Mario AI Competition, which was run in association with the IEEE Games Innovation Conference and the IEEE Symposium on Computational Intelligence and Games. The focus of the competition was on developing controllers that could play a version of Super Mario Bros as well as possible. We describe the motivations for holding this competition, the challenges associated with developing artificial intelligence for platform games, the software and API developed for the competition, the competition rules and organization, the submitted controllers and the results. We conclude the paper by discussing what the outcomes of the competition can teach us both about developing platform game AI and about organizing game AI competitions. The first two authors are the organizers of the competition, while the third author is the winner of the competition.","4248":"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.","4249":"\nPurpose\nThe service sector is at an inflection point with regard to productivity gains and service industrialization similar to the industrial revolution in manufacturing that started in the eighteenth century. Robotics in combination with rapidly improving technologies like artificial intelligence (AI), mobile, cloud, big data and biometrics will bring opportunities for a wide range of innovations that have the potential to dramatically change service industries. The purpose of this paper is to explore the potential role service robots will play in the future and to advance a research agenda for service researchers.\n\n\nDesign\/methodology\/approach\nThis paper uses a conceptual approach that is rooted in the service, robotics and AI literature.\n\n\nFindings\nThe contribution of this paper is threefold. First, it provides a definition of service robots, describes their key attributes, contrasts their features and capabilities with those of frontline employees, and provides an understanding for which types of service tasks robots will dominate and where humans will dominate. Second, this paper examines consumer perceptions, beliefs and behaviors as related to service robots, and advances the service robot acceptance model. Third, it provides an overview of the ethical questions surrounding robot-delivered services at the individual, market and societal level.\n\n\nPractical implications\nThis paper helps service organizations and their management, service robot innovators, programmers and developers, and policymakers better understand the implications of a ubiquitous deployment of service robots.\n\n\nOriginality\/value\nThis is the first conceptual paper that systematically examines key dimensions of robot-delivered frontline service and explores how these will differ in the future.\n","4250":null,"4251":"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be \"entangled\" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.","4252":"One might imagine that AI systems with harmless goals will be harmless. This paper instead shows that intelligent systems will need to be carefully designed to prevent them from behaving in harmful ways. We identify a number of \u201cdrives\u201d that will appear in sufficiently advanced AI systems of any design. We call them drives because they are tendencies which will be present unless explicitly counteracted. We start by showing that goal-seeking systems will have drives to model their own operation and to improve themselves. We then show that self-improving systems will be driven to clarify their goals and represent them as economic utility functions. They will also strive for their actions to approximate rational economic behavior. This will lead almost all systems to protect their utility functions from modification and their utility measurement systems from corruption. We also discuss some exceptional systems which will want to modify their utility functions. We next discuss the drive toward self-protection which causes systems try to prevent themselves from being harmed. Finally we examine drives toward the acquisition of resources and toward their efficient utilization. We end with a discussion of how to incorporate these insights in designing intelligent technology which will lead to a positive future for humanity.","4253":"During the outbreak time of COVID-19, computed tomography (CT) is a useful manner for diagnosing COVID-19 patients. Due to privacy issues, publicly available COVID-19 CT datasets are highly difficult to obtain, which hinders the research and development of AI-powered diagnosis methods of COVID-19 based on CTs. To address this issue, we build an open-sourced dataset -- COVID-CT, which contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19 CTs. The utility of this dataset is confirmed by a senior radiologist who has been diagnosing and treating COVID-19 patients since the outbreak of this pandemic. We also perform experimental studies which further demonstrate that this dataset is useful for developing AI-based diagnosis models of COVID-19. Using this dataset, we develop diagnosis methods based on multi-task learning and self-supervised learning, that achieve an F1 of 0.90, an AUC of 0.98, and an accuracy of 0.89. According to the senior radiologist, models with such performance are good enough for clinical usage. The data and code are available at this https URL","4254":"We consider the fundamental problem of solving quadratic systems of equations in yi=|\u3008ai,x\u3009|2,\u2009i=1,\u2026,m , and x\u2208\u211dn is unknown. We propose a novel method, which starts with an initial guess computed by means of a spectral method and proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach [13]. There are several key distinguishing features, most notably a distinct objective functional and novel update rules, which operate in an adaptive fashion and drop terms bearing too much influence on the search direction. These careful selection rules provide a tighter initial guess, better descent directions, and thus enhanced practical performance. On the theoretical side, we prove that for certain unstructured models of quadratic systems, our algorithms return the correct solution in linear time, i.e., in time proportional to reading the data {ai} and {yi} as soon as the ratio m\/n between the number of equations and unknowns exceeds a fixed numerical constant. We extend the theory to deal with noisy systems in which we only have yi\u2248|\u3008ai,x\u3009|2 and prove that our algorithms achieve a statistical accuracy, which is nearly unimprovable. We complement our theoretical study with numerical examples showing that solving random quadratic systems is both computationally and statistically not much harder than solving linear systems of the same size\u2014hence the title of this paper. For instance, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least squares problem of the same size. \u00a9 2016 Wiley Periodicals, Inc.","4255":"This introduction to this special issue discusses artificial intelligence (AI), commonly defined as \u201ca system\u2019s ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.\u201d It summarizes seven articles published in this special issue that present a wide variety of perspectives on AI, authored by several of the world\u2019s leading experts and specialists in AI. It concludes by offering a comprehensive outlook on the future of AI, drawing on micro-, meso-, and macro-perspectives.","4256":"The 1998 Planning Competition at the AI Planning Systems Conference was the first of its kind. Its goal was to create planning domains that a wide variety of planning researchers could agree on to make comparison among planners more meaningful, measure overall progress in the field, and set up a framework for long-term creation of a repository of problems in a standard notation. A rules committee for the competition was created in 1997 and had long discussions on how the contest should go. One result of these discussions was the pddl notation for planning domains. This notation was used to set up a set of planning problems and get a modest problem repository started. As a result, five planning systems were able to compete when the contest took place in June 1998. All these systems solved problems in the strips framework, with some slight extensions. The attempt to find domains for other forms of planning foundered because of technical and organizational problems. In spite of this problem, the competition achieved its goals partially in that it con-firmed that substantial progress had occurred in some subfields of planning, and it allowed qualitative comparison among different planning algorithms. It is urged that the competition continue to take place and to evolve.","4257":"We present a new AI task - Embodied Question Answering(EmbodiedQA) - where an agent is spawned at a random location in a 3D environment and asked a question ('What color is the car?'). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question ('orange'). EmbodiedQA requires a range of AI skills - language understanding, visual recognition, active perception, goal-driven navigation, commonsense reasoning, long-term memory, and grounding language into actions. In this work, we develop a dataset of questions and answers in House3D environments [1], evaluation metrics, and a hierarchical model trained with imitation and reinforcement learning.","4258":"The title of this technical report says almost everything: this is indeed \"a short bibliography on AI and the arts\". It is presented in four sections: General Arguments, Proposals, and Approaches (31 references); Artificial Intelligence in Music (124 references); Artificial Intelligence in Literature and the Performing Arts (13 references), and Artificial Intelligence and Visual Art (57 references). About a quarter of these have short abstracts. Creating a bibliography can be a monumental task, and this bibliography should be viewed as a good and useful start, though it is by no means complete. For comparison, consider the 4,585-entry bibliography Computer Applications in Music by Deta Davis (A-REditions). No direct comparison is intended (or possible), but my point is that many more papers are likely to exist. As a rough check, I looked for several pre-1990 AI and Music articles and books (including my own, of course) in the bibliography. Out of five papers from well-known sources, only one was listed. On the other hand, I discovered a number of papers in this report that were unknown to me, so I am grateful to have a new source of references. In their introduction, the authors acknowledge the need for more references and even offer.a cup of coffee in reward for each new one. I will be sending a number of contributions, so the next time anyone is in Vienna, the coffee is on me. I hope the authors will continue to collect abstracts and publish an updated report in the future.","4259":"From the Publisher: \nResearchers now agree that intelligence always manifests itself in behavior - thus it is behavior that we must understand. An exciting new field has grown around the study of behavior-based intelligence, also known as embodied cognitive science, \"new AI,\" and \"behavior-based AI.\". \"Rolf Pfeifer and Christian Scheier provide a systematic introduction to this new way of thinking about intelligence and computers. After discussing concepts and approaches such as subsumption architecture, Braitenberg vehicles, evolutionary robotics, artificial life, self-organization, and learning, the authors derive a set of principles and a coherent framework for the study of naturally and artificially intelligent systems, or autonomous agents. This framework is based on a synthetic methodology whose goal is understanding by designing and building.","4260":"Background Large language models such as ChatGPT can produce increasingly realistic text, with unknown information on the accuracy and integrity of using these models in scientific writing. Methods We gathered ten research abstracts from five high impact factor medical journals (n=50) and asked ChatGPT to generate research abstracts based on their titles and journals. We evaluated the abstracts using an artificial intelligence (AI) output detector, plagiarism detector, and had blinded human reviewers try to distinguish whether abstracts were original or generated. Results All ChatGPT-generated abstracts were written clearly but only 8% correctly followed the specific journal\u2019s formatting requirements. Most generated abstracts were detected using the AI output detector, with scores (higher meaning more likely to be generated) of median [interquartile range] of 99.98% [12.73, 99.98] compared with very low probability of AI-generated output in the original abstracts of 0.02% [0.02, 0.09]. The AUROC of the AI output detector was 0.94. Generated abstracts scored very high on originality using the plagiarism detector (100% [100, 100] originality). Generated abstracts had a similar patient cohort size as original abstracts, though the exact numbers were fabricated. When given a mixture of original and general abstracts, blinded human reviewers correctly identified 68% of generated abstracts as being generated by ChatGPT, but incorrectly identified 14% of original abstracts as being generated. Reviewers indicated that it was surprisingly difficult to differentiate between the two, but that the generated abstracts were vaguer and had a formulaic feel to the writing. Conclusion ChatGPT writes believable scientific abstracts, though with completely generated data. These are original without any plagiarism detected but are often identifiable using an AI output detector and skeptical human reviewers. Abstract evaluation for journals and medical conferences must adapt policy and practice to maintain rigorous scientific standards; we suggest inclusion of AI output detectors in the editorial process and clear disclosure if these technologies are used. The boundaries of ethical and acceptable use of large language models to help scientific writing remain to be determined.","4261":"The last decade witnessed increasingly rapid progress in self\u2010driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence (AI). The objective of this paper is to survey the current state\u2010of\u2010the\u2010art on deep learning technologies used in autonomous driving. We start by presenting AI\u2010based self\u2010driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration, and motion control algorithms. We investigate both the modular perception\u2010planning\u2010action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing AI architectures for autonomous driving, such as their safety, training data sources, and computational hardware. The comparison presented in this survey helps gain insight into the strengths and limitations of deep learning and AI approaches for autonomous driving and assist with design choices.","4262":null,"4263":"Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players\u2019 beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game. Description AI masters Diplomacy The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players\u2019 motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. \u2014YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.","4264":"In parallel with the rapid adoption of artificial intelligence (AI) empowered by advances in AI research, there has been growing awareness and concerns of data privacy. Recent significant developments in the data regulation landscape have prompted a seismic shift in interest toward privacy-preserving AI. This has contributed to the popularity of Federated Learning (FL), the leading paradigm for the training of machine learning models on data silos in a privacy-preserving manner. In this survey, we explore the domain of personalized FL (PFL) to address the fundamental challenges of FL on heterogeneous data, a universal characteristic inherent in all real-world datasets. We analyze the key motivations for PFL and present a unique taxonomy of PFL techniques categorized according to the key challenges and personalization strategies in PFL. We highlight their key ideas, challenges, opportunities, and envision promising future trajectories of research toward a new PFL architectural design, realistic PFL benchmarking, and trustworthy PFL approaches.","4265":null,"4266":"Artificial intelligence (AI) plays a growing role in remote sensing (RS). Applications of AI, particularly machine learning algorithms, range from initial image processing to high-level data understanding and knowledge discovery. AI techniques have emerged as a powerful strategy for analyzing RS data and led to remarkable breakthroughs in all RS fields. Given this period of breathtaking evolution, this work aims to provide a comprehensive review of the recent achievements of AI algorithms and applications in RS data analysis. The review includes more than 270 research papers, covering the following major aspects of AI innovation for RS: machine learning, computational intelligence, AI explicability, data mining, natural language processing (NLP), and AI security. We conclude this review by identifying promising directions for future research.","4267":null,"4268":"The Internet of Things (IoT) is penetrating many facets of our daily life with the proliferation of intelligent services and applications empowered by artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and processing that may not be feasible in realistic application scenarios due to the high scalability of modern IoT networks and growing data privacy concerns. Federated Learning (FL) has emerged as a distributed collaborative AI approach that can enable many intelligent IoT applications, by allowing for AI training at distributed IoT devices without the need for data sharing. In this article, we provide a comprehensive survey of the emerging applications of FL in IoT networks, beginning from an introduction to the recent advances in FL and IoT to a discussion of their integration. Particularly, we explore and analyze the potential of FL for enabling a wide range of IoT services, including IoT data sharing, data offloading and caching, attack detection, localization, mobile crowdsensing, and IoT privacy and security. We then provide an extensive survey of the use of FL in various key IoT applications such as smart healthcare, smart transportation, Unmanned Aerial Vehicles (UAVs), smart cities, and smart industry. The important lessons learned from this review of the FL-IoT services and applications are also highlighted. We complete this survey by highlighting the current challenges and possible directions for future research in this booming area.","4269":"Many problems in AI study can be traced back to the confusion of different research goals. In this paper, five typical ways to define AI are clarified, analyzed, and compared. It is argued that though they are all legitimate research goals, they lead the research to very different directions, and most of them have trouble to give AI a proper identity. Finally, a working definition of AI is proposed, which has important advantages over the alternatives.","4270":"From the Publisher: \nLearn how AI experts create intelligent game objects and characters with this firstvolume in the AI Game Programming Wisdom series. This unique collection of articles gives programmers and developers access to the insights and wisdom of over thirty AI pros. Each article delves deep into key AI game programming issues and provides insightful new ideas and techniques that can be easily integrated into your own games. Everything from general AI architectures, rule based systems, level-of-detail AI, scripting language issues, to expert systems, fuzzy logic, neural networks, and genetic algorithms are thoroughly covered. If you're a game programmer (AI\/logic, front-end, user interface, tools, graphics, etc.) this comprehensive resource will help you take your skills and knowledge to the next level. \nKEY FEATURES \n- Contains new AI techniques and solutions written by over thirty industry experts \n- Provides comprehensive coverage of all aspects of AI programming \n- Includes ready-to-use ideas and code \n- Provides skill enhancement for beginning\/ intermediate programmers, and insightful new ideas for the pros \n- Companion CD includes all code for easy implementation \nAuthor Biography: Steve Rabin is a 10-year video game industry veteran working at Nintendo of America. He's written AI for three published games and was a contributor to both Game Programming Gems 1 and 2. He was also the AI section editor for Game Programming Gems 2. He's spoken on AI at the Game Developers Conference and holds a degree in Computer Engineering from the University of Washington, where he specialized in robotics.","4271":"Current approaches to adaptive game AI typically require numerous trials to learn effective behavior (i.e., game adaptation is not rapid). In addition, game developers are concerned that applying adaptive game AI may result in uncontrollable and unpredictable behavior (i.e., game adaptation is not reliable). These characteristics hamper the incorporation of adaptive game AI in commercially available video games. In this paper, we discuss an alternative to these current approaches. Our alternative approach to adaptive game AI has as its goal adapting rapidly and reliably to game circumstances. Our approach can be classified in the area of case-based adaptive game AI. In the approach, domain knowledge required to adapt to game circumstances is gathered automatically by the game AI, and is exploited immediately (i.e., without trials and without resource-intensive learning) to evoke effective behavior in a controlled manner in online play. We performed experiments that test case-based adaptive game AI on three different maps in a commercial real-time strategy (RTS) game. From our results, we may conclude that case-based adaptive game AI provides a strong basis for effectively adapting game AI in video games.","4272":"Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https:\/\/github.com\/salesforce\/CodeGen.","4273":"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community1.","4274":"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.","4275":"Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction to identify the target objects or categories before executing visual recognition tasks. Such systems lack the ability to actively reason and comprehend implicit user intentions. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction pairs, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with atoken and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving: 1) complex reasoning; 2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation image-instruction pairs results in further performance enhancement. Experiments show our method not only unlocks new reasoning segmentation capabilities but also proves effective in both complex reasoning segmentation and standard referring segmentation tasks. Code, models, and demo are at https:\/\/github.com\/dvlab-research\/LISA.","4276":"Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendations, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https:\/\/github.com\/SAI990323\/TALLRec.","4277":"Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.","4278":"Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.","4279":"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https:\/\/gorilla.cs.berkeley.edu","4280":"High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\u00d7 with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https:\/\/github.com\/vllm-project\/vllm.","4281":"Visual instruction tuning large language model(LLM) on image-text pairs has achieved general-purpose vision-language abilities. However, the lack of region-text pairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin (the second place is 75.6%) and almost reaching human-level performance of 85.0%. The code, dataset, and demo can be found at https:\/\/github.com\/jshilong\/GPT4RoI.","4282":"Large language models have shown their remarkable capabilities as a general interface for various language-related applications. Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others. The challenge is to use a single model for performing diverse vision-language tasks effectively with simple multi-modal instructions. Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks. We propose using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models. Our model and codes are available at https:\/\/minigpt-v2.github.io\/","4283":"Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation. In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models. We also open-sourced our model and part of the data at https:\/\/github.com\/PKU-YuanGroup\/ChatLaw.","4284":"We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.","4285":"Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https:\/\/github.com\/Paitesanshi\/LLM-Agent-Survey.","4286":"Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https:\/\/github.com\/OpenGVLab\/InternGPT. The code shall be released at https:\/\/github.com\/OpenGVLab\/VisionLLM.","4287":"This paper aims to efficiently enable Large Language Models (LLMs) to use multimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have shown great potential for tool usage through sophisticated prompt engineering. Nevertheless, these models typically rely on prohibitive computational costs and publicly inaccessible data. To address these challenges, we propose the GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools. It generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts. By using the Low-Rank Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs to solve a range of visual problems, including visual comprehension and image generation. Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways. Extensive experiments demonstrate the effectiveness of our method on various language models, which not only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot capacity for unseen tools. The code and demo are available at https:\/\/github.com\/StevenGrove\/GPT4Tools.","4288":"We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input\/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https:\/\/google-research.github.io\/seanet\/audiopalm\/examples","4289":"In the past decades, recommender systems have attracted much attention in both research and industry communities, and a large number of studies have been devoted to developing effective recommendation models. Basically speaking, these models mainly learn the underlying user preference from historical behavior data, and then estimate the user-item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs), we take a different approach to developing the recommendation models, considering recommendation as instruction following by LLMs. The key idea is that the preferences or needs of a user can be expressed in natural language descriptions (called instructions), so that LLMs can understand and further execute the instruction for fulfilling the recommendation task. Instead of using public APIs of LLMs, we instruction tune an open-source LLM (3B Flan-T5-XL), in order to better adapt LLMs to recommender systems. For this purpose, we first design a general instruction format for describing the preference, intention, task form and context of a user in natural language. Then we manually design 39 instruction templates and automatically generate a large amount of user-personalized instruction data (252K instructions) with varying types of preferences and intentions. To demonstrate the effectiveness of our approach, we instantiate the instruction templates into several widely-studied recommendation (or search) tasks, and conduct extensive experiments on these tasks with real-world datasets. Experiment results show that the proposed approach can outperform several competitive baselines, including the powerful GPT-3.5, on these evaluation tasks. Our approach sheds light on developing more user-friendly recommender systems, in which users can freely communicate with the system and obtain more accurate recommendations via natural language instructions.","4290":"Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https:\/\/robot-help.github.io","4291":"Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced abilitY. The Valley consists of a LLM, a temporal modeling module, a visual encoder, and a simple projection module designed to bridge visual and textual modes. To empower Valley with video comprehension and instruction-following capabilities, we construct a video instruction dataset and adopt a two-stage tuning procedure to train it. Specifically, we employ ChatGPT to facilitate the construction of task-oriented conversation data encompassing various tasks, including multi-shot captions, long video descriptions, action recognition, causal relationship inference, etc. Subsequently, we adopt a pre-training-then-instructions-tuned pipeline to align visual and textual modalities and improve the instruction-following capability of Valley. Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.","4292":"The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm \u2014 Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. Our code and dataset can be found at https:\/\/github.com\/jizhi-zhang\/FaiRLLM.","4293":"In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \\url{https:\/\/github.com\/jieyilong\/tree-of-thought-puzzle-solver}.","4294":"Although large language models have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule the solution steps prior to implementation. Thus we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem solving. This paper proposes a self-planning code generation method with large language model, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, the language model plans out the solution steps from the intent combined with in-context learning. Then it enters the implementation phase, where the model generates code step by step, guided by the solution steps. The effectiveness of self-planning code generation has been rigorously evaluated on multiple code generation datasets and the results have demonstrated a marked superiority over naive direct generation approaches with language model. The improvement in performance is substantial, highlighting the significance of self-planning in code generation tasks.","4295":"In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \\emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \\textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\\ie \\emph{reasoning}). Specially, we propose an \\emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\\url{https:\/\/github.com\/RUCAIBox\/StructGPT}.","4296":"Research on prompting has shown excellent performance with little or even no supervised training across many tasks. However, prompting for machine translation is still under-explored in the literature. We fill this gap by offering a systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection. We further explore the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the testbed show that 1) the number and the quality of prompt examples matter, where using suboptimal examples degenerates translation; 2) several features of prompt examples, such as semantic similarity, show significant Spearman correlation with their prompting performance; yet, none of the correlations are strong enough; 3) using pseudo parallel prompt examples constructed from monolingual data via zero-shot prompting could improve translation; and 4) improved performance is achievable by transferring knowledge from prompt examples selected in other settings. We finally provide an analysis on the model outputs and discuss several problems that prompting still suffers from.","4297":"Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.","4298":"We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foster future development in healthcare AI. A demo of Radiology-GPT is available at https:\/\/huggingface.co\/spaces\/allen-eric\/radiology-gpt.","4299":"When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.","4300":"Large-language models like ChatGPT have recently received a great deal of attention. To assess ChatGPT in the field of genetics, we compared its performance to human respondents in answering genetics questions (involving 13,636 responses) that had been posted on social media platforms starting in 2021. Overall, ChatGPT did not perform significantly differently than human respondents, but did significantly better on memorization-type questions versus critical thinking questions, frequently provided different answers when asked questions multiple times, and provided plausible explanations for both correct and incorrect answers.","4301":null,"4302":"There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.","4303":"Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near chance distinguishing GPT-3's responses from those of an\"actual human philosopher\".","4304":null,"4305":"While LLMs have made it possible to rapidly prototype new ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains \u2013 a key step to lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We find from pilot studies that users need support transforming data between steps of a chain, as well as debugging the chain at multiple granularities. To address these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four designers and developers, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to even more complex tasks, as well as supporting low-fi chain prototyping.","4306":null,"4307":"Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by \u201cunit-testing\u201d sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.","4308":null,"4309":"Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins.","4310":"We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.","4311":"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.","4312":"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.","4313":"The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https:\/\/minigpt-4.github.io\/.","4314":"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https:\/\/github.com\/princeton-nlp\/tree-of-thought-llm.","4315":"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https:\/\/github.com\/nlpxucan\/WizardLM","4316":"We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https:\/\/github.com\/QwenLM\/Qwen-VL .","4317":"The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their\"cognitive\"processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https:\/\/github.com\/camel-ai\/camel.","4318":"We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making.","4319":"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.","4320":"Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.","4321":"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.","4322":"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https:\/\/aka.ms\/kosmos-2.","4323":"Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.","4324":"Large language models have led to state-of-the-art accuracies across several tasks. However, training these models efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP\/s on 3072 GPUs (per-GPU throughput of 52% of theoretical peak).","4325":"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.","4326":"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.","4327":"Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT\u2019s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT\u2019s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44\/100), 42% (42\/100), 64.4% (56\/87), and 57.8% (59\/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT\u2019s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183\/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT\u2019s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.","4328":"Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.","4329":"Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization.","4330":"Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.","4331":null,"4332":"Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.","4333":"Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https:\/\/github.com\/nlpxucan\/WizardLM","4334":"Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.","4335":"Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, and not model size, is the key to the LLM's zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LMM summaries are judged to be on par with human written summaries.","4336":"In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\"win\/tie\/lose\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https:\/\/github.com\/i-Eval\/FairEval} to facilitate future research.","4337":"Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.","4338":"Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.","4339":"This article discusses OpenAI's ChatGPT, a generative pre\u2010trained transformer, which uses natural language processing to fulfill text\u2010based user requests (i.e., a \u201cchatbot\u201d). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT\u20103, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.","4340":"The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token\/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https:\/\/github.com\/FMInference\/FlexGen","4341":"Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https:\/\/0nutation.github.io\/SpeechGPT.github.io\/.","4342":"Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for ChatGPT to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.","4343":"Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com\/sunnweiwei\/RankGPT.","4344":"We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (\"LLM thoughts\") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by>31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.","4345":"Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.","4346":"Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with these insights, zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators. The code and processed datasets are available at https:\/\/github.com\/RUCAIBox\/LLMRank.","4347":"Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images\/instructions, and yields a 84.5\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.","4348":"Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search\/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.","4349":"This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.","4350":"The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \\url{https:\/\/github.com\/OFA-Sys\/ExpertLLaMA}.","4351":"Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.","4352":"We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https:\/\/voyager.minedojo.org\/.","4353":"Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner to generate candidate programs and test them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.","4354":"We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\\sim 600 \\times$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.","4355":"Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually match or improve performance on a range of downstream tasks. Code available at https:\/\/github.com\/yuqingd\/ellm.","4356":"Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https:\/\/github.com\/horseee\/LLM-Pruner","4357":"We introduce Chain of Knowledge (CoK), a framework that augments large language models with structured knowledge bases to improve factual correctness and reduce hallucination. Compared to previous works which only retrieve unstructured texts, CoK leverages structured knowledge bases which support complex queries and offer more direct factual statements. To assist large language models to effectively query knowledge bases, we propose a query generator model with contrastive instruction-tuning. As the query generator is separate from the frozen large language model, our framework is modular and thus easily adapted to various knowledge sources and models. Experiments show that our framework signi\ufb01cantly enhances the factual correctness of large language models on knowledge-intensive tasks. Our code is available at https:\/\/github.com\/DAMO-NLP-SG\/chain-of-knowledge.","4358":"Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface.","4359":"Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https:\/\/github.com\/BradyFU\/Awesome-Multimodal-Large-Language-Models.","4360":"With the rapid popularity of large language models such as ChatGPT and GPT-4, a growing amount of attention is paid to their safety concerns. These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information. Evaluating and enhancing their safety is particularly essential for the wide application of large language models (LLMs). To further promote the safe deployment of LLMs, we develop a Chinese LLM safety assessment benchmark. Our benchmark explores the comprehensive safety performance of LLMs from two perspectives: 8 kinds of typical safety scenarios and 6 types of more challenging instruction attacks. Our benchmark is based on a straightforward process in which it provides the test prompts and evaluates the safety of the generated responses from the evaluated model. In evaluation, we utilize the LLM's strong evaluation ability and develop it as a safety evaluator by prompting. On top of this benchmark, we conduct safety assessments and analyze 15 LLMs including the OpenAI GPT series and other well-known Chinese LLMs, where we observe some interesting findings. For example, we find that instruction attacks are more likely to expose safety issues of all LLMs. Moreover, to promote the development and deployment of safe, responsible, and ethical AI, we publicly release SafetyPrompts including 100k augmented prompts and responses by LLMs.","4361":"Large language models have demonstrated remarkable performance across various natural language processing tasks; however, their efficacy in more challenging and domain-specific tasks remains less explored. This paper introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating large language models.In order to align the evaluation results with humans as much as possible, we designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. We evaluated the ChatGPT model on GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement. To further scrutinize the model's responses, we incorporate human evaluations.In conclusion, this research contributes a robust evaluation benchmark for future large-scale language models and offers valuable insights into the limitations of such models.","4362":"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to\"morally self-correct\"-- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.","4363":"Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.","4364":null,"4365":"This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.","4366":null,"4367":"Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. Code and datasets are available at https:\/\/github.com\/zjunlp\/EasyEdit.","4368":"An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary\"chatGPT jailbreaks\", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.","4369":null,"4370":"Large language models (LLMs) have promised a revolution in answering complex questions using the ChatGPT model. Its application in chemistry is still in its infancy. This viewpoint addresses the question of how well ChatGPT understands chemistry by posing five simple tasks in different subareas of chemistry.","4371":"Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.","4372":"Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https:\/\/github.com\/EleutherAI\/pythia","4373":"We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video\/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.","4374":"A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.","4375":"Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https:\/\/github.com\/naver-ai\/almost","4376":"We show that transformer-based large language models are computationally universal when augmented with an external memory. Any deterministic language model that conditions on strings of bounded length is equivalent to a finite automaton, hence computationally limited. However, augmenting such models with a read-write memory creates the possibility of processing arbitrarily large inputs and, potentially, simulating any algorithm. We establish that an existing large language model, Flan-U-PaLM 540B, can be combined with an associative read-write memory to exactly simulate the execution of a universal Turing machine, $U_{15,2}$. A key aspect of the finding is that it does not require any modification of the language model weights. Instead, the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts.","4377":"Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.","4378":"As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.","4379":"This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.","4380":"In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs.","4381":"Artificial intelligence has the potential to open insight into the structure of proteins at the scale of evolution. It has only recently been possible to extend protein structure prediction to two hundred million cataloged proteins. Characterizing the structures of the exponentially growing billions of protein sequences revealed by large scale gene sequencing experiments would necessitate a break-through in the speed of folding. Here we show that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction. Leveraging the insight that language models learn evolutionary patterns across millions of sequences, we train models up to 15B parameters, the largest language model of proteins to date. As the language models are scaled they learn information that enables prediction of the three-dimensional structure of a protein at the resolution of individual atoms. This results in prediction that is up to 60x faster than state-of-the-art while maintaining resolution and accuracy. Building on this, we present the ESM Metage-nomic Atlas. This is the first large-scale structural characterization of metagenomic proteins, with more than 617 million structures. The atlas reveals more than 225 million high confidence predictions, including millions whose structures are novel in comparison with experimentally determined structures, giving an unprecedented view into the vast breadth and diversity of the structures of some of the least understood proteins on earth.","4382":"Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https:\/\/github.com\/VHellendoorn\/Code-LMs, which enables future research and application in this area. We have an online appendix at https:\/\/arxiv.org\/abs\/2202.13169.","4383":"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.","4384":"Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.","4385":"By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19\/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and\/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https:\/\/sites.google.com\/view\/automatic-prompt-engineer.","4386":"Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these large models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on a single GPU or even on a multi-GPU server; and b) the number of compute operations required to train these models can result in unrealistically long training times. New methods of model parallelism such as tensor and pipeline parallelism have been proposed to address these challenges; unfortunately, naive usage leads to fundamental scaling issues at thousands of GPUs due to various reasons, e.g., expensive cross-node communication or idle periods waiting on other devices. In this work, we show how to compose different types of parallelism methods (tensor, pipeline, and data paralleism) to scale to thousands of GPUs, achieving a two-order-of-magnitude increase in the sizes of models we can efficiently train compared to existing systems. We discuss various implementations of pipeline parallelism and propose a novel schedule that can improve throughput by more than 10% with comparable memory footprint compared to previously-proposed approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. The composition of these techniques allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP\/s on 3072 GPUs with achieved per-GPU throughput of 52% of peak; previous efforts to train similar-sized models achieve much lower throughput (36% of theoretical peak). Our code has been open-sourced at https:\/\/github.com\/nvidia\/megatron-lm.","4387":"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.","4388":"This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.","4389":"The latest generation of large neural language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work, we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a generative language model to write a story. We evaluated Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences. For example, the language model is able to engage in open-ended conversation about the story, respond to writers\u2019 custom requests expressed in natural language (such as \u201drewrite this text to be more Dickensian\u201d), and generate suggestions that serve to unblock writers in the creative process. Based on these results, we discuss design implications for future human-AI co-writing systems.","4390":"Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https:\/\/github.com\/wyu97\/GenRead.","4391":null,"4392":"The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.","4393":"We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.","4394":"We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a\"hyper-accuracy distortion\"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.","4395":"Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.","4396":"The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading\/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.","4397":"Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.","4398":"When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from incorrect CoT. 1","4399":"Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\u2018faster\u2019) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https:\/\/code-as-policies.github.io","4400":"Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure. With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines -- by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models doesn't tend to suffer from dimension-dependent performance degradation. Code to reproduce results can be found at https:\/\/github.com\/lxuechen\/private-transformers.","4401":"Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction. Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics. To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).","4402":"While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g.,\"A\") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.","4403":"We introduce LAVILA, a new approach to learning video-language representations by leveraging Large Language Models (LLMs). We repurpose pre-trained LLMs to be conditioned on visual input, and finetune them to create automatic video narrators. Our auto-generated narrations offer a number of advantages, including dense coverage of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text. The video-language embedding learned contrastively with these narrations outperforms the previous state-of-the-art on multiple first-person and third-person video tasks, both in zero-shot and finetuned setups. Most notably, Lavilaobtains an absolute gain of 10.1% on EGTEA classification and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks. Furthermore, LaVilatrained with only half the narrations from the Ego4D dataset outperforms models trained on the full set, and shows positive scaling behavior on increasing pre-training data and model size.","4404":"We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.","4405":"This paper presents a dynamic, real-time approach to detecting anomalous blockchain transactions. The proposed tool, BlockGPT, generates tracing representations of blockchain activity and trains from scratch a large language model to act as a real-time Intrusion Detection System. Unlike traditional methods, BlockGPT is designed to offer an unrestricted search space and does not rely on predefined rules or patterns, enabling it to detect a broader range of anomalies. We demonstrate the effectiveness of BlockGPT through its use as an anomaly detection tool for Ethereum transactions. In our experiments, it effectively identifies abnormal transactions among a dataset of 68M transactions and has a batched throughput of 2284 transactions per second on average. Our results show that, BlockGPT identifies abnormal transactions by ranking 49 out of 124 attacks among the top-3 most abnormal transactions interacting with their victim contracts. This work makes contributions to the field of blockchain transaction analysis by introducing a custom data encoding compatible with the transformer architecture, a domain-specific tokenization technique, and a tree encoding method specifically crafted for the Ethereum Virtual Machine (EVM) trace representation.","4406":"Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), owing to their excellent understanding and generation abilities. Remarkably, what further sets these models apart is the massive amounts of world knowledge they internalize during pretraining. While many downstream applications provide the model with an informational context to aid its performance on the underlying task, how the model's world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model's memorized knowledge. This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size. As a solution, we propose a novel method - Knowledge Aware FineTuning (KAFT) - to strengthen both controllability and robustness by incorporating counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.","4407":"Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model\u2019s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https:\/\/github.com\/itsnamgyu\/reasoning-teacher.","4408":"Vision-language models (VLMs) such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what features the model uses to construct its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.","4409":"Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.","4410":"Abstract Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.","4411":"The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.","4412":"In this paper, we consider the problem of sparsifying BERT models, which are a key building block for natural language processing, in order to reduce their storage and computational cost. We introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate second-order information, which we show to yield state-of-the-art results in both stages of language tasks: pre-training and fine-tuning. Specifically, oBERT extends existing work on second-order pruning by allowing for pruning weight blocks, and is the first such method that is applicable at BERT scale. Second, we investigate compounding compression approaches to obtain highly compressed but accurate models for deployment on edge devices. These models significantly push boundaries of the current state-of-the-art sparse BERT models with respect to all metrics: model size, inference speed and task accuracy. For example, relative to the dense BERT-base, we obtain 10x model size compression with < 1% accuracy drop, 10x CPU-inference speedup with < 2% accuracy drop, and 29x CPU-inference speedup with < 7.5% accuracy drop. Our code, fully integrated with Transformers and SparseML, is available at https:\/\/github.com\/neuralmagic\/sparseml\/tree\/main\/research\/optimal_BERT_surgeon_oBERT.","4413":"Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a large language model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans nor does it explain the full extent of their behavior-despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how humans develop the ability to reason about the mental states of others, other mechanisms are also\u00a0responsible.","4414":"In this paper, we introduce the Chinese corpus from CLUE organization, CLUECorpus2020, a large-scale corpus that can be used directly for self-supervised learning such as pre-training of a language model, or language generation. It has 100G raw corpus with 35 billion Chinese characters, which is retrieved from Common Crawl. To better understand this corpus, we conduct language understanding experiments on both small and large scale, and results show that the models trained on this corpus can achieve excellent performance on Chinese. We release a new Chinese vocabulary with a size of 8K, which is only one-third of the vocabulary size used in Chinese Bert released by Google. It saves computational cost and memory while works as good as original vocabulary. We also release both large and tiny versions of the pre-trained model on this corpus. The former achieves the state-of-the-art result, and the latter retains most precision while accelerating training and prediction speed for eight times compared to Bert-base. To facilitate future work on self-supervised learning on Chinese, we release our dataset, new vocabulary, codes, and pre-trained models on Github.","4415":"We propose a method for using a large language model, such as GPT-3, to simulate responses of different humans in a given context. We test our method by attempting to repro- duce well-established economic, psycholinguistic, and social experiments. The method requires prompt templates for each experiment. Simulations are run by varying the (hypotheti-cal) subject details, such as name, and analyzing the text gen- erated by the language model. To validate our methodology, we use GPT-3 to simulate the Ultimatum Game , garden path sentences , risk aversion , and the Milgram Shock experiments. In order to address concerns of exposure to these studies in training data, we also evaluate simulations on novel variants of these studies. We show that it is possible to simulate re- sponses of different people and that their responses are consistent with prior human studies from the literature. Across all studies, the distributions generated by larger language models better align with prior experimental results, suggesting a trend that future language models may be used for even more faithful simulations of human responses. Our use of a lan- guage model for simulation is contrasted with anthropomor-phic views of a language model as having its own behavior.","4416":null,"4417":"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25\u00d7 parameters of BERT Large , demonstrating its generalizability to different downstream tasks.","4418":"With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.","4419":"Typhoon is a series of Thai large language models (LLMs) developed specifically for the Thai language. This technical report presents challenges and insights in developing Thai LLMs, including data preparation, pretraining, instruction-tuning, and evaluation. As one of the challenges of low-resource languages is the amount of pretraining data, we apply continual training to transfer existing world knowledge from a strong LLM. To evaluate the Thai knowledge encapsulated in each model from the pretraining stage, we develop ThaiExam, a benchmark based on examinations for high-school students and investment professionals in Thailand. In addition, we fine-tune Typhoon to follow Thai instructions, and we evaluate instruction-tuned models on Thai instruction datasets as well as translation, summarization, and question-answering tasks. Experimental results on a suite of Thai benchmarks show that Typhoon outperforms all open-source Thai language models, and its performance is on par with GPT-3.5 in Thai while having only 7 billion parameters and being 2.62 times more efficient in tokenizing Thai text.","4420":"Large language models (LMs) offer unprecedented language generation capabilities and exciting opportunities for interaction design. However, their highly context-dependent capabilities are difficult to grasp and are often subjectively interpreted. In this paper, we argue that by curating and analyzing large interaction datasets, the HCI community can foster more incisive examinations of LMs\u2019 generative capabilities. Exemplifying this approach, we present CoAuthor, a dataset designed for revealing GPT-3\u2019s capabilities in assisting creative and argumentative writing. CoAuthor captures rich interactions between 63 writers and four instances of GPT-3 across 1445 writing sessions. We demonstrate that CoAuthor can address questions about GPT-3\u2019s language, ideation, and collaboration capabilities, and reveal its contribution as a writing \u201ccollaborator\u201d under various definitions of good collaboration. Finally, we discuss how this work may facilitate a more principled discussion around LMs\u2019 promises and pitfalls in relation to interaction design. The dataset and an interface for replaying the writing sessions are publicly available at https:\/\/coauthor.stanford.edu.","4421":null,"4422":"Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service. It allows users to design task-specific prompts to query the PTMs through some black-box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually unavailable. Can we optimize the task prompts by only accessing the model inference APIs? This paper proposes the black-box tuning framework to optimize the continuous prompt prepended to the input text via derivative-free optimization. Instead of optimizing in the original high-dimensional prompt space, which is intractable for traditional derivative-free optimization, we perform optimization in a randomly generated subspace due to the low intrinsic dimensionality of large PTMs. The experimental results show that the black-box tuning with RoBERTa on a few labeled samples not only significantly outperforms manual prompt and GPT-3's in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning.","4423":"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.","4424":"The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone. In the process of building BLOOM--the Big Science Large Open-science Open-access Multilingual language model--our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization. In addition, we study the impact of various popular pre-training corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to choose the target model size, shape, and training setup. All our models and code are open-sourced at https:\/\/huggingface.co\/bigscience .","4425":"It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.","4426":"On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.","4427":"Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models\u2019 novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.","4428":"Large pre-trained language models such as GPT-3 [10], Codex [11], and Coogle's language model [7] are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems.","4429":"Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https:\/\/aka.ms\/BLURB.","4430":"In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as \u2018make this melodramatic\u2019 or \u2018insert a metaphor.\u2019","4431":"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https:\/\/github.com\/VinAIResearch\/BERTweet","4432":"Human developers can produce code with cybersecurity bugs. Can emerging \u2018smart\u2019 code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI\u2019s Codex and AI21\u2019s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information\u2014 both semantically and syntactically\u2014with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model\u2019s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.","4433":"State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both the high-data and low-data regimes, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.","4434":"Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval. However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential. In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space. We use the recently proposed Condenser pre-training architecture, which learns to condense information into the dense vector through LM pre-training. On top of it, we propose coCondenser, which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space. Experiments on MS-MARCO, Natural Question, and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation, synthesis, or filtering, and the need for large batch training. It shows comparable performance to RocketQA, a state-of-the-art, heavily engineered system, using simple small batch fine-tuning.","4435":"We propose a large margin criterion for training neural language models. Conventionally, neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However, we demonstrate that PPL may not be the best metric to optimize in some tasks, and further propose a large margin formulation. The proposed method aims to enlarge the margin between the \u201cgood\u201d and \u201cbad\u201d sentences in a task-specific sense. It is trained end-to-end and can be widely applied to tasks that involve re-scoring of generated text. Compared with minimum-PPL training, our method gains up to 1.1 WER reduction for speech recognition and 1.0 BLEU increase for machine translation.","4436":"Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at this https URL.","4437":"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization. Our structured approach achieves significant inference speedups while matching or outperforming our unstructured pruning baseline at various sparsity levels. We apply our method to state of the art models on the enwiki8 dataset and obtain a 1.19 perplexity score with just 5M parameters, vastly outperforming a model of the same size trained from scratch. We also demonstrate that our method can be applied to language model fine-tuning by pruning the BERT model on several downstream classification benchmarks.","4438":"Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.","4439":"We present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model. The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space. Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained models in accuracy, but also improves pretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of its pretraining GPU hours. With the same pretraining steps of standard base\/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points.","4440":"Recently, large language models such as GPT-2 have shown themselves to be extremely adept at text generation and have also been able to achieve high-quality results in many downstream NLP tasks such as text classification, sentiment analysis and question answering with the aid of fine-tuning. We present a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects. Our approach is demonstrated to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks.","4441":"Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP.","4442":"In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.","4443":"Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at this https URL. We hope this will be useful for practitioners and researchers working on financial NLP tasks.","4444":null,"4445":"The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.","4446":"Existing language model compression methods mostly use a simple L2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.","4447":"Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory\/power scenarios (e.g., mobile). In this paper, we propose an effective way to fine-tune multiple down-stream generation tasks simultaneously using a single, large pretrained model. The experiments on five diverse language generation tasks show that by just using an additional 2-3% parameters for each task, our model can maintain or even improve the performance of fine-tuning the whole model.","4448":"Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.","4449":"With a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC)tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these mod-els and techniques can generalize to out-of-domain and unseen RC tasks. To enhance the generalization ability, we propose a multi-task learning framework that learns the shared representation across different tasks. Our model is built on top of a large pre-trained language model, such as XLNet, and then fine-tuned on multiple RC datasets. Experimental results show the effectiveness of our methods, with an average Exact Match score of 56.59 and an average F1 score of 68.98, which significantly improves the BERT-Large baseline by8.39 and 7.22, respectively","4450":"Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled\/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset. We release the code at: https:\/\/github.com\/google-research\/distilling-step-by-step .","4451":"Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) have suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Recurrent Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pre-trained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is capable of generating human-like responses to persuade people to donate to a charity.","4452":"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.","4453":"Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models\u2019 results in the non-zero-shot setting. LAION-Audio-630K1 and the proposed model 2 are both available to the public.","4454":"Attention-based recurrent neural encoder-decoder models present an elegant solution to the automatic speech recognition problem. This approach folds the acoustic model, pronunciation model, and language model into a single network and requires only a parallel corpus of speech and text for training. However, unlike in conventional approaches that combine separate acoustic and language models, it is not clear how to use additional (unpaired) text. While there has been previous work on methods addressing this problem, a thorough comparison among methods is still lacking. In this paper, we compare a suite of past methods and some of our own proposed methods for using unpaired text data to improve encoder-decoder models. For evaluation, we use the medium-sized Switchboard data set and the large-scale Google voice search and dictation data sets. Our results confirm the benefits of using unpaired text across a range of methods and data sets. Surprisingly, for first-pass decoding, the rather simple approach of shallow fusion performs best across data sets. However, for Google data sets we find that cold fusion has a lower oracle error rate and outperforms other approaches after second-pass rescoring on the Google voice search data set.","4455":"Within human sentence processing, it is known that there are large effects of a word\u2019s probability in context on how long it takes to read it. This relationship has been quanti\ufb01ed using information-theoretic surprisal, or the amount of new information conveyed by a word. Here, we compare surprisals derived from a col-lection of language models derived from n -grams, neural networks, and a combination of both. We show that the models\u2019 psychological predictive power improves as a tight linear function of language model linguistic quality. We also show that the size of the effect of surprisal is estimated consistently across all types of language models. These \ufb01ndings point toward surprising robustness of surprisal estimates and suggest that surprisal estimated by low-quality language models are not biased.","4456":"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot\/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).","4457":"We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https:\/\/imagen.research.google\/ for an overview of the results.","4458":"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.","4459":"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classi\ufb01cation. Although large unlabeled text corpora are abundant, labeled data for learning these speci\ufb01c tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative \ufb01ne-tuning on each speci\ufb01c task. In contrast to previous approaches, we make use of task-aware input transformations during \ufb01ne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, signi\ufb01cantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).","4460":"Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com\/llm-attacks\/llm-attacks.","4461":"While current approaches to action recognition on presegmented video clips already achieve high accuracies, temporal action detection is still far from comparably good results. Automatically locating and classifying the relevant action segments in videos of varying lengths proves to be a challenging task. We propose a novel method for temporal action detection including statistical length and language modeling to represent temporal and contextual structure. Our approach aims at globally optimizing the joint probability of three components, a length and language model and a discriminative action model, without making intermediate decisions. The problem of finding the most likely action sequence and the corresponding segment boundaries in an exponentially large search space is addressed by dynamic programming. We provide an extensive evaluation of each model component on Thumos 14, a large action detection dataset, and report state-of-the-art results on three datasets.","4462":"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1\/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.","4463":"A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.","4464":"n-gram statistical language model has been successfully applied to capture programming patterns to support code completion and suggestion. However, the approaches using n-gram face challenges in capturing the patterns at higher levels of abstraction due to the mismatch between the sequence nature in n-grams and the structure nature of syntax and semantics in source code. This paper presents GraLan, a graph-based statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute the appearance probabilities of any graphs given the observed (sub)graphs. We use GraLan to develop an API suggestion engine and an AST-based language model, ASTLan. ASTLan supports the suggestion of the next valid syntactic template and the detection of common syntactic templates. Our empirical evaluation on a large corpus of open-source projects has shown that our engine is more accurate in API code suggestion than the state-of-the-art approaches, and in 75% of the cases, it can correctly suggest the API with only five candidates. ASTLan has also high accuracy in suggesting the next syntactic template and is able to detect many useful and common syntactic templates.","4465":"We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https:\/\/unified-io.allenai.org.","4466":"State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a \u201cfoundation\u201d, that targets all modalities at once-a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.","4467":"Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.","4468":"We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, \\vatex is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on \\vatex: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the \\vatex dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using \\vatex for other video-and-language research.","4469":"Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.","4470":"Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16 percentage points (75.17%->91.68% accuracy) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available available at https:\/\/github.com\/amazon-science\/mm-cot.","4471":"Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.","4472":"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).","4473":"Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \\textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-$\\alpha$, with up to 200 billion parameters. PanGu-$\\alpha$ is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-$\\alpha$, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-$\\alpha$ in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-$\\alpha$ in performing various tasks under few-shot or zero-shot settings.","4474":"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.","4475":"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU\/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.","4476":"We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).","4477":"The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https:\/\/github.com\/aub-mind\/araBERT hoping to encourage research and applications for Arabic NLP.","4478":"We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo\/bar instead of negative\/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.","4479":"Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https:\/\/github.com\/posgnu\/rci-agent.","4480":"Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. Unlike OpenAI CLIP that adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in limited GPU resources. We further construct a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER and OpenAI CLIP on various downstream tasks.","4481":"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.","4482":"Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https:\/\/voxposer.github.io","4483":"Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's\"hands and eyes,\"while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https:\/\/say-can.github.io\/.","4484":"Large \u201cinstruction-tuned\u201d language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.","4485":null,"4486":"Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.","4487":"Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.","4488":"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.","4489":"Model parallelism has become a necessity for training modern large-scale deep language models. In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property. This enables a more fine-grained pipeline compared with previous work. With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models. We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration. We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods. The code for reproduction can be found at https:\/\/github.com\/zhuohan123\/terapipe","4490":"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).","4491":"Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning","4492":"Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.","4493":"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.","4494":"In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.","4495":"Introduction This study presents COVID-Twitter-BERT (CT-BERT), a transformer-based model that is pre-trained on a large corpus of COVID-19 related Twitter messages. CT-BERT is specifically designed to be used on COVID-19 content, particularly from social media, and can be utilized for various natural language processing tasks such as classification, question-answering, and chatbots. This paper aims to evaluate the performance of CT-BERT on different classification datasets and compare it with BERT-LARGE, its base model. Methods The study utilizes CT-BERT, which is pre-trained on a large corpus of COVID-19 related Twitter messages. The authors evaluated the performance of CT-BERT on five different classification datasets, including one in the target domain. The model's performance is compared to its base model, BERT-LARGE, to measure the marginal improvement. The authors also provide detailed information on the training process and the technical specifications of the model. Results The results indicate that CT-BERT outperforms BERT-LARGE with a marginal improvement of 10-30% on all five classification datasets. The largest improvements are observed in the target domain. The authors provide detailed performance metrics and discuss the significance of these results. Discussion The study demonstrates the potential of pre-trained transformer models, such as CT-BERT, for COVID-19 related natural language processing tasks. The results indicate that CT-BERT can improve the classification performance on COVID-19 related content, especially on social media. These findings have important implications for various applications, such as monitoring public sentiment and developing chatbots to provide COVID-19 related information. The study also highlights the importance of using domain-specific pre-trained models for specific natural language processing tasks. Overall, this work provides a valuable contribution to the development of COVID-19 related NLP models.","4496":"Abstract Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https:\/\/github.com\/THU-KEG\/KEPLER.","4497":"Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at this https URL.","4498":"In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image\/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni\/multi-modal encoder\/decoder) and depends on external modules such as object detectors\/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. Codes are released at \\url{https:\/\/github.com\/microsoft\/GenerativeImage2Text}.","4499":"Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.","4500":"We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.","4501":"We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM.","4502":"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model\/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.","4503":"Large language models (LLMs) have demonstrated significant universal capabilities as few\/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines.","4504":"We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.","4505":"Whereas conventional spoken language understanding (SLU) systems map speech to text, and then text to intent, end-to-end SLU systems map speech directly to intent through a single trainable model. Achieving high accuracy with these end-to-end models without a large amount of training data is difficult. We propose a method to reduce the data requirements of end-to-end SLU in which the model is first pre-trained to predict words and phonemes, thus learning good features for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and show that our method improves performance both when the full dataset is used for training and when only a small subset is used. We also describe preliminary experiments to gauge the model's ability to generalize to new phrases not heard during training.","4506":"Language models pretrained on text from a wide variety of sources form the foundation of today\u2019s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task\u2019s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.","4507":"Scaling up neural networks has led to remarkable performance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guidance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has primarily used private data & models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training distribution plays a key role in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public CLIP models, to ensure reproducibility and make scaling laws research more accessible. Source code and instructions to reproduce this study is available at https:\/\/github.eom\/LAION-AI\/sealing-laws-openelip.","4508":"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.","4509":"Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only at https:\/\/github.com\/project-baize\/baize-chatbot. An online demo is also available at https:\/\/huggingface.co\/spaces\/project-baize\/chat-with-baize.","4510":"Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at https:\/\/github.com\/salesforce\/ALBEF\/.","4511":"Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues we present RankGen, a 1.2B parameter encoder model for English that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and (2) sequences generated from a large language model conditioned on the prefix. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We release our model checkpoints, code, and human preference data with explanations to facilitate future research.","4512":"Recently, pre-trained language representation flourishes as the mainstay of the natural language understanding community, e.g., BERT. These pre-trained language representations can create state-of-the-art results on a wide range of downstream tasks. Along with continuous significant performance improvement, the size and complexity of these pre-trained neural models continue to increase rapidly. Is it possible to compress these large-scale language representation models? How will the pruned language representation affect the downstream multi-task transfer learning objectives? In this paper, we propose Reweighted Proximal Pruning (RPP), a new pruning method specifically designed for a large-scale language representation model. Through experiments on SQuAD and the GLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for both the pre-training task and the downstream multiple fine-tuning tasks at high prune ratio. RPP provides a new perspective to help us analyze what large-scale language representation might learn. Additionally, RPP makes it possible to deploy a large state-of-the-art language representation model such as BERT on a series of distinct devices (e.g., online servers, mobile phones, and edge devices).","4513":"Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https:\/\/github.com\/agencyenterprise\/PromptInject.","4514":"Large language models such as GPT-3 and PaLM have shown remarkable performance in few-shot learning. However, they still struggle with reasoning tasks such as the arithmetic benchmark GSM8K. Recent advances deliberately guide the language model to generate a chain of reasoning steps before producing the \ufb01nal answer, success-fully boosting the GSM8K benchmark from 17 . 9% to 58 . 1% in terms of problem solving rate. In this paper, we propose a new approach, D I V E RS E (Diverse Veri\ufb01er on Reasoning Step), to further advance their reasoning capability. D I V E RS E \ufb01rst explores different prompts to enhance the diversity in reasoning paths. Second, D I V E RS E intro-duces a veri\ufb01er to distinguish good answers from bad answers for a better weighted voting. Finally, D I V E RS E veri\ufb01es the correctness of each single step rather than all the steps in a whole. We conduct extensive experiments using the latest language model code-davinci-002 and demonstrate that D I - V E RS E can achieve new state-of-the-art performance on six out of eight reasoning benchmarks (e.g., GSM8K 74 . 4% \u2192 83 . 2% ), out-performing the PaLM model with 540B parameters.","4515":"Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train\"open-book\"QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.","4516":"Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.","4517":"We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https:\/\/github.com\/google-research\/url-nlp.","4518":null,"4519":"Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, we have no opportunity to compare models and acquisition functions. This paper provides a large-scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by-Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.","4520":"We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.","4521":"We present PandaGPT, an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities. Our pilot experiments show that PandaGPT can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios. More interestingly, PandaGPT can take multimodal inputs simultaneously and compose their semantics naturally. For example, PandaGPT can connect how objects look in an image\/video and how they sound in an audio. To do so, PandaGPT combines the multimodal encoders from ImageBind and the large language models from Vicuna. Notably, only aligned image-text pairs are required for the training of PandaGPT. Thanks to the strong capability of ImageBind in embedding data from different modalities into the same space, PandaGPT displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an initial step toward building AGI that can perceive and understand inputs in different modalities holistically, as we humans do.","4522":"Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.","4523":"Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \u201ctask descriptions\u201d in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.","4524":"Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1\/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.","4525":"Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","4526":"The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.","4527":"Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.","4528":"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.","4529":"Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross\/self-attention upon visual and textual tokens. However, cross\/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.","4530":"We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https:\/\/github.com\/pytorch\/fairseq\/","4531":"Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories\u2014local, long-term, and external memory\u2014at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.","4532":"We study the characteristics of large online social networks through an extensive analysis of over 1.9 million MySpace profiles in an effort to understand who is using these networks and how they are being used. We study MySpace through a comparative study over three different, but related, facets: (i) the sociability of users in MySpace based on relationship, messaging, and group participation; (ii) the demographic characteristics of MySpace users in terms of age, gender, and location, and a study of how these factors correlate with their privacy preferences; and (iii) the text artifacts of MySpace users, which can be used to construct language models that distinguish MySpace users not just by who they say they are but also by the language model they employ. We find a number of surprising results and conjecture several potential research directions based on our observations","4533":"In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.","4534":"Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages. They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models. This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages. Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model. The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).","4535":"Masked language models (MLMs) conventionally mask 15% of tokens due to the belief that more masking would leave insufficient context to learn good representations; this masking rate has been widely used, regardless of model sizes or masking strategies. In this work, we revisit this important choice of MLM pre-training. We first establish that 15% is not universally optimal, and larger models should adopt a higher masking rate. Specifically, we find that masking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD. Interestingly, an extremely high masking rate of 80% can still preserve 95% fine-tuning performance and most of the accuracy in linguistic probing, challenging the conventional wisdom about the role of the masking rate. We then examine the interplay between masking rates and masking strategies and find that uniform masking requires a higher masking rate compared to sophisticated masking strategies such as span or PMI masking. Finally, we argue that increasing the masking rate has two distinct effects: it leads to more corruption, which makes the prediction task more difficult; it also enables more predictions, which benefits optimization. Using this framework, we revisit BERT\u2019s 80-10-10 corruption strategy. Together, our results contribute to a better understanding of MLM pre-training.","4536":"Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial-intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional fine-tuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. Our models and code are open sourced for widespread adoption in protein engineering. A record of this paper's Transparent Peer Review process is included in the supplemental information.","4537":"Large-scale pre-trained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from inefficiency and linguistic signal overwhelmed by long visual sequences in cross-modal alignment. To address both problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections.mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, including image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability on vision-language and video-language tasks. The code and pre-trained models are available at https:\/\/github.com\/alibaba\/AliceMind","4538":"This paper presents a novel approach for multi-lingual sentiment classification in short texts. This is a challenging task as the amount of training data in languages other than English is very limited. Previously proposed multi-lingual approaches typically require to establish a correspondence to English for which powerful classifiers are already available. In contrast, our method does not require such supervision. We leverage large amounts of weakly-supervised data in various languages to train a multi-layer convolutional network and demonstrate the importance of using pre-training of such networks. We thoroughly evaluate our approach on various multi-lingual datasets, including the recent SemEval-2016 sentiment prediction benchmark (Task 4), where we achieved state-of-the-art performance. We also compare the performance of our model trained individually for each language to a variant trained for all languages at once. We show that the latter model reaches slightly worse - but still acceptable - performance when compared to the single language model, while benefiting from better generalization properties across languages.","4539":"Recent progress has shown that large-scale pre-training using contrastive image-text pairs can be a promising alternative for high-quality visual representation learning from natural language supervision. Benefiting from a broader source of supervision, this new paradigm exhibits impressive transferability to downstream classification tasks and datasets. However, the problem of transferring the knowledge learned from image-text pairs to more complex dense prediction tasks has barely been visited. In this work, we present a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP. Specifically, we convert the original image-text matching problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models. By further using the contextual information from the image to prompt the language model, we are able to facilitate our model to better exploit the pretrained knowledge. Our method is model-agnostic, which can be applied to arbitrary dense prediction systems and various pre-trained visual backbones including both CLIP models and ImageNet pre-trained models. Extensive experiments demonstrate the superior performance of our methods on semantic segmentation, object detection, and instance segmentation tasks. Code is available at https:\/\/github.com\/raoyongming\/DenseCLIP.","4540":"A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https:\/\/stereoset.mit.edu.","4541":"We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard\/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\\% WER on AMI-IHM, 4.7\\% WER on Switchboard, 8.3\\% WER on CallHome, and 1.3\\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\\% WER without a language model, which compares to 38.6\\% WER to a strong HMM baseline with a language model.","4542":"We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of MoME, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA, NLVR2 and image-text retrieval. The code and pretrained models are available at https:\/\/aka.ms\/vlmo.","4543":"In this work, we study the large-scale pretraining of BERT-Large with differentially private SGD (DP-SGD). We show that combined with a careful implementation, scaling up the batch size to millions (i.e., mega-batches) improves the utility of the DP-SGD step for BERT; we also enhance its efficiency by using an increasing batch size schedule. Our implementation builds on the recent work of [SVK20], who demonstrated that the overhead of a DP-SGD step is minimized with effective use of JAX [BFH+18, FJL18] primitives in conjunction with the XLA compiler [XLA17]. Our implementation achieves a masked language model accuracy of 60.5% at a batch size of 2M, for $\\epsilon = 5.36$. To put this number in perspective, non-private BERT models achieve an accuracy of $\\sim$70%.","4544":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.","4545":"We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units.","4546":"We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.","4547":"Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.","4548":"Large corpora are ubiquitous in today's world and memory\nquickly becomes the limiting factor in practical applications\nof the Vector Space Model (VSM). We identify gap in existing\nVSM implementations, which is their scalability and ease of\nuse. We describe a Natural Language Processing software\nframework which is based on the idea of document streaming,\ni.e. processing corpora document after document, in a memory\nindependent fashion. In this framework, we implement several\npopular algorithms for topical inference, including Latent\nSemantic Analysis and Latent Dirichlet Allocation, in a way\nthat makes them completely independent of the training corpus\nsize. Particular emphasis is placed on straightforward and\nintuitive framework design, so that modifications and\nextensions of the methods and\/or their application by\ninterested practitioners are effortless. We demonstrate the\nusefulness of our approach on a real-world scenario of\ncomputing document similarities within an existing digital\nlibrary DML-CZ.","4549":"Many state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are hybrids of neural networks and Hidden Markov Models (HMMs). Recently, more direct end-to-end methods have been investigated, in which neural architectures were trained to model sequences of characters [1,2]. To our knowledge, all these approaches relied on Connectionist Temporal Classification [3] modules. We investigate an alternative method for sequence modelling based on an attention mechanism that allows a Recurrent Neural Network (RNN) to learn alignments between sequences of input frames and output labels. We show how this setup can be applied to LVCSR by integrating the decoding RNN with an n-gram language model and by speeding up its operation by constraining selections made by the attention mechanism and by reducing the source sequence lengths by pooling information over time. Recognition accuracies similar to other HMM-free RNN-based approaches are reported for the Wall Street Journal corpus.","4550":"BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. We release the code and pre-trained model in GitHub\\footnote{\\url{this https URL}}.","4551":"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video\/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.","4552":"In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage representation to attend words appeared in itself, which is called \"Inner-Attention\" in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of \"Inner-Attention\" mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.","4553":"Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.","4554":"Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.","4555":"We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.","4556":"Neural networks provide new possibilities to automatically learn complex language patterns and query-document relations. Neural IR models have achieved promising results in learning query-document relevance patterns, but few explorations have been done on understanding the text content of a query or a document. This paper studies leveraging a recently-proposed contextual neural language model, BERT, to provide deeper text understanding for IR. Experimental results demonstrate that the contextual text representations from BERT are more effective than traditional word embeddings. Compared to bag-of-words retrieval models, the contextual language model can better leverage language structures, bringing large improvements on queries written in natural languages. Combining the text understanding ability with search knowledge leads to an enhanced pre-trained BERT model that can benefit related search tasks where training data are limited.","4557":"This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https:\/\/github.com\/LuoweiZhou\/VLP.","4558":"Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.","4559":"Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance. Code is available in https:\/\/github.com\/zjunlp\/DART.","4560":"Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.","4561":null,"4562":"In this paper, we introduce a new vision-language pre-trained model -- ImageBERT -- for image-text joint embedding. Our model is a Transformer-based model, which takes different modalities as input and models the relationship between them. The model is pre-trained on four tasks simultaneously: Masked Language Modeling (MLM), Masked Object Classification (MOC), Masked Region Feature Regression (MRFR), and Image Text Matching (ITM). To further enhance the pre-training quality, we have collected a Large-scale weAk-supervised Image-Text (LAIT) dataset from Web. We first pre-train the model on this dataset, then conduct a second stage pre-training on Conceptual Captions and SBU Captions. Our experiments show that multi-stage pre-training strategy outperforms single-stage pre-training. We also fine-tune and evaluate our pre-trained ImageBERT model on image retrieval and text retrieval tasks, and achieve new state-of-the-art results on both MSCOCO and Flickr30k datasets.","4563":"Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions -- highlighting further the nuances involved in careful evaluation of LM toxicity.","4564":null,"4565":"Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result\u2014it further improves the performance even when added to the already very strong model.","4566":"Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.","4567":"Pre-Trained Vision-Language Models (VL-PTMs) have shown promising capabilities in grounding natural language in image data, facilitating a broad variety of cross-modal tasks. However, we note that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for large amounts of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. To address the challenge, we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap. In this way, CPT enables strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs outperform their fine-tuned counterparts by a large margin (e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard deviation reduction on average with one shot in RefCOCO evaluation). We make the data and code for this paper publicly available at https:\/\/github.com\/thunlp\/CPT.","4568":"Building ASR models across many languages is a challenging multi-task learning problem due to large variations and heavily unbalanced data. Existing work has shown positive transfer from high resource to low resource languages. However, degradations on high resource languages are commonly observed due to interference from the heterogeneous multilingual data and reduction in per-language capacity. We conduct a capacity study on a 15-language task, with the amount of data per language varying from 7.6K to 53.5K hours. We adopt GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that (1) scaling the number of model parameters is an effective way to solve the capacity bottleneck - our 500M-param model already outperforms monolingual baselines and scaling it to 1B and 10B brought further quality gains; (2) larger models are not only more data efficient, but also more efficient in terms of training cost as measured in TPU days - the 1B-param model reaches the same accuracy at 34% of training time as the 500M-param model; (3) given a fixed capacity budget, adding depth works better than width and large encoders do better than large decoders; (4) with continuous training, they can be adapted to new languages and domains.","4569":"We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.","4570":"We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https:\/\/github.com\/VinAIResearch\/PhoBERT","4571":"Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.","4572":"Much of vision-and-language research focuses on a small but diverse set of independent tasks and supporting datasets often studied in isolation; however, the visually-grounded language understanding skills required for success at these tasks overlap significantly. In this work, we investigate these relationships between vision-and-language tasks by developing a large-scale, multi-task model. Our approach culminates in a single model on 12 datasets from four broad categories of task including visual question answering, caption-based image retrieval, grounding referring expressions, and multimodal verification. Compared to independently trained single-task models, this represents a reduction from approximately 3 billion parameters to 270 million while simultaneously improving performance by 2.05 points on average across tasks. We use our multi-task framework to perform in-depth analysis of the effect of joint training diverse tasks. Further, we show that finetuning task-specific models from our single multi-task model can lead to further improvements, achieving performance at or above the state-of-the-art.","4573":"Abstract Pre-trained Transformer-based models have achieved state-of-the-art performance for various Natural Language Processing (NLP) tasks. However, these models often have billions of parameters, and thus are too resource- hungry and computation-intensive to suit low- capability devices or applications with strict latency requirements. One potential remedy for this is model compression, which has attracted considerable research attention. Here, we summarize the research in compressing Transformers, focusing on the especially popular BERT model. In particular, we survey the state of the art in compression for BERT, we clarify the current best practices for compressing large-scale Transformer models, and we provide insights into the workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving lightweight, accurate, and generic NLP models.","4574":"Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\\% and 67.75\\% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well. Code is available at \\url{this https URL .","4575":"In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.","4576":"Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can\"prompt\"the LM with the review and the label description\"Does the user like this movie?\", and ask whether the next word is\"yes\"or\"no\". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by fine-tuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-the-box might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.","4577":"Cross-cultural studies suggest that access to a conventional language containing words that can be used for counting is essential to develop representations of large exact numbers. However, cultures that lack a conventional counting system typically differ from cultures that have such systems, not only in language but also in many other ways. As a result, it is difficult to isolate the effects of language on the development of number representations. Here we examine the numerical abilities of individuals who lack conventional language for number (deaf individuals who do not have access to a usable model for language, spoken or signed) but who live in a numerate culture (Nicaragua) and thus have access to other aspects of culture that might foster the development of number. These deaf individuals develop their own gestures, called homesigns, to communicate. We show that homesigners use gestures to communicate about number. However, they do not consistently extend the correct number of fingers when communicating about sets greater than three, nor do they always correctly match the number of items in one set to a target set when that target set is greater than three. Thus, even when integrated into a numerate society, individuals who lack input from a conventional language do not spontaneously develop representations of large exact numerosities.","4578":"Abstract We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture. Our model uses extended short-term context by caching local hidden states\u2014similar to transformer-XL\u2014and global long-term memory by retrieving a set of nearest neighbor tokens at each timestep. We design a gating function to adaptively combine multiple information sources to make a prediction. This mechanism allows the model to use either local context, short-term memory, or long-term memory (or any combination of them) on an ad hoc basis depending on the context. Experiments on word-based and character-based language modeling datasets demonstrate the efficacy of our proposed method compared to strong baselines.","4579":"Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.","4580":"Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed and outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.","4581":"The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.","4582":"Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning.However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed.To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners.For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks.Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at https:\/\/github.com\/woojeongjin\/FewVLM","4583":"Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.","4584":"Pretrained language models have shown success in many natural language processing tasks. Many works explore to incorporate the knowledge into the language models. In the biomedical domain, experts have taken decades of effort on building large-scale knowledge bases. For example, UMLS contains millions of entities with their synonyms and defines hundreds of relations among entities. Leveraging this knowledge can benefit a variety of downstream tasks such as named entity recognition and relation extraction. To this end, we propose KeBioLM, a biomedical pretrained language model that explicitly leverages knowledge from the UMLS knowledge bases. Specifically, we extract entities from PubMed abstracts and link them to UMLS. We then train a knowledge-aware language model that firstly applies a text-only encoding layer to learn entity representation and then applies a text-entity fusion encoding to aggregate entity representation. In addition, we add two training objectives as entity detection and entity linking. Experiments on the named entity recognition and relation extraction tasks from the BLURB benchmark demonstrate the effectiveness of our approach. Further analysis on a collected probing dataset shows that our model has better ability to model medical knowledge.","4585":"Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.","4586":"When pre-trained on large unsupervised textual corpora, language models are able to store and retrieve factual knowledge to some extent, making it possible to use them directly for zero-shot cloze-style question answering. However, storing factual knowledge in a fixed number of weights of a language model clearly has limitations. Previous approaches have successfully provided access to information outside the model weights using supervised architectures that combine an information retrieval system with a machine reading component. In this paper, we go a step further and integrate information from a retrieval system with a pre-trained language model in a purely unsupervised way. We report that augmenting pre-trained language models in this way dramatically improves performance and that the resulting system, despite being unsupervised, is competitive with a supervised machine reading baseline. Furthermore, processing query and context with different segment tokens allows BERT to utilize its Next Sentence Prediction pre-trained classifier to determine whether the context is relevant or not, substantially improving BERT's zero-shot cloze-style question-answering performance and making its predictions robust to noisy contexts.","4587":"Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme consists of masked language modeling combined with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to very large sequence lengths. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains state-of-the-art performance on multiple benchmarks covering diverse protein properties (including protein structure, post translational modifications and biophysical attributes), despite using a far smaller model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data. Code and pretrained model weights are available at https:\/\/github.com\/nadavbra\/protein_bert.","4588":"We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM\u2019s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.","4589":"We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu.","4590":"Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 \u2013> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.","4591":"In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.","4592":"Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named \"vokenization\" that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call \"vokens\"). The \"vokenizer\" is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models publicly available at this https URL","4593":"Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages.","4594":"Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (i) PKD-Last: learning from the last k layers; and (ii) PKD-Skip: learning from every k layers. These two patient distillation schemes enable the exploitation of rich information in the teacher\u2019s hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with a significant gain in training efficiency, without sacrificing model accuracy.","4595":"Large-scale pre-trained language models have shown impressive results on language understanding benchmarks like GLUE and SuperGLUE, improving considerably over other pre-training methods like distributed representations (GloVe) and purely supervised approaches. We introduce the Dual Intent and Entity Transformer (DIET) architecture, and study the effectiveness of different pre-trained representations on intent and entity prediction, two common dialogue language understanding tasks. DIET advances the state of the art on a complex multi-domain NLU dataset and achieves similarly high performance on other simpler datasets. Surprisingly, we show that there is no clear benefit to using large pre-trained models for this task, and in fact DIET improves upon the current state of the art even in a purely supervised setup without any pre-trained embeddings. Our best performing model outperforms fine-tuning BERT and is about six times faster to train.","4596":"A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial that models can be built quickly and are highly accurate. We present a large-scale study across 18 established biomedical and clinical NLP tasks to determine which of several popular open-source biomedical and clinical NLP models work well in different settings. Furthermore, we apply recent advances in pretraining to train new biomedical language models, and carefully investigate the effect of various design choices on downstream performance. Our best models perform well in all of our benchmarks, and set new State-of-the-Art in 9 tasks. We release these models in the hope that they can help the community to speed up and increase the accuracy of BioNLP and text mining applications.","4597":"Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel framework for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the error propagation, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our model achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.","4598":"There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively expensive at this scale. Besides, many of the people who analyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse.\n We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use.","4599":"Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.","4600":"Movie and TV subtitles are a highly valuable resource for the compilation of parallel corpora thanks to their availability in large numbers and across many languages. However, the quality of the resulting sentence alignments is often lower than for other parallel corpora. This paper presents a new major release of the OpenSubtitles collection of parallel corpora, which is extracted from a total of 3.7 million subtitles spread over 60 languages. In addition to a substantial increase in the corpus size (about 30 % compared to the previous version), this new release associates explicit quality scores to each sentence alignment. These scores are determined by a feedforward neural network based on simple language-independent features and estimated on a sample of aligned sentence pairs. Evaluation results show that the model is able predict lexical translation probabilities with a root mean square error of 0.07 (coef\ufb01cient of determination R 2 = 0.47). Based on the scores produced by this regression model, the parallel corpora can be \ufb01ltered to prune out low-quality alignments.","4601":"LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.","4602":"Search engines are a critical tool for intelligence analysis. A number of innovations for search have been introduced since research with an emphasis on analyst needs began in the TIPSTER project. For example, the Inquery search engine introduced support for specification of complex queries in a probabilistic inference network framework. Recent research on language model-ing has led to the development of Indri, a search engine that combines the best features of inference nets and language modeling in an architecture designed for large-scale applications. In this paper, we describe the Indri system and show how the query language is designed to support modern language technologies. We also present results demonstrating that Indri is both effective and efficient.","4603":"Semantic Role Labeling (SRL) has proved to be a valuable tool for performing automatic analysis of natural language texts. Currently however, most systems rely on a large training set, which is manually annotated, an effort that needs to be repeated whenever different languages or a different set of semantic roles is used in a certain application. A possible solution for this problem is semi-supervised learning, where a small set of training examples is automatically expanded using unlabeled texts. We present the Latent Words Language Model, which is a language model that learns word similarities from unlabeled texts. We use these similarities for different semi-supervised SRL methods as additional features or to automatically expand a small training set. We evaluate the methods on the PropBank dataset and find that for small training sizes our best performing system achieves an error reduction of 33.27% F1-measure compared to a state-of-the-art supervised baseline.","4604":"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.","4605":"We present a scalable joint language model designed to utilize fine-grain syntactic tags. We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora. We advocate the use of relatively simple tags that do not require deep linguistic knowledge of the language but provide more structural information than POS tags and can be derived from automatically generated parse trees - a combination of properties that allows easy adoption of this model for new languages. We propose two fine-grain tagsets and evaluate our model using these tags, as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions.","4606":"We address the problem of synthesizing code completions for programs using APIs. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls. Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments. Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90% of the cases.","4607":"Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model\u2019s latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.","4608":"Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values. We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.","4609":"Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. \nWe present a practical method for the federated learning of deep networks that proves robust to the unbalanced and non-IID data distributions that naturally arise. This method allows high-quality models to be trained in relatively few rounds of communication, the principal constraint for federated learning. The key insight is that despite the non-convex loss functions we optimize, parameter averaging over updates from multiple clients produces surprisingly good results, for example decreasing the communication needed to train an LSTM language model by two orders of magnitude.","4610":"Language models have traditionally been estimated based on relative frequencies, using count statistics that can be extracted from huge amounts of text data. More recently, it has been found that neural networks are particularly powerful at estimating probability distributions over word sequences, giving substantial improvements over state-of-the-art count models. However, the performance of neural network language models strongly depends on their architectural structure. This paper compares count models to feedforward, recurrent, and long short-term memory (LSTM) neural network variants on two large-vocabulary speech recognition tasks. We evaluate the models in terms of perplexity and word error rate, experimentally validating the strong correlation of the two quantities, which we find to hold regardless of the underlying type of the language model. Furthermore, neural networks incur an increased computational complexity compared to count models, and they differently model context dependences, often exceeding the number of words that are taken into account by count based approaches. These differences require efficient search methods for neural networks, and we analyze the potential improvements that can be obtained when applying advanced algorithms to the rescoring of word lattices on large-scale setups.","4611":"Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model\u2019s embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT\u2019s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.","4612":"Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.","4613":"In this paper, a system for the reading of totally unconstrained handwritten text is presented. The kernel of the system is a hidden Markov model (HMM) for handwriting recognition. The HMM is enhanced by a statistical language model. Thus linguistic knowledge beyond the lexicon level is incorporated in the recognition process. Another novel feature of the system is that the HMM is applied in such a way that the difficult problem of segmenting a line of text into individual words is avoided. A number of experiments with various language models and large vocabularies have been conducted. The language models used in the system were also analytically compared based on their perplexity.","4614":"A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.","4615":"This paper describes a framework for modeling human activities as temporally structured processes. Our approach is motivated by the inherently hierarchical nature of human activities and the close correspondence between human actions and speech: We model action units using Hidden Markov Models, much like words in speech. These action units then form the building blocks to model complex human activities as sentences using an action grammar. To evaluate our approach, we collected a large dataset of daily cooking activities: The dataset includes a total of 52 participants, each performing a total of 10 cooking activities in multiple real-life kitchens, resulting in over 77 hours of video footage. We evaluate the HTK toolkit, a state-of-the-art speech recognition engine, in combination with multiple video feature descriptors, for both the recognition of cooking activities (e.g., making pancakes) as well as the semantic parsing of videos into action units (e.g., cracking eggs). Our results demonstrate the benefits of structured temporal generative approaches over existing discriminative approaches in coping with the complexity of human daily life activities.","4616":"We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.","4617":"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest.","4618":"We explore unsupervised language model adaptation techniques for Statistical Machine Translation. The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection. Specific language models are then build from the retrieved data and interpolated with a general background model. Experiments show significant improvements when translating with these adapted language models.","4619":null,"4620":"In this paper, we propose the TBCNN-pair model to recognize entailment and contradiction between two sentences. In our model, a tree-based convolutional neural network (TBCNN) captures sentence-level semantics; then heuristic matching layers like concatenation, element-wise product\/difference combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin.","4621":"We describe how to effectively train neural network based language models on large data sets. Fast convergence during training and better overall performance is observed when the training data are sorted by their relevance. We introduce hash-based implementation of a maximum entropy model, that can be trained as a part of the neural network model. This leads to significant reduction of computational complexity. We achieved around 10% relative reduction of word error rate on English Broadcast News speech recognition task, against large 4-gram model trained on 400M tokens.","4622":"Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a model to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our model is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including reading comprehension, sentiment analysis, and named-entity recognition and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.","4623":"The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling. \nIn this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.","4624":"How do neural language models keep track of number agreement between subject and verb? We show that \u2018diagnostic classifiers\u2019, trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented. Moreover, they give us insight into when and where number information is corrupted in cases where the language model ends up making agreement errors. To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences. Results from such an intervention reveal a large increase in the language model\u2019s accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.","4625":"In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.","4626":"We present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, low-power \"motes,\" each of which execute concurrent, reactive programs that must operate with severe memory and power constraints.nesC's contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption).nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.","4627":"We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences.","4628":"The transformer-based pre-trained language model BERT has helped to improve state-of-the-art performance on many natural language processing (NLP) tasks. Using the same architecture and parameters, we developed and evaluated a monolingual Dutch BERT model called BERTje. Compared to the multilingual BERT model, which includes Dutch but is only based on Wikipedia text, BERTje is based on a large and diverse dataset of 2.4 billion tokens. BERTje consistently outperforms the equally-sized multilingual BERT model on downstream NLP tasks (part-of-speech tagging, named-entity recognition, semantic role labeling, and sentiment analysis). Our pre-trained Dutch BERT model is made available at this https URL.","4629":"We are concerned with the discovery of hierarchical relationships from large-scale unstructured similarity scores. For this purpose, we study different models of hyperbolic space and find that learning embeddings in the Lorentz model is substantially more efficient than in the Poincar\\'e-ball model. We show that the proposed approach allows us to learn high-quality embeddings of large taxonomies which yield improvements over Poincar\\'e embeddings, especially in low dimensions. Lastly, we apply our model to discover hierarchies in two real-world datasets: we show that an embedding in hyperbolic space can reveal important aspects of a company's organizational structure as well as reveal historical relationships between language families.","4630":"We propose algorithms to train production-quality n-gram language models using federated learning. Federated learning is a distributed computation platform that can be used to train global models for portable devices such as smart phones. Federated learning is especially relevant for applications handling privacy-sensitive data, such as virtual keyboards, because training is performed without the users\u2019 data ever leaving their devices. While the principles of federated learning are fairly generic, its methodology assumes that the underlying models are neural networks. However, virtual keyboards are typically powered by n-gram language models for latency reasons. We propose to train a recurrent neural network language model using the decentralized FederatedAveraging algorithm and to approximate this federated model server-side with an n-gram model that can be deployed to devices for fast inference. Our technical contributions include ways of handling large vocabularies, algorithms to correct capitalization errors in user data, and efficient finite state transducer algorithms to convert word language models to word-piece language models and vice versa. The n-gram language models trained with federated learning are compared to n-grams trained with traditional server-based algorithms using A\/B tests on tens of millions of users of a virtual keyboard. Results are presented for two languages, American English and Brazilian Portuguese. This work demonstrates that high-quality n-gram language models can be trained directly on client mobile devices without sensitive training data ever leaving the devices.","4631":"Cloze tests are widely adopted in language exams to evaluate students\u2019 language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset CLOTH, containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, CLOTH requires a deeper language understanding and a wider attention span than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a language model trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending the long-term context to be the key bottleneck.","4632":"Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images\/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.","4633":"Large language models have been proven quite beneficial for a variety of automatic speech recognition tasks in Google. We summarize results on Voice Search and a few YouTube speech transcription tasks to highlight the impact that one can expect from increasing both the amount of training data, and the size of the language model estimated from such data. Depending on the task, availability and amount of training data used, language model size and amount of work and care put into integrating them in the lattice rescoring step we observe reductions in word error rate between 6% and 10% relative, for systems on a wide range of operating points between 17% and 52% word error rate.","4634":"In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the Free917 and WebQuestions benchmark datasets show our semantic parser improves over the state of the art.","4635":"This paper describes Jaql, a declarative scripting language for analyzing large semistructured datasets in parallel using Hadoop\u2019s MapReduce framework. Jaql is currently used in IBM\u2019s InfoSphere BigInsights [5] and Cognos Consumer Insight [9] products. Jaql\u2019s design features are: (1) a flexible data model, (2) reusability, (3) varying levels of abstraction, and (4) scalability. Jaql\u2019s data model is inspired by JSON and can be used to represent datasets that vary from flat, relational tables to collections of semistructured documents. A Jaql script can start without any schema and evolve over time from a partial to a rigid schema. Reusability is provided through the use of higher-order functions and by packaging related functions into modules. Most Jaql scripts work at a high level of abstraction for concise specification of logical operations (e.g., join), but Jaql\u2019s notion of physical transparency also provides a lower level of abstraction if necessary. This allows users to pin down the evaluation plan of a script for greater control or even add new operators. The Jaql compiler automatically rewrites Jaql scripts so they can run in parallel on Hadoop. In addition to describing Jaql\u2019s design, we present the results of scale-up experiments on Hadoop running Jaql scripts for intranet data analysis and log processing.","4636":"We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.","4637":"This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.","4638":"A language model (LM) is calculated as the probability of a word sequence that provides the solution to word prediction for a variety of information systems. A recurrent neural network (RNN) is powerful to learn the large-span dynamics of a word sequence in the continuous space. However, the training of the RNN-LM is an ill-posed problem because of too many parameters from a large dictionary size and a high-dimensional hidden layer. This paper presents a Bayesian approach to regularize the RNN-LM and apply it for continuous speech recognition. We aim to penalize the too complicated RNN-LM by compensating for the uncertainty of the estimated model parameters, which is represented by a Gaussian prior. The objective function in a Bayesian classification network is formed as the regularized cross-entropy error function. The regularized model is constructed not only by calculating the regularized parameters according to the maximum a posteriori criterion but also by estimating the Gaussian hyperparameter by maximizing the marginal likelihood. A rapid approximation to a Hessian matrix is developed to implement the Bayesian RNN-LM (BRNN-LM) by selecting a small set of salient outer-products. The proposed BRNN-LM achieves a sparser model than the RNN-LM. Experiments on different corpora show the robustness of system performance by applying the rapid BRNN-LM under different conditions.","4639":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.","4640":"Large language models (LMs) are able to in-context learn\u2014perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required\u2014randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.","4641":"The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.","4642":"We contribute 5-gram counts and language models trained on the Common Crawl corpus, a collection over 9 billion web pages. This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate. By preserving singletons, we were able to use Kneser-Ney smoothing to build large language models. This paper describes how the corpus was processed with emphasis on the problems that arise in working with data at this scale. Our unpruned Kneser-Ney English $5$-gram language model, built on 975 billion deduplicated tokens, contains over 500 billion unique n-grams. We show gains of 0.5-1.4 BLEU by using large language models to translate into various languages.","4643":"Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.","4644":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \u201cvirtual tokens\u201d. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.","4645":"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https:\/\/parti.research.google\/ for high-resolution images.","4646":"Recently, Google launched YouTube Kids, a mobile application for children, that uses a speech recognizer built specifically for recognizing children\u2019s speech. In this paper we present techniques we explored to build such a system. We describe the use of a neural network classifier to identify matched acoustic training data, filtering data for language modeling to reduce the chance of producing offensive results. We also compare long short-term memory (LSTM) recurrent networks to convolutional, LSTM, deep neural networks (CLDNN). We found that a CLDNN acoustic model outperforms an LSTM across a variety of different conditions, but does not specifically model child speech relatively better than adult. Overall, these findings allow us to build a successful, state-of-the-art large vocabulary speech recognizer for both children and adults.","4647":"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.","4648":"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics.","4649":"The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https:\/\/github.com\/nlpyang\/geval","4650":"Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https:\/\/github.com\/bigscience-workshop\/t-zero and all prompts are available at https:\/\/github.com\/bigscience-workshop\/promptsource.","4651":"We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.","4652":"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https:\/\/github.com\/ bigscience-workshop\/xmtf.","4653":"The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.","4654":"Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.","4655":"Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users' intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowd-source the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60--82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5--11%.","4656":"Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.","4657":"A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world\u2019s languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean.","4658":"We present a simple algorithm to efficiently train language models with noise-contrastive estimation (NCE) on graphics processing units (GPUs). Our NCE-trained language models achieve significantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in Chelba et al. (2013). When incorporated into a strong Arabic-English machine translation system they give a strong boost in translation quality. We release a toolkit so that others may also train large-scale, large vocabulary LSTM language models with NCE, parallelizing computation across multiple GPUs.","4659":"As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.","4660":"Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models \u2014 a sentiment model and a toxicity model \u2014 applied on online comments in English language from four different genres.","4661":"This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https:\/\/github.com\/microsoft\/Swin-Transformer.","4662":"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.","4663":"This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.","4664":"In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc). The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy. Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82% and 69%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.","4665":"The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by finding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the first giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is significantly better at the code suggestion task than previous models. More broadly, our approach provides a new \u201clens\u201d for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program's core logic based solely on general information theoretic criteria.","4666":"Logistic regression analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach that uses a Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach to a range of document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or ridge logistic regression combined with feature selection. We describe our model fitting algorithm, our open source implementations (BBR and BMR), and experimental results.","4667":"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT\u2019s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT\u2019s attention.","4668":"In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.","4669":"In this work, we explore \u201cprompt tuning,\u201d a simple yet effective mechanism for learning \u201csoft prompts\u201d to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3\u2019s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \u201ccloses the gap\u201d and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \u201cprefix tuning\u201d of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \u201cprompt ensembling.\u201d We release code and model checkpoints to reproduce our experiments.","4670":"DryadLINQ is a system and a set of language extensions that enable a new programming model for large scale distributed computing. It generalizes previous execution environments such as SQL, MapReduce, and Dryad in two ways: by adopting an expressive data model of strongly typed .NET objects; and by supporting general-purpose imperative and declarative operations on datasets within a traditional high-level programming language. \n \nA DryadLINQ program is a sequential program composed of LINQ expressions performing arbitrary side-effect-free transformations on datasets, and can be written and debugged using standard .NET development tools. The DryadLINQ system automatically and transparently translates the data-parallel portions of the program into a distributed execution plan which is passed to the Dryad execution platform. Dryad, which has been in continuous operation for several years on production clusters made up of thousands of computers, ensures efficient, reliable execution of this plan. \n \nWe describe the implementation of the DryadLINQ compiler and runtime. We evaluate DryadLINQ on a varied set of programs drawn from domains such as web-graph analysis, large-scale log mining, and machine learning. We show that excellent absolute performance can be attained--a general-purpose sort of 1012 Bytes of data executes in 319 seconds on a 240-computer, 960- disk cluster--as well as demonstrating near-linear scaling of execution time on representative applications as we vary the number of computers used for a job.","4671":"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.","4672":"Research in speech recognition and machine translation is boosting the use of large scale n-gram language models. We present an open source toolkit that permits to efficiently handle language models with billions of n-grams on conventional machines. The IRSTLM toolkit supports distribution of ngram collection and smoothing over a computer cluster, language model compression through probability quantization, lazy-loading of huge language models from disk. IRSTLM has been so far successfully deployed with the Moses toolkit for statistical machine translation and with the FBK-irst speech recognition system. Efficiency of the tool is reported on a speech transcription task of Italian political speeches using a language model of 1.1 billion four-grams.","4673":"Several recent works have considered the problem of generating reviews (or \u2018tips\u2019) as a form of explanation as to why a recommendation might match a customer\u2019s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users\u2019 decision-making process. We seek to introduce new datasets and methods to address the recommendation justification task. In terms of data, we first propose an \u2018extractive\u2019 approach to identify review segments which justify users\u2019 intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.","4674":"Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale\/color denoising, and real image denoising). The source code and pre-trained models are available at https:\/\/github.com\/swz30\/Restormer.","4675":"Modeling neural tissue is an important tool to investigate biological neural networks. Until recently, most of this modeling has been done using numerical methods. In the European research project \"FACETS\" this computational approach is complemented by different kinds of neuromorphic systems. A special emphasis lies in the usability of these systems for neuroscience. To accomplish this goal an integrated software\/hardware framework has been developed which is centered around a unified neural system description language, called PyNN, that allows the scientist to describe a model and execute it in a transparent fashion on either a neuromorphic hardware system or a numerical simulator. A very large analog neuromorphic hardware system developed within FACETS is able to use complex neural models as well as realistic network topologies, i.e. it can realize more than 10000 synapses per neuron, to allow the direct execution of models which previously could have been simulated numerically only.","4676":"Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.","4677":"This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the\"tug-of-war\"dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at https:\/\/github.com\/microsoft\/DeBERTa.","4678":"Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.","4679":"Program synthesis strives to generate a computer program as a solution to a given problem speci\ufb01cation. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent speci\ufb01cation faced in prior approaches. Our new approach casts the process of writing a speci\ufb01cation and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the speci\ufb01cation is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called C ODE G EN , on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our \ufb01ndings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model C ODE G EN (with up to 16B parameters trained on TPU-v4) outperforms OpenAI\u2019s Codex on the HumanEval benchmark. We make the training library JAX FORMER including checkpoints available as open source contribution: https:\/\/github.com\/salesforce\/CodeGen . to achieve program synthesis: (1) the vastness of the search space, and (2) the dif\ufb01culty of properly specifying user intent. In an attempt to address these challenges, we propose and train C ODE G EN , an interactive code generation model for program synthesis. In addition, we develop a new multi-turn programming benchmark to investigate the programming synthesis capacities of C ODE G EN . enhances as a function of the model size and data size. It suggests that the capacity of conversational program synthesis scales as a function of the model size and data size. The models are simply trained with an autoregressive language modeling objective. While the model and the data scale up, conversational capacity emerges.","4680":"Generating step-by-step\"chain-of-thought\"rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the\"Self-Taught Reasoner\"(STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.","4681":"Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. \nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.","4682":"Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.","4683":"When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \u201cfantastic\u201d and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.","4684":"Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https:\/\/memit.baulab.info.","4685":"Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations.","4686":"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and pre-trained models will be made publicly available at this https URL.","4687":"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.","4688":"Image captioning is a fundamental task in vision-language understanding, where the model predicts a textual informative caption to a given input image. In this paper, we present a simple approach to address this task. We use CLIP encoding as a prefix to the caption, by employing a simple mapping network, and then fine-tunes a language model to generate the image captions. The recently proposed CLIP model contains rich semantic features which were trained with textual context, making it best for vision-language perception. Our key idea is that together with a pre-trained language model (GPT2), we obtain a wide understanding of both visual and textual data. Hence, our approach only requires rather quick training to produce a competent captioning model. Without additional annotations or pre-training, it efficiently generates meaningful captions for large-scale and diverse datasets. Surprisingly, our method works well even when only the mapping network is trained, while both CLIP and the language model remain frozen, allowing a lighter architecture with less trainable parameters. Through quantitative evaluation, we demonstrate our model achieves comparable results to state-of-the-art methods on the challenging Conceptual Captions and nocaps datasets, while it is simpler, faster, and lighter. Our code is available in https:\/\/github.com\/rmokady\/CLIP_prefix_caption.","4689":"Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https:\/\/github.com\/airsplay\/lxmert","4690":"We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.","4691":"Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.","4692":"\n \n We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.\n \n","4693":"We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.","4694":"Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been dicult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades rst yielded and then made practical these powerful learning models. When appropriate, we reconcile conicting notation and nomenclature. Our goal is to provide a selfcontained explication of the state of the art together with a historical perspective and references to primary research.","4695":"Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.","4696":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.","4697":"Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.","4698":"As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN\/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.","4699":"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https URL.","4700":"We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own selfcritiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.","4701":"Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.","4702":"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions\u2014training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.","4703":"Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record. The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology. DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.","4704":"We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering\/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.","4705":"Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric by training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open source a training dataset that covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems of WMT. We open-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.","4706":"For many low-resource languages, spoken language resources are more likely to be annotated with translations than transcriptions. This bilingual speech data can be used for word-spotting, spoken document retrieval, and even for documentation of endangered languages. We experiment with the neural, attentional model applied to this data. On phoneto-word alignment and translation reranking tasks, we achieve large improvements relative to several baselines. On the more challenging speech-to-word alignment task, our model nearly matches GIZA++\u2019s performance on gold transcriptions, but without recourse to transcriptions or to a lexicon.","4707":"Recently, pre-trained language models have achieved remarkable success in a broad range of natural language processing tasks. However, in multilingual setting, it is extremely resource-consuming to pre-train a deep language model over large-scale corpora for each language. Instead of exhaustively pre-training monolingual language models independently, an alternative solution is to pre-train a powerful multilingual deep language model over large-scale corpora in hundreds of languages. However, the vocabulary size for each language in such a model is relatively small, especially for low-resource languages. This limitation inevitably hinders the performance of these multilingual models on tasks such as sequence labeling, wherein in-depth token-level or sentence-level understanding is essential. In this paper, inspired by previous methods designed for monolingual settings, we investigate two approaches (i.e., joint mapping and mixture mapping) based on a pre-trained multilingual model BERT for addressing the out-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Experimental results show that using mixture mapping is more promising. To the best of our knowledge, this is the first work that attempts to address and discuss the OOV issue in multilingual settings.","4708":"We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training\/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task training data, and outperforms much bigger models with nearly 8x parameters.","4709":"Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.","4710":"Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT\u201914 English-French and WMT\u201916 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.","4711":"Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields.","4712":"In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active toward a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target. Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria), it reproduced 28% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.","4713":"In this paper, we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems. We present an efficient low-memory method for constructing high-order approximate n-gram frequency counts. The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint. We show that this method easily scales to billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine. Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods.","4714":"The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.","4715":"We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.","4716":"This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.","4717":"Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level. We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).","4718":"This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.","4719":"This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.","4720":"We present CLIP2Video network to transfer the image-language pre-training model to video-text retrieval in an end-to-end manner. Leading approaches in the domain of video-and-language learning try to distill the spatio-temporal video features and multi-modal interaction between videos and languages from a large-scale video-text dataset. Different from them, we leverage pretrained image-language model, simplify it as a two-stage framework with co-learning of image-text and enhancing temporal relations between video frames and video-text respectively, make it able to train on comparatively small datasets. Specifically, based on the spatial semantics captured by Contrastive Language-Image Pretraining (CLIP) model, our model involves a Temporal Difference Block to capture motions at fine temporal video frames, and a Temporal Alignment Block to re-align the tokens of video clips and phrases and enhance the multi-modal correlation. We conduct thorough ablation studies, and achieve state-of-the-art performance on major text-to-video and video-to-text retrieval benchmarks, including new records of retrieval accuracy on MSR-VTT, MSVD and VATEX.","4721":"We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g. RoBERTa) and generation models (e.g. BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.","4722":"In statistical language modeling, one technique to reduce the problematic eects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the eects of applying such a technique to higherorder n-gram models trained on large corpora. We introduce a modification of the exchange clustering algorithm with improved eciency for certain partially class-based models and a distributed version of this algorithm to eciently obtain automatic word classifications for large vocabularies (>1 million words) using such large training corpora (>30 billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with wordbased n-gram models in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.","4723":"Abstract \u26a0 This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model\u2019s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1","4724":"Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on $\\bf{5}$ widely used deep learning tasks ($\\bf{18}$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English$\\to$German translation ($\\bf{30.91}$ BLEU) and WMT14 English$\\to$French translation ($\\bf{43.95}$ BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub{\\url{https:\/\/github.com\/dropreg\/R-Drop}}.","4725":"Adapting large-scale pretrained language models to downstream tasks via fine-tuning is the standard method for achieving state-of-the-art performance on NLP benchmarks. However, fine-tuning all weights of models with millions or billions of parameters is sample-inefficient, unstable in low-resource settings, and wasteful as it requires storing a separate copy of the model for each task. Recent work has developed parameter-efficient fine-tuning methods, but these approaches either still require a relatively large number of parameters or underperform standard fine-tuning. In this work, we propose Compacter, a method for fine-tuning large-scale language models with a better trade-off between task performance and the number of trainable parameters than prior work. Compacter accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers. Specifically, Compacter inserts task-specific weight matrices into a pretrained model's weights, which are computed efficiently as a sum of Kronecker products between shared\"slow\"weights and\"fast\"rank-one matrices defined per Compacter layer. By only training 0.047% of a pretrained model's parameters, Compacter performs on par with standard fine-tuning on GLUE and outperforms standard fine-tuning on SuperGLUE and low-resource settings. Our code is publicly available at~\\url{https:\/\/github.com\/rabeehk\/compacter}.","4726":"In this paper we describe the development of an accurate, smallfootprint, large vocabulary speech recognizer for mobile devices. To achieve the best recognition accuracy, state-of-the-art deep neural networks (DNNs) are adopted as acoustic models. A variety of speedup techniques for DNN score computation are used to enable real-time operation on mobile devices. To reduce the memory and disk usage, on-the-fly language model (LM) rescoring is performed with a compressed n-gram LM. We were able to build an accurate and compact system that runs well below real-time on a Nexus 4 Android phone. Index Terms: Deep neural networks, embedded speech recognition, SIMD, LM compression.","4727":"We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective \u201cdepth\u201d of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments. The code is available at https:\/\/github.com\/locuslab\/deq.","4728":"Companies providing cloud-scale services have an increasing need to store and analyze massive data sets such as search logs and click streams. For cost and performance reasons, processing is typically done on large clusters of shared-nothing commodity machines. It is imperative to develop a programming model that hides the complexity of the underlying system but provides flexibility by allowing users to extend functionality to meet a variety of requirements. \n \nIn this paper, we present a new declarative and extensible scripting language, SCOPE (Structured Computations Optimized for Parallel Execution), targeted for this type of massive data analysis. The language is designed for ease of use with no explicit parallelism, while being amenable to efficient parallel execution on large clusters. SCOPE borrows several features from SQL. Data is modeled as sets of rows composed of typed columns. The select statement is retained with inner joins, outer joins, and aggregation allowed. Users can easily define their own functions and implement their own versions of operators: extractors (parsing and constructing rows from a file), processors (row-wise processing), reducers (group-wise processing), and combiners (combining rows from two inputs). SCOPE supports nesting of expressions but also allows a computation to be specified as a series of steps, in a manner often preferred by programmers. We also describe how scripts are compiled into efficient, parallel execution plans and executed on large clusters.","4729":"Hitherto, the major challenge to sign language recognition is how to develop approaches that scale well with increasing vocabulary size. We present an approach to large vocabulary, continuous Chinese sign language (CSL) recognition that uses phonemes instead of whole signs as the basic units. Since the number of phonemes is limited, HMM-based training and recognition of the CSL signal becomes more tractable and has the potential to recognize enlarged vocabularies. Furthermore, the proposed method facilitates the CSL recognition when the finger-alphabet is blended with gestures. About 2400 phonemes are defined for CSL. One HMM is built for each phoneme, and then the signs are encoded based on these phonemes. A decoder that uses a tree-structured network is presented. Clustering of the Gaussians on the states, the language model and N-best-pass is used to improve the performance of the system. Experiments on a 5119 sign vocabulary are carried out, and the result is exciting.","4730":"\n \n Twitter sentiment analysis (TSA) has become a hot research topic in recent years. The goal of this task is to discover the attitude or opinion of the tweets, which is typically formulated as a machine learning based text classification problem. Some methods use manually labeled data to train fully supervised models, while others use some noisy labels, such as emoticons and hashtags, for model training. In general, we can only get a limited number of training data for the fully supervised models because it is very labor-intensive and time-consuming to manually label the tweets. As for the models with noisy labels, it is hard for them to achieve satisfactory performance due to the noise in the labels although it is easy to get a large amount of data for training. Hence, the best strategy is to utilize both manually labeled data and noisy labeled data for training. However, how to seamlessly integrate these two different kinds of data into the same learning framework is still a challenge. In this paper, we present a novel model, called emoticon smoothed language model (ESLAM), to handle this challenge. The basic idea is to train a language model based on the manually labeled data, and then use the noisy emoticon data for smoothing. Experiments on real data sets demonstrate that ESLAM can effectively integrate both kinds of data to outperform those methods using only one of them.\n \n","4731":"SciDB [4, 3] is a new open-source data management system intended primarily for use in application domains that involve very large (petabyte) scale array data; for example, scientific applications such as astronomy, remote sensing and climate modeling, bio-science information management, risk management systems in financial applications, and the analysis of web log data. In this talk we will describe our set of motivating examples and use them to explain the features of SciDB. We then briefly give an overview of the project 'in flight', explaining our novel storage manager, array data model, query language, and extensibility frameworks.","4732":"This paper describes ongoing work on a new approach for language modeling for large vocabulary continuous speech recognition. Almost all state.. o. f-the-art systems use statistical n-gram language models estimated on text corpora. One principle problem with such language models is the fact that many of the n-grams are never observed even in very large training corpora, and therefore it is common to back-off to a lower-order model. In this paper we propose to address this problem by carrying out the estimation task in a continuous space, enabling a smooth interpolation of the probabilities. A neural network is used to learn the projection of the words onto a continuous space and to estimate the n-gram probabilities. The connectionist language model is being evaluated on the DARPA HUB5 conversational telephone speech recognition task and preliminary results show consistent improvements in both perplexity and word error rate.","4733":"During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-of-the-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time.","4734":"Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.","4735":"It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning.Our code is available at github.com\/liujch1998\/GKP","4736":"The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters\u2014small learnt bottleneck layers inserted within each layer of a pre-trained model\u2014 ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic \u201cstiching-in\u201d of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml","4737":"Named Entity Recognition is a well established information extraction task with many state of the art systems existing for a variety of languages. Most systems rely on language speci\ufb01c resources, large annotated corpora, gazetteers and feature engineering to perform well monolingually. In this paper, we introduce an attentional neural model which only uses language universal phonological character representations with word embeddings to achieve state of the art performance in a monolingual setting using super-vision and which can quickly adapt to a new language with minimal or no data. We demonstrate that phonological character representations facilitate cross-lingual transfer, out-perform orthographic representations and incorporating both attention and phonological features improves statistical ef\ufb01ciency of the model in 0-shot and low data transfer settings with no task speci\ufb01c feature engineering in the source or target language.","4738":"A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score.","4739":"Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.","4740":"Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model. While bi-lingual parallel data are expensive to generate, monolingual data are relatively common. Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language. This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. The method exploits the existence of comparable text---multiple texts in the target language that discuss the same or similar stories as found in the source language document. For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to the source documents. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system.","4741":"Inferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple\u2019s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches."},"source":{"0":"bibsonomy","1":"bibsonomy","2":"bibsonomy","3":"bibsonomy","4":"bibsonomy","5":"bibsonomy","6":"bibsonomy","7":"bibsonomy","8":"bibsonomy","9":"bibsonomy","10":"bibsonomy","11":"bibsonomy","12":"bibsonomy","13":"bibsonomy","14":"bibsonomy","15":"bibsonomy","16":"bibsonomy","17":"bibsonomy","18":"bibsonomy","19":"bibsonomy","20":"bibsonomy","21":"bibsonomy","22":"bibsonomy","23":"bibsonomy","24":"bibsonomy","25":"bibsonomy","26":"bibsonomy","27":"bibsonomy","28":"bibsonomy","29":"bibsonomy","30":"bibsonomy","31":"bibsonomy","32":"bibsonomy","33":"bibsonomy","34":"bibsonomy","35":"bibsonomy","36":"bibsonomy","37":"bibsonomy","38":"bibsonomy","39":"bibsonomy","40":"bibsonomy","41":"bibsonomy","42":"bibsonomy","43":"bibsonomy","44":"bibsonomy","45":"bibsonomy","46":"bibsonomy","47":"bibsonomy","48":"bibsonomy","49":"bibsonomy","50":"bibsonomy","51":"bibsonomy","52":"bibsonomy","53":"bibsonomy","54":"bibsonomy","55":"bibsonomy","56":"bibsonomy","57":"bibsonomy","58":"bibsonomy","59":"bibsonomy","60":"bibsonomy","61":"bibsonomy","62":"bibsonomy","63":"bibsonomy","64":"bibsonomy","65":"bibsonomy","66":"bibsonomy","67":"bibsonomy","68":"bibsonomy","69":"bibsonomy","70":"bibsonomy","71":"bibsonomy","72":"bibsonomy","73":"bibsonomy","74":"bibsonomy","75":"bibsonomy","76":"bibsonomy","77":"bibsonomy","78":"bibsonomy","79":"bibsonomy","80":"bibsonomy","81":"bibsonomy","82":"bibsonomy","83":"bibsonomy","84":"bibsonomy","85":"bibsonomy","86":"bibsonomy","87":"bibsonomy","88":"bibsonomy","89":"bibsonomy","90":"bibsonomy","91":"bibsonomy","92":"bibsonomy","93":"bibsonomy","94":"bibsonomy","95":"bibsonomy","96":"bibsonomy","97":"bibsonomy","98":"bibsonomy","99":"bibsonomy","100":"bibsonomy","101":"bibsonomy","102":"bibsonomy","103":"bibsonomy","104":"bibsonomy","105":"bibsonomy","106":"bibsonomy","107":"bibsonomy","108":"bibsonomy","109":"bibsonomy","110":"bibsonomy","111":"bibsonomy","112":"bibsonomy","113":"bibsonomy","114":"bibsonomy","115":"bibsonomy","116":"bibsonomy","117":"bibsonomy","118":"bibsonomy","119":"bibsonomy","120":"bibsonomy","121":"bibsonomy","122":"bibsonomy","123":"bibsonomy","124":"bibsonomy","125":"bibsonomy","126":"bibsonomy","127":"bibsonomy","128":"bibsonomy","129":"bibsonomy","130":"bibsonomy","131":"bibsonomy","132":"bibsonomy","133":"bibsonomy","134":"bibsonomy","135":"bibsonomy","136":"bibsonomy","137":"bibsonomy","138":"bibsonomy","139":"bibsonomy","140":"bibsonomy","141":"bibsonomy","142":"bibsonomy","143":"bibsonomy","144":"bibsonomy","145":"bibsonomy","146":"bibsonomy","147":"bibsonomy","148":"bibsonomy","149":"bibsonomy","150":"bibsonomy","151":"bibsonomy","152":"bibsonomy","153":"bibsonomy","154":"bibsonomy","155":"bibsonomy","156":"bibsonomy","157":"bibsonomy","158":"bibsonomy","159":"bibsonomy","160":"bibsonomy","161":"bibsonomy","162":"bibsonomy","163":"bibsonomy","164":"bibsonomy","165":"bibsonomy","166":"bibsonomy","167":"bibsonomy","168":"bibsonomy","169":"bibsonomy","170":"bibsonomy","171":"bibsonomy","172":"bibsonomy","173":"bibsonomy","174":"bibsonomy","175":"bibsonomy","176":"bibsonomy","177":"bibsonomy","178":"bibsonomy","179":"bibsonomy","180":"bibsonomy","181":"bibsonomy","182":"bibsonomy","183":"bibsonomy","184":"bibsonomy","185":"bibsonomy","186":"bibsonomy","187":"bibsonomy","188":"bibsonomy","189":"bibsonomy","190":"bibsonomy","191":"bibsonomy","192":"bibsonomy","193":"bibsonomy","194":"bibsonomy","195":"bibsonomy","196":"bibsonomy","197":"bibsonomy","198":"bibsonomy","199":"bibsonomy","200":"bibsonomy","201":"bibsonomy","202":"bibsonomy","203":"bibsonomy","204":"bibsonomy","205":"bibsonomy","206":"bibsonomy","207":"bibsonomy","208":"bibsonomy","209":"bibsonomy","210":"bibsonomy","211":"bibsonomy","212":"bibsonomy","213":"bibsonomy","214":"bibsonomy","215":"bibsonomy","216":"bibsonomy","217":"bibsonomy","218":"bibsonomy","219":"bibsonomy","220":"bibsonomy","221":"bibsonomy","222":"bibsonomy","223":"bibsonomy","224":"bibsonomy","225":"bibsonomy","226":"bibsonomy","227":"bibsonomy","228":"bibsonomy","229":"bibsonomy","230":"bibsonomy","231":"bibsonomy","232":"bibsonomy","233":"bibsonomy","234":"bibsonomy","235":"bibsonomy","236":"bibsonomy","237":"bibsonomy","238":"bibsonomy","239":"bibsonomy","240":"bibsonomy","241":"bibsonomy","242":"bibsonomy","243":"bibsonomy","244":"bibsonomy","245":"bibsonomy","246":"bibsonomy","247":"bibsonomy","248":"bibsonomy","249":"bibsonomy","250":"bibsonomy","251":"bibsonomy","252":"bibsonomy","253":"bibsonomy","254":"bibsonomy","255":"bibsonomy","256":"bibsonomy","257":"bibsonomy","258":"bibsonomy","259":"bibsonomy","260":"bibsonomy","261":"bibsonomy","262":"bibsonomy","263":"bibsonomy","264":"bibsonomy","265":"bibsonomy","266":"bibsonomy","267":"bibsonomy","268":"bibsonomy","269":"bibsonomy","270":"bibsonomy","271":"bibsonomy","272":"bibsonomy","273":"bibsonomy","274":"bibsonomy","275":"bibsonomy","276":"bibsonomy","277":"bibsonomy","278":"bibsonomy","279":"bibsonomy","280":"bibsonomy","281":"bibsonomy","282":"bibsonomy","283":"bibsonomy","284":"bibsonomy","285":"bibsonomy","286":"bibsonomy","287":"bibsonomy","288":"bibsonomy","289":"bibsonomy","290":"bibsonomy","291":"bibsonomy","292":"bibsonomy","293":"bibsonomy","294":"bibsonomy","295":"bibsonomy","296":"bibsonomy","297":"bibsonomy","298":"bibsonomy","299":"bibsonomy","300":"bibsonomy","301":"bibsonomy","302":"bibsonomy","303":"bibsonomy","304":"bibsonomy","305":"bibsonomy","306":"bibsonomy","307":"bibsonomy","308":"bibsonomy","309":"bibsonomy","310":"bibsonomy","311":"bibsonomy","312":"bibsonomy","313":"bibsonomy","314":"bibsonomy","315":"bibsonomy","316":"bibsonomy","317":"bibsonomy","318":"bibsonomy","319":"bibsonomy","320":"bibsonomy","321":"bibsonomy","322":"bibsonomy","323":"bibsonomy","324":"bibsonomy","325":"bibsonomy","326":"bibsonomy","327":"bibsonomy","328":"bibsonomy","329":"bibsonomy","330":"bibsonomy","331":"bibsonomy","332":"bibsonomy","333":"bibsonomy","334":"bibsonomy","335":"bibsonomy","336":"bibsonomy","337":"bibsonomy","338":"bibsonomy","339":"bibsonomy","340":"bibsonomy","341":"bibsonomy","342":"bibsonomy","343":"bibsonomy","344":"bibsonomy","345":"bibsonomy","346":"bibsonomy","347":"bibsonomy","348":"bibsonomy","349":"bibsonomy","350":"bibsonomy","351":"bibsonomy","352":"bibsonomy","353":"bibsonomy","354":"bibsonomy","355":"bibsonomy","356":"bibsonomy","357":"bibsonomy","358":"bibsonomy","359":"bibsonomy","360":"bibsonomy","361":"bibsonomy","362":"bibsonomy","363":"bibsonomy","364":"bibsonomy","365":"bibsonomy","366":"bibsonomy","367":"bibsonomy","368":"bibsonomy","369":"bibsonomy","370":"bibsonomy","371":"bibsonomy","372":"bibsonomy","373":"bibsonomy","374":"bibsonomy","375":"bibsonomy","376":"bibsonomy","377":"bibsonomy","378":"bibsonomy","379":"bibsonomy","380":"bibsonomy","381":"bibsonomy","382":"bibsonomy","383":"bibsonomy","384":"bibsonomy","385":"bibsonomy","386":"bibsonomy","387":"bibsonomy","388":"bibsonomy","389":"bibsonomy","390":"bibsonomy","391":"bibsonomy","392":"bibsonomy","393":"bibsonomy","394":"bibsonomy","395":"bibsonomy","396":"bibsonomy","397":"bibsonomy","398":"bibsonomy","399":"bibsonomy","400":"bibsonomy","401":"bibsonomy","402":"bibsonomy","403":"bibsonomy","404":"bibsonomy","405":"bibsonomy","406":"bibsonomy","407":"bibsonomy","408":"bibsonomy","409":"bibsonomy","410":"bibsonomy","411":"bibsonomy","412":"bibsonomy","413":"bibsonomy","414":"bibsonomy","415":"bibsonomy","416":"bibsonomy","417":"bibsonomy","418":"bibsonomy","419":"bibsonomy","420":"bibsonomy","421":"bibsonomy","422":"bibsonomy","423":"bibsonomy","424":"bibsonomy","425":"bibsonomy","426":"bibsonomy","427":"bibsonomy","428":"bibsonomy","429":"bibsonomy","430":"bibsonomy","431":"bibsonomy","432":"bibsonomy","433":"bibsonomy","434":"bibsonomy","435":"bibsonomy","436":"bibsonomy","437":"bibsonomy","438":"bibsonomy","439":"bibsonomy","440":"bibsonomy","441":"bibsonomy","442":"bibsonomy","443":"bibsonomy","444":"bibsonomy","445":"bibsonomy","446":"bibsonomy","447":"bibsonomy","448":"bibsonomy","449":"bibsonomy","450":"bibsonomy","451":"bibsonomy","452":"bibsonomy","453":"bibsonomy","454":"bibsonomy","455":"bibsonomy","456":"bibsonomy","457":"bibsonomy","458":"bibsonomy","459":"bibsonomy","460":"bibsonomy","461":"bibsonomy","462":"bibsonomy","463":"bibsonomy","464":"bibsonomy","465":"bibsonomy","466":"bibsonomy","467":"bibsonomy","468":"bibsonomy","469":"bibsonomy","470":"bibsonomy","471":"bibsonomy","472":"bibsonomy","473":"bibsonomy","474":"bibsonomy","475":"bibsonomy","476":"bibsonomy","477":"bibsonomy","478":"bibsonomy","479":"bibsonomy","480":"bibsonomy","481":"bibsonomy","482":"bibsonomy","483":"bibsonomy","484":"bibsonomy","485":"bibsonomy","486":"bibsonomy","487":"bibsonomy","488":"bibsonomy","489":"bibsonomy","490":"bibsonomy","491":"bibsonomy","492":"bibsonomy","493":"bibsonomy","494":"bibsonomy","495":"bibsonomy","496":"bibsonomy","497":"bibsonomy","498":"bibsonomy","499":"bibsonomy","500":"bibsonomy","501":"bibsonomy","502":"bibsonomy","503":"bibsonomy","504":"bibsonomy","505":"bibsonomy","506":"bibsonomy","507":"bibsonomy","508":"bibsonomy","509":"bibsonomy","510":"bibsonomy","511":"bibsonomy","512":"bibsonomy","513":"bibsonomy","514":"bibsonomy","515":"bibsonomy","516":"bibsonomy","517":"bibsonomy","518":"bibsonomy","519":"bibsonomy","520":"bibsonomy","521":"bibsonomy","522":"bibsonomy","523":"bibsonomy","524":"bibsonomy","525":"bibsonomy","526":"bibsonomy","527":"bibsonomy","528":"bibsonomy","529":"bibsonomy","530":"bibsonomy","531":"bibsonomy","532":"bibsonomy","533":"bibsonomy","534":"bibsonomy","535":"bibsonomy","536":"bibsonomy","537":"bibsonomy","538":"bibsonomy","539":"bibsonomy","540":"bibsonomy","541":"bibsonomy","542":"bibsonomy","543":"bibsonomy","544":"bibsonomy","545":"bibsonomy","546":"bibsonomy","547":"bibsonomy","548":"bibsonomy","549":"bibsonomy","550":"bibsonomy","551":"bibsonomy","552":"bibsonomy","553":"bibsonomy","554":"bibsonomy","555":"bibsonomy","556":"bibsonomy","557":"bibsonomy","558":"bibsonomy","559":"bibsonomy","560":"bibsonomy","561":"bibsonomy","562":"bibsonomy","563":"bibsonomy","564":"bibsonomy","565":"bibsonomy","566":"bibsonomy","567":"bibsonomy","568":"bibsonomy","569":"bibsonomy","570":"bibsonomy","571":"bibsonomy","572":"bibsonomy","573":"bibsonomy","574":"bibsonomy","575":"bibsonomy","576":"bibsonomy","577":"bibsonomy","578":"bibsonomy","579":"bibsonomy","580":"bibsonomy","581":"bibsonomy","582":"bibsonomy","583":"bibsonomy","584":"bibsonomy","585":"bibsonomy","586":"bibsonomy","587":"bibsonomy","588":"bibsonomy","589":"bibsonomy","590":"bibsonomy","591":"bibsonomy","592":"bibsonomy","593":"bibsonomy","594":"bibsonomy","595":"bibsonomy","596":"bibsonomy","597":"bibsonomy","598":"bibsonomy","599":"bibsonomy","600":"bibsonomy","601":"bibsonomy","602":"bibsonomy","603":"bibsonomy","604":"bibsonomy","605":"bibsonomy","606":"bibsonomy","607":"bibsonomy","608":"bibsonomy","609":"bibsonomy","610":"bibsonomy","611":"bibsonomy","612":"bibsonomy","613":"bibsonomy","614":"bibsonomy","615":"bibsonomy","616":"bibsonomy","617":"bibsonomy","618":"bibsonomy","619":"bibsonomy","620":"bibsonomy","621":"bibsonomy","622":"bibsonomy","623":"bibsonomy","624":"bibsonomy","625":"bibsonomy","626":"bibsonomy","627":"bibsonomy","628":"bibsonomy","629":"bibsonomy","630":"bibsonomy","631":"bibsonomy","632":"bibsonomy","633":"bibsonomy","634":"bibsonomy","635":"bibsonomy","636":"bibsonomy","637":"bibsonomy","638":"bibsonomy","639":"bibsonomy","640":"bibsonomy","641":"bibsonomy","642":"bibsonomy","643":"bibsonomy","644":"bibsonomy","645":"bibsonomy","646":"bibsonomy","647":"bibsonomy","648":"bibsonomy","649":"bibsonomy","650":"bibsonomy","651":"bibsonomy","652":"bibsonomy","653":"bibsonomy","654":"bibsonomy","655":"bibsonomy","656":"bibsonomy","657":"bibsonomy","658":"bibsonomy","659":"bibsonomy","660":"bibsonomy","661":"bibsonomy","662":"bibsonomy","663":"bibsonomy","664":"bibsonomy","665":"bibsonomy","666":"bibsonomy","667":"bibsonomy","668":"bibsonomy","669":"bibsonomy","670":"bibsonomy","671":"bibsonomy","672":"bibsonomy","673":"bibsonomy","674":"bibsonomy","675":"bibsonomy","676":"bibsonomy","677":"bibsonomy","678":"bibsonomy","679":"bibsonomy","680":"bibsonomy","681":"bibsonomy","682":"bibsonomy","683":"bibsonomy","684":"bibsonomy","685":"bibsonomy","686":"bibsonomy","687":"bibsonomy","688":"bibsonomy","689":"bibsonomy","690":"bibsonomy","691":"bibsonomy","692":"bibsonomy","693":"bibsonomy","694":"bibsonomy","695":"bibsonomy","696":"bibsonomy","697":"bibsonomy","698":"bibsonomy","699":"bibsonomy","700":"bibsonomy","701":"bibsonomy","702":"bibsonomy","703":"bibsonomy","704":"bibsonomy","705":"bibsonomy","706":"bibsonomy","707":"bibsonomy","708":"bibsonomy","709":"bibsonomy","710":"bibsonomy","711":"bibsonomy","712":"bibsonomy","713":"bibsonomy","714":"bibsonomy","715":"bibsonomy","716":"bibsonomy","717":"bibsonomy","718":"bibsonomy","719":"bibsonomy","720":"bibsonomy","721":"bibsonomy","722":"bibsonomy","723":"bibsonomy","724":"bibsonomy","725":"bibsonomy","726":"bibsonomy","727":"bibsonomy","728":"bibsonomy","729":"bibsonomy","730":"bibsonomy","731":"bibsonomy","732":"bibsonomy","733":"bibsonomy","734":"bibsonomy","735":"bibsonomy","736":"bibsonomy","737":"bibsonomy","738":"bibsonomy","739":"bibsonomy","740":"bibsonomy","741":"bibsonomy","742":"bibsonomy","743":"bibsonomy","744":"bibsonomy","745":"bibsonomy","746":"bibsonomy","747":"bibsonomy","748":"bibsonomy","749":"bibsonomy","750":"bibsonomy","751":"bibsonomy","752":"bibsonomy","753":"bibsonomy","754":"bibsonomy","755":"bibsonomy","756":"bibsonomy","757":"bibsonomy","758":"bibsonomy","759":"bibsonomy","760":"bibsonomy","761":"bibsonomy","762":"bibsonomy","763":"bibsonomy","764":"bibsonomy","765":"bibsonomy","766":"bibsonomy","767":"bibsonomy","768":"bibsonomy","769":"bibsonomy","770":"bibsonomy","771":"bibsonomy","772":"bibsonomy","773":"bibsonomy","774":"bibsonomy","775":"bibsonomy","776":"bibsonomy","777":"bibsonomy","778":"bibsonomy","779":"bibsonomy","780":"bibsonomy","781":"bibsonomy","782":"bibsonomy","783":"bibsonomy","784":"bibsonomy","785":"bibsonomy","786":"bibsonomy","787":"bibsonomy","788":"bibsonomy","789":"bibsonomy","790":"bibsonomy","791":"bibsonomy","792":"bibsonomy","793":"bibsonomy","794":"bibsonomy","795":"bibsonomy","796":"bibsonomy","797":"bibsonomy","798":"bibsonomy","799":"bibsonomy","800":"bibsonomy","801":"bibsonomy","802":"bibsonomy","803":"bibsonomy","804":"bibsonomy","805":"bibsonomy","806":"bibsonomy","807":"bibsonomy","808":"bibsonomy","809":"bibsonomy","810":"bibsonomy","811":"bibsonomy","812":"bibsonomy","813":"bibsonomy","814":"bibsonomy","815":"bibsonomy","816":"bibsonomy","817":"bibsonomy","818":"bibsonomy","819":"bibsonomy","820":"bibsonomy","821":"bibsonomy","822":"bibsonomy","823":"bibsonomy","824":"bibsonomy","825":"bibsonomy","826":"bibsonomy","827":"bibsonomy","828":"bibsonomy","829":"bibsonomy","830":"bibsonomy","831":"bibsonomy","832":"bibsonomy","833":"bibsonomy","834":"bibsonomy","835":"bibsonomy","836":"bibsonomy","837":"bibsonomy","838":"bibsonomy","839":"bibsonomy","840":"bibsonomy","841":"bibsonomy","842":"bibsonomy","843":"bibsonomy","844":"bibsonomy","845":"bibsonomy","846":"bibsonomy","847":"bibsonomy","848":"bibsonomy","849":"bibsonomy","850":"bibsonomy","851":"bibsonomy","852":"bibsonomy","853":"bibsonomy","854":"bibsonomy","855":"bibsonomy","856":"bibsonomy","857":"bibsonomy","858":"bibsonomy","859":"bibsonomy","860":"bibsonomy","861":"bibsonomy","862":"bibsonomy","863":"bibsonomy","864":"bibsonomy","865":"bibsonomy","866":"bibsonomy","867":"bibsonomy","868":"bibsonomy","869":"bibsonomy","870":"bibsonomy","871":"bibsonomy","872":"bibsonomy","873":"bibsonomy","874":"bibsonomy","875":"bibsonomy","876":"bibsonomy","877":"bibsonomy","878":"bibsonomy","879":"bibsonomy","880":"bibsonomy","881":"bibsonomy","882":"bibsonomy","883":"bibsonomy","884":"bibsonomy","885":"bibsonomy","886":"bibsonomy","887":"bibsonomy","888":"bibsonomy","889":"bibsonomy","890":"bibsonomy","891":"bibsonomy","892":"bibsonomy","893":"bibsonomy","894":"bibsonomy","895":"bibsonomy","896":"bibsonomy","897":"bibsonomy","898":"bibsonomy","899":"bibsonomy","900":"bibsonomy","901":"bibsonomy","902":"bibsonomy","903":"bibsonomy","904":"bibsonomy","905":"bibsonomy","906":"bibsonomy","907":"bibsonomy","908":"bibsonomy","909":"bibsonomy","910":"bibsonomy","911":"bibsonomy","912":"bibsonomy","913":"bibsonomy","914":"bibsonomy","915":"bibsonomy","916":"bibsonomy","917":"bibsonomy","918":"bibsonomy","919":"bibsonomy","920":"bibsonomy","921":"bibsonomy","922":"bibsonomy","923":"bibsonomy","924":"bibsonomy","925":"bibsonomy","926":"bibsonomy","927":"bibsonomy","928":"bibsonomy","929":"bibsonomy","930":"bibsonomy","931":"bibsonomy","932":"bibsonomy","933":"bibsonomy","934":"bibsonomy","935":"bibsonomy","936":"bibsonomy","937":"bibsonomy","938":"bibsonomy","939":"bibsonomy","940":"bibsonomy","941":"bibsonomy","942":"bibsonomy","943":"bibsonomy","944":"bibsonomy","945":"bibsonomy","946":"bibsonomy","947":"bibsonomy","948":"bibsonomy","949":"bibsonomy","950":"bibsonomy","951":"bibsonomy","952":"bibsonomy","953":"bibsonomy","954":"bibsonomy","955":"bibsonomy","956":"bibsonomy","957":"bibsonomy","958":"bibsonomy","959":"bibsonomy","960":"bibsonomy","961":"bibsonomy","962":"bibsonomy","963":"bibsonomy","964":"bibsonomy","965":"bibsonomy","966":"bibsonomy","967":"bibsonomy","968":"bibsonomy","969":"bibsonomy","970":"bibsonomy","971":"bibsonomy","972":"bibsonomy","973":"bibsonomy","974":"bibsonomy","975":"bibsonomy","976":"bibsonomy","977":"bibsonomy","978":"bibsonomy","979":"bibsonomy","980":"bibsonomy","981":"bibsonomy","982":"bibsonomy","983":"bibsonomy","984":"bibsonomy","985":"bibsonomy","986":"bibsonomy","987":"bibsonomy","988":"bibsonomy","989":"bibsonomy","990":"bibsonomy","991":"bibsonomy","992":"bibsonomy","993":"bibsonomy","994":"bibsonomy","995":"bibsonomy","996":"bibsonomy","997":"bibsonomy","998":"bibsonomy","999":"bibsonomy","1000":"bibsonomy","1001":"bibsonomy","1002":"bibsonomy","1003":"bibsonomy","1004":"bibsonomy","1005":"bibsonomy","1006":"bibsonomy","1007":"bibsonomy","1008":"bibsonomy","1009":"bibsonomy","1010":"bibsonomy","1011":"bibsonomy","1012":"bibsonomy","1013":"bibsonomy","1014":"bibsonomy","1015":"bibsonomy","1016":"bibsonomy","1017":"bibsonomy","1018":"bibsonomy","1019":"bibsonomy","1020":"bibsonomy","1021":"bibsonomy","1022":"bibsonomy","1023":"bibsonomy","1024":"bibsonomy","1025":"bibsonomy","1026":"bibsonomy","1027":"bibsonomy","1028":"bibsonomy","1029":"bibsonomy","1030":"bibsonomy","1031":"bibsonomy","1032":"bibsonomy","1033":"bibsonomy","1034":"bibsonomy","1035":"bibsonomy","1036":"bibsonomy","1037":"bibsonomy","1038":"bibsonomy","1039":"bibsonomy","1040":"bibsonomy","1041":"bibsonomy","1042":"bibsonomy","1043":"bibsonomy","1044":"bibsonomy","1045":"bibsonomy","1046":"bibsonomy","1047":"bibsonomy","1048":"bibsonomy","1049":"bibsonomy","1050":"bibsonomy","1051":"bibsonomy","1052":"bibsonomy","1053":"bibsonomy","1054":"bibsonomy","1055":"bibsonomy","1056":"bibsonomy","1057":"bibsonomy","1058":"bibsonomy","1059":"bibsonomy","1060":"bibsonomy","1061":"bibsonomy","1062":"bibsonomy","1063":"bibsonomy","1064":"bibsonomy","1065":"bibsonomy","1066":"bibsonomy","1067":"bibsonomy","1068":"bibsonomy","1069":"bibsonomy","1070":"bibsonomy","1071":"bibsonomy","1072":"bibsonomy","1073":"bibsonomy","1074":"bibsonomy","1075":"bibsonomy","1076":"bibsonomy","1077":"bibsonomy","1078":"bibsonomy","1079":"bibsonomy","1080":"bibsonomy","1081":"bibsonomy","1082":"bibsonomy","1083":"bibsonomy","1084":"bibsonomy","1085":"bibsonomy","1086":"bibsonomy","1087":"bibsonomy","1088":"bibsonomy","1089":"bibsonomy","1090":"bibsonomy","1091":"bibsonomy","1092":"bibsonomy","1093":"bibsonomy","1094":"bibsonomy","1095":"bibsonomy","1096":"bibsonomy","1097":"bibsonomy","1098":"bibsonomy","1099":"bibsonomy","1100":"bibsonomy","1101":"bibsonomy","1102":"bibsonomy","1103":"bibsonomy","1104":"bibsonomy","1105":"bibsonomy","1106":"bibsonomy","1107":"bibsonomy","1108":"bibsonomy","1109":"bibsonomy","1110":"bibsonomy","1111":"bibsonomy","1112":"bibsonomy","1113":"bibsonomy","1114":"bibsonomy","1115":"bibsonomy","1116":"bibsonomy","1117":"bibsonomy","1118":"bibsonomy","1119":"bibsonomy","1120":"bibsonomy","1121":"bibsonomy","1122":"bibsonomy","1123":"bibsonomy","1124":"bibsonomy","1125":"bibsonomy","1126":"bibsonomy","1127":"bibsonomy","1128":"bibsonomy","1129":"bibsonomy","1130":"bibsonomy","1131":"bibsonomy","1132":"bibsonomy","1133":"bibsonomy","1134":"bibsonomy","1135":"bibsonomy","1136":"bibsonomy","1137":"bibsonomy","1138":"bibsonomy","1139":"bibsonomy","1140":"bibsonomy","1141":"bibsonomy","1142":"bibsonomy","1143":"bibsonomy","1144":"bibsonomy","1145":"bibsonomy","1146":"bibsonomy","1147":"bibsonomy","1148":"bibsonomy","1149":"bibsonomy","1150":"bibsonomy","1151":"bibsonomy","1152":"bibsonomy","1153":"bibsonomy","1154":"bibsonomy","1155":"bibsonomy","1156":"bibsonomy","1157":"bibsonomy","1158":"bibsonomy","1159":"bibsonomy","1160":"bibsonomy","1161":"bibsonomy","1162":"bibsonomy","1163":"bibsonomy","1164":"bibsonomy","1165":"bibsonomy","1166":"bibsonomy","1167":"bibsonomy","1168":"bibsonomy","1169":"bibsonomy","1170":"bibsonomy","1171":"bibsonomy","1172":"bibsonomy","1173":"bibsonomy","1174":"bibsonomy","1175":"bibsonomy","1176":"bibsonomy","1177":"bibsonomy","1178":"bibsonomy","1179":"bibsonomy","1180":"bibsonomy","1181":"bibsonomy","1182":"bibsonomy","1183":"bibsonomy","1184":"bibsonomy","1185":"bibsonomy","1186":"bibsonomy","1187":"bibsonomy","1188":"bibsonomy","1189":"bibsonomy","1190":"bibsonomy","1191":"bibsonomy","1192":"bibsonomy","1193":"bibsonomy","1194":"bibsonomy","1195":"bibsonomy","1196":"bibsonomy","1197":"bibsonomy","1198":"bibsonomy","1199":"bibsonomy","1200":"bibsonomy","1201":"bibsonomy","1202":"bibsonomy","1203":"bibsonomy","1204":"bibsonomy","1205":"bibsonomy","1206":"bibsonomy","1207":"bibsonomy","1208":"bibsonomy","1209":"bibsonomy","1210":"bibsonomy","1211":"bibsonomy","1212":"bibsonomy","1213":"bibsonomy","1214":"bibsonomy","1215":"bibsonomy","1216":"bibsonomy","1217":"bibsonomy","1218":"bibsonomy","1219":"bibsonomy","1220":"bibsonomy","1221":"bibsonomy","1222":"bibsonomy","1223":"bibsonomy","1224":"bibsonomy","1225":"bibsonomy","1226":"bibsonomy","1227":"bibsonomy","1228":"bibsonomy","1229":"bibsonomy","1230":"bibsonomy","1231":"bibsonomy","1232":"bibsonomy","1233":"bibsonomy","1234":"bibsonomy","1235":"bibsonomy","1236":"bibsonomy","1237":"bibsonomy","1238":"bibsonomy","1239":"bibsonomy","1240":"bibsonomy","1241":"bibsonomy","1242":"bibsonomy","1243":"bibsonomy","1244":"bibsonomy","1245":"bibsonomy","1246":"bibsonomy","1247":"bibsonomy","1248":"bibsonomy","1249":"bibsonomy","1250":"bibsonomy","1251":"bibsonomy","1252":"bibsonomy","1253":"bibsonomy","1254":"bibsonomy","1255":"bibsonomy","1256":"bibsonomy","1257":"bibsonomy","1258":"bibsonomy","1259":"bibsonomy","1260":"bibsonomy","1261":"bibsonomy","1262":"bibsonomy","1263":"bibsonomy","1264":"bibsonomy","1265":"bibsonomy","1266":"bibsonomy","1267":"bibsonomy","1268":"bibsonomy","1269":"bibsonomy","1270":"bibsonomy","1271":"bibsonomy","1272":"bibsonomy","1273":"bibsonomy","1274":"bibsonomy","1275":"bibsonomy","1276":"bibsonomy","1277":"bibsonomy","1278":"bibsonomy","1279":"bibsonomy","1280":"bibsonomy","1281":"bibsonomy","1282":"bibsonomy","1283":"bibsonomy","1284":"bibsonomy","1285":"bibsonomy","1286":"bibsonomy","1287":"bibsonomy","1288":"bibsonomy","1289":"bibsonomy","1290":"bibsonomy","1291":"bibsonomy","1292":"bibsonomy","1293":"bibsonomy","1294":"bibsonomy","1295":"bibsonomy","1296":"bibsonomy","1297":"bibsonomy","1298":"bibsonomy","1299":"bibsonomy","1300":"bibsonomy","1301":"bibsonomy","1302":"bibsonomy","1303":"bibsonomy","1304":"bibsonomy","1305":"bibsonomy","1306":"bibsonomy","1307":"bibsonomy","1308":"bibsonomy","1309":"bibsonomy","1310":"bibsonomy","1311":"bibsonomy","1312":"bibsonomy","1313":"bibsonomy","1314":"bibsonomy","1315":"bibsonomy","1316":"bibsonomy","1317":"bibsonomy","1318":"bibsonomy","1319":"bibsonomy","1320":"bibsonomy","1321":"bibsonomy","1322":"bibsonomy","1323":"bibsonomy","1324":"bibsonomy","1325":"bibsonomy","1326":"bibsonomy","1327":"bibsonomy","1328":"bibsonomy","1329":"bibsonomy","1330":"bibsonomy","1331":"bibsonomy","1332":"bibsonomy","1333":"bibsonomy","1334":"bibsonomy","1335":"bibsonomy","1336":"bibsonomy","1337":"bibsonomy","1338":"bibsonomy","1339":"bibsonomy","1340":"bibsonomy","1341":"bibsonomy","1342":"bibsonomy","1343":"bibsonomy","1344":"bibsonomy","1345":"bibsonomy","1346":"bibsonomy","1347":"bibsonomy","1348":"bibsonomy","1349":"bibsonomy","1350":"bibsonomy","1351":"bibsonomy","1352":"bibsonomy","1353":"bibsonomy","1354":"bibsonomy","1355":"bibsonomy","1356":"bibsonomy","1357":"bibsonomy","1358":"bibsonomy","1359":"bibsonomy","1360":"bibsonomy","1361":"bibsonomy","1362":"bibsonomy","1363":"bibsonomy","1364":"bibsonomy","1365":"bibsonomy","1366":"bibsonomy","1367":"bibsonomy","1368":"bibsonomy","1369":"bibsonomy","1370":"bibsonomy","1371":"bibsonomy","1372":"bibsonomy","1373":"bibsonomy","1374":"bibsonomy","1375":"bibsonomy","1376":"bibsonomy","1377":"bibsonomy","1378":"bibsonomy","1379":"bibsonomy","1380":"bibsonomy","1381":"bibsonomy","1382":"bibsonomy","1383":"bibsonomy","1384":"bibsonomy","1385":"bibsonomy","1386":"bibsonomy","1387":"bibsonomy","1388":"bibsonomy","1389":"bibsonomy","1390":"bibsonomy","1391":"bibsonomy","1392":"bibsonomy","1393":"bibsonomy","1394":"bibsonomy","1395":"bibsonomy","1396":"bibsonomy","1397":"bibsonomy","1398":"bibsonomy","1399":"bibsonomy","1400":"bibsonomy","1401":"bibsonomy","1402":"bibsonomy","1403":"bibsonomy","1404":"bibsonomy","1405":"bibsonomy","1406":"bibsonomy","1407":"bibsonomy","1408":"bibsonomy","1409":"bibsonomy","1410":"bibsonomy","1411":"bibsonomy","1412":"bibsonomy","1413":"bibsonomy","1414":"bibsonomy","1415":"bibsonomy","1416":"bibsonomy","1417":"bibsonomy","1418":"bibsonomy","1419":"bibsonomy","1420":"bibsonomy","1421":"bibsonomy","1422":"bibsonomy","1423":"bibsonomy","1424":"bibsonomy","1425":"bibsonomy","1426":"bibsonomy","1427":"bibsonomy","1428":"bibsonomy","1429":"bibsonomy","1430":"bibsonomy","1431":"bibsonomy","1432":"bibsonomy","1433":"bibsonomy","1434":"bibsonomy","1435":"bibsonomy","1436":"bibsonomy","1437":"bibsonomy","1438":"bibsonomy","1439":"bibsonomy","1440":"bibsonomy","1441":"bibsonomy","1442":"bibsonomy","1443":"bibsonomy","1444":"bibsonomy","1445":"bibsonomy","1446":"bibsonomy","1447":"bibsonomy","1448":"bibsonomy","1449":"bibsonomy","1450":"bibsonomy","1451":"bibsonomy","1452":"bibsonomy","1453":"bibsonomy","1454":"bibsonomy","1455":"bibsonomy","1456":"bibsonomy","1457":"bibsonomy","1458":"bibsonomy","1459":"bibsonomy","1460":"bibsonomy","1461":"bibsonomy","1462":"bibsonomy","1463":"bibsonomy","1464":"bibsonomy","1465":"bibsonomy","1466":"bibsonomy","1467":"bibsonomy","1468":"bibsonomy","1469":"bibsonomy","1470":"bibsonomy","1471":"bibsonomy","1472":"bibsonomy","1473":"bibsonomy","1474":"bibsonomy","1475":"bibsonomy","1476":"bibsonomy","1477":"bibsonomy","1478":"bibsonomy","1479":"bibsonomy","1480":"bibsonomy","1481":"bibsonomy","1482":"bibsonomy","1483":"bibsonomy","1484":"bibsonomy","1485":"bibsonomy","1486":"bibsonomy","1487":"bibsonomy","1488":"bibsonomy","1489":"bibsonomy","1490":"bibsonomy","1491":"bibsonomy","1492":"bibsonomy","1493":"bibsonomy","1494":"bibsonomy","1495":"bibsonomy","1496":"bibsonomy","1497":"bibsonomy","1498":"bibsonomy","1499":"bibsonomy","1500":"bibsonomy","1501":"bibsonomy","1502":"bibsonomy","1503":"bibsonomy","1504":"bibsonomy","1505":"bibsonomy","1506":"bibsonomy","1507":"bibsonomy","1508":"bibsonomy","1509":"bibsonomy","1510":"bibsonomy","1511":"bibsonomy","1512":"bibsonomy","1513":"bibsonomy","1514":"bibsonomy","1515":"bibsonomy","1516":"bibsonomy","1517":"bibsonomy","1518":"bibsonomy","1519":"bibsonomy","1520":"bibsonomy","1521":"bibsonomy","1522":"bibsonomy","1523":"bibsonomy","1524":"bibsonomy","1525":"bibsonomy","1526":"bibsonomy","1527":"bibsonomy","1528":"bibsonomy","1529":"bibsonomy","1530":"bibsonomy","1531":"bibsonomy","1532":"bibsonomy","1533":"bibsonomy","1534":"bibsonomy","1535":"bibsonomy","1536":"bibsonomy","1537":"bibsonomy","1538":"bibsonomy","1539":"bibsonomy","1540":"bibsonomy","1541":"bibsonomy","1542":"bibsonomy","1543":"bibsonomy","1544":"bibsonomy","1545":"bibsonomy","1546":"bibsonomy","1547":"bibsonomy","1548":"bibsonomy","1549":"bibsonomy","1550":"bibsonomy","1551":"bibsonomy","1552":"bibsonomy","1553":"bibsonomy","1554":"bibsonomy","1555":"bibsonomy","1556":"bibsonomy","1557":"bibsonomy","1558":"bibsonomy","1559":"bibsonomy","1560":"bibsonomy","1561":"bibsonomy","1562":"bibsonomy","1563":"bibsonomy","1564":"bibsonomy","1565":"bibsonomy","1566":"bibsonomy","1567":"bibsonomy","1568":"bibsonomy","1569":"bibsonomy","1570":"bibsonomy","1571":"bibsonomy","1572":"bibsonomy","1573":"bibsonomy","1574":"bibsonomy","1575":"bibsonomy","1576":"bibsonomy","1577":"bibsonomy","1578":"bibsonomy","1579":"bibsonomy","1580":"bibsonomy","1581":"bibsonomy","1582":"bibsonomy","1583":"bibsonomy","1584":"bibsonomy","1585":"bibsonomy","1586":"bibsonomy","1587":"bibsonomy","1588":"bibsonomy","1589":"bibsonomy","1590":"bibsonomy","1591":"bibsonomy","1592":"bibsonomy","1593":"bibsonomy","1594":"bibsonomy","1595":"bibsonomy","1596":"bibsonomy","1597":"bibsonomy","1598":"bibsonomy","1599":"bibsonomy","1600":"bibsonomy","1601":"bibsonomy","1602":"bibsonomy","1603":"bibsonomy","1604":"bibsonomy","1605":"bibsonomy","1606":"bibsonomy","1607":"bibsonomy","1608":"bibsonomy","1609":"bibsonomy","1610":"bibsonomy","1611":"bibsonomy","1612":"bibsonomy","1613":"bibsonomy","1614":"bibsonomy","1615":"bibsonomy","1616":"bibsonomy","1617":"bibsonomy","1618":"bibsonomy","1619":"bibsonomy","1620":"bibsonomy","1621":"bibsonomy","1622":"bibsonomy","1623":"bibsonomy","1624":"bibsonomy","1625":"bibsonomy","1626":"bibsonomy","1627":"bibsonomy","1628":"bibsonomy","1629":"bibsonomy","1630":"bibsonomy","1631":"bibsonomy","1632":"bibsonomy","1633":"bibsonomy","1634":"bibsonomy","1635":"bibsonomy","1636":"bibsonomy","1637":"bibsonomy","1638":"bibsonomy","1639":"bibsonomy","1640":"bibsonomy","1641":"bibsonomy","1642":"bibsonomy","1643":"bibsonomy","1644":"bibsonomy","1645":"bibsonomy","1646":"bibsonomy","1647":"bibsonomy","1648":"bibsonomy","1649":"bibsonomy","1650":"bibsonomy","1651":"bibsonomy","1652":"bibsonomy","1653":"bibsonomy","1654":"bibsonomy","1655":"bibsonomy","1656":"bibsonomy","1657":"bibsonomy","1658":"bibsonomy","1659":"bibsonomy","1660":"bibsonomy","1661":"bibsonomy","1662":"bibsonomy","1663":"bibsonomy","1664":"bibsonomy","1665":"bibsonomy","1666":"bibsonomy","1667":"bibsonomy","1668":"bibsonomy","1669":"bibsonomy","1670":"bibsonomy","1671":"bibsonomy","1672":"bibsonomy","1673":"bibsonomy","1674":"bibsonomy","1675":"bibsonomy","1676":"bibsonomy","1677":"bibsonomy","1678":"bibsonomy","1679":"bibsonomy","1680":"bibsonomy","1681":"bibsonomy","1682":"bibsonomy","1683":"bibsonomy","1684":"bibsonomy","1685":"bibsonomy","1686":"bibsonomy","1687":"bibsonomy","1688":"bibsonomy","1689":"bibsonomy","1690":"bibsonomy","1691":"bibsonomy","1692":"bibsonomy","1693":"bibsonomy","1694":"bibsonomy","1695":"bibsonomy","1696":"bibsonomy","1697":"bibsonomy","1698":"bibsonomy","1699":"bibsonomy","1700":"bibsonomy","1701":"bibsonomy","1702":"bibsonomy","1703":"bibsonomy","1704":"bibsonomy","1705":"bibsonomy","1706":"bibsonomy","1707":"bibsonomy","1708":"bibsonomy","1709":"bibsonomy","1710":"bibsonomy","1711":"bibsonomy","1712":"bibsonomy","1713":"bibsonomy","1714":"bibsonomy","1715":"bibsonomy","1716":"bibsonomy","1717":"bibsonomy","1718":"bibsonomy","1719":"bibsonomy","1720":"bibsonomy","1721":"bibsonomy","1722":"bibsonomy","1723":"bibsonomy","1724":"bibsonomy","1725":"bibsonomy","1726":"bibsonomy","1727":"bibsonomy","1728":"bibsonomy","1729":"bibsonomy","1730":"bibsonomy","1731":"bibsonomy","1732":"bibsonomy","1733":"bibsonomy","1734":"bibsonomy","1735":"bibsonomy","1736":"bibsonomy","1737":"bibsonomy","1738":"bibsonomy","1739":"bibsonomy","1740":"bibsonomy","1741":"bibsonomy","1742":"bibsonomy","1743":"bibsonomy","1744":"bibsonomy","1745":"bibsonomy","1746":"bibsonomy","1747":"bibsonomy","1748":"bibsonomy","1749":"bibsonomy","1750":"bibsonomy","1751":"bibsonomy","1752":"bibsonomy","1753":"bibsonomy","1754":"bibsonomy","1755":"bibsonomy","1756":"bibsonomy","1757":"bibsonomy","1758":"bibsonomy","1759":"bibsonomy","1760":"bibsonomy","1761":"bibsonomy","1762":"bibsonomy","1763":"bibsonomy","1764":"bibsonomy","1765":"bibsonomy","1766":"bibsonomy","1767":"bibsonomy","1768":"bibsonomy","1769":"bibsonomy","1770":"bibsonomy","1771":"bibsonomy","1772":"bibsonomy","1773":"bibsonomy","1774":"bibsonomy","1775":"bibsonomy","1776":"bibsonomy","1777":"bibsonomy","1778":"bibsonomy","1779":"bibsonomy","1780":"bibsonomy","1781":"bibsonomy","1782":"bibsonomy","1783":"bibsonomy","1784":"bibsonomy","1785":"bibsonomy","1786":"bibsonomy","1787":"bibsonomy","1788":"bibsonomy","1789":"bibsonomy","1790":"bibsonomy","1791":"bibsonomy","1792":"bibsonomy","1793":"bibsonomy","1794":"bibsonomy","1795":"bibsonomy","1796":"bibsonomy","1797":"bibsonomy","1798":"bibsonomy","1799":"bibsonomy","1800":"bibsonomy","1801":"bibsonomy","1802":"bibsonomy","1803":"bibsonomy","1804":"bibsonomy","1805":"bibsonomy","1806":"bibsonomy","1807":"bibsonomy","1808":"bibsonomy","1809":"bibsonomy","1810":"bibsonomy","1811":"bibsonomy","1812":"bibsonomy","1813":"bibsonomy","1814":"bibsonomy","1815":"bibsonomy","1816":"bibsonomy","1817":"bibsonomy","1818":"bibsonomy","1819":"bibsonomy","1820":"bibsonomy","1821":"bibsonomy","1822":"bibsonomy","1823":"bibsonomy","1824":"bibsonomy","1825":"bibsonomy","1826":"bibsonomy","1827":"bibsonomy","1828":"bibsonomy","1829":"bibsonomy","1830":"bibsonomy","1831":"bibsonomy","1832":"bibsonomy","1833":"bibsonomy","1834":"bibsonomy","1835":"bibsonomy","1836":"bibsonomy","1837":"bibsonomy","1838":"bibsonomy","1839":"bibsonomy","1840":"bibsonomy","1841":"bibsonomy","1842":"bibsonomy","1843":"bibsonomy","1844":"bibsonomy","1845":"bibsonomy","1846":"bibsonomy","1847":"bibsonomy","1848":"bibsonomy","1849":"bibsonomy","1850":"bibsonomy","1851":"bibsonomy","1852":"bibsonomy","1853":"bibsonomy","1854":"bibsonomy","1855":"bibsonomy","1856":"bibsonomy","1857":"bibsonomy","1858":"bibsonomy","1859":"bibsonomy","1860":"bibsonomy","1861":"bibsonomy","1862":"bibsonomy","1863":"bibsonomy","1864":"bibsonomy","1865":"bibsonomy","1866":"bibsonomy","1867":"bibsonomy","1868":"bibsonomy","1869":"bibsonomy","1870":"bibsonomy","1871":"bibsonomy","1872":"bibsonomy","1873":"bibsonomy","1874":"bibsonomy","1875":"bibsonomy","1876":"bibsonomy","1877":"bibsonomy","1878":"bibsonomy","1879":"bibsonomy","1880":"bibsonomy","1881":"bibsonomy","1882":"bibsonomy","1883":"bibsonomy","1884":"bibsonomy","1885":"bibsonomy","1886":"bibsonomy","1887":"bibsonomy","1888":"bibsonomy","1889":"bibsonomy","1890":"bibsonomy","1891":"bibsonomy","1892":"bibsonomy","1893":"bibsonomy","1894":"bibsonomy","1895":"bibsonomy","1896":"bibsonomy","1897":"bibsonomy","1898":"bibsonomy","1899":"bibsonomy","1900":"bibsonomy","1901":"bibsonomy","1902":"bibsonomy","1903":"bibsonomy","1904":"bibsonomy","1905":"bibsonomy","1906":"bibsonomy","1907":"bibsonomy","1908":"bibsonomy","1909":"bibsonomy","1910":"bibsonomy","1911":"bibsonomy","1912":"bibsonomy","1913":"bibsonomy","1914":"bibsonomy","1915":"bibsonomy","1916":"bibsonomy","1917":"bibsonomy","1918":"bibsonomy","1919":"bibsonomy","1920":"bibsonomy","1921":"bibsonomy","1922":"bibsonomy","1923":"bibsonomy","1924":"bibsonomy","1925":"bibsonomy","1926":"bibsonomy","1927":"bibsonomy","1928":"bibsonomy","1929":"bibsonomy","1930":"bibsonomy","1931":"bibsonomy","1932":"bibsonomy","1933":"bibsonomy","1934":"bibsonomy","1935":"bibsonomy","1936":"bibsonomy","1937":"bibsonomy","1938":"bibsonomy","1939":"bibsonomy","1940":"bibsonomy","1941":"bibsonomy","1942":"bibsonomy","1943":"bibsonomy","1944":"bibsonomy","1945":"bibsonomy","1946":"bibsonomy","1947":"bibsonomy","1948":"bibsonomy","1949":"bibsonomy","1950":"bibsonomy","1951":"bibsonomy","1952":"bibsonomy","1953":"bibsonomy","1954":"bibsonomy","1955":"bibsonomy","1956":"bibsonomy","1957":"bibsonomy","1958":"bibsonomy","1959":"bibsonomy","1960":"bibsonomy","1961":"bibsonomy","1962":"bibsonomy","1963":"bibsonomy","1964":"bibsonomy","1965":"bibsonomy","1966":"bibsonomy","1967":"bibsonomy","1968":"bibsonomy","1969":"bibsonomy","1970":"bibsonomy","1971":"bibsonomy","1972":"bibsonomy","1973":"bibsonomy","1974":"bibsonomy","1975":"bibsonomy","1976":"bibsonomy","1977":"bibsonomy","1978":"bibsonomy","1979":"bibsonomy","1980":"bibsonomy","1981":"bibsonomy","1982":"bibsonomy","1983":"bibsonomy","1984":"bibsonomy","1985":"bibsonomy","1986":"bibsonomy","1987":"bibsonomy","1988":"bibsonomy","1989":"bibsonomy","1990":"bibsonomy","1991":"bibsonomy","1992":"bibsonomy","1993":"bibsonomy","1994":"bibsonomy","1995":"bibsonomy","1996":"bibsonomy","1997":"bibsonomy","1998":"bibsonomy","1999":"bibsonomy","2000":"bibsonomy","2001":"bibsonomy","2002":"bibsonomy","2003":"bibsonomy","2004":"bibsonomy","2005":"bibsonomy","2006":"bibsonomy","2007":"bibsonomy","2008":"bibsonomy","2009":"bibsonomy","2010":"bibsonomy","2011":"bibsonomy","2012":"bibsonomy","2013":"bibsonomy","2014":"bibsonomy","2015":"bibsonomy","2016":"bibsonomy","2017":"bibsonomy","2018":"bibsonomy","2019":"bibsonomy","2020":"bibsonomy","2021":"bibsonomy","2022":"bibsonomy","2023":"bibsonomy","2024":"bibsonomy","2025":"bibsonomy","2026":"bibsonomy","2027":"bibsonomy","2028":"bibsonomy","2029":"bibsonomy","2030":"bibsonomy","2031":"bibsonomy","2032":"bibsonomy","2033":"bibsonomy","2034":"bibsonomy","2035":"bibsonomy","2036":"bibsonomy","2037":"bibsonomy","2038":"bibsonomy","2039":"bibsonomy","2040":"bibsonomy","2041":"bibsonomy","2042":"bibsonomy","2043":"bibsonomy","2044":"bibsonomy","2045":"bibsonomy","2046":"bibsonomy","2047":"bibsonomy","2048":"bibsonomy","2049":"bibsonomy","2050":"bibsonomy","2051":"bibsonomy","2052":"bibsonomy","2053":"bibsonomy","2054":"bibsonomy","2055":"bibsonomy","2056":"bibsonomy","2057":"bibsonomy","2058":"bibsonomy","2059":"bibsonomy","2060":"bibsonomy","2061":"bibsonomy","2062":"bibsonomy","2063":"bibsonomy","2064":"bibsonomy","2065":"bibsonomy","2066":"bibsonomy","2067":"bibsonomy","2068":"bibsonomy","2069":"bibsonomy","2070":"bibsonomy","2071":"bibsonomy","2072":"bibsonomy","2073":"bibsonomy","2074":"bibsonomy","2075":"bibsonomy","2076":"bibsonomy","2077":"bibsonomy","2078":"bibsonomy","2079":"bibsonomy","2080":"bibsonomy","2081":"bibsonomy","2082":"bibsonomy","2083":"bibsonomy","2084":"bibsonomy","2085":"bibsonomy","2086":"bibsonomy","2087":"bibsonomy","2088":"bibsonomy","2089":"bibsonomy","2090":"bibsonomy","2091":"bibsonomy","2092":"bibsonomy","2093":"bibsonomy","2094":"bibsonomy","2095":"bibsonomy","2096":"bibsonomy","2097":"bibsonomy","2098":"bibsonomy","2099":"bibsonomy","2100":"bibsonomy","2101":"bibsonomy","2102":"bibsonomy","2103":"bibsonomy","2104":"bibsonomy","2105":"bibsonomy","2106":"bibsonomy","2107":"bibsonomy","2108":"bibsonomy","2109":"bibsonomy","2110":"bibsonomy","2111":"bibsonomy","2112":"bibsonomy","2113":"bibsonomy","2114":"bibsonomy","2115":"bibsonomy","2116":"bibsonomy","2117":"bibsonomy","2118":"bibsonomy","2119":"bibsonomy","2120":"bibsonomy","2121":"bibsonomy","2122":"bibsonomy","2123":"bibsonomy","2124":"bibsonomy","2125":"bibsonomy","2126":"bibsonomy","2127":"bibsonomy","2128":"bibsonomy","2129":"bibsonomy","2130":"bibsonomy","2131":"bibsonomy","2132":"bibsonomy","2133":"bibsonomy","2134":"bibsonomy","2135":"bibsonomy","2136":"bibsonomy","2137":"bibsonomy","2138":"bibsonomy","2139":"bibsonomy","2140":"bibsonomy","2141":"bibsonomy","2142":"bibsonomy","2143":"bibsonomy","2144":"bibsonomy","2145":"bibsonomy","2146":"bibsonomy","2147":"bibsonomy","2148":"bibsonomy","2149":"bibsonomy","2150":"bibsonomy","2151":"bibsonomy","2152":"bibsonomy","2153":"bibsonomy","2154":"bibsonomy","2155":"bibsonomy","2156":"bibsonomy","2157":"bibsonomy","2158":"bibsonomy","2159":"bibsonomy","2160":"bibsonomy","2161":"bibsonomy","2162":"bibsonomy","2163":"bibsonomy","2164":"bibsonomy","2165":"bibsonomy","2166":"bibsonomy","2167":"bibsonomy","2168":"bibsonomy","2169":"bibsonomy","2170":"bibsonomy","2171":"bibsonomy","2172":"bibsonomy","2173":"bibsonomy","2174":"bibsonomy","2175":"bibsonomy","2176":"bibsonomy","2177":"bibsonomy","2178":"bibsonomy","2179":"bibsonomy","2180":"bibsonomy","2181":"bibsonomy","2182":"bibsonomy","2183":"bibsonomy","2184":"bibsonomy","2185":"bibsonomy","2186":"bibsonomy","2187":"bibsonomy","2188":"bibsonomy","2189":"bibsonomy","2190":"bibsonomy","2191":"bibsonomy","2192":"bibsonomy","2193":"bibsonomy","2194":"bibsonomy","2195":"bibsonomy","2196":"bibsonomy","2197":"bibsonomy","2198":"bibsonomy","2199":"bibsonomy","2200":"bibsonomy","2201":"bibsonomy","2202":"bibsonomy","2203":"bibsonomy","2204":"bibsonomy","2205":"bibsonomy","2206":"bibsonomy","2207":"bibsonomy","2208":"bibsonomy","2209":"bibsonomy","2210":"bibsonomy","2211":"bibsonomy","2212":"bibsonomy","2213":"bibsonomy","2214":"bibsonomy","2215":"bibsonomy","2216":"bibsonomy","2217":"bibsonomy","2218":"bibsonomy","2219":"bibsonomy","2220":"bibsonomy","2221":"bibsonomy","2222":"bibsonomy","2223":"bibsonomy","2224":"bibsonomy","2225":"bibsonomy","2226":"bibsonomy","2227":"bibsonomy","2228":"bibsonomy","2229":"bibsonomy","2230":"bibsonomy","2231":"bibsonomy","2232":"bibsonomy","2233":"bibsonomy","2234":"bibsonomy","2235":"bibsonomy","2236":"bibsonomy","2237":"bibsonomy","2238":"bibsonomy","2239":"bibsonomy","2240":"bibsonomy","2241":"bibsonomy","2242":"bibsonomy","2243":"bibsonomy","2244":"bibsonomy","2245":"bibsonomy","2246":"bibsonomy","2247":"bibsonomy","2248":"bibsonomy","2249":"bibsonomy","2250":"bibsonomy","2251":"bibsonomy","2252":"bibsonomy","2253":"bibsonomy","2254":"bibsonomy","2255":"bibsonomy","2256":"bibsonomy","2257":"bibsonomy","2258":"bibsonomy","2259":"bibsonomy","2260":"bibsonomy","2261":"bibsonomy","2262":"bibsonomy","2263":"bibsonomy","2264":"bibsonomy","2265":"bibsonomy","2266":"bibsonomy","2267":"bibsonomy","2268":"bibsonomy","2269":"bibsonomy","2270":"bibsonomy","2271":"bibsonomy","2272":"bibsonomy","2273":"bibsonomy","2274":"bibsonomy","2275":"bibsonomy","2276":"bibsonomy","2277":"bibsonomy","2278":"bibsonomy","2279":"bibsonomy","2280":"bibsonomy","2281":"bibsonomy","2282":"bibsonomy","2283":"bibsonomy","2284":"bibsonomy","2285":"bibsonomy","2286":"bibsonomy","2287":"bibsonomy","2288":"bibsonomy","2289":"bibsonomy","2290":"bibsonomy","2291":"bibsonomy","2292":"bibsonomy","2293":"bibsonomy","2294":"bibsonomy","2295":"bibsonomy","2296":"bibsonomy","2297":"bibsonomy","2298":"bibsonomy","2299":"bibsonomy","2300":"bibsonomy","2301":"bibsonomy","2302":"bibsonomy","2303":"bibsonomy","2304":"bibsonomy","2305":"bibsonomy","2306":"bibsonomy","2307":"bibsonomy","2308":"bibsonomy","2309":"bibsonomy","2310":"bibsonomy","2311":"bibsonomy","2312":"bibsonomy","2313":"bibsonomy","2314":"bibsonomy","2315":"bibsonomy","2316":"bibsonomy","2317":"bibsonomy","2318":"bibsonomy","2319":"bibsonomy","2320":"bibsonomy","2321":"bibsonomy","2322":"bibsonomy","2323":"bibsonomy","2324":"bibsonomy","2325":"bibsonomy","2326":"bibsonomy","2327":"bibsonomy","2328":"bibsonomy","2329":"bibsonomy","2330":"bibsonomy","2331":"bibsonomy","2332":"bibsonomy","2333":"bibsonomy","2334":"bibsonomy","2335":"bibsonomy","2336":"bibsonomy","2337":"bibsonomy","2338":"bibsonomy","2339":"bibsonomy","2340":"bibsonomy","2341":"bibsonomy","2342":"bibsonomy","2343":"bibsonomy","2344":"bibsonomy","2345":"bibsonomy","2346":"bibsonomy","2347":"bibsonomy","2348":"bibsonomy","2349":"bibsonomy","2350":"bibsonomy","2351":"bibsonomy","2352":"bibsonomy","2353":"bibsonomy","2354":"bibsonomy","2355":"bibsonomy","2356":"bibsonomy","2357":"bibsonomy","2358":"bibsonomy","2359":"bibsonomy","2360":"bibsonomy","2361":"bibsonomy","2362":"bibsonomy","2363":"bibsonomy","2364":"bibsonomy","2365":"bibsonomy","2366":"bibsonomy","2367":"bibsonomy","2368":"bibsonomy","2369":"bibsonomy","2370":"bibsonomy","2371":"bibsonomy","2372":"bibsonomy","2373":"bibsonomy","2374":"bibsonomy","2375":"bibsonomy","2376":"bibsonomy","2377":"bibsonomy","2378":"bibsonomy","2379":"bibsonomy","2380":"bibsonomy","2381":"bibsonomy","2382":"bibsonomy","2383":"bibsonomy","2384":"bibsonomy","2385":"bibsonomy","2386":"bibsonomy","2387":"bibsonomy","2388":"bibsonomy","2389":"bibsonomy","2390":"bibsonomy","2391":"bibsonomy","2392":"bibsonomy","2393":"bibsonomy","2394":"bibsonomy","2395":"bibsonomy","2396":"bibsonomy","2397":"bibsonomy","2398":"bibsonomy","2399":"bibsonomy","2400":"bibsonomy","2401":"bibsonomy","2402":"bibsonomy","2403":"bibsonomy","2404":"bibsonomy","2405":"bibsonomy","2406":"bibsonomy","2407":"bibsonomy","2408":"bibsonomy","2409":"bibsonomy","2410":"bibsonomy","2411":"bibsonomy","2412":"bibsonomy","2413":"bibsonomy","2414":"bibsonomy","2415":"bibsonomy","2416":"bibsonomy","2417":"bibsonomy","2418":"bibsonomy","2419":"bibsonomy","2420":"bibsonomy","2421":"bibsonomy","2422":"bibsonomy","2423":"bibsonomy","2424":"bibsonomy","2425":"bibsonomy","2426":"bibsonomy","2427":"bibsonomy","2428":"bibsonomy","2429":"bibsonomy","2430":"bibsonomy","2431":"bibsonomy","2432":"bibsonomy","2433":"bibsonomy","2434":"bibsonomy","2435":"bibsonomy","2436":"bibsonomy","2437":"bibsonomy","2438":"bibsonomy","2439":"bibsonomy","2440":"bibsonomy","2441":"bibsonomy","2442":"bibsonomy","2443":"bibsonomy","2444":"bibsonomy","2445":"bibsonomy","2446":"bibsonomy","2447":"bibsonomy","2448":"bibsonomy","2449":"bibsonomy","2450":"bibsonomy","2451":"bibsonomy","2452":"bibsonomy","2453":"bibsonomy","2454":"bibsonomy","2455":"bibsonomy","2456":"bibsonomy","2457":"bibsonomy","2458":"bibsonomy","2459":"bibsonomy","2460":"bibsonomy","2461":"bibsonomy","2462":"bibsonomy","2463":"bibsonomy","2464":"bibsonomy","2465":"bibsonomy","2466":"bibsonomy","2467":"bibsonomy","2468":"bibsonomy","2469":"bibsonomy","2470":"bibsonomy","2471":"bibsonomy","2472":"bibsonomy","2473":"bibsonomy","2474":"bibsonomy","2475":"bibsonomy","2476":"bibsonomy","2477":"bibsonomy","2478":"bibsonomy","2479":"bibsonomy","2480":"bibsonomy","2481":"bibsonomy","2482":"bibsonomy","2483":"bibsonomy","2484":"bibsonomy","2485":"bibsonomy","2486":"bibsonomy","2487":"bibsonomy","2488":"bibsonomy","2489":"bibsonomy","2490":"bibsonomy","2491":"bibsonomy","2492":"bibsonomy","2493":"bibsonomy","2494":"bibsonomy","2495":"bibsonomy","2496":"bibsonomy","2497":"bibsonomy","2498":"bibsonomy","2499":"bibsonomy","2500":"bibsonomy","2501":"bibsonomy","2502":"bibsonomy","2503":"bibsonomy","2504":"bibsonomy","2505":"bibsonomy","2506":"bibsonomy","2507":"bibsonomy","2508":"bibsonomy","2509":"bibsonomy","2510":"bibsonomy","2511":"bibsonomy","2512":"bibsonomy","2513":"bibsonomy","2514":"bibsonomy","2515":"bibsonomy","2516":"bibsonomy","2517":"bibsonomy","2518":"bibsonomy","2519":"bibsonomy","2520":"bibsonomy","2521":"bibsonomy","2522":"bibsonomy","2523":"bibsonomy","2524":"bibsonomy","2525":"bibsonomy","2526":"bibsonomy","2527":"bibsonomy","2528":"bibsonomy","2529":"bibsonomy","2530":"bibsonomy","2531":"bibsonomy","2532":"bibsonomy","2533":"bibsonomy","2534":"bibsonomy","2535":"bibsonomy","2536":"bibsonomy","2537":"bibsonomy","2538":"bibsonomy","2539":"bibsonomy","2540":"bibsonomy","2541":"bibsonomy","2542":"bibsonomy","2543":"bibsonomy","2544":"bibsonomy","2545":"bibsonomy","2546":"bibsonomy","2547":"bibsonomy","2548":"bibsonomy","2549":"bibsonomy","2550":"bibsonomy","2551":"bibsonomy","2552":"bibsonomy","2553":"bibsonomy","2554":"bibsonomy","2555":"bibsonomy","2556":"bibsonomy","2557":"bibsonomy","2558":"bibsonomy","2559":"bibsonomy","2560":"bibsonomy","2561":"bibsonomy","2562":"bibsonomy","2563":"bibsonomy","2564":"bibsonomy","2565":"bibsonomy","2566":"bibsonomy","2567":"bibsonomy","2568":"bibsonomy","2569":"bibsonomy","2570":"bibsonomy","2571":"bibsonomy","2572":"bibsonomy","2573":"bibsonomy","2574":"bibsonomy","2575":"bibsonomy","2576":"bibsonomy","2577":"bibsonomy","2578":"bibsonomy","2579":"bibsonomy","2580":"bibsonomy","2581":"bibsonomy","2582":"bibsonomy","2583":"bibsonomy","2584":"bibsonomy","2585":"bibsonomy","2586":"bibsonomy","2587":"bibsonomy","2588":"bibsonomy","2589":"bibsonomy","2590":"bibsonomy","2591":"bibsonomy","2592":"bibsonomy","2593":"bibsonomy","2594":"bibsonomy","2595":"bibsonomy","2596":"bibsonomy","2597":"bibsonomy","2598":"bibsonomy","2599":"bibsonomy","2600":"bibsonomy","2601":"bibsonomy","2602":"bibsonomy","2603":"bibsonomy","2604":"bibsonomy","2605":"bibsonomy","2606":"bibsonomy","2607":"bibsonomy","2608":"bibsonomy","2609":"bibsonomy","2610":"bibsonomy","2611":"bibsonomy","2612":"bibsonomy","2613":"bibsonomy","2614":"bibsonomy","2615":"bibsonomy","2616":"bibsonomy","2617":"bibsonomy","2618":"bibsonomy","2619":"bibsonomy","2620":"bibsonomy","2621":"bibsonomy","2622":"bibsonomy","2623":"bibsonomy","2624":"bibsonomy","2625":"bibsonomy","2626":"bibsonomy","2627":"bibsonomy","2628":"bibsonomy","2629":"bibsonomy","2630":"bibsonomy","2631":"bibsonomy","2632":"bibsonomy","2633":"bibsonomy","2634":"bibsonomy","2635":"bibsonomy","2636":"bibsonomy","2637":"bibsonomy","2638":"bibsonomy","2639":"bibsonomy","2640":"bibsonomy","2641":"bibsonomy","2642":"bibsonomy","2643":"bibsonomy","2644":"bibsonomy","2645":"bibsonomy","2646":"bibsonomy","2647":"bibsonomy","2648":"bibsonomy","2649":"bibsonomy","2650":"bibsonomy","2651":"bibsonomy","2652":"bibsonomy","2653":"bibsonomy","2654":"bibsonomy","2655":"bibsonomy","2656":"bibsonomy","2657":"bibsonomy","2658":"bibsonomy","2659":"bibsonomy","2660":"bibsonomy","2661":"bibsonomy","2662":"bibsonomy","2663":"bibsonomy","2664":"bibsonomy","2665":"bibsonomy","2666":"bibsonomy","2667":"bibsonomy","2668":"bibsonomy","2669":"bibsonomy","2670":"bibsonomy","2671":"bibsonomy","2672":"bibsonomy","2673":"bibsonomy","2674":"bibsonomy","2675":"bibsonomy","2676":"bibsonomy","2677":"bibsonomy","2678":"bibsonomy","2679":"bibsonomy","2680":"bibsonomy","2681":"bibsonomy","2682":"bibsonomy","2683":"bibsonomy","2684":"bibsonomy","2685":"bibsonomy","2686":"bibsonomy","2687":"bibsonomy","2688":"bibsonomy","2689":"bibsonomy","2690":"bibsonomy","2691":"bibsonomy","2692":"bibsonomy","2693":"bibsonomy","2694":"bibsonomy","2695":"bibsonomy","2696":"bibsonomy","2697":"bibsonomy","2698":"bibsonomy","2699":"bibsonomy","2700":"bibsonomy","2701":"bibsonomy","2702":"bibsonomy","2703":"bibsonomy","2704":"bibsonomy","2705":"bibsonomy","2706":"bibsonomy","2707":"bibsonomy","2708":"bibsonomy","2709":"bibsonomy","2710":"bibsonomy","2711":"bibsonomy","2712":"bibsonomy","2713":"bibsonomy","2714":"bibsonomy","2715":"bibsonomy","2716":"bibsonomy","2717":"bibsonomy","2718":"bibsonomy","2719":"bibsonomy","2720":"bibsonomy","2721":"bibsonomy","2722":"bibsonomy","2723":"bibsonomy","2724":"bibsonomy","2725":"bibsonomy","2726":"bibsonomy","2727":"bibsonomy","2728":"bibsonomy","2729":"bibsonomy","2730":"bibsonomy","2731":"bibsonomy","2732":"bibsonomy","2733":"bibsonomy","2734":"bibsonomy","2735":"bibsonomy","2736":"bibsonomy","2737":"bibsonomy","2738":"bibsonomy","2739":"bibsonomy","2740":"bibsonomy","2741":"bibsonomy","2742":"bibsonomy","2743":"bibsonomy","2744":"bibsonomy","2745":"bibsonomy","2746":"bibsonomy","2747":"bibsonomy","2748":"bibsonomy","2749":"bibsonomy","2750":"bibsonomy","2751":"bibsonomy","2752":"bibsonomy","2753":"bibsonomy","2754":"bibsonomy","2755":"bibsonomy","2756":"bibsonomy","2757":"bibsonomy","2758":"bibsonomy","2759":"bibsonomy","2760":"bibsonomy","2761":"bibsonomy","2762":"bibsonomy","2763":"bibsonomy","2764":"bibsonomy","2765":"bibsonomy","2766":"bibsonomy","2767":"bibsonomy","2768":"bibsonomy","2769":"bibsonomy","2770":"bibsonomy","2771":"bibsonomy","2772":"bibsonomy","2773":"bibsonomy","2774":"bibsonomy","2775":"bibsonomy","2776":"bibsonomy","2777":"bibsonomy","2778":"bibsonomy","2779":"bibsonomy","2780":"bibsonomy","2781":"bibsonomy","2782":"bibsonomy","2783":"bibsonomy","2784":"bibsonomy","2785":"bibsonomy","2786":"bibsonomy","2787":"bibsonomy","2788":"bibsonomy","2789":"bibsonomy","2790":"bibsonomy","2791":"bibsonomy","2792":"bibsonomy","2793":"bibsonomy","2794":"bibsonomy","2795":"bibsonomy","2796":"bibsonomy","2797":"bibsonomy","2798":"bibsonomy","2799":"bibsonomy","2800":"bibsonomy","2801":"bibsonomy","2802":"bibsonomy","2803":"bibsonomy","2804":"bibsonomy","2805":"bibsonomy","2806":"bibsonomy","2807":"bibsonomy","2808":"bibsonomy","2809":"bibsonomy","2810":"bibsonomy","2811":"bibsonomy","2812":"bibsonomy","2813":"bibsonomy","2814":"bibsonomy","2815":"bibsonomy","2816":"bibsonomy","2817":"bibsonomy","2818":"bibsonomy","2819":"bibsonomy","2820":"bibsonomy","2821":"bibsonomy","2822":"bibsonomy","2823":"bibsonomy","2824":"bibsonomy","2825":"bibsonomy","2826":"bibsonomy","2827":"bibsonomy","2828":"bibsonomy","2829":"bibsonomy","2830":"bibsonomy","2831":"bibsonomy","2832":"bibsonomy","2833":"bibsonomy","2834":"bibsonomy","2835":"bibsonomy","2836":"bibsonomy","2837":"bibsonomy","2838":"bibsonomy","2839":"bibsonomy","2840":"bibsonomy","2841":"bibsonomy","2842":"bibsonomy","2843":"bibsonomy","2844":"bibsonomy","2845":"bibsonomy","2846":"bibsonomy","2847":"bibsonomy","2848":"bibsonomy","2849":"bibsonomy","2850":"bibsonomy","2851":"bibsonomy","2852":"bibsonomy","2853":"bibsonomy","2854":"bibsonomy","2855":"bibsonomy","2856":"bibsonomy","2857":"bibsonomy","2858":"bibsonomy","2859":"bibsonomy","2860":"bibsonomy","2861":"bibsonomy","2862":"bibsonomy","2863":"bibsonomy","2864":"bibsonomy","2865":"bibsonomy","2866":"bibsonomy","2867":"bibsonomy","2868":"bibsonomy","2869":"bibsonomy","2870":"bibsonomy","2871":"bibsonomy","2872":"bibsonomy","2873":"bibsonomy","2874":"bibsonomy","2875":"bibsonomy","2876":"bibsonomy","2877":"bibsonomy","2878":"bibsonomy","2879":"bibsonomy","2880":"bibsonomy","2881":"bibsonomy","2882":"bibsonomy","2883":"bibsonomy","2884":"bibsonomy","2885":"bibsonomy","2886":"bibsonomy","2887":"bibsonomy","2888":"bibsonomy","2889":"bibsonomy","2890":"bibsonomy","2891":"bibsonomy","2892":"bibsonomy","2893":"bibsonomy","2894":"bibsonomy","2895":"bibsonomy","2896":"bibsonomy","2897":"bibsonomy","2898":"bibsonomy","2899":"bibsonomy","2900":"bibsonomy","2901":"bibsonomy","2902":"bibsonomy","2903":"bibsonomy","2904":"bibsonomy","2905":"bibsonomy","2906":"bibsonomy","2907":"bibsonomy","2908":"bibsonomy","2909":"bibsonomy","2910":"bibsonomy","2911":"bibsonomy","2912":"bibsonomy","2913":"bibsonomy","2914":"bibsonomy","2915":"bibsonomy","2916":"bibsonomy","2917":"bibsonomy","2918":"bibsonomy","2919":"bibsonomy","2920":"bibsonomy","2921":"bibsonomy","2922":"bibsonomy","2923":"bibsonomy","2924":"bibsonomy","2925":"bibsonomy","2926":"bibsonomy","2927":"bibsonomy","2928":"bibsonomy","2929":"bibsonomy","2930":"bibsonomy","2931":"bibsonomy","2932":"bibsonomy","2933":"bibsonomy","2934":"bibsonomy","2935":"bibsonomy","2936":"bibsonomy","2937":"bibsonomy","2938":"bibsonomy","2939":"bibsonomy","2940":"bibsonomy","2941":"bibsonomy","2942":"bibsonomy","2943":"bibsonomy","2944":"bibsonomy","2945":"bibsonomy","2946":"bibsonomy","2947":"bibsonomy","2948":"bibsonomy","2949":"bibsonomy","2950":"bibsonomy","2951":"bibsonomy","2952":"bibsonomy","2953":"bibsonomy","2954":"bibsonomy","2955":"bibsonomy","2956":"bibsonomy","2957":"bibsonomy","2958":"bibsonomy","2959":"bibsonomy","2960":"bibsonomy","2961":"bibsonomy","2962":"bibsonomy","2963":"bibsonomy","2964":"bibsonomy","2965":"bibsonomy","2966":"bibsonomy","2967":"bibsonomy","2968":"bibsonomy","2969":"bibsonomy","2970":"bibsonomy","2971":"bibsonomy","2972":"bibsonomy","2973":"bibsonomy","2974":"bibsonomy","2975":"bibsonomy","2976":"bibsonomy","2977":"bibsonomy","2978":"bibsonomy","2979":"bibsonomy","2980":"bibsonomy","2981":"bibsonomy","2982":"bibsonomy","2983":"bibsonomy","2984":"bibsonomy","2985":"bibsonomy","2986":"bibsonomy","2987":"bibsonomy","2988":"bibsonomy","2989":"bibsonomy","2990":"bibsonomy","2991":"bibsonomy","2992":"bibsonomy","2993":"bibsonomy","2994":"bibsonomy","2995":"bibsonomy","2996":"bibsonomy","2997":"bibsonomy","2998":"bibsonomy","2999":"bibsonomy","3000":"bibsonomy","3001":"bibsonomy","3002":"bibsonomy","3003":"bibsonomy","3004":"bibsonomy","3005":"bibsonomy","3006":"bibsonomy","3007":"bibsonomy","3008":"bibsonomy","3009":"bibsonomy","3010":"bibsonomy","3011":"bibsonomy","3012":"bibsonomy","3013":"bibsonomy","3014":"bibsonomy","3015":"bibsonomy","3016":"bibsonomy","3017":"bibsonomy","3018":"bibsonomy","3019":"bibsonomy","3020":"bibsonomy","3021":"bibsonomy","3022":"bibsonomy","3023":"bibsonomy","3024":"bibsonomy","3025":"bibsonomy","3026":"bibsonomy","3027":"bibsonomy","3028":"bibsonomy","3029":"bibsonomy","3030":"bibsonomy","3031":"bibsonomy","3032":"bibsonomy","3033":"bibsonomy","3034":"bibsonomy","3035":"bibsonomy","3036":"bibsonomy","3037":"bibsonomy","3038":"bibsonomy","3039":"bibsonomy","3040":"bibsonomy","3041":"bibsonomy","3042":"bibsonomy","3043":"bibsonomy","3044":"bibsonomy","3045":"bibsonomy","3046":"bibsonomy","3047":"bibsonomy","3048":"bibsonomy","3049":"bibsonomy","3050":"bibsonomy","3051":"bibsonomy","3052":"bibsonomy","3053":"bibsonomy","3054":"bibsonomy","3055":"bibsonomy","3056":"bibsonomy","3057":"bibsonomy","3058":"bibsonomy","3059":"bibsonomy","3060":"bibsonomy","3061":"bibsonomy","3062":"bibsonomy","3063":"bibsonomy","3064":"bibsonomy","3065":"bibsonomy","3066":"bibsonomy","3067":"bibsonomy","3068":"bibsonomy","3069":"bibsonomy","3070":"bibsonomy","3071":"bibsonomy","3072":"bibsonomy","3073":"bibsonomy","3074":"bibsonomy","3075":"bibsonomy","3076":"bibsonomy","3077":"bibsonomy","3078":"bibsonomy","3079":"bibsonomy","3080":"bibsonomy","3081":"bibsonomy","3082":"bibsonomy","3083":"bibsonomy","3084":"bibsonomy","3085":"bibsonomy","3086":"bibsonomy","3087":"bibsonomy","3088":"bibsonomy","3089":"bibsonomy","3090":"bibsonomy","3091":"bibsonomy","3092":"bibsonomy","3093":"bibsonomy","3094":"bibsonomy","3095":"bibsonomy","3096":"bibsonomy","3097":"bibsonomy","3098":"bibsonomy","3099":"bibsonomy","3100":"bibsonomy","3101":"bibsonomy","3102":"bibsonomy","3103":"bibsonomy","3104":"bibsonomy","3105":"bibsonomy","3106":"bibsonomy","3107":"bibsonomy","3108":"bibsonomy","3109":"bibsonomy","3110":"bibsonomy","3111":"bibsonomy","3112":"bibsonomy","3113":"bibsonomy","3114":"bibsonomy","3115":"bibsonomy","3116":"bibsonomy","3117":"bibsonomy","3118":"bibsonomy","3119":"bibsonomy","3120":"bibsonomy","3121":"bibsonomy","3122":"bibsonomy","3123":"bibsonomy","3124":"bibsonomy","3125":"bibsonomy","3126":"bibsonomy","3127":"bibsonomy","3128":"bibsonomy","3129":"bibsonomy","3130":"bibsonomy","3131":"bibsonomy","3132":"bibsonomy","3133":"bibsonomy","3134":"bibsonomy","3135":"bibsonomy","3136":"bibsonomy","3137":"bibsonomy","3138":"bibsonomy","3139":"bibsonomy","3140":"bibsonomy","3141":"bibsonomy","3142":"bibsonomy","3143":"bibsonomy","3144":"bibsonomy","3145":"bibsonomy","3146":"bibsonomy","3147":"bibsonomy","3148":"bibsonomy","3149":"bibsonomy","3150":"bibsonomy","3151":"bibsonomy","3152":"bibsonomy","3153":"bibsonomy","3154":"bibsonomy","3155":"bibsonomy","3156":"bibsonomy","3157":"bibsonomy","3158":"bibsonomy","3159":"bibsonomy","3160":"bibsonomy","3161":"bibsonomy","3162":"bibsonomy","3163":"bibsonomy","3164":"bibsonomy","3165":"bibsonomy","3166":"bibsonomy","3167":"bibsonomy","3168":"bibsonomy","3169":"bibsonomy","3170":"bibsonomy","3171":"bibsonomy","3172":"bibsonomy","3173":"bibsonomy","3174":"bibsonomy","3175":"bibsonomy","3176":"bibsonomy","3177":"bibsonomy","3178":"bibsonomy","3179":"bibsonomy","3180":"bibsonomy","3181":"bibsonomy","3182":"bibsonomy","3183":"bibsonomy","3184":"bibsonomy","3185":"bibsonomy","3186":"bibsonomy","3187":"bibsonomy","3188":"bibsonomy","3189":"bibsonomy","3190":"bibsonomy","3191":"bibsonomy","3192":"bibsonomy","3193":"bibsonomy","3194":"bibsonomy","3195":"bibsonomy","3196":"bibsonomy","3197":"bibsonomy","3198":"bibsonomy","3199":"bibsonomy","3200":"bibsonomy","3201":"bibsonomy","3202":"bibsonomy","3203":"bibsonomy","3204":"bibsonomy","3205":"bibsonomy","3206":"bibsonomy","3207":"bibsonomy","3208":"bibsonomy","3209":"bibsonomy","3210":"bibsonomy","3211":"bibsonomy","3212":"bibsonomy","3213":"bibsonomy","3214":"bibsonomy","3215":"bibsonomy","3216":"bibsonomy","3217":"bibsonomy","3218":"bibsonomy","3219":"bibsonomy","3220":"bibsonomy","3221":"bibsonomy","3222":"bibsonomy","3223":"bibsonomy","3224":"bibsonomy","3225":"bibsonomy","3226":"bibsonomy","3227":"bibsonomy","3228":"bibsonomy","3229":"bibsonomy","3230":"bibsonomy","3231":"bibsonomy","3232":"bibsonomy","3233":"bibsonomy","3234":"bibsonomy","3235":"bibsonomy","3236":"bibsonomy","3237":"bibsonomy","3238":"bibsonomy","3239":"bibsonomy","3240":"bibsonomy","3241":"bibsonomy","3242":"bibsonomy","3243":"bibsonomy","3244":"bibsonomy","3245":"bibsonomy","3246":"bibsonomy","3247":"bibsonomy","3248":"bibsonomy","3249":"bibsonomy","3250":"bibsonomy","3251":"bibsonomy","3252":"bibsonomy","3253":"bibsonomy","3254":"bibsonomy","3255":"bibsonomy","3256":"bibsonomy","3257":"bibsonomy","3258":"bibsonomy","3259":"bibsonomy","3260":"bibsonomy","3261":"bibsonomy","3262":"bibsonomy","3263":"bibsonomy","3264":"bibsonomy","3265":"bibsonomy","3266":"bibsonomy","3267":"bibsonomy","3268":"bibsonomy","3269":"bibsonomy","3270":"bibsonomy","3271":"bibsonomy","3272":"bibsonomy","3273":"bibsonomy","3274":"bibsonomy","3275":"bibsonomy","3276":"bibsonomy","3277":"bibsonomy","3278":"bibsonomy","3279":"bibsonomy","3280":"bibsonomy","3281":"bibsonomy","3282":"bibsonomy","3283":"bibsonomy","3284":"bibsonomy","3285":"bibsonomy","3286":"bibsonomy","3287":"bibsonomy","3288":"bibsonomy","3289":"bibsonomy","3290":"bibsonomy","3291":"bibsonomy","3292":"bibsonomy","3293":"bibsonomy","3294":"bibsonomy","3295":"bibsonomy","3296":"bibsonomy","3297":"bibsonomy","3298":"bibsonomy","3299":"bibsonomy","3300":"bibsonomy","3301":"bibsonomy","3302":"bibsonomy","3303":"bibsonomy","3304":"bibsonomy","3305":"bibsonomy","3306":"bibsonomy","3307":"bibsonomy","3308":"bibsonomy","3309":"bibsonomy","3310":"bibsonomy","3311":"bibsonomy","3312":"bibsonomy","3313":"bibsonomy","3314":"bibsonomy","3315":"bibsonomy","3316":"bibsonomy","3317":"bibsonomy","3318":"bibsonomy","3319":"bibsonomy","3320":"bibsonomy","3321":"bibsonomy","3322":"bibsonomy","3323":"bibsonomy","3324":"bibsonomy","3325":"bibsonomy","3326":"bibsonomy","3327":"bibsonomy","3328":"bibsonomy","3329":"bibsonomy","3330":"bibsonomy","3331":"bibsonomy","3332":"bibsonomy","3333":"bibsonomy","3334":"bibsonomy","3335":"bibsonomy","3336":"bibsonomy","3337":"bibsonomy","3338":"bibsonomy","3339":"bibsonomy","3340":"bibsonomy","3341":"bibsonomy","3342":"bibsonomy","3343":"bibsonomy","3344":"bibsonomy","3345":"bibsonomy","3346":"bibsonomy","3347":"bibsonomy","3348":"bibsonomy","3349":"bibsonomy","3350":"bibsonomy","3351":"bibsonomy","3352":"bibsonomy","3353":"bibsonomy","3354":"bibsonomy","3355":"bibsonomy","3356":"bibsonomy","3357":"bibsonomy","3358":"bibsonomy","3359":"bibsonomy","3360":"bibsonomy","3361":"bibsonomy","3362":"bibsonomy","3363":"bibsonomy","3364":"semantic_scholar","3365":"semantic_scholar","3366":"semantic_scholar","3367":"semantic_scholar","3368":"semantic_scholar","3369":"semantic_scholar","3370":"semantic_scholar","3371":"semantic_scholar","3372":"semantic_scholar","3373":"semantic_scholar","3374":"semantic_scholar","3375":"semantic_scholar","3376":"semantic_scholar","3377":"semantic_scholar","3378":"semantic_scholar","3379":"semantic_scholar","3380":"semantic_scholar","3381":"semantic_scholar","3382":"semantic_scholar","3383":"semantic_scholar","3384":"semantic_scholar","3385":"semantic_scholar","3386":"semantic_scholar","3387":"semantic_scholar","3388":"semantic_scholar","3389":"semantic_scholar","3390":"semantic_scholar","3391":"semantic_scholar","3392":"semantic_scholar","3393":"semantic_scholar","3394":"semantic_scholar","3395":"semantic_scholar","3396":"semantic_scholar","3397":"semantic_scholar","3398":"semantic_scholar","3399":"semantic_scholar","3400":"semantic_scholar","3401":"semantic_scholar","3402":"semantic_scholar","3403":"semantic_scholar","3404":"semantic_scholar","3405":"semantic_scholar","3406":"semantic_scholar","3407":"semantic_scholar","3408":"semantic_scholar","3409":"semantic_scholar","3410":"semantic_scholar","3411":"semantic_scholar","3412":"semantic_scholar","3413":"semantic_scholar","3414":"semantic_scholar","3415":"semantic_scholar","3416":"semantic_scholar","3417":"semantic_scholar","3418":"semantic_scholar","3419":"semantic_scholar","3420":"semantic_scholar","3421":"semantic_scholar","3422":"semantic_scholar","3423":"semantic_scholar","3424":"semantic_scholar","3425":"semantic_scholar","3426":"semantic_scholar","3427":"semantic_scholar","3428":"semantic_scholar","3429":"semantic_scholar","3430":"semantic_scholar","3431":"semantic_scholar","3432":"semantic_scholar","3433":"semantic_scholar","3434":"semantic_scholar","3435":"semantic_scholar","3436":"semantic_scholar","3437":"semantic_scholar","3438":"semantic_scholar","3439":"semantic_scholar","3440":"semantic_scholar","3441":"semantic_scholar","3442":"semantic_scholar","3443":"semantic_scholar","3444":"semantic_scholar","3445":"semantic_scholar","3446":"semantic_scholar","3447":"semantic_scholar","3448":"semantic_scholar","3449":"semantic_scholar","3450":"semantic_scholar","3451":"semantic_scholar","3452":"semantic_scholar","3453":"semantic_scholar","3454":"semantic_scholar","3455":"semantic_scholar","3456":"semantic_scholar","3457":"semantic_scholar","3458":"semantic_scholar","3459":"semantic_scholar","3460":"semantic_scholar","3461":"semantic_scholar","3462":"semantic_scholar","3463":"semantic_scholar","3464":"semantic_scholar","3465":"semantic_scholar","3466":"semantic_scholar","3467":"semantic_scholar","3468":"semantic_scholar","3469":"semantic_scholar","3470":"semantic_scholar","3471":"semantic_scholar","3472":"semantic_scholar","3473":"semantic_scholar","3474":"semantic_scholar","3475":"semantic_scholar","3476":"semantic_scholar","3477":"semantic_scholar","3478":"semantic_scholar","3479":"semantic_scholar","3480":"semantic_scholar","3481":"semantic_scholar","3482":"semantic_scholar","3483":"semantic_scholar","3484":"semantic_scholar","3485":"semantic_scholar","3486":"semantic_scholar","3487":"semantic_scholar","3488":"semantic_scholar","3489":"semantic_scholar","3490":"semantic_scholar","3491":"semantic_scholar","3492":"semantic_scholar","3493":"semantic_scholar","3494":"semantic_scholar","3495":"semantic_scholar","3496":"semantic_scholar","3497":"semantic_scholar","3498":"semantic_scholar","3499":"semantic_scholar","3500":"semantic_scholar","3501":"semantic_scholar","3502":"semantic_scholar","3503":"semantic_scholar","3504":"semantic_scholar","3505":"semantic_scholar","3506":"semantic_scholar","3507":"semantic_scholar","3508":"semantic_scholar","3509":"semantic_scholar","3510":"semantic_scholar","3511":"semantic_scholar","3512":"semantic_scholar","3513":"semantic_scholar","3514":"semantic_scholar","3515":"semantic_scholar","3516":"semantic_scholar","3517":"semantic_scholar","3518":"semantic_scholar","3519":"semantic_scholar","3520":"semantic_scholar","3521":"semantic_scholar","3522":"semantic_scholar","3523":"semantic_scholar","3524":"semantic_scholar","3525":"semantic_scholar","3526":"semantic_scholar","3527":"semantic_scholar","3528":"semantic_scholar","3529":"semantic_scholar","3530":"semantic_scholar","3531":"semantic_scholar","3532":"semantic_scholar","3533":"semantic_scholar","3534":"semantic_scholar","3535":"semantic_scholar","3536":"semantic_scholar","3537":"semantic_scholar","3538":"semantic_scholar","3539":"semantic_scholar","3540":"semantic_scholar","3541":"semantic_scholar","3542":"semantic_scholar","3543":"semantic_scholar","3544":"semantic_scholar","3545":"semantic_scholar","3546":"semantic_scholar","3547":"semantic_scholar","3548":"semantic_scholar","3549":"semantic_scholar","3550":"semantic_scholar","3551":"semantic_scholar","3552":"semantic_scholar","3553":"semantic_scholar","3554":"semantic_scholar","3555":"semantic_scholar","3556":"semantic_scholar","3557":"semantic_scholar","3558":"semantic_scholar","3559":"semantic_scholar","3560":"semantic_scholar","3561":"semantic_scholar","3562":"semantic_scholar","3563":"semantic_scholar","3564":"semantic_scholar","3565":"semantic_scholar","3566":"semantic_scholar","3567":"semantic_scholar","3568":"semantic_scholar","3569":"semantic_scholar","3570":"semantic_scholar","3571":"semantic_scholar","3572":"semantic_scholar","3573":"semantic_scholar","3574":"semantic_scholar","3575":"semantic_scholar","3576":"semantic_scholar","3577":"semantic_scholar","3578":"semantic_scholar","3579":"semantic_scholar","3580":"semantic_scholar","3581":"semantic_scholar","3582":"semantic_scholar","3583":"semantic_scholar","3584":"semantic_scholar","3585":"semantic_scholar","3586":"semantic_scholar","3587":"semantic_scholar","3588":"semantic_scholar","3589":"semantic_scholar","3590":"semantic_scholar","3591":"semantic_scholar","3592":"semantic_scholar","3593":"semantic_scholar","3594":"semantic_scholar","3595":"semantic_scholar","3596":"semantic_scholar","3597":"semantic_scholar","3598":"semantic_scholar","3599":"semantic_scholar","3600":"semantic_scholar","3601":"semantic_scholar","3602":"semantic_scholar","3603":"semantic_scholar","3604":"semantic_scholar","3605":"semantic_scholar","3606":"semantic_scholar","3607":"semantic_scholar","3608":"semantic_scholar","3609":"semantic_scholar","3610":"semantic_scholar","3611":"semantic_scholar","3612":"semantic_scholar","3613":"semantic_scholar","3614":"semantic_scholar","3615":"semantic_scholar","3616":"semantic_scholar","3617":"semantic_scholar","3618":"semantic_scholar","3619":"semantic_scholar","3620":"semantic_scholar","3621":"semantic_scholar","3622":"semantic_scholar","3623":"semantic_scholar","3624":"semantic_scholar","3625":"semantic_scholar","3626":"semantic_scholar","3627":"semantic_scholar","3628":"semantic_scholar","3629":"semantic_scholar","3630":"semantic_scholar","3631":"semantic_scholar","3632":"semantic_scholar","3633":"semantic_scholar","3634":"semantic_scholar","3635":"semantic_scholar","3636":"semantic_scholar","3637":"semantic_scholar","3638":"semantic_scholar","3639":"semantic_scholar","3640":"semantic_scholar","3641":"semantic_scholar","3642":"semantic_scholar","3643":"semantic_scholar","3644":"semantic_scholar","3645":"semantic_scholar","3646":"semantic_scholar","3647":"semantic_scholar","3648":"semantic_scholar","3649":"semantic_scholar","3650":"semantic_scholar","3651":"semantic_scholar","3652":"semantic_scholar","3653":"semantic_scholar","3654":"semantic_scholar","3655":"semantic_scholar","3656":"semantic_scholar","3657":"semantic_scholar","3658":"semantic_scholar","3659":"semantic_scholar","3660":"semantic_scholar","3661":"semantic_scholar","3662":"semantic_scholar","3663":"semantic_scholar","3664":"semantic_scholar","3665":"semantic_scholar","3666":"semantic_scholar","3667":"semantic_scholar","3668":"semantic_scholar","3669":"semantic_scholar","3670":"semantic_scholar","3671":"semantic_scholar","3672":"semantic_scholar","3673":"semantic_scholar","3674":"semantic_scholar","3675":"semantic_scholar","3676":"semantic_scholar","3677":"semantic_scholar","3678":"semantic_scholar","3679":"semantic_scholar","3680":"semantic_scholar","3681":"semantic_scholar","3682":"semantic_scholar","3683":"semantic_scholar","3684":"semantic_scholar","3685":"semantic_scholar","3686":"semantic_scholar","3687":"semantic_scholar","3688":"semantic_scholar","3689":"semantic_scholar","3690":"semantic_scholar","3691":"semantic_scholar","3692":"semantic_scholar","3693":"semantic_scholar","3694":"semantic_scholar","3695":"semantic_scholar","3696":"semantic_scholar","3697":"semantic_scholar","3698":"semantic_scholar","3699":"semantic_scholar","3700":"semantic_scholar","3701":"semantic_scholar","3702":"semantic_scholar","3703":"semantic_scholar","3704":"semantic_scholar","3705":"semantic_scholar","3706":"semantic_scholar","3707":"semantic_scholar","3708":"semantic_scholar","3709":"semantic_scholar","3710":"semantic_scholar","3711":"semantic_scholar","3712":"semantic_scholar","3713":"semantic_scholar","3714":"semantic_scholar","3715":"semantic_scholar","3716":"semantic_scholar","3717":"semantic_scholar","3718":"semantic_scholar","3719":"semantic_scholar","3720":"semantic_scholar","3721":"semantic_scholar","3722":"semantic_scholar","3723":"semantic_scholar","3724":"semantic_scholar","3725":"semantic_scholar","3726":"semantic_scholar","3727":"semantic_scholar","3728":"semantic_scholar","3729":"semantic_scholar","3730":"semantic_scholar","3731":"semantic_scholar","3732":"semantic_scholar","3733":"semantic_scholar","3734":"semantic_scholar","3735":"semantic_scholar","3736":"semantic_scholar","3737":"semantic_scholar","3738":"semantic_scholar","3739":"semantic_scholar","3740":"semantic_scholar","3741":"semantic_scholar","3742":"semantic_scholar","3743":"semantic_scholar","3744":"semantic_scholar","3745":"semantic_scholar","3746":"semantic_scholar","3747":"semantic_scholar","3748":"semantic_scholar","3749":"semantic_scholar","3750":"semantic_scholar","3751":"semantic_scholar","3752":"semantic_scholar","3753":"semantic_scholar","3754":"semantic_scholar","3755":"semantic_scholar","3756":"semantic_scholar","3757":"semantic_scholar","3758":"semantic_scholar","3759":"semantic_scholar","3760":"semantic_scholar","3761":"semantic_scholar","3762":"semantic_scholar","3763":"semantic_scholar","3764":"semantic_scholar","3765":"semantic_scholar","3766":"semantic_scholar","3767":"semantic_scholar","3768":"semantic_scholar","3769":"semantic_scholar","3770":"semantic_scholar","3771":"semantic_scholar","3772":"semantic_scholar","3773":"semantic_scholar","3774":"semantic_scholar","3775":"semantic_scholar","3776":"semantic_scholar","3777":"semantic_scholar","3778":"semantic_scholar","3779":"semantic_scholar","3780":"semantic_scholar","3781":"semantic_scholar","3782":"semantic_scholar","3783":"semantic_scholar","3784":"semantic_scholar","3785":"semantic_scholar","3786":"semantic_scholar","3787":"semantic_scholar","3788":"semantic_scholar","3789":"semantic_scholar","3790":"semantic_scholar","3791":"semantic_scholar","3792":"semantic_scholar","3793":"semantic_scholar","3794":"semantic_scholar","3795":"semantic_scholar","3796":"semantic_scholar","3797":"semantic_scholar","3798":"semantic_scholar","3799":"semantic_scholar","3800":"semantic_scholar","3801":"semantic_scholar","3802":"semantic_scholar","3803":"semantic_scholar","3804":"semantic_scholar","3805":"semantic_scholar","3806":"semantic_scholar","3807":"semantic_scholar","3808":"semantic_scholar","3809":"semantic_scholar","3810":"semantic_scholar","3811":"semantic_scholar","3812":"semantic_scholar","3813":"semantic_scholar","3814":"semantic_scholar","3815":"semantic_scholar","3816":"semantic_scholar","3817":"semantic_scholar","3818":"semantic_scholar","3819":"semantic_scholar","3820":"semantic_scholar","3821":"semantic_scholar","3822":"semantic_scholar","3823":"semantic_scholar","3824":"semantic_scholar","3825":"semantic_scholar","3826":"semantic_scholar","3827":"semantic_scholar","3828":"semantic_scholar","3829":"semantic_scholar","3830":"semantic_scholar","3831":"semantic_scholar","3832":"semantic_scholar","3833":"semantic_scholar","3834":"semantic_scholar","3835":"semantic_scholar","3836":"semantic_scholar","3837":"semantic_scholar","3838":"semantic_scholar","3839":"semantic_scholar","3840":"semantic_scholar","3841":"semantic_scholar","3842":"semantic_scholar","3843":"semantic_scholar","3844":"semantic_scholar","3845":"semantic_scholar","3846":"semantic_scholar","3847":"semantic_scholar","3848":"semantic_scholar","3849":"semantic_scholar","3850":"semantic_scholar","3851":"semantic_scholar","3852":"semantic_scholar","3853":"semantic_scholar","3854":"semantic_scholar","3855":"semantic_scholar","3856":"semantic_scholar","3857":"semantic_scholar","3858":"semantic_scholar","3859":"semantic_scholar","3860":"semantic_scholar","3861":"semantic_scholar","3862":"semantic_scholar","3863":"semantic_scholar","3864":"semantic_scholar","3865":"semantic_scholar","3866":"semantic_scholar","3867":"semantic_scholar","3868":"semantic_scholar","3869":"semantic_scholar","3870":"semantic_scholar","3871":"semantic_scholar","3872":"semantic_scholar","3873":"semantic_scholar","3874":"semantic_scholar","3875":"semantic_scholar","3876":"semantic_scholar","3877":"semantic_scholar","3878":"semantic_scholar","3879":"semantic_scholar","3880":"semantic_scholar","3881":"semantic_scholar","3882":"semantic_scholar","3883":"semantic_scholar","3884":"semantic_scholar","3885":"semantic_scholar","3886":"semantic_scholar","3887":"semantic_scholar","3888":"semantic_scholar","3889":"semantic_scholar","3890":"semantic_scholar","3891":"semantic_scholar","3892":"semantic_scholar","3893":"semantic_scholar","3894":"semantic_scholar","3895":"semantic_scholar","3896":"semantic_scholar","3897":"semantic_scholar","3898":"semantic_scholar","3899":"semantic_scholar","3900":"semantic_scholar","3901":"semantic_scholar","3902":"semantic_scholar","3903":"semantic_scholar","3904":"semantic_scholar","3905":"semantic_scholar","3906":"semantic_scholar","3907":"semantic_scholar","3908":"semantic_scholar","3909":"semantic_scholar","3910":"semantic_scholar","3911":"semantic_scholar","3912":"semantic_scholar","3913":"semantic_scholar","3914":"semantic_scholar","3915":"semantic_scholar","3916":"semantic_scholar","3917":"semantic_scholar","3918":"semantic_scholar","3919":"semantic_scholar","3920":"semantic_scholar","3921":"semantic_scholar","3922":"semantic_scholar","3923":"semantic_scholar","3924":"semantic_scholar","3925":"semantic_scholar","3926":"semantic_scholar","3927":"semantic_scholar","3928":"semantic_scholar","3929":"semantic_scholar","3930":"semantic_scholar","3931":"semantic_scholar","3932":"semantic_scholar","3933":"semantic_scholar","3934":"semantic_scholar","3935":"semantic_scholar","3936":"semantic_scholar","3937":"semantic_scholar","3938":"semantic_scholar","3939":"semantic_scholar","3940":"semantic_scholar","3941":"semantic_scholar","3942":"semantic_scholar","3943":"semantic_scholar","3944":"semantic_scholar","3945":"semantic_scholar","3946":"semantic_scholar","3947":"semantic_scholar","3948":"semantic_scholar","3949":"semantic_scholar","3950":"semantic_scholar","3951":"semantic_scholar","3952":"semantic_scholar","3953":"semantic_scholar","3954":"semantic_scholar","3955":"semantic_scholar","3956":"semantic_scholar","3957":"semantic_scholar","3958":"semantic_scholar","3959":"semantic_scholar","3960":"semantic_scholar","3961":"semantic_scholar","3962":"semantic_scholar","3963":"semantic_scholar","3964":"semantic_scholar","3965":"semantic_scholar","3966":"semantic_scholar","3967":"semantic_scholar","3968":"semantic_scholar","3969":"semantic_scholar","3970":"semantic_scholar","3971":"semantic_scholar","3972":"semantic_scholar","3973":"semantic_scholar","3974":"semantic_scholar","3975":"semantic_scholar","3976":"semantic_scholar","3977":"semantic_scholar","3978":"semantic_scholar","3979":"semantic_scholar","3980":"semantic_scholar","3981":"semantic_scholar","3982":"semantic_scholar","3983":"semantic_scholar","3984":"semantic_scholar","3985":"semantic_scholar","3986":"semantic_scholar","3987":"semantic_scholar","3988":"semantic_scholar","3989":"semantic_scholar","3990":"semantic_scholar","3991":"semantic_scholar","3992":"semantic_scholar","3993":"semantic_scholar","3994":"semantic_scholar","3995":"semantic_scholar","3996":"semantic_scholar","3997":"semantic_scholar","3998":"semantic_scholar","3999":"semantic_scholar","4000":"semantic_scholar","4001":"semantic_scholar","4002":"semantic_scholar","4003":"semantic_scholar","4004":"semantic_scholar","4005":"semantic_scholar","4006":"semantic_scholar","4007":"semantic_scholar","4008":"semantic_scholar","4009":"semantic_scholar","4010":"semantic_scholar","4011":"semantic_scholar","4012":"semantic_scholar","4013":"semantic_scholar","4014":"semantic_scholar","4015":"semantic_scholar","4016":"semantic_scholar","4017":"semantic_scholar","4018":"semantic_scholar","4019":"semantic_scholar","4020":"semantic_scholar","4021":"semantic_scholar","4022":"semantic_scholar","4023":"semantic_scholar","4024":"semantic_scholar","4025":"semantic_scholar","4026":"semantic_scholar","4027":"semantic_scholar","4028":"semantic_scholar","4029":"semantic_scholar","4030":"semantic_scholar","4031":"semantic_scholar","4032":"semantic_scholar","4033":"semantic_scholar","4034":"semantic_scholar","4035":"semantic_scholar","4036":"semantic_scholar","4037":"semantic_scholar","4038":"semantic_scholar","4039":"semantic_scholar","4040":"semantic_scholar","4041":"semantic_scholar","4042":"semantic_scholar","4043":"semantic_scholar","4044":"semantic_scholar","4045":"semantic_scholar","4046":"semantic_scholar","4047":"semantic_scholar","4048":"semantic_scholar","4049":"semantic_scholar","4050":"semantic_scholar","4051":"semantic_scholar","4052":"semantic_scholar","4053":"semantic_scholar","4054":"semantic_scholar","4055":"semantic_scholar","4056":"semantic_scholar","4057":"semantic_scholar","4058":"semantic_scholar","4059":"semantic_scholar","4060":"semantic_scholar","4061":"semantic_scholar","4062":"semantic_scholar","4063":"semantic_scholar","4064":"semantic_scholar","4065":"semantic_scholar","4066":"semantic_scholar","4067":"semantic_scholar","4068":"semantic_scholar","4069":"semantic_scholar","4070":"semantic_scholar","4071":"semantic_scholar","4072":"semantic_scholar","4073":"semantic_scholar","4074":"semantic_scholar","4075":"semantic_scholar","4076":"semantic_scholar","4077":"semantic_scholar","4078":"semantic_scholar","4079":"semantic_scholar","4080":"semantic_scholar","4081":"semantic_scholar","4082":"semantic_scholar","4083":"semantic_scholar","4084":"semantic_scholar","4085":"semantic_scholar","4086":"semantic_scholar","4087":"semantic_scholar","4088":"semantic_scholar","4089":"semantic_scholar","4090":"semantic_scholar","4091":"semantic_scholar","4092":"semantic_scholar","4093":"semantic_scholar","4094":"semantic_scholar","4095":"semantic_scholar","4096":"semantic_scholar","4097":"semantic_scholar","4098":"semantic_scholar","4099":"semantic_scholar","4100":"semantic_scholar","4101":"semantic_scholar","4102":"semantic_scholar","4103":"semantic_scholar","4104":"semantic_scholar","4105":"semantic_scholar","4106":"semantic_scholar","4107":"semantic_scholar","4108":"semantic_scholar","4109":"semantic_scholar","4110":"semantic_scholar","4111":"semantic_scholar","4112":"semantic_scholar","4113":"semantic_scholar","4114":"semantic_scholar","4115":"semantic_scholar","4116":"semantic_scholar","4117":"semantic_scholar","4118":"semantic_scholar","4119":"semantic_scholar","4120":"semantic_scholar","4121":"semantic_scholar","4122":"semantic_scholar","4123":"semantic_scholar","4124":"semantic_scholar","4125":"semantic_scholar","4126":"semantic_scholar","4127":"semantic_scholar","4128":"semantic_scholar","4129":"semantic_scholar","4130":"semantic_scholar","4131":"semantic_scholar","4132":"semantic_scholar","4133":"semantic_scholar","4134":"semantic_scholar","4135":"semantic_scholar","4136":"semantic_scholar","4137":"semantic_scholar","4138":"semantic_scholar","4139":"semantic_scholar","4140":"semantic_scholar","4141":"semantic_scholar","4142":"semantic_scholar","4143":"semantic_scholar","4144":"semantic_scholar","4145":"semantic_scholar","4146":"semantic_scholar","4147":"semantic_scholar","4148":"semantic_scholar","4149":"semantic_scholar","4150":"semantic_scholar","4151":"semantic_scholar","4152":"semantic_scholar","4153":"semantic_scholar","4154":"semantic_scholar","4155":"semantic_scholar","4156":"semantic_scholar","4157":"semantic_scholar","4158":"semantic_scholar","4159":"semantic_scholar","4160":"semantic_scholar","4161":"semantic_scholar","4162":"semantic_scholar","4163":"semantic_scholar","4164":"semantic_scholar","4165":"semantic_scholar","4166":"semantic_scholar","4167":"semantic_scholar","4168":"semantic_scholar","4169":"semantic_scholar","4170":"semantic_scholar","4171":"semantic_scholar","4172":"semantic_scholar","4173":"semantic_scholar","4174":"semantic_scholar","4175":"semantic_scholar","4176":"semantic_scholar","4177":"semantic_scholar","4178":"semantic_scholar","4179":"semantic_scholar","4180":"semantic_scholar","4181":"semantic_scholar","4182":"semantic_scholar","4183":"semantic_scholar","4184":"semantic_scholar","4185":"semantic_scholar","4186":"semantic_scholar","4187":"semantic_scholar","4188":"semantic_scholar","4189":"semantic_scholar","4190":"semantic_scholar","4191":"semantic_scholar","4192":"semantic_scholar","4193":"semantic_scholar","4194":"semantic_scholar","4195":"semantic_scholar","4196":"semantic_scholar","4197":"semantic_scholar","4198":"semantic_scholar","4199":"semantic_scholar","4200":"semantic_scholar","4201":"semantic_scholar","4202":"semantic_scholar","4203":"semantic_scholar","4204":"semantic_scholar","4205":"semantic_scholar","4206":"semantic_scholar","4207":"semantic_scholar","4208":"semantic_scholar","4209":"semantic_scholar","4210":"semantic_scholar","4211":"semantic_scholar","4212":"semantic_scholar","4213":"semantic_scholar","4214":"semantic_scholar","4215":"semantic_scholar","4216":"semantic_scholar","4217":"semantic_scholar","4218":"semantic_scholar","4219":"semantic_scholar","4220":"semantic_scholar","4221":"semantic_scholar","4222":"semantic_scholar","4223":"semantic_scholar","4224":"semantic_scholar","4225":"semantic_scholar","4226":"semantic_scholar","4227":"semantic_scholar","4228":"semantic_scholar","4229":"semantic_scholar","4230":"semantic_scholar","4231":"semantic_scholar","4232":"semantic_scholar","4233":"semantic_scholar","4234":"semantic_scholar","4235":"semantic_scholar","4236":"semantic_scholar","4237":"semantic_scholar","4238":"semantic_scholar","4239":"semantic_scholar","4240":"semantic_scholar","4241":"semantic_scholar","4242":"semantic_scholar","4243":"semantic_scholar","4244":"semantic_scholar","4245":"semantic_scholar","4246":"semantic_scholar","4247":"semantic_scholar","4248":"semantic_scholar","4249":"semantic_scholar","4250":"semantic_scholar","4251":"semantic_scholar","4252":"semantic_scholar","4253":"semantic_scholar","4254":"semantic_scholar","4255":"semantic_scholar","4256":"semantic_scholar","4257":"semantic_scholar","4258":"semantic_scholar","4259":"semantic_scholar","4260":"semantic_scholar","4261":"semantic_scholar","4262":"semantic_scholar","4263":"semantic_scholar","4264":"semantic_scholar","4265":"semantic_scholar","4266":"semantic_scholar","4267":"semantic_scholar","4268":"semantic_scholar","4269":"semantic_scholar","4270":"semantic_scholar","4271":"semantic_scholar","4272":"semantic_scholar","4273":"semantic_scholar","4274":"semantic_scholar","4275":"semantic_scholar","4276":"semantic_scholar","4277":"semantic_scholar","4278":"semantic_scholar","4279":"semantic_scholar","4280":"semantic_scholar","4281":"semantic_scholar","4282":"semantic_scholar","4283":"semantic_scholar","4284":"semantic_scholar","4285":"semantic_scholar","4286":"semantic_scholar","4287":"semantic_scholar","4288":"semantic_scholar","4289":"semantic_scholar","4290":"semantic_scholar","4291":"semantic_scholar","4292":"semantic_scholar","4293":"semantic_scholar","4294":"semantic_scholar","4295":"semantic_scholar","4296":"semantic_scholar","4297":"semantic_scholar","4298":"semantic_scholar","4299":"semantic_scholar","4300":"semantic_scholar","4301":"semantic_scholar","4302":"semantic_scholar","4303":"semantic_scholar","4304":"semantic_scholar","4305":"semantic_scholar","4306":"semantic_scholar","4307":"semantic_scholar","4308":"semantic_scholar","4309":"semantic_scholar","4310":"semantic_scholar","4311":"semantic_scholar","4312":"semantic_scholar","4313":"semantic_scholar","4314":"semantic_scholar","4315":"semantic_scholar","4316":"semantic_scholar","4317":"semantic_scholar","4318":"semantic_scholar","4319":"semantic_scholar","4320":"semantic_scholar","4321":"semantic_scholar","4322":"semantic_scholar","4323":"semantic_scholar","4324":"semantic_scholar","4325":"semantic_scholar","4326":"semantic_scholar","4327":"semantic_scholar","4328":"semantic_scholar","4329":"semantic_scholar","4330":"semantic_scholar","4331":"semantic_scholar","4332":"semantic_scholar","4333":"semantic_scholar","4334":"semantic_scholar","4335":"semantic_scholar","4336":"semantic_scholar","4337":"semantic_scholar","4338":"semantic_scholar","4339":"semantic_scholar","4340":"semantic_scholar","4341":"semantic_scholar","4342":"semantic_scholar","4343":"semantic_scholar","4344":"semantic_scholar","4345":"semantic_scholar","4346":"semantic_scholar","4347":"semantic_scholar","4348":"semantic_scholar","4349":"semantic_scholar","4350":"semantic_scholar","4351":"semantic_scholar","4352":"semantic_scholar","4353":"semantic_scholar","4354":"semantic_scholar","4355":"semantic_scholar","4356":"semantic_scholar","4357":"semantic_scholar","4358":"semantic_scholar","4359":"semantic_scholar","4360":"semantic_scholar","4361":"semantic_scholar","4362":"semantic_scholar","4363":"semantic_scholar","4364":"semantic_scholar","4365":"semantic_scholar","4366":"semantic_scholar","4367":"semantic_scholar","4368":"semantic_scholar","4369":"semantic_scholar","4370":"semantic_scholar","4371":"semantic_scholar","4372":"semantic_scholar","4373":"semantic_scholar","4374":"semantic_scholar","4375":"semantic_scholar","4376":"semantic_scholar","4377":"semantic_scholar","4378":"semantic_scholar","4379":"semantic_scholar","4380":"semantic_scholar","4381":"semantic_scholar","4382":"semantic_scholar","4383":"semantic_scholar","4384":"semantic_scholar","4385":"semantic_scholar","4386":"semantic_scholar","4387":"semantic_scholar","4388":"semantic_scholar","4389":"semantic_scholar","4390":"semantic_scholar","4391":"semantic_scholar","4392":"semantic_scholar","4393":"semantic_scholar","4394":"semantic_scholar","4395":"semantic_scholar","4396":"semantic_scholar","4397":"semantic_scholar","4398":"semantic_scholar","4399":"semantic_scholar","4400":"semantic_scholar","4401":"semantic_scholar","4402":"semantic_scholar","4403":"semantic_scholar","4404":"semantic_scholar","4405":"semantic_scholar","4406":"semantic_scholar","4407":"semantic_scholar","4408":"semantic_scholar","4409":"semantic_scholar","4410":"semantic_scholar","4411":"semantic_scholar","4412":"semantic_scholar","4413":"semantic_scholar","4414":"semantic_scholar","4415":"semantic_scholar","4416":"semantic_scholar","4417":"semantic_scholar","4418":"semantic_scholar","4419":"semantic_scholar","4420":"semantic_scholar","4421":"semantic_scholar","4422":"semantic_scholar","4423":"semantic_scholar","4424":"semantic_scholar","4425":"semantic_scholar","4426":"semantic_scholar","4427":"semantic_scholar","4428":"semantic_scholar","4429":"semantic_scholar","4430":"semantic_scholar","4431":"semantic_scholar","4432":"semantic_scholar","4433":"semantic_scholar","4434":"semantic_scholar","4435":"semantic_scholar","4436":"semantic_scholar","4437":"semantic_scholar","4438":"semantic_scholar","4439":"semantic_scholar","4440":"semantic_scholar","4441":"semantic_scholar","4442":"semantic_scholar","4443":"semantic_scholar","4444":"semantic_scholar","4445":"semantic_scholar","4446":"semantic_scholar","4447":"semantic_scholar","4448":"semantic_scholar","4449":"semantic_scholar","4450":"semantic_scholar","4451":"semantic_scholar","4452":"semantic_scholar","4453":"semantic_scholar","4454":"semantic_scholar","4455":"semantic_scholar","4456":"semantic_scholar","4457":"semantic_scholar","4458":"semantic_scholar","4459":"semantic_scholar","4460":"semantic_scholar","4461":"semantic_scholar","4462":"semantic_scholar","4463":"semantic_scholar","4464":"semantic_scholar","4465":"semantic_scholar","4466":"semantic_scholar","4467":"semantic_scholar","4468":"semantic_scholar","4469":"semantic_scholar","4470":"semantic_scholar","4471":"semantic_scholar","4472":"semantic_scholar","4473":"semantic_scholar","4474":"semantic_scholar","4475":"semantic_scholar","4476":"semantic_scholar","4477":"semantic_scholar","4478":"semantic_scholar","4479":"semantic_scholar","4480":"semantic_scholar","4481":"semantic_scholar","4482":"semantic_scholar","4483":"semantic_scholar","4484":"semantic_scholar","4485":"semantic_scholar","4486":"semantic_scholar","4487":"semantic_scholar","4488":"semantic_scholar","4489":"semantic_scholar","4490":"semantic_scholar","4491":"semantic_scholar","4492":"semantic_scholar","4493":"semantic_scholar","4494":"semantic_scholar","4495":"semantic_scholar","4496":"semantic_scholar","4497":"semantic_scholar","4498":"semantic_scholar","4499":"semantic_scholar","4500":"semantic_scholar","4501":"semantic_scholar","4502":"semantic_scholar","4503":"semantic_scholar","4504":"semantic_scholar","4505":"semantic_scholar","4506":"semantic_scholar","4507":"semantic_scholar","4508":"semantic_scholar","4509":"semantic_scholar","4510":"semantic_scholar","4511":"semantic_scholar","4512":"semantic_scholar","4513":"semantic_scholar","4514":"semantic_scholar","4515":"semantic_scholar","4516":"semantic_scholar","4517":"semantic_scholar","4518":"semantic_scholar","4519":"semantic_scholar","4520":"semantic_scholar","4521":"semantic_scholar","4522":"semantic_scholar","4523":"semantic_scholar","4524":"semantic_scholar","4525":"semantic_scholar","4526":"semantic_scholar","4527":"semantic_scholar","4528":"semantic_scholar","4529":"semantic_scholar","4530":"semantic_scholar","4531":"semantic_scholar","4532":"semantic_scholar","4533":"semantic_scholar","4534":"semantic_scholar","4535":"semantic_scholar","4536":"semantic_scholar","4537":"semantic_scholar","4538":"semantic_scholar","4539":"semantic_scholar","4540":"semantic_scholar","4541":"semantic_scholar","4542":"semantic_scholar","4543":"semantic_scholar","4544":"semantic_scholar","4545":"semantic_scholar","4546":"semantic_scholar","4547":"semantic_scholar","4548":"semantic_scholar","4549":"semantic_scholar","4550":"semantic_scholar","4551":"semantic_scholar","4552":"semantic_scholar","4553":"semantic_scholar","4554":"semantic_scholar","4555":"semantic_scholar","4556":"semantic_scholar","4557":"semantic_scholar","4558":"semantic_scholar","4559":"semantic_scholar","4560":"semantic_scholar","4561":"semantic_scholar","4562":"semantic_scholar","4563":"semantic_scholar","4564":"semantic_scholar","4565":"semantic_scholar","4566":"semantic_scholar","4567":"semantic_scholar","4568":"semantic_scholar","4569":"semantic_scholar","4570":"semantic_scholar","4571":"semantic_scholar","4572":"semantic_scholar","4573":"semantic_scholar","4574":"semantic_scholar","4575":"semantic_scholar","4576":"semantic_scholar","4577":"semantic_scholar","4578":"semantic_scholar","4579":"semantic_scholar","4580":"semantic_scholar","4581":"semantic_scholar","4582":"semantic_scholar","4583":"semantic_scholar","4584":"semantic_scholar","4585":"semantic_scholar","4586":"semantic_scholar","4587":"semantic_scholar","4588":"semantic_scholar","4589":"semantic_scholar","4590":"semantic_scholar","4591":"semantic_scholar","4592":"semantic_scholar","4593":"semantic_scholar","4594":"semantic_scholar","4595":"semantic_scholar","4596":"semantic_scholar","4597":"semantic_scholar","4598":"semantic_scholar","4599":"semantic_scholar","4600":"semantic_scholar","4601":"semantic_scholar","4602":"semantic_scholar","4603":"semantic_scholar","4604":"semantic_scholar","4605":"semantic_scholar","4606":"semantic_scholar","4607":"semantic_scholar","4608":"semantic_scholar","4609":"semantic_scholar","4610":"semantic_scholar","4611":"semantic_scholar","4612":"semantic_scholar","4613":"semantic_scholar","4614":"semantic_scholar","4615":"semantic_scholar","4616":"semantic_scholar","4617":"semantic_scholar","4618":"semantic_scholar","4619":"semantic_scholar","4620":"semantic_scholar","4621":"semantic_scholar","4622":"semantic_scholar","4623":"semantic_scholar","4624":"semantic_scholar","4625":"semantic_scholar","4626":"semantic_scholar","4627":"semantic_scholar","4628":"semantic_scholar","4629":"semantic_scholar","4630":"semantic_scholar","4631":"semantic_scholar","4632":"semantic_scholar","4633":"semantic_scholar","4634":"semantic_scholar","4635":"semantic_scholar","4636":"semantic_scholar","4637":"semantic_scholar","4638":"semantic_scholar","4639":"semantic_scholar","4640":"semantic_scholar","4641":"semantic_scholar","4642":"semantic_scholar","4643":"semantic_scholar","4644":"semantic_scholar","4645":"semantic_scholar","4646":"semantic_scholar","4647":"semantic_scholar","4648":"semantic_scholar","4649":"semantic_scholar","4650":"semantic_scholar","4651":"semantic_scholar","4652":"semantic_scholar","4653":"semantic_scholar","4654":"semantic_scholar","4655":"semantic_scholar","4656":"semantic_scholar","4657":"semantic_scholar","4658":"semantic_scholar","4659":"semantic_scholar","4660":"semantic_scholar","4661":"semantic_scholar","4662":"semantic_scholar","4663":"semantic_scholar","4664":"semantic_scholar","4665":"semantic_scholar","4666":"semantic_scholar","4667":"semantic_scholar","4668":"semantic_scholar","4669":"semantic_scholar","4670":"semantic_scholar","4671":"semantic_scholar","4672":"semantic_scholar","4673":"semantic_scholar","4674":"semantic_scholar","4675":"semantic_scholar","4676":"semantic_scholar","4677":"semantic_scholar","4678":"semantic_scholar","4679":"semantic_scholar","4680":"semantic_scholar","4681":"semantic_scholar","4682":"semantic_scholar","4683":"semantic_scholar","4684":"semantic_scholar","4685":"semantic_scholar","4686":"semantic_scholar","4687":"semantic_scholar","4688":"semantic_scholar","4689":"semantic_scholar","4690":"semantic_scholar","4691":"semantic_scholar","4692":"semantic_scholar","4693":"semantic_scholar","4694":"semantic_scholar","4695":"semantic_scholar","4696":"semantic_scholar","4697":"semantic_scholar","4698":"semantic_scholar","4699":"semantic_scholar","4700":"semantic_scholar","4701":"semantic_scholar","4702":"semantic_scholar","4703":"semantic_scholar","4704":"semantic_scholar","4705":"semantic_scholar","4706":"semantic_scholar","4707":"semantic_scholar","4708":"semantic_scholar","4709":"semantic_scholar","4710":"semantic_scholar","4711":"semantic_scholar","4712":"semantic_scholar","4713":"semantic_scholar","4714":"semantic_scholar","4715":"semantic_scholar","4716":"semantic_scholar","4717":"semantic_scholar","4718":"semantic_scholar","4719":"semantic_scholar","4720":"semantic_scholar","4721":"semantic_scholar","4722":"semantic_scholar","4723":"semantic_scholar","4724":"semantic_scholar","4725":"semantic_scholar","4726":"semantic_scholar","4727":"semantic_scholar","4728":"semantic_scholar","4729":"semantic_scholar","4730":"semantic_scholar","4731":"semantic_scholar","4732":"semantic_scholar","4733":"semantic_scholar","4734":"semantic_scholar","4735":"semantic_scholar","4736":"semantic_scholar","4737":"semantic_scholar","4738":"semantic_scholar","4739":"semantic_scholar","4740":"semantic_scholar","4741":"semantic_scholar"}}